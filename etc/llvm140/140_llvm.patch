diff --git a/clang-tools-extra/clang-tidy/altera/SingleWorkItemBarrierCheck.cpp b/clang-tools-extra/clang-tidy/altera/SingleWorkItemBarrierCheck.cpp
index 759c81c34ca6..056b6d1189bf 100644
--- a/clang-tools-extra/clang-tidy/altera/SingleWorkItemBarrierCheck.cpp
+++ b/clang-tools-extra/clang-tidy/altera/SingleWorkItemBarrierCheck.cpp
@@ -26,7 +26,7 @@ void SingleWorkItemBarrierCheck::registerMatchers(MatchFinder *Finder) {
       functionDecl(
           allOf(
               // That are OpenCL kernels...
-              hasAttr(attr::Kind::OpenCLKernel),
+              hasAttr(attr::Kind::ComputeKernel),
               // And call a barrier function (either 1.x or 2.x version)...
               forEachDescendant(callExpr(callee(functionDecl(hasAnyName(
                                              "barrier", "work_group_barrier"))))
diff --git a/clang/include/clang-c/Index.h b/clang/include/clang-c/Index.h
index b0d7ef509c26..1edd6ee0f657 100644
--- a/clang/include/clang-c/Index.h
+++ b/clang/include/clang-c/Index.h
@@ -2631,7 +2631,7 @@ enum CXCursorKind {
   CXCursor_NoDuplicateAttr = 411,
   CXCursor_CUDAConstantAttr = 412,
   CXCursor_CUDADeviceAttr = 413,
-  CXCursor_CUDAGlobalAttr = 414,
+  CXCursor_ComputeKernelAttr = 414,
   CXCursor_CUDAHostAttr = 415,
   CXCursor_CUDASharedAttr = 416,
   CXCursor_VisibilityAttr = 417,
@@ -3341,6 +3341,7 @@ enum CXTypeKind {
   CXType_Pipe = 120,
 
   /* OpenCL builtin types. */
+#if 0 // OpenCL image types with access qualifiers
   CXType_OCLImage1dRO = 121,
   CXType_OCLImage1dArrayRO = 122,
   CXType_OCLImage1dBufferRO = 123,
@@ -3377,6 +3378,24 @@ enum CXTypeKind {
   CXType_OCLImage2dMSAADepthRW = 154,
   CXType_OCLImage2dArrayMSAADepthRW = 155,
   CXType_OCLImage3dRW = 156,
+#else // OpenCL image types without access qualifiers
+  CXType_OCLImage1d = 121,
+  CXType_OCLImage1dArray = 122,
+  CXType_OCLImage1dBuffer = 123,
+  CXType_OCLImage2d = 124,
+  CXType_OCLImage2dArray = 125,
+  CXType_OCLImage2dDepth = 126,
+  CXType_OCLImage2dArrayDepth = 127,
+  CXType_OCLImage2dMSAA = 128,
+  CXType_OCLImage2dArrayMSAA = 129,
+  CXType_OCLImage2dMSAADepth = 130,
+  CXType_OCLImage2dArrayMSAADepth = 131,
+  CXType_OCLImage3d = 132,
+  CXType_OCLImageCube = 133,
+  CXType_OCLImageCubeArray = 134,
+  CXType_OCLImageCubeDepth = 135,
+  CXType_OCLImageCubeArrayDepth = 136,
+#endif
   CXType_OCLSampler = 157,
   CXType_OCLEvent = 158,
   CXType_OCLQueue = 159,
@@ -3429,6 +3448,11 @@ enum CXCallingConv {
   CXCallingConv_AArch64VectorCall = 16,
   CXCallingConv_SwiftAsync = 17,
 
+  CXCallingConv_FloorFunction = 40,
+  CXCallingConv_FloorKernel = 41,
+  CXCallingConv_FloorVertex = 42,
+  CXCallingConv_FloorFragment = 43,
+
   CXCallingConv_Invalid = 100,
   CXCallingConv_Unexposed = 200
 };
diff --git a/clang/include/clang/AST/ASTContext.h b/clang/include/clang/AST/ASTContext.h
index b9aa2d2cfadb..a20c784d7b6b 100644
--- a/clang/include/clang/AST/ASTContext.h
+++ b/clang/include/clang/AST/ASTContext.h
@@ -35,6 +35,7 @@
 #include "clang/Basic/LangOptions.h"
 #include "clang/Basic/Linkage.h"
 #include "clang/Basic/NoSanitizeList.h"
+#include "clang/Basic/OpenCLOptions.h"
 #include "clang/Basic/OperatorKinds.h"
 #include "clang/Basic/PartialDiagnostic.h"
 #include "clang/Basic/ProfileList.h"
@@ -603,6 +604,12 @@ private:
   ///  this ASTContext object.
   LangOptions &LangOpts;
 
+  /// enabled/used OpenCL features and options (used by Sema)
+  OpenCLOptions OpenCLFeatures;
+
+  /// module-level flag if the FP contract was ever disabled
+  bool disabledFPContract = false;
+
   /// NoSanitizeList object that is used by sanitizers to decide which
   /// entities should not be instrumented.
   std::unique_ptr<NoSanitizeList> NoSanitizeL;
@@ -764,6 +771,12 @@ public:
 
   const LangOptions& getLangOpts() const { return LangOpts; }
 
+  OpenCLOptions& getOpenCLFeatures() { return OpenCLFeatures; }
+  const OpenCLOptions& getOpenCLFeatures() const { return OpenCLFeatures; }
+
+  void disableFPContract() { disabledFPContract = true; }
+  bool isFPContractDisabled() const { return disabledFPContract; }
+
   // If this condition is false, typo correction must be performed eagerly
   // rather than delayed in many places, as it makes use of dependent types.
   // the condition is false for clang's C-only codepath, as it doesn't support
diff --git a/clang/include/clang/AST/Expr.h b/clang/include/clang/AST/Expr.h
index 991abef73363..2b1de8c1eb66 100644
--- a/clang/include/clang/AST/Expr.h
+++ b/clang/include/clang/AST/Expr.h
@@ -6236,6 +6236,16 @@ public:
     BI_First = 0
   };
 
+  // The ABI values for various atomic memory orderings.
+  enum AtomicOrderingKind {
+    AO_ABI_memory_order_relaxed = 0,
+    AO_ABI_memory_order_consume = 1,
+    AO_ABI_memory_order_acquire = 2,
+    AO_ABI_memory_order_release = 3,
+    AO_ABI_memory_order_acq_rel = 4,
+    AO_ABI_memory_order_seq_cst = 5
+  };
+
 private:
   /// Location of sub-expressions.
   /// The location of Scope sub-expression is NumSubExprs - 1, which is
diff --git a/clang/include/clang/AST/GlobalDecl.h b/clang/include/clang/AST/GlobalDecl.h
index 8cb56fb4ae90..cfe582b9454c 100644
--- a/clang/include/clang/AST/GlobalDecl.h
+++ b/clang/include/clang/AST/GlobalDecl.h
@@ -59,7 +59,7 @@ class GlobalDecl {
   void Init(const Decl *D) {
     assert(!isa<CXXConstructorDecl>(D) && "Use other ctor with ctor decls!");
     assert(!isa<CXXDestructorDecl>(D) && "Use other ctor with dtor decls!");
-    assert(!D->hasAttr<CUDAGlobalAttr>() && "Use other ctor with GPU kernels!");
+    assert(!D->hasAttr<ComputeKernelAttr>() && "Use other ctor with GPU kernels!");
 
     Value.setPointer(D);
   }
@@ -69,7 +69,7 @@ public:
   GlobalDecl(const VarDecl *D) { Init(D);}
   GlobalDecl(const FunctionDecl *D, unsigned MVIndex = 0)
       : MultiVersionIndex(MVIndex) {
-    if (!D->hasAttr<CUDAGlobalAttr>()) {
+    if (!D->hasAttr<ComputeKernelAttr>()) {
       Init(D);
       return;
     }
@@ -77,7 +77,7 @@ public:
   }
   GlobalDecl(const FunctionDecl *D, KernelReferenceKind Kind)
       : Value(D, unsigned(Kind)) {
-    assert(D->hasAttr<CUDAGlobalAttr>() && "Decl is not a GPU kernel!");
+    assert(D->hasAttr<ComputeKernelAttr>() && "Decl is not a GPU kernel!");
   }
   GlobalDecl(const NamedDecl *D) { Init(D); }
   GlobalDecl(const BlockDecl *D) { Init(D); }
@@ -121,7 +121,7 @@ public:
   unsigned getMultiVersionIndex() const {
     assert(isa<FunctionDecl>(
                getDecl()) &&
-               !cast<FunctionDecl>(getDecl())->hasAttr<CUDAGlobalAttr>() &&
+               !cast<FunctionDecl>(getDecl())->hasAttr<ComputeKernelAttr>() &&
            !isa<CXXConstructorDecl>(getDecl()) &&
            !isa<CXXDestructorDecl>(getDecl()) &&
            "Decl is not a plain FunctionDecl!");
@@ -130,7 +130,7 @@ public:
 
   KernelReferenceKind getKernelReferenceKind() const {
     assert(isa<FunctionDecl>(getDecl()) &&
-           cast<FunctionDecl>(getDecl())->hasAttr<CUDAGlobalAttr>() &&
+           cast<FunctionDecl>(getDecl())->hasAttr<ComputeKernelAttr>() &&
            "Decl is not a GPU kernel!");
     return static_cast<KernelReferenceKind>(Value.getInt());
   }
@@ -151,8 +151,12 @@ public:
   }
 
   static KernelReferenceKind getDefaultKernelReference(const FunctionDecl *D) {
+#if 0
     return D->getLangOpts().CUDAIsDevice ? KernelReferenceKind::Kernel
                                          : KernelReferenceKind::Stub;
+#else // we always compile device code
+    return KernelReferenceKind::Kernel;
+#endif
   }
 
   GlobalDecl getWithDecl(const Decl *D) {
@@ -177,7 +181,7 @@ public:
 
   GlobalDecl getWithMultiVersionIndex(unsigned Index) {
     assert(isa<FunctionDecl>(getDecl()) &&
-           !cast<FunctionDecl>(getDecl())->hasAttr<CUDAGlobalAttr>() &&
+           !cast<FunctionDecl>(getDecl())->hasAttr<ComputeKernelAttr>() &&
            !isa<CXXConstructorDecl>(getDecl()) &&
            !isa<CXXDestructorDecl>(getDecl()) &&
            "Decl is not a plain FunctionDecl!");
@@ -188,7 +192,7 @@ public:
 
   GlobalDecl getWithKernelReferenceKind(KernelReferenceKind Kind) {
     assert(isa<FunctionDecl>(getDecl()) &&
-           cast<FunctionDecl>(getDecl())->hasAttr<CUDAGlobalAttr>() &&
+           cast<FunctionDecl>(getDecl())->hasAttr<ComputeKernelAttr>() &&
            "Decl is not a GPU kernel!");
     GlobalDecl Result(*this);
     Result.Value.setInt(unsigned(Kind));
diff --git a/clang/include/clang/AST/Mangle.h b/clang/include/clang/AST/Mangle.h
index 7d02f08e0120..3335738dcd17 100644
--- a/clang/include/clang/AST/Mangle.h
+++ b/clang/include/clang/AST/Mangle.h
@@ -38,6 +38,7 @@ namespace clang {
   struct ThisAdjustment;
   struct ThunkInfo;
   class VarDecl;
+  class FieldDecl;
 
 /// MangleContext - Context for tracking state which persists across multiple
 /// calls to the C++ name mangler.
@@ -165,6 +166,9 @@ public:
   /// across translation units so it can be used with LTO.
   virtual void mangleTypeName(QualType T, raw_ostream &) = 0;
 
+  virtual void mangleMetalFieldName(const FieldDecl *D, const CXXRecordDecl* RD, raw_ostream &) {}
+  virtual void mangleMetalGeneric(const std::string& name, QualType Ty, const CXXRecordDecl* RD, raw_ostream &) {}
+
   /// @}
 };
 
diff --git a/clang/include/clang/AST/OperationKinds.def b/clang/include/clang/AST/OperationKinds.def
index b05b9d81569e..f959a4eabfa5 100644
--- a/clang/include/clang/AST/OperationKinds.def
+++ b/clang/include/clang/AST/OperationKinds.def
@@ -354,6 +354,12 @@ CAST_OPERATION(BuiltinFnToFnPtr)
 // queue_t, etc.)
 CAST_OPERATION(ZeroToOCLOpaqueType)
 
+// Convert a zero value for OpenCL event_t initialization.
+CAST_OPERATION(ZeroToOCLEvent)
+
+// Convert a zero value for OpenCL queue_t initialization.
+CAST_OPERATION(ZeroToOCLQueue)
+
 // Convert a pointer to a different address space.
 CAST_OPERATION(AddressSpaceConversion)
 
diff --git a/clang/include/clang/AST/Type.h b/clang/include/clang/AST/Type.h
index 722add6cd877..c2771e6faf95 100644
--- a/clang/include/clang/AST/Type.h
+++ b/clang/include/clang/AST/Type.h
@@ -459,8 +459,33 @@ public:
   /// Add the qualifiers from the given set to this set, given that
   /// they don't conflict.
   void addConsistentQualifiers(Qualifiers qs) {
-    assert(getAddressSpace() == qs.getAddressSpace() ||
-           !hasAddressSpace() || !qs.hasAddressSpace());
+    // fix address space, otherwise assert
+    if ((!hasAddressSpace() && qs.hasAddressSpace()) ||
+        (hasAddressSpace() && !qs.hasAddressSpace())) {
+      // -> adding an address space or keeping an existing address space, let it pass
+    } else if (getAddressSpace() == qs.getAddressSpace()) {
+      // -> both have the same address space, all okay
+    } else {
+      // -> different address spaces
+      if (getAddressSpace() == LangAS::Default ||
+          getAddressSpace() == LangAS::opencl_private ||
+          getAddressSpace() == LangAS::opencl_generic) {
+        // allow adjustment to new address space for private/generic,
+        // this usually happens when going to a more specific one
+        removeAddressSpace();
+      } else if (qs.getAddressSpace() == LangAS::Default ||
+                 qs.getAddressSpace() == LangAS::opencl_private ||
+                 qs.getAddressSpace() == LangAS::opencl_generic) {
+        // ignore less specific address space
+        qs.removeAddressSpace();
+      } else {
+        // really mismatching address space
+        // -> assert and keep original
+        assert(false && "address space mismatch");
+        qs.removeAddressSpace();
+      }
+    }
+
     assert(getObjCGCAttr() == qs.getObjCGCAttr() ||
            !hasObjCGCAttr() || !qs.hasObjCGCAttr());
     assert(getObjCLifetime() == qs.getObjCLifetime() ||
@@ -475,7 +500,18 @@ public:
   ///   every address space is a superset of itself.
   /// CL2.0 adds:
   ///   __generic is a superset of any address space except for __constant.
-  static bool isAddressSpaceSupersetOf(LangAS A, LangAS B) {
+  static bool isAddressSpaceSupersetOf(LangAS A, LangAS B, bool check_as = true,
+                                       bool allow_default_as_cast = false) {
+    if (allow_default_as_cast) {
+      if (A == LangAS::Default || B == LangAS::Default) {
+        return true;
+      }
+    }
+
+    if (!check_as) {
+      return true; // !check_as -> always pass
+    }
+
     // Address spaces must match exactly.
     return A == B ||
            // Otherwise in OpenCLC v2.0 s6.5.5: every address space except
@@ -505,15 +541,18 @@ public:
 
   /// Returns true if the address space in these qualifiers is equal to or
   /// a superset of the address space in the argument qualifiers.
-  bool isAddressSpaceSupersetOf(Qualifiers other) const {
-    return isAddressSpaceSupersetOf(getAddressSpace(), other.getAddressSpace());
+  bool isAddressSpaceSupersetOf(Qualifiers other, bool check_as = true,
+                                bool allow_default_as_cast = false) const {
+    return isAddressSpaceSupersetOf(getAddressSpace(), other.getAddressSpace(),
+                                    check_as, allow_default_as_cast);
   }
 
   /// Determines if these qualifiers compatibly include another set.
   /// Generally this answers the question of whether an object with the other
   /// qualifiers can be safely used as an object with these qualifiers.
-  bool compatiblyIncludes(Qualifiers other) const {
-    return isAddressSpaceSupersetOf(other) &&
+  bool compatiblyIncludes(Qualifiers other, bool check_as = true,
+                          bool allow_default_as_cast = false) const {
+    return isAddressSpaceSupersetOf(other, check_as, allow_default_as_cast) &&
            // ObjC GC qualifiers can match, be added, or be removed, but can't
            // be changed.
            (getObjCGCAttr() == other.getObjCGCAttr() || !hasObjCGCAttr() ||
@@ -1999,6 +2038,7 @@ public:
   bool isComplexType() const;      // C99 6.2.5p11 (complex)
   bool isAnyComplexType() const;   // C99 6.2.5p11 (complex) + Complex Int.
   bool isFloatingType() const;     // C99 6.2.5p11 (real floating + complex)
+  bool isDoubleType() const;       // (double + long double)
   bool isHalfType() const;         // OpenCL 6.1.1.1, NEON (IEEE 754-2008 half)
   bool isFloat16Type() const;      // C11 extension ISO/IEC TS 18661
   bool isBFloat16Type() const;
@@ -2012,6 +2052,12 @@ public:
   bool isFundamentalType() const;
   bool isCompoundType() const;
 
+  // Vector categories
+  bool isFloatingVecType() const;
+  bool isDoubleVecType() const;
+  bool isIntegerVecType() const;
+  bool isRealVecType() const;
+
   // Type Predicates: Check to see if this type is structurally the specified
   // type, ignoring typedefs and qualifiers.
   bool isFunctionType() const;
@@ -2114,11 +2160,15 @@ public:
 
   bool isImageType() const;                     // Any OpenCL image type
 
+  bool isAggregateImageType() const;            // struct/class containing only image*_t members
+  bool isArrayImageType(bool single_field_arr) const; // array of aggregate images
+
   bool isSamplerT() const;                      // OpenCL sampler_t
   bool isEventT() const;                        // OpenCL event_t
   bool isClkEventT() const;                     // OpenCL clk_event_t
   bool isQueueT() const;                        // OpenCL queue_t
   bool isReserveIDT() const;                    // OpenCL reserve_id_t
+  bool isExecType() const;                      // OpenCL 2.0 execution model types
 
 #define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \
   bool is##Id##Type() const;
@@ -6591,7 +6641,7 @@ inline FunctionType::ExtInfo getFunctionExtInfo(QualType t) {
 inline bool QualType::isMoreQualifiedThan(QualType other) const {
   Qualifiers MyQuals = getQualifiers();
   Qualifiers OtherQuals = other.getQualifiers();
-  return (MyQuals != OtherQuals && MyQuals.compatiblyIncludes(OtherQuals));
+  return (MyQuals != OtherQuals && MyQuals.compatiblyIncludes(OtherQuals, false));
 }
 
 /// Determine whether this type is at last
@@ -6605,7 +6655,7 @@ inline bool QualType::isAtLeastAsQualifiedAs(QualType other) const {
   if (getUnqualifiedType()->isVoidType())
     OtherQuals.removeUnaligned();
 
-  return getQualifiers().compatiblyIncludes(OtherQuals);
+  return getQualifiers().compatiblyIncludes(OtherQuals, false);
 }
 
 /// If Type is a reference type (e.g., const
@@ -6876,6 +6926,11 @@ inline bool Type::isReserveIDT() const {
   return isSpecificBuiltinType(BuiltinType::OCLReserveID);
 }
 
+inline bool Type::isExecType() const {
+  return isSpecificBuiltinType(BuiltinType::OCLQueue) ||
+         isSpecificBuiltinType(BuiltinType::OCLClkEvent);
+}
+
 inline bool Type::isImageType() const {
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) is##Id##Type() ||
   return
@@ -6914,7 +6969,8 @@ inline bool Type::isOCLExtOpaqueType() const {
 
 inline bool Type::isOpenCLSpecificType() const {
   return isSamplerT() || isEventT() || isImageType() || isClkEventT() ||
-         isQueueT() || isReserveIDT() || isPipeType() || isOCLExtOpaqueType();
+         isQueueT() || isReserveIDT() || isPipeType() || isOCLExtOpaqueType() ||
+         isAggregateImageType();
 }
 
 inline bool Type::isTemplateTypeParmType() const {
diff --git a/clang/include/clang/Basic/AddressSpaces.h b/clang/include/clang/Basic/AddressSpaces.h
index 99bb67fd26d1..a4f3dbb47cf9 100644
--- a/clang/include/clang/Basic/AddressSpaces.h
+++ b/clang/include/clang/Basic/AddressSpaces.h
@@ -39,6 +39,9 @@ enum class LangAS : unsigned {
   opencl_global_device,
   opencl_global_host,
 
+  // Vulkan specific address spaces.
+  vulkan_input,
+
   // CUDA specific address spaces.
   cuda_device,
   cuda_constant,
diff --git a/clang/include/clang/Basic/Attr.td b/clang/include/clang/Basic/Attr.td
index 31ca7d21b902..2d732362f434 100644
--- a/clang/include/clang/Basic/Attr.td
+++ b/clang/include/clang/Basic/Attr.td
@@ -140,7 +140,7 @@ def FunctionLike : SubsetSubject<DeclBase,
                                  "functions, function pointers">;
 
 def OpenCLKernelFunction
-    : SubsetSubject<Function, [{S->hasAttr<OpenCLKernelAttr>()}],
+    : SubsetSubject<Function, [{S->hasAttr<ComputeKernelAttr>()}],
                     "kernel functions">;
 
 // HasFunctionProto is a more strict version of FunctionLike, so it should
@@ -1088,13 +1088,6 @@ def CPUDispatch : InheritableAttr {
 
 // CUDA attributes are spelled __attribute__((attr)) or __declspec(__attr__),
 // and they do not receive a [[]] spelling.
-def CUDAConstant : InheritableAttr {
-  let Spellings = [GNU<"constant">, Declspec<"__constant__">];
-  let Subjects = SubjectList<[Var]>;
-  let LangOpts = [CUDA];
-  let Documentation = [Undocumented];
-}
-
 def CUDACudartBuiltin : IgnoredAttr {
   let Spellings = [GNU<"cudart_builtin">, Declspec<"__cudart_builtin__">];
   let LangOpts = [CUDA];
@@ -1134,14 +1127,6 @@ def CUDADeviceBuiltinTextureType : InheritableAttr {
 def : MutualExclusions<[CUDADeviceBuiltinSurfaceType,
                         CUDADeviceBuiltinTextureType]>;
 
-def CUDAGlobal : InheritableAttr {
-  let Spellings = [GNU<"global">, Declspec<"__global__">];
-  let Subjects = SubjectList<[Function]>;
-  let LangOpts = [CUDA];
-  let Documentation = [Undocumented];
-}
-def : MutualExclusions<[CUDADevice, CUDAGlobal]>;
-
 def CUDAHost : InheritableAttr {
   let Spellings = [GNU<"host">, Declspec<"__host__">];
   let Subjects = SubjectList<[Function]>;
@@ -1149,7 +1134,6 @@ def CUDAHost : InheritableAttr {
   let Documentation = [Undocumented];
   let SimpleHandler = 1;
 }
-def : MutualExclusions<[CUDAGlobal, CUDAHost]>;
 
 def HIPManaged : InheritableAttr {
   let Spellings = [GNU<"managed">, Declspec<"__managed__">];
@@ -1176,14 +1160,6 @@ def CUDALaunchBounds : InheritableAttr {
   let Documentation = [Undocumented];
 }
 
-def CUDAShared : InheritableAttr {
-  let Spellings = [GNU<"shared">, Declspec<"__shared__">];
-  let Subjects = SubjectList<[Var]>;
-  let LangOpts = [CUDA];
-  let Documentation = [Undocumented];
-}
-def : MutualExclusions<[CUDAConstant, CUDAShared, HIPManaged]>;
-
 def SYCLKernel : InheritableAttr {
   let Spellings = [Clang<"sycl_kernel">];
   let Subjects = SubjectList<[FunctionTmpl]>;
@@ -1205,81 +1181,191 @@ def CXX11NoReturn : InheritableAttr {
   let SimpleHandler = 1;
 }
 
-// Similar to CUDA, OpenCL attributes do not receive a [[]] spelling because
-// the specification does not expose them with one currently.
-def OpenCLKernel : InheritableAttr {
-  let Spellings = [Keyword<"__kernel">, Keyword<"kernel">];
-  let Subjects = SubjectList<[Function], ErrorDiag>;
+// floor attributes
+def ComputeKernel : DeclOrTypeAttr {
+  let Spellings = [GNU<"compute_kernel">, CXX11<"","compute_kernel", 200809>];
   let Documentation = [Undocumented];
   let SimpleHandler = 1;
 }
 
-def OpenCLUnrollHint : StmtAttr {
-  let Spellings = [GNU<"opencl_unroll_hint">];
-  let Subjects = SubjectList<[ForStmt, CXXForRangeStmt, WhileStmt, DoStmt],
-                             ErrorDiag, "'for', 'while', and 'do' statements">;
-  let Args = [UnsignedArgument<"UnrollHint", /*opt*/1>];
-  let Documentation = [OpenCLUnrollHintDocs];
+def GraphicsVertexShader : DeclOrTypeAttr {
+  let Spellings = [GNU<"vertex_shader">, CXX11<"","vertex_shader", 200809>];
+  let Documentation = [Undocumented];
+  let SimpleHandler = 1;
 }
 
-def OpenCLIntelReqdSubGroupSize: InheritableAttr {
-  let Spellings = [GNU<"intel_reqd_sub_group_size">];
-  let Args = [UnsignedArgument<"SubGroupSize">];
-  let Subjects = SubjectList<[Function], ErrorDiag>;
-  let Documentation = [OpenCLIntelReqdSubGroupSizeDocs];
+def GraphicsFragmentShader : DeclOrTypeAttr {
+  let Spellings = [GNU<"fragment_shader">, CXX11<"","fragment_shader", 200809>];
+  let Documentation = [Undocumented];
+  let SimpleHandler = 1;
 }
 
-// This attribute is both a type attribute, and a declaration attribute (for
-// parameter variables).
-def OpenCLAccess : Attr {
-  let Spellings = [Keyword<"__read_only">, Keyword<"read_only">,
-                   Keyword<"__write_only">, Keyword<"write_only">,
-                   Keyword<"__read_write">, Keyword<"read_write">];
-  let Subjects = SubjectList<[ParmVar, TypedefName], ErrorDiag>;
-  let Accessors = [Accessor<"isReadOnly", [Keyword<"__read_only">,
-                                           Keyword<"read_only">]>,
-                   Accessor<"isReadWrite", [Keyword<"__read_write">,
-                                            Keyword<"read_write">]>,
-                   Accessor<"isWriteOnly", [Keyword<"__write_only">,
-                                            Keyword<"write_only">]>];
+def RetRange : InheritableAttr {
+  let Spellings = [GNU<"range">, CXX11<"","range", 200809>];
+  let Args = [ExprArgument<"LowerBound">, ExprArgument<"UpperBound">];
+  let Documentation = [Undocumented];
+}
+
+def ImageAccess : Attr {
+  let Spellings = [GNU<"image_read_only">, CXX11<"","image_read_only", 200809>,
+                   GNU<"image_write_only">, CXX11<"","image_write_only", 200809>,
+                   GNU<"image_read_write">, CXX11<"","image_read_write", 200809>];
+  let Accessors = [Accessor<"isReadOnly", [GNU<"image_read_only">,
+										   CXX11<"","image_read_only", 200809>]>,
+                   Accessor<"isReadWrite", [GNU<"image_read_write">,
+											CXX11<"","image_read_write", 200809>]>,
+                   Accessor<"isWriteOnly", [GNU<"image_write_only">,
+											CXX11<"","image_write_only", 200809>]>];
   let Documentation = [OpenCLAccessDocs];
 }
 
-def OpenCLPrivateAddressSpace : TypeAttr {
-  let Spellings = [Keyword<"__private">, Keyword<"private">, Clang<"opencl_private">];
+def PrivateAddressSpace : TypeAttr {
+  let Spellings = [GNU<"private_as">, CXX11<"","private_as", 200809>];
   let Documentation = [OpenCLAddressSpacePrivateDocs];
 }
 
-def OpenCLGlobalAddressSpace : TypeAttr {
-  let Spellings = [Keyword<"__global">, Keyword<"global">, Clang<"opencl_global">];
+def GlobalAddressSpace : TypeAttr {
+  let Spellings = [GNU<"global_as">, CXX11<"","global_as", 200809>];
   let Documentation = [OpenCLAddressSpaceGlobalDocs];
 }
 
-def OpenCLGlobalDeviceAddressSpace : TypeAttr {
-  let Spellings = [Clang<"opencl_global_device">];
-  let Documentation = [OpenCLAddressSpaceGlobalExtDocs];
+def LocalAddressSpace : TypeAttr {
+  let Spellings = [GNU<"local_as">, CXX11<"","local_as", 200809>];
+  let Documentation = [OpenCLAddressSpaceLocalDocs];
 }
 
-def OpenCLGlobalHostAddressSpace : TypeAttr {
-  let Spellings = [Clang<"opencl_global_host">];
-  let Documentation = [OpenCLAddressSpaceGlobalExtDocs];
+def ConstantAddressSpace : TypeAttr {
+  let Spellings = [GNU<"constant_as">, CXX11<"","constant_as", 200809>];
+  let Documentation = [OpenCLAddressSpaceConstantDocs];
 }
 
-def OpenCLLocalAddressSpace : TypeAttr {
-  let Spellings = [Keyword<"__local">, Keyword<"local">, Clang<"opencl_local">];
-  let Documentation = [OpenCLAddressSpaceLocalDocs];
+def GenericAddressSpace : TypeAttr {
+  let Spellings = [GNU<"generic_as">, CXX11<"","generic_as", 200809>];
+  let Documentation = [OpenCLAddressSpaceGenericDocs];
 }
 
-def OpenCLConstantAddressSpace : TypeAttr {
-  let Spellings = [Keyword<"__constant">, Keyword<"constant">, Clang<"opencl_constant">];
-  let Documentation = [OpenCLAddressSpaceConstantDocs];
+// CUDA address space attrs are not type attrs, so they need to be handled specially
+def CUDAShared : InheritableAttr {
+  let Spellings = [GNU<"local_cuda">, CXX11<"","local_cuda", 200809>];
+  let Subjects = SubjectList<[Var]>;
+  let LangOpts = [CUDA];
+  let Documentation = [Undocumented];
 }
 
-def OpenCLGenericAddressSpace : TypeAttr {
-  let Spellings = [Keyword<"__generic">, Keyword<"generic">, Clang<"opencl_generic">];
-  let Documentation = [OpenCLAddressSpaceGenericDocs];
+def FloorArgBuffer : DeclOrTypeAttr {
+  let Spellings = [GNU<"floor_arg_buffer">, CXX11<"","floor_arg_buffer", 200809>];
+  let Documentation = [Undocumented];
+}
+
+def CUDAConstant : InheritableAttr {
+  let Spellings = [GNU<"constant_cuda">, CXX11<"","constant_cuda", 200809>];
+  let Subjects = SubjectList<[Var]>;
+  let LangOpts = [CUDA];
+  let Documentation = [Undocumented];
 }
 
+def FloorImageDataType : InheritableAttr {
+  let Spellings = [CXX11<"floor","image_data_type", 200809>];
+  let Args = [TypeArgument<"ImageDataType">];
+  let TemplateDependent = 1;
+  let Documentation = [Undocumented];
+}
+
+def FloorImageFlags : InheritableAttr {
+  let Spellings = [CXX11<"floor","image_flags", 200809>];
+  let Args = [ExprArgument<"ImageFlags">];
+  let TemplateDependent = 1;
+  let AdditionalMembers = [{
+    uint64_t eval_flags = 0;
+    uint64_t getEvalFlags() const {
+      return eval_flags;
+    }
+    void setEvalFlags(uint64_t flags) {
+      eval_flags = flags;
+    }
+    bool isReadOnly() const {
+        return (eval_flags & 0xC00u) == 0x400u;
+    }
+    bool isWriteOnly() const {
+        return (eval_flags & 0xC00u) == 0x800u;
+    }
+    bool isReadWrite() const {
+        return (eval_flags & 0xC00u) == 0xC00u;
+    }
+  }];
+  let Documentation = [Undocumented];
+}
+
+// on aggregate types this signals that they can be converted/coerced to the corresponding clang/llvm vector type
+def VectorCompat : InheritableAttr {
+  let Spellings = [GNU<"vector_compat">, CXX11<"","vector_compat", 200809>];
+  let Documentation = [Undocumented];
+}
+
+// fbo color output location
+def GraphicsFBOColorLocation : InheritableAttr {
+  let Spellings = [GNU<"color">, CXX11<"","color", 200809>];
+  let Args = [ExprArgument<"ColorLocation">];
+  let TemplateDependent = 1;
+  let AdditionalMembers = [{
+  unsigned int eval_location = 0;
+  unsigned int getEvalLocation() const {
+    return eval_location;
+  }
+  void setEvalLocation(unsigned int loc) {
+    eval_location = loc;
+  }
+  }];
+  let Documentation = [Undocumented];
+}
+
+// fbo explicit writable depth with depth type
+def GraphicsFBODepthType : InheritableAttr {
+  let Spellings = [GNU<"depth">, CXX11<"","depth", 200809>];
+  let Args = [EnumArgument<"DepthQualifier", "DepthQualifierType",
+                           ["any", "greater", "less"],
+                           ["FBODepthTypeAny", "FBODepthTypeGreater", "FBODepthTypeLess"]>];
+  let Documentation = [Undocumented];
+}
+
+// vertex output position (used in vertex output structs)
+def GraphicsVertexPosition : InheritableAttr {
+  let Spellings = [GNU<"position">, CXX11<"","position", 200809>];
+  let Documentation = [Undocumented];
+}
+
+// vertex/point output size (used in vertex output structs)
+def GraphicsPointSize : InheritableAttr {
+  let Spellings = [GNU<"point_size">, CXX11<"","point_size", 200809>];
+  let Documentation = [Undocumented];
+}
+
+// stage input (just fragment shader input right now)
+def GraphicsStageInput : Attr {
+  let Spellings = [GNU<"stage_input">, CXX11<"","stage_input", 200809>];
+  let Subjects = SubjectList<[ParmVar], ErrorDiag>;
+  let Documentation = [Undocumented];
+}
+
+// Similar to CUDA, OpenCL attributes do not receive a [[]] spelling because
+// the specification does not expose them with one currently.
+def OpenCLUnrollHint : StmtAttr {
+  let Spellings = [GNU<"opencl_unroll_hint">];
+  let Subjects = SubjectList<[ForStmt, CXXForRangeStmt, WhileStmt, DoStmt],
+                             ErrorDiag, "'for', 'while', and 'do' statements">;
+  let Args = [UnsignedArgument<"UnrollHint", /*opt*/1>];
+  let Documentation = [OpenCLUnrollHintDocs];
+}
+
+def OpenCLIntelReqdSubGroupSize: InheritableAttr {
+  let Spellings = [GNU<"intel_reqd_sub_group_size">];
+  let Args = [UnsignedArgument<"SubGroupSize">];
+  let Subjects = SubjectList<[Function], ErrorDiag>;
+  let Documentation = [OpenCLIntelReqdSubGroupSizeDocs];
+}
+
+// This attribute is both a type attribute, and a declaration attribute (for
+// parameter variables).
 def OpenCLNoSVM : Attr {
   let Spellings = [GNU<"nosvm">];
   let Subjects = SubjectList<[Var]>;
diff --git a/clang/include/clang/Basic/Builtins.def b/clang/include/clang/Basic/Builtins.def
index 7d331a86126f..045e12e62e03 100644
--- a/clang/include/clang/Basic/Builtins.def
+++ b/clang/include/clang/Basic/Builtins.def
@@ -139,10 +139,12 @@ BUILTIN(__builtin_huge_val, "d", "nc")
 BUILTIN(__builtin_huge_valf, "f", "nc")
 BUILTIN(__builtin_huge_vall, "Ld", "nc")
 BUILTIN(__builtin_huge_valf128, "LLd", "nc")
+BUILTIN(__builtin_huge_valh, "h", "nc")
 BUILTIN(__builtin_inf  , "d"   , "nc")
 BUILTIN(__builtin_inff , "f"   , "nc")
 BUILTIN(__builtin_infl , "Ld"  , "nc")
 BUILTIN(__builtin_inff128 , "LLd"  , "nc")
+BUILTIN(__builtin_infh , "h"   , "nc")
 BUILTIN(__builtin_labs , "LiLi"  , "Fnc")
 BUILTIN(__builtin_llabs, "LLiLLi", "Fnc")
 BUILTIN(__builtin_ldexp , "ddi"  , "Fne")
@@ -157,10 +159,12 @@ BUILTIN(__builtin_nan,  "dcC*" , "FnU")
 BUILTIN(__builtin_nanf, "fcC*" , "FnU")
 BUILTIN(__builtin_nanl, "LdcC*", "FnU")
 BUILTIN(__builtin_nanf128, "LLdcC*", "FnU")
+BUILTIN(__builtin_nanh, "hcC*" , "ncF")
 BUILTIN(__builtin_nans,  "dcC*" , "FnU")
 BUILTIN(__builtin_nansf, "fcC*" , "FnU")
 BUILTIN(__builtin_nansl, "LdcC*", "FnU")
 BUILTIN(__builtin_nansf128, "LLdcC*", "FnU")
+BUILTIN(__builtin_nansh, "hcC*" , "ncF")
 BUILTIN(__builtin_powi , "ddi"  , "Fnc")
 BUILTIN(__builtin_powif, "ffi"  , "Fnc")
 BUILTIN(__builtin_powil, "LdLdi", "Fnc")
diff --git a/clang/include/clang/Basic/BuiltinsNVPTX.def b/clang/include/clang/Basic/BuiltinsNVPTX.def
index 7afee4dbc80b..aa3281a83033 100644
--- a/clang/include/clang/Basic/BuiltinsNVPTX.def
+++ b/clang/include/clang/Basic/BuiltinsNVPTX.def
@@ -19,13 +19,21 @@
 
 #pragma push_macro("SM_70")
 #pragma push_macro("SM_72")
+#pragma push_macro("SM_73")
 #pragma push_macro("SM_75")
 #pragma push_macro("SM_80")
+#pragma push_macro("SM_82")
 #pragma push_macro("SM_86")
-#define SM_86 "sm_86"
-#define SM_80 "sm_80|" SM_86
+#pragma push_macro("SM_87")
+#pragma push_macro("SM_88")
+#define SM_88 "sm_88"
+#define SM_87 "sm_87|" SM_88
+#define SM_86 "sm_86|" SM_87
+#define SM_82 "sm_82|" SM_86
+#define SM_80 "sm_80|" SM_82
 #define SM_75 "sm_75|" SM_80
-#define SM_72 "sm_72|" SM_75
+#define SM_73 "sm_73|" SM_75
+#define SM_72 "sm_72|" SM_73
 #define SM_70 "sm_70|" SM_72
 
 #pragma push_macro("SM_60")
@@ -33,6 +41,7 @@
 
 #pragma push_macro("PTX60")
 #pragma push_macro("PTX61")
+#pragma push_macro("PTX62")
 #pragma push_macro("PTX63")
 #pragma push_macro("PTX64")
 #pragma push_macro("PTX65")
@@ -41,7 +50,9 @@
 #pragma push_macro("PTX72")
 #pragma push_macro("PTX73")
 #pragma push_macro("PTX74")
-#define PTX74 "ptx74"
+#pragma push_macro("PTX75")
+#define PTX75 "ptx75"
+#define PTX74 "ptx74|" PTX75
 #define PTX73 "ptx73|" PTX74
 #define PTX72 "ptx72|" PTX73
 #define PTX71 "ptx71|" PTX72
@@ -49,7 +60,8 @@
 #define PTX65 "ptx65|" PTX70
 #define PTX64 "ptx64|" PTX65
 #define PTX63 "ptx63|" PTX64
-#define PTX61 "ptx61|" PTX63
+#define PTX62 "ptx62|" PTX63
+#define PTX61 "ptx61|" PTX62
 #define PTX60 "ptx60|" PTX61
 
 #pragma push_macro("AND")
@@ -814,11 +826,16 @@ TARGET_BUILTIN(__nvvm_cp_async_wait_all, "v", "", AND(SM_80,PTX70))
 #pragma pop_macro("SM_60")
 #pragma pop_macro("SM_70")
 #pragma pop_macro("SM_72")
+#pragma pop_macro("SM_73")
 #pragma pop_macro("SM_75")
 #pragma pop_macro("SM_80")
+#pragma pop_macro("SM_82")
 #pragma pop_macro("SM_86")
+#pragma pop_macro("SM_87")
+#pragma pop_macro("SM_88")
 #pragma pop_macro("PTX60")
 #pragma pop_macro("PTX61")
+#pragma pop_macro("PTX62")
 #pragma pop_macro("PTX63")
 #pragma pop_macro("PTX64")
 #pragma pop_macro("PTX65")
@@ -827,3 +844,4 @@ TARGET_BUILTIN(__nvvm_cp_async_wait_all, "v", "", AND(SM_80,PTX70))
 #pragma pop_macro("PTX72")
 #pragma pop_macro("PTX73")
 #pragma pop_macro("PTX74")
+#pragma pop_macro("PTX75")
diff --git a/clang/include/clang/Basic/CodeGenOptions.def b/clang/include/clang/Basic/CodeGenOptions.def
index 610123260175..a5c3632b9d91 100644
--- a/clang/include/clang/Basic/CodeGenOptions.def
+++ b/clang/include/clang/Basic/CodeGenOptions.def
@@ -88,6 +88,17 @@ CODEGENOPT(EmitVersionIdentMetadata , 1, 1) ///< Emit compiler version metadata.
 CODEGENOPT(EmitGcovArcs      , 1, 0) ///< Emit coverage data files, aka. GCDA.
 CODEGENOPT(EmitGcovNotes     , 1, 0) ///< Emit coverage "notes" files, aka GCNO.
 CODEGENOPT(EmitOpenCLArgMetadata , 1, 0) ///< Emit OpenCL kernel arg metadata.
+CODEGENOPT(MetalIntelWorkarounds , 1, 0) ///< Enable Intel GPU specific fixes and workarounds.
+CODEGENOPT(MetalNvidiaWorkarounds , 1, 0) ///< Enable Nvidia GPU specific fixes and workarounds.
+CODEGENOPT(MetalNoArrayImage , 1, 0) ///< Disable native array of images support.
+CODEGENOPT(MetalSoftPrintf , 1, 0) ///< Enables support of software printf emulation.
+CODEGENOPT(SPIRIntelWorkarounds , 1, 0) ///< Enable Intel SPIR specific fixes and workarounds.
+VALUE_CODEGENOPT(VulkanIUBSize, 32, 256) ///< max supported size of Vulkan inline uniform blocks
+VALUE_CODEGENOPT(VulkanIUBCount, 32, 4) ///< max supported count of Vulkan inline uniform blocks
+CODEGENOPT(VulkanLLVMPreStructurizationPass , 1, 0) ///< Enable LLVM CFG structurization pass prior to actual structurization.
+CODEGENOPT(VulkanSoftPrintf , 1, 0) ///< Enables support of software printf emulation.
+CODEGENOPT(GraphicsPrimitiveID , 1, 0) ///< Enables support for builtin primitive id.
+CODEGENOPT(GraphicsBarycentricCoord , 1, 0) ///< Enables support for builtin primitive id.
 CODEGENOPT(EmulatedTLS       , 1, 0) ///< Set by default or -f[no-]emulated-tls.
 CODEGENOPT(ExplicitEmulatedTLS , 1, 0) ///< Set if -f[no-]emulated-tls is used.
 /// Embed Bitcode mode (off/all/bitcode/marker).
diff --git a/clang/include/clang/Basic/CodeGenOptions.h b/clang/include/clang/Basic/CodeGenOptions.h
index 6a0bce0ad80a..46ab5c779515 100644
--- a/clang/include/clang/Basic/CodeGenOptions.h
+++ b/clang/include/clang/Basic/CodeGenOptions.h
@@ -229,6 +229,9 @@ public:
   /// function instead of to trap instructions.
   std::string TrapFuncName;
 
+  /// OpenCL compile options to embed in the SPIR metadata
+  std::string SPIRCompileOptions;
+
   /// A list of dependent libraries.
   std::vector<std::string> DependentLibraries;
 
diff --git a/clang/include/clang/Basic/Cuda.h b/clang/include/clang/Basic/Cuda.h
index ef2023cabf7f..7b02c431f7b3 100644
--- a/clang/include/clang/Basic/Cuda.h
+++ b/clang/include/clang/Basic/Cuda.h
@@ -33,9 +33,10 @@ enum class CudaVersion {
   CUDA_112,
   CUDA_113,
   CUDA_114,
-  FULLY_SUPPORTED = CUDA_114,
+  CUDA_115,
+  FULLY_SUPPORTED = CUDA_115,
   PARTIALLY_SUPPORTED =
-      CUDA_114, // Partially supported. Proceed with a warning.
+      CUDA_115, // Partially supported. Proceed with a warning.
   NEW = 10000,  // Too new. Issue a warning, but allow using it.
 };
 const char *CudaVersionToString(CudaVersion V);
@@ -59,9 +60,13 @@ enum class CudaArch {
   SM_62,
   SM_70,
   SM_72,
+  SM_73,
   SM_75,
   SM_80,
+  SM_82,
   SM_86,
+  SM_87,
+  SM_88,
   GFX600,
   GFX601,
   GFX602,
diff --git a/clang/include/clang/Basic/DiagnosticDriverKinds.td b/clang/include/clang/Basic/DiagnosticDriverKinds.td
index b823ade0eafb..4bed29f51d04 100644
--- a/clang/include/clang/Basic/DiagnosticDriverKinds.td
+++ b/clang/include/clang/Basic/DiagnosticDriverKinds.td
@@ -206,6 +206,9 @@ def err_drv_nested_config_file: Error<
 def err_drv_arg_requires_bitcode_input: Error<
   "option '%0' requires input to be LLVM bitcode">;
 
+def err_drv_floor_function_info : Error<
+  "unable to open floor function info file">, DefaultFatal;
+
 def err_target_unsupported_arch
   : Error<"the target architecture '%0' is not supported by the target '%1'">;
 def err_cpu_unsupported_isa
diff --git a/clang/include/clang/Basic/DiagnosticSemaKinds.td b/clang/include/clang/Basic/DiagnosticSemaKinds.td
index d37c8e9266e9..d378291eebc3 100644
--- a/clang/include/clang/Basic/DiagnosticSemaKinds.td
+++ b/clang/include/clang/Basic/DiagnosticSemaKinds.td
@@ -10085,6 +10085,8 @@ def err_event_t_addr_space_qual : Error<
   "the event_t type can only be used with __private address space qualifier">;
 def err_expected_kernel_void_return_type : Error<
   "kernel must have void return type">;
+def err_pointer_to_image : Error<
+  "pointer to image is invalid">;
 def err_sampler_initializer_not_integer : Error<
   "sampler_t initialization requires 32-bit integer, not %0">;
 def warn_sampler_initializer_invalid_bits : Warning<
diff --git a/clang/include/clang/Basic/LangOptions.def b/clang/include/clang/Basic/LangOptions.def
index 565ecd94f977..0d1bc72f1f9c 100644
--- a/clang/include/clang/Basic/LangOptions.def
+++ b/clang/include/clang/Basic/LangOptions.def
@@ -219,6 +219,11 @@ ENUM_LANGOPT(DefaultCallingConv, DefaultCallingConvention, 3, DCC_None, "default
 
 LANGOPT(ShortEnums        , 1, 0, "short enum types")
 
+LANGOPT(Metal             , 1, 0, "Metal")
+LANGOPT(MetalVersion      , 32, 200, "Metal version")
+LANGOPT(Vulkan            , 1, 0, "Vulkan")
+LANGOPT(VulkanVersion     , 32, 120, "Vulkan version")
+LANGOPT(FloorHostCompute  , 1, 0, "floor host-compute")
 LANGOPT(OpenCL            , 1, 0, "OpenCL")
 LANGOPT(OpenCLVersion     , 32, 0, "OpenCL C version")
 LANGOPT(OpenCLCPlusPlus   , 1, 0, "C++ for OpenCL")
@@ -296,6 +301,8 @@ LANGOPT(FastRelaxedMath , 1, 0, "OpenCL fast relaxed math")
 BENIGN_LANGOPT(CLNoSignedZero , 1, 0, "Permit Floating Point optimization without regard to signed zeros")
 COMPATIBLE_LANGOPT(CLUnsafeMath , 1, 0, "Unsafe Floating Point Math")
 COMPATIBLE_LANGOPT(CLFiniteMathOnly , 1, 0, "__FINITE_MATH_ONLY__ predefined macro")
+LANGOPT(CLSamplerOpaque, 1, 0, "Emit sampler as a pointer to opaque structure")
+LANGOPT(CLVerifySPIR , 1, 0, "cl-verify-spir flag used")
 /// FP_CONTRACT mode (on/off/fast).
 BENIGN_ENUM_LANGOPT(DefaultFPContractMode, FPModeKind, 2, FPM_Off, "FP contraction type")
 COMPATIBLE_LANGOPT(ExpStrictFP, 1, false, "Enable experimental strict floating point")
diff --git a/clang/include/clang/Basic/LangOptions.h b/clang/include/clang/Basic/LangOptions.h
index 35b33c2e0971..3ab1cd7b7d91 100644
--- a/clang/include/clang/Basic/LangOptions.h
+++ b/clang/include/clang/Basic/LangOptions.h
@@ -286,6 +286,12 @@ public:
     ExtendTo64
   };
 
+  std::fstream* floor_function_info { nullptr };
+  unsigned int floor_image_capabilities { 0 };
+  bool metal_no_array_image { false };
+  bool metal_soft_printf { false };
+  bool vulkan_soft_printf { false };
+
 public:
   /// The used language standard.
   LangStandard::Kind LangStd;
diff --git a/clang/include/clang/Basic/LangStandard.h b/clang/include/clang/Basic/LangStandard.h
index b0785409628c..4d0bda8c111f 100644
--- a/clang/include/clang/Basic/LangStandard.h
+++ b/clang/include/clang/Basic/LangStandard.h
@@ -33,6 +33,8 @@ enum class Language : uint8_t {
   ObjCXX,
   OpenCL,
   OpenCLCXX,
+  Metal,
+  Vulkan,
   CUDA,
   RenderScript,
   HIP,
diff --git a/clang/include/clang/Basic/LangStandards.def b/clang/include/clang/Basic/LangStandards.def
index 6056cfd65bbb..f0eb9e564ad4 100644
--- a/clang/include/clang/Basic/LangStandards.def
+++ b/clang/include/clang/Basic/LangStandards.def
@@ -170,16 +170,28 @@ LANGSTANDARD_ALIAS_DEPR(opencl10, "cl")
 
 LANGSTANDARD(opencl11, "cl1.1",
              OpenCL, "OpenCL 1.1",
-             LineComment | C99 | Digraphs | HexFloat | OpenCL)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
 LANGSTANDARD(opencl12, "cl1.2",
              OpenCL, "OpenCL 1.2",
-             LineComment | C99 | Digraphs | HexFloat | OpenCL)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
 LANGSTANDARD(opencl20, "cl2.0",
              OpenCL, "OpenCL 2.0",
-             LineComment | C99 | Digraphs | HexFloat | OpenCL)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(opencl21, "cl2.1",
+             OpenCL, "OpenCL 2.1",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(opencl22, "cl2.2",
+             OpenCL, "OpenCL 2.2",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
 LANGSTANDARD(opencl30, "cl3.0",
              OpenCL, "OpenCL 3.0",
-             LineComment | C99 | Digraphs | HexFloat | OpenCL)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
 
 LANGSTANDARD(openclcpp10, "clc++1.0",
              OpenCL, "C++ for OpenCL 1.0",
@@ -201,13 +213,37 @@ LANGSTANDARD_ALIAS_DEPR(openclcpp10, "CLC++")
 LANGSTANDARD_ALIAS_DEPR(openclcpp10, "CLC++1.0")
 LANGSTANDARD_ALIAS_DEPR(openclcpp2021, "CLC++2021")
 
+// Metal
+LANGSTANDARD(metal20, "metal2.0", Metal, "Metal 2.0",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(metal21, "metal2.1", Metal, "Metal 2.1",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(metal22, "metal2.2", Metal, "Metal 2.2",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(metal23, "metal2.3", Metal, "Metal 2.3",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+LANGSTANDARD(metal24, "metal2.4", Metal, "Metal 2.4",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+
+// Vulkan
+LANGSTANDARD(vulkan12, "vulkan1.2", Vulkan, "Vulkan C++ 1.2",
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode | OpenCL)
+
 // CUDA
 LANGSTANDARD(cuda, "cuda", CUDA, "NVIDIA CUDA(tm)",
-             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | Digraphs)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode)
 
 // HIP
 LANGSTANDARD(hip, "hip", HIP, "HIP",
-             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | Digraphs)
+             LineComment | CPlusPlus | CPlusPlus11 | CPlusPlus14 | CPlusPlus17 |
+             CPlusPlus20 | CPlusPlus2b | Digraphs | HexFloat | GNUMode)
 
 #undef LANGSTANDARD
 #undef LANGSTANDARD_ALIAS
diff --git a/clang/include/clang/Basic/OpenCLExtensions.def b/clang/include/clang/Basic/OpenCLExtensions.def
index a053a0e9adb5..a475b4db22fc 100644
--- a/clang/include/clang/Basic/OpenCLExtensions.def
+++ b/clang/include/clang/Basic/OpenCLExtensions.def
@@ -47,7 +47,7 @@
 
 #ifndef OPENCL_GENERIC_EXTENSION
 #ifndef OPENCLEXTNAME
-#pragma error "macro OPENCLEXTNAME or OPENCL_GENERIC_EXTENSION is required"
+#error "macro OPENCLEXTNAME or OPENCL_GENERIC_EXTENSION is required"
 #else
 #define OPENCL_GENERIC_EXTENSION(ext, ...) OPENCLEXTNAME(ext)
 #endif // OPENCLEXTNAME
@@ -118,6 +118,13 @@ OPENCL_OPTIONALCOREFEATURE(__opencl_c_program_scope_global_variables, false, 300
 OPENCL_OPTIONALCOREFEATURE(__opencl_c_fp64, false, 300, OCL_C_30)
 OPENCL_OPTIONALCOREFEATURE(__opencl_c_images, false, 300, OCL_C_30)
 
+// Vulkan capabilities and extensions
+OPENCL_EXTENSION(vk_capability_int16, true, 120)
+OPENCL_EXTENSION(vk_capability_int64, true, 120)
+OPENCL_EXTENSION(vk_capability_float16, true, 120)
+OPENCL_EXTENSION(vk_capability_float64, true, 120)
+OPENCL_EXTENSION(vk_capability_multiview, true, 120)
+
 #undef OPENCL_OPTIONALCOREFEATURE
 #undef OPENCL_COREFEATURE
 #undef OPENCL_GENERIC_EXTENSION
diff --git a/clang/include/clang/Basic/OpenCLImageTypes.def b/clang/include/clang/Basic/OpenCLImageTypes.def
index ada5892c06b3..43eb2e59c4bc 100644
--- a/clang/include/clang/Basic/OpenCLImageTypes.def
+++ b/clang/include/clang/Basic/OpenCLImageTypes.def
@@ -21,22 +21,42 @@
 #define IMAGE_READ_WRITE_TYPE(Type, Id, Ext)
 
 #elif defined(GENERIC_IMAGE_TYPE_EXT)
+#if 1 // NOTE: same as below
+#define IMAGE_READ_TYPE(Type, Id, Ext) GENERIC_IMAGE_TYPE_EXT(Type, Id##Ty, Ext)
+#define IMAGE_WRITE_TYPE(Type, Id, Ext)
+#define IMAGE_READ_WRITE_TYPE(Type, Id, Ext)
+#else
 #define IMAGE_READ_TYPE(Type, Id, Ext) GENERIC_IMAGE_TYPE_EXT(Type, Id##ROTy, Ext)
 #define IMAGE_WRITE_TYPE(Type, Id, Ext) GENERIC_IMAGE_TYPE_EXT(Type, Id##WOTy, Ext)
 #define IMAGE_READ_WRITE_TYPE(Type, Id, Ext) GENERIC_IMAGE_TYPE_EXT(Type, Id##RWTy, Ext)
+#endif
+
+#else
+
+// NOTE/TODO:
+// this new method of using ro/wo/rw specific image types does not yet
+// work everywhere and is generally incompatible to SPIR 1.2 itself
+// -> use the new facilities, but keep using the old/standard type names
+#if 1
+
+#define IMAGE_READ_TYPE(Type, Id, Ext) IMAGE_TYPE(Type, Id, Id##Ty, , )
+#define IMAGE_WRITE_TYPE(Type, Id, Ext)
+#define IMAGE_READ_WRITE_TYPE(Type, Id, Ext)
 
 #else
 #ifndef IMAGE_READ_TYPE
 #define IMAGE_READ_TYPE(Type, Id, Ext) \
-          IMAGE_TYPE(Type, Id##RO, Id##ROTy,  read_only, ro)
+          IMAGE_TYPE(Type, Id##RO, Id##ROTy,  read_only, _ro)
 #endif
 #ifndef IMAGE_WRITE_TYPE
 #define IMAGE_WRITE_TYPE(Type, Id, Ext) \
-          IMAGE_TYPE(Type, Id##WO, Id##WOTy, write_only, wo)
+          IMAGE_TYPE(Type, Id##WO, Id##WOTy, write_only, _wo)
 #endif
 #ifndef IMAGE_READ_WRITE_TYPE
 #define IMAGE_READ_WRITE_TYPE(Type, Id, Ext) \
-          IMAGE_TYPE(Type, Id##RW, Id##RWTy, read_write, rw)
+          IMAGE_TYPE(Type, Id##RW, Id##RWTy, read_write, _rw)
+#endif
+
 #endif
 
 #endif
@@ -53,6 +73,10 @@ IMAGE_READ_TYPE(image2d_array_msaa, OCLImage2dArrayMSAA, "cl_khr_gl_msaa_sharing
 IMAGE_READ_TYPE(image2d_msaa_depth, OCLImage2dMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_READ_TYPE(image2d_array_msaa_depth, OCLImage2dArrayMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_READ_TYPE(image3d, OCLImage3d, "")
+IMAGE_READ_TYPE(imagecube, OCLImageCube, "")
+IMAGE_READ_TYPE(imagecube_array, OCLImageCubeArray, "")
+IMAGE_READ_TYPE(imagecube_depth, OCLImageCubeDepth, "")
+IMAGE_READ_TYPE(imagecube_array_depth, OCLImageCubeArrayDepth, "")
 
 IMAGE_WRITE_TYPE(image1d, OCLImage1d, "")
 IMAGE_WRITE_TYPE(image1d_array, OCLImage1dArray, "")
@@ -66,6 +90,10 @@ IMAGE_WRITE_TYPE(image2d_array_msaa, OCLImage2dArrayMSAA, "cl_khr_gl_msaa_sharin
 IMAGE_WRITE_TYPE(image2d_msaa_depth, OCLImage2dMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_WRITE_TYPE(image2d_array_msaa_depth, OCLImage2dArrayMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_WRITE_TYPE(image3d, OCLImage3d, "")
+IMAGE_WRITE_TYPE(imagecube, OCLImageCube, "")
+IMAGE_WRITE_TYPE(imagecube_array, OCLImageCubeArray, "")
+IMAGE_WRITE_TYPE(imagecube_depth, OCLImageCubeDepth, "")
+IMAGE_WRITE_TYPE(imagecube_array_depth, OCLImageCubeArrayDepth, "")
 
 IMAGE_READ_WRITE_TYPE(image1d, OCLImage1d, "")
 IMAGE_READ_WRITE_TYPE(image1d_array, OCLImage1dArray, "")
@@ -79,6 +107,10 @@ IMAGE_READ_WRITE_TYPE(image2d_array_msaa, OCLImage2dArrayMSAA, "cl_khr_gl_msaa_s
 IMAGE_READ_WRITE_TYPE(image2d_msaa_depth, OCLImage2dMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_READ_WRITE_TYPE(image2d_array_msaa_depth, OCLImage2dArrayMSAADepth, "cl_khr_gl_msaa_sharing")
 IMAGE_READ_WRITE_TYPE(image3d, OCLImage3d, "")
+IMAGE_READ_WRITE_TYPE(imagecube, OCLImageCube, "")
+IMAGE_READ_WRITE_TYPE(imagecube_array, OCLImageCubeArray, "")
+IMAGE_READ_WRITE_TYPE(imagecube_depth, OCLImageCubeDepth, "")
+IMAGE_READ_WRITE_TYPE(imagecube_array_depth, OCLImageCubeArrayDepth, "")
 
 #undef IMAGE_TYPE
 #undef GENERIC_IMAGE_TYPE
diff --git a/clang/include/clang/Basic/OpenCLOptions.h b/clang/include/clang/Basic/OpenCLOptions.h
index d6cb1a210519..3f4ef036a8a4 100644
--- a/clang/include/clang/Basic/OpenCLOptions.h
+++ b/clang/include/clang/Basic/OpenCLOptions.h
@@ -31,8 +31,10 @@ enum OpenCLVersionID : unsigned int {
   OCL_C_11 = 0x2,
   OCL_C_12 = 0x4,
   OCL_C_20 = 0x8,
-  OCL_C_30 = 0x10,
-  OCL_C_ALL = 0x1f,
+  OCL_C_21 = 0x10,
+  OCL_C_22 = 0x20,
+  OCL_C_30 = 0x40,
+  OCL_C_ALL = OCL_C_10 | OCL_C_11 | OCL_C_12 | OCL_C_20 | OCL_C_21 | OCL_C_22 | OCL_C_30,
   OCL_C_11P = OCL_C_ALL ^ OCL_C_10,              // OpenCL C 1.1+
   OCL_C_12P = OCL_C_ALL ^ (OCL_C_10 | OCL_C_11), // OpenCL C 1.2+
 };
@@ -49,6 +51,10 @@ static inline OpenCLVersionID encodeOpenCLVersion(unsigned OpenCLVersion) {
     return OCL_C_12;
   case 200:
     return OCL_C_20;
+  case 210:
+    return OCL_C_21;
+  case 220:
+    return OCL_C_22;
   case 300:
     return OCL_C_30;
   }
diff --git a/clang/include/clang/Basic/Specifiers.h b/clang/include/clang/Basic/Specifiers.h
index 66cdba3f912e..5c31a17a320b 100644
--- a/clang/include/clang/Basic/Specifiers.h
+++ b/clang/include/clang/Basic/Specifiers.h
@@ -90,6 +90,11 @@ namespace clang {
     TST_atomic,           // C11 _Atomic
 #define GENERIC_IMAGE_TYPE(ImgType, Id) TST_##ImgType##_t, // OpenCL image types
 #include "clang/Basic/OpenCLImageTypes.def"
+    TST_sampler_t,        // OpenCL sampler_t
+    TST_event_t,          // OpenCL event_t
+    TST_queue_t,          // OpenCL queue_t
+    TST_clk_event_t,      // OpenCL clk_event_t
+    TST_reserve_id_t,     // OpenCL reserve_id_t
     TST_error // erroneous type
   };
 
@@ -273,8 +278,12 @@ namespace clang {
     CC_AAPCS,       // __attribute__((pcs("aapcs")))
     CC_AAPCS_VFP,   // __attribute__((pcs("aapcs-vfp")))
     CC_IntelOclBicc, // __attribute__((intel_ocl_bicc))
-    CC_SpirFunction, // default for OpenCL functions on SPIR target
-    CC_OpenCLKernel, // inferred for OpenCL kernels
+    // NOTE: don't go above 15 for anything that is actually used by clang
+    CC_FloorFunction, // default for OpenCL/SPIR, Metal/AIR, CUDA and Vulkan/SPIR-V functions (non entry points)
+    CC_FloorKernel,   // inferred for OpenCL/SPIR, Metal/AIR, CUDA and Vulkan/SPIR-V kernels
+    CC_FloorVertex,   // inferred for Metal/AIR and Vulkan/SPIR-V vertex shaders
+    CC_FloorFragment, // inferred for Metal/AIR and Vulkan/SPIR-V fragment shaders
+    // ^^^ 15
     CC_Swift,        // __attribute__((swiftcall))
     CC_SwiftAsync,        // __attribute__((swiftasynccall))
     CC_PreserveMost, // __attribute__((preserve_most))
@@ -292,8 +301,10 @@ namespace clang {
     case CC_X86RegCall:
     case CC_X86Pascal:
     case CC_X86VectorCall:
-    case CC_SpirFunction:
-    case CC_OpenCLKernel:
+    case CC_FloorFunction:
+    case CC_FloorKernel:
+    case CC_FloorVertex:
+    case CC_FloorFragment:
     case CC_Swift:
     case CC_SwiftAsync:
       return false;
diff --git a/clang/include/clang/Basic/TargetInfo.h b/clang/include/clang/Basic/TargetInfo.h
index 77a510462a65..9b34bd786ead 100644
--- a/clang/include/clang/Basic/TargetInfo.h
+++ b/clang/include/clang/Basic/TargetInfo.h
@@ -1471,6 +1471,7 @@ public:
       default:
         return CCCR_Warning;
       case CC_C:
+      case CC_FloorKernel:
         return CCCR_OK;
     }
   }
diff --git a/clang/include/clang/Basic/TargetOptions.h b/clang/include/clang/Basic/TargetOptions.h
index 81c15adb8248..92a9f3f715dd 100644
--- a/clang/include/clang/Basic/TargetOptions.h
+++ b/clang/include/clang/Basic/TargetOptions.h
@@ -69,7 +69,7 @@ public:
   std::vector<std::string> OpenCLExtensionsAsWritten;
 
   /// If given, enables support for __int128_t and __uint128_t types.
-  bool ForceEnableInt128 = false;
+  bool ForceEnableInt128 = true;
 
   /// \brief If enabled, use 32-bit pointers for accessing const/local/shared
   /// address space.
diff --git a/clang/include/clang/Basic/TokenKinds.def b/clang/include/clang/Basic/TokenKinds.def
index 0dd5936aa3e6..e4d17e90d06b 100644
--- a/clang/include/clang/Basic/TokenKinds.def
+++ b/clang/include/clang/Basic/TokenKinds.def
@@ -566,35 +566,20 @@ KEYWORD(__forceinline               , KEYMS)
 KEYWORD(__unaligned                 , KEYMS)
 KEYWORD(__super                     , KEYMS)
 
-// OpenCL address space qualifiers
-KEYWORD(__global                    , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__local                     , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__constant                  , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__private                   , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__generic                   , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("global", __global            , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("local", __local              , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("constant", __constant        , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("private", __private          , KEYOPENCLC)
-ALIAS("generic", __generic          , KEYOPENCLC | KEYOPENCLCXX)
-// OpenCL function qualifiers
-KEYWORD(__kernel                    , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("kernel", __kernel            , KEYOPENCLC | KEYOPENCLCXX)
-// OpenCL access qualifiers
-KEYWORD(__read_only                 , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__write_only                , KEYOPENCLC | KEYOPENCLCXX)
-KEYWORD(__read_write                , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("read_only", __read_only      , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("write_only", __write_only    , KEYOPENCLC | KEYOPENCLCXX)
-ALIAS("read_write", __read_write    , KEYOPENCLC | KEYOPENCLCXX)
+
 // OpenCL builtins
-KEYWORD(__builtin_astype            , KEYOPENCLC | KEYOPENCLCXX)
-UNARY_EXPR_OR_TYPE_TRAIT(vec_step, VecStep, KEYOPENCLC | KEYOPENCLCXX | KEYALTIVEC | KEYZVECTOR)
-#define GENERIC_IMAGE_TYPE(ImgType, Id) KEYWORD(ImgType##_t, KEYOPENCLC | KEYOPENCLCXX)
-#include "clang/Basic/OpenCLImageTypes.def"
-KEYWORD(pipe                        , KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(__builtin_astype            , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(sampler_t                   , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(event_t                     , KEYCXX | KEYOPENCLC| KEYOPENCLCXX)
+UNARY_EXPR_OR_TYPE_TRAIT(vec_step, VecStep, KEYCXX | KEYOPENCLC | KEYOPENCLCXX | KEYALTIVEC | KEYZVECTOR)
+
+// OpenCL 2.0
+KEYWORD(queue_t                     , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(clk_event_t                 , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(reserve_id_t                , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
+KEYWORD(pipe                        , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
 // C++ for OpenCL s2.3.1: addrspace_cast operator
-KEYWORD(addrspace_cast              , KEYOPENCLCXX)
+KEYWORD(addrspace_cast              , KEYCXX | KEYOPENCLC | KEYOPENCLCXX)
 
 // OpenMP Type Traits
 UNARY_EXPR_OR_TYPE_TRAIT(__builtin_omp_required_simd_align, OpenMPRequiredSimdAlign, KEYALL)
diff --git a/clang/include/clang/CodeGen/BackendUtil.h b/clang/include/clang/CodeGen/BackendUtil.h
index 77d500079f01..b97ce4a380a1 100644
--- a/clang/include/clang/CodeGen/BackendUtil.h
+++ b/clang/include/clang/CodeGen/BackendUtil.h
@@ -30,6 +30,11 @@ namespace clang {
   enum BackendAction {
     Backend_EmitAssembly,  ///< Emit native assembly files
     Backend_EmitBC,        ///< Emit LLVM bitcode files
+    Backend_EmitBC32,      ///< Emit LLVM 3.2 bitcode files
+    Backend_EmitBC50,      ///< Emit LLVM 5.0 bitcode files
+    Backend_EmitSPIRV,     ///< Emit SPIR-V bitcode files
+    Backend_EmitSPIRVContainer,    ///< Emit container with SPIR-V bitcode files
+    Backend_EmitMetalLib,          ///< Emit Metal Library
     Backend_EmitLL,        ///< Emit human-readable LLVM assembly
     Backend_EmitNothing,   ///< Don't emit anything (benchmarking mode)
     Backend_EmitMCNull,    ///< Run CodeGen, but don't emit anything
diff --git a/clang/include/clang/CodeGen/CodeGenAction.h b/clang/include/clang/CodeGen/CodeGenAction.h
index b5721344046d..3cd849714950 100644
--- a/clang/include/clang/CodeGen/CodeGenAction.h
+++ b/clang/include/clang/CodeGen/CodeGenAction.h
@@ -95,6 +95,36 @@ public:
   EmitBCAction(llvm::LLVMContext *_VMContext = nullptr);
 };
 
+class EmitBC32Action : public CodeGenAction {
+  virtual void anchor();
+public:
+  EmitBC32Action(llvm::LLVMContext *_VMContext = nullptr);
+};
+
+class EmitBC50Action : public CodeGenAction {
+  virtual void anchor();
+public:
+  EmitBC50Action(llvm::LLVMContext *_VMContext = nullptr);
+};
+
+class EmitSPIRVAction : public CodeGenAction {
+  virtual void anchor();
+public:
+  EmitSPIRVAction(llvm::LLVMContext *_VMContext = nullptr);
+};
+
+class EmitSPIRVContainerAction : public CodeGenAction {
+  virtual void anchor();
+public:
+  EmitSPIRVContainerAction(llvm::LLVMContext *_VMContext = nullptr);
+};
+
+class EmitMetalLibAction : public CodeGenAction {
+  virtual void anchor();
+public:
+  EmitMetalLibAction(llvm::LLVMContext *_VMContext = nullptr);
+};
+
 class EmitLLVMAction : public CodeGenAction {
   virtual void anchor();
 public:
diff --git a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td
index 0d3c053e89ae..348108e6dc76 100644
--- a/clang/include/clang/Driver/Options.td
+++ b/clang/include/clang/Driver/Options.td
@@ -1506,7 +1506,7 @@ defm jump_tables : BoolFOption<"jump-tables",
   NegFlag<SetTrue, [CC1Option], "Do not use">, PosFlag<SetFalse, [], "Use">,
   BothFlags<[], " jump tables for lowering switches">>;
 defm force_enable_int128 : BoolFOption<"force-enable-int128",
-  TargetOpts<"ForceEnableInt128">, DefaultFalse,
+  TargetOpts<"ForceEnableInt128">, DefaultTrue,
   PosFlag<SetTrue, [CC1Option], "Enable">, NegFlag<SetFalse, [], "Disable">,
   BothFlags<[], " support for int128_t type">>;
 defm keep_static_consts : BoolFOption<"keep-static-consts",
@@ -5469,6 +5469,16 @@ def emit_pch : Flag<["-"], "emit-pch">,
   HelpText<"Generate pre-compiled header file">;
 def emit_llvm_bc : Flag<["-"], "emit-llvm-bc">,
   HelpText<"Build ASTs then convert to LLVM, emit .bc file">;
+def emit_llvm_bc_32 : Flag<["-"], "llvm-bc-32">,
+  HelpText<"Output LLVM 3.2 compatible bitcode">;
+def emit_llvm_bc_50 : Flag<["-"], "llvm-bc-50">,
+  HelpText<"Output LLVM 5.0 compatible bitcode">;
+def emit_spirv : Flag<["-"], "emit-spirv">,
+  HelpText<"Build ASTs then convert to LLVM, then convert to SPIR-V, emit .spv file">;
+def emit_spirv_container : Flag<["-"], "emit-spirv-container">,
+  HelpText<"Build ASTs then convert to LLVM, then convert to multiple SPIR-V modules, emit .spvc file">;
+def emit_metallib : Flag<["-"], "emit-metallib">,
+  HelpText<"Builds Metal/AIR bitcode and puts it in a .metallib file">;
 def emit_llvm_only : Flag<["-"], "emit-llvm-only">,
   HelpText<"Build ASTs and convert to LLVM, discarding output">;
 def emit_codegen_only : Flag<["-"], "emit-codegen-only">,
@@ -5831,6 +5841,15 @@ def disable_pragma_debug_crash : Flag<["-"], "disable-pragma-debug-crash">,
   HelpText<"Disable any #pragma clang __debug that can lead to crashing behavior. This is meant for testing.">,
   MarshallingInfoFlag<PreprocessorOpts<"DisablePragmaDebugCrash">>;
 
+//===----------------------------------------------------------------------===//
+// libfloor Options (applying to OpenCL, CUDA, Metal and Vulkan)
+//===----------------------------------------------------------------------===//
+
+def floor_function_info : Joined<["-"], "floor-function-info=">,
+  HelpText<"floor function info output file">;
+def floor_image_capabilities : Joined<["-"], "floor-image-capabilities=">,
+  HelpText<"image read and write capabilities">;
+
 //===----------------------------------------------------------------------===//
 // OpenCL Options
 //===----------------------------------------------------------------------===//
@@ -5839,6 +5858,16 @@ def cl_ext_EQ : CommaJoined<["-"], "cl-ext=">,
   HelpText<"OpenCL only. Enable or disable OpenCL extensions. The argument is a comma-separated sequence of one or more extension names, each prefixed by '+' or '-'.">,
   MarshallingInfoStringVector<TargetOpts<"OpenCLExtensionsAsWritten">>;
 
+// SPIR generator options
+def cl_spir_compile_options : Separate<["-"], "cl-spir-compile-options">,
+  HelpText<"SPIR compilation options to record in metadata">;
+def cl_sampler_type : Separate<["-"], "cl-sampler-type">,
+  HelpText<"OpenCL only. Specify type of sampler to emit. Valid values: \"opaque\"(default), \"i32\"">;
+def cl_verify_spir : Flag<["-"], "cl-verify-spir">,
+  HelpText<"OpenCL/SPIR only. Runs the Khronos SPIR verifier on the final LLVM IR.">;
+def cl_spir_intel_workarounds : Flag<["-"], "cl-spir-intel-workarounds">,
+  HelpText<"Enable Intel SPIR specific fixes and workarounds">;
+
 //===----------------------------------------------------------------------===//
 // CUDA Options
 //===----------------------------------------------------------------------===//
@@ -5856,6 +5885,41 @@ def fno_cuda_host_device_constexpr : Flag<["-"], "fno-cuda-host-device-constexpr
   HelpText<"Don't treat unattributed constexpr functions as __host__ __device__.">,
   MarshallingInfoNegativeFlag<LangOpts<"CUDAHostDeviceConstexpr">>;
 
+//===----------------------------------------------------------------------===//
+// AIR/Metal Options
+//===----------------------------------------------------------------------===//
+
+def metal_intel_workarounds : Flag<["-"], "metal-intel-workarounds">,
+  HelpText<"Enable Intel GPU specific fixes and workarounds">;
+def metal_nvidia_workarounds : Flag<["-"], "metal-nvidia-workarounds">,
+  HelpText<"Enable Nvidia GPU specific fixes and workarounds">;
+def metal_no_array_image : Flag<["-"], "metal-no-array-image">,
+  HelpText<"Disables native support for array of images">;
+def metal_soft_printf : Flag<["-"], "metal-soft-printf">,
+  HelpText<"Enables support of software printf emulation">;
+
+//===----------------------------------------------------------------------===//
+// Vulkan Options
+//===----------------------------------------------------------------------===//
+
+def vulkan_iub_size_EQ : Joined<["-"], "vulkan-iub-size=">,
+  HelpText<"Sets the Vulkan inline uniform block size">;
+def vulkan_iub_count_EQ : Joined<["-"], "vulkan-iub-count=">,
+  HelpText<"Sets the Vulkan inline uniform block count">;
+def vulkan_llvm_pre_structurization_pass : Flag<["-"], "vulkan-llvm-pre-structurization-pass">,
+  HelpText<"Enable LLVM CFG structurization pass prior to actual structurization">;
+def vulkan_soft_printf : Flag<["-"], "vulkan-soft-printf">,
+  HelpText<"Enables support of software printf emulation">;
+
+//===----------------------------------------------------------------------===//
+// Graphics Options
+//===----------------------------------------------------------------------===//
+
+def graphics_primitive_id : Flag<["-"], "graphics-primitive-id">,
+  HelpText<"Enables support for primitive id query in fragment shaders">;
+def graphics_barycentric_coord : Flag<["-"], "graphics-barycentric-coord">,
+  HelpText<"Enables support for barycentric coordinate query in fragment shaders">;
+
 //===----------------------------------------------------------------------===//
 // OpenMP Options
 //===----------------------------------------------------------------------===//
diff --git a/clang/include/clang/Driver/Types.def b/clang/include/clang/Driver/Types.def
index 997eea445c22..b8c6ea3a0d01 100644
--- a/clang/include/clang/Driver/Types.def
+++ b/clang/include/clang/Driver/Types.def
@@ -39,6 +39,8 @@ TYPE("cpp-output",               PP_C,         INVALID,         "i",      phases
 TYPE("c",                        C,            PP_C,            "c",      phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("cl",                       CL,           PP_C,            "cl",     phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("clcpp",                    CLCXX,        PP_CXX,          "clcpp",  phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("metal",                    METAL,        PP_CXX,          "cpp",    phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("vulkan",                   VULKAN,       PP_CXX,          "cpp",    phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("cuda-cpp-output",          PP_CUDA,      INVALID,         "cui",    phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("cuda",                     CUDA,         PP_CUDA,         "cu",     phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("cuda",                     CUDA_DEVICE,  PP_CUDA,         "cu",     phases::Preprocess, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
@@ -58,11 +60,15 @@ TYPE("renderscript",             RenderScript, PP_C,            "rs",     phases
 // C family input files to precompile.
 TYPE("c-header-cpp-output",      PP_CHeader,   INVALID,         "i",      phases::Precompile)
 TYPE("c-header",                 CHeader,      PP_CHeader,      "h",      phases::Preprocess, phases::Precompile)
-TYPE("cl-header",                CLHeader,     PP_CHeader,      "h",      phases::Preprocess, phases::Precompile)
+TYPE("cl-header",                CLHeader,     PP_CXXHeader,    "h",      phases::Preprocess, phases::Precompile)
+TYPE("cuda-header",              CUDAHeader,   PP_CUDAHeader,   "hpp",    phases::Preprocess, phases::Precompile)
+TYPE("metal-header",             MetalHeader,  PP_CXXHeader,    "hpp",    phases::Preprocess, phases::Precompile)
+TYPE("vulkan-header",            VulkanHeader, PP_CXXHeader,    "hpp",    phases::Preprocess, phases::Precompile)
 TYPE("objective-c-header-cpp-output", PP_ObjCHeader, INVALID,   "mi",     phases::Precompile)
 TYPE("objective-c-header",       ObjCHeader,   PP_ObjCHeader,   "h",      phases::Preprocess, phases::Precompile)
 TYPE("c++-header-cpp-output",    PP_CXXHeader, INVALID,         "ii",     phases::Precompile)
-TYPE("c++-header",               CXXHeader,    PP_CXXHeader,    "hh",     phases::Preprocess, phases::Precompile)
+TYPE("cuda-header-cpp-output",   PP_CUDAHeader, PP_CXXHeader,   "ii",     phases::Precompile)
+TYPE("c++-header",               CXXHeader,    PP_CXXHeader,    "hpp",    phases::Preprocess, phases::Precompile)
 TYPE("objective-c++-header-cpp-output", PP_ObjCXXHeader, INVALID, "mii",  phases::Precompile)
 TYPE("objective-c++-header",     ObjCXXHeader, PP_ObjCXXHeader, "h",      phases::Preprocess, phases::Precompile)
 TYPE("c++-module",               CXXModule,    PP_CXXModule,    "cppm",   phases::Preprocess, phases::Precompile, phases::Compile, phases::Backend, phases::Assemble, phases::Link)
@@ -80,6 +86,8 @@ TYPE("java",                     Java,         INVALID,         nullptr,  phases
 // outputs should use the standard suffixes.
 TYPE("ir",                       LLVM_IR,      INVALID,         "ll",     phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("ir",                       LLVM_BC,      INVALID,         "bc",     phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("ir",                       LLVM_BC_50,   INVALID,         "bc",     phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("ir",                       LLVM_BC_32,   INVALID,         "bc",     phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("lto-ir",                   LTO_IR,       INVALID,         "s",      phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("lto-bc",                   LTO_BC,       INVALID,         "o",      phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 
@@ -100,4 +108,7 @@ TYPE("dSYM",                     dSYM,         INVALID,         "dSYM",   phases
 TYPE("dependencies",             Dependencies, INVALID,         "d",      phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("cuda-fatbin",              CUDA_FATBIN,  INVALID,         "fatbin", phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("hip-fatbin",               HIP_FATBIN,   INVALID,         "hipfb",  phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("spirv",                    SPIRV,        INVALID,         "spv",    phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("spirvc",                   SPIRVC,       INVALID,         "spvc",   phases::Compile, phases::Backend, phases::Assemble, phases::Link)
+TYPE("metallib",                 METALLIB,     INVALID,         "metallib", phases::Compile, phases::Backend, phases::Assemble, phases::Link)
 TYPE("none",                     Nothing,      INVALID,         nullptr,  phases::Compile, phases::Backend, phases::Assemble, phases::Link)
diff --git a/clang/include/clang/Frontend/FrontendOptions.h b/clang/include/clang/Frontend/FrontendOptions.h
index 41ea45ca0b10..587b79187331 100644
--- a/clang/include/clang/Frontend/FrontendOptions.h
+++ b/clang/include/clang/Frontend/FrontendOptions.h
@@ -60,6 +60,21 @@ enum ActionKind {
   /// Emit a .bc file.
   EmitBC,
 
+  /// Emit a LLVM 3.2 .bc file.
+  EmitBC32,
+
+  /// Emit a LLVM 5.0 .bc file.
+  EmitBC50,
+
+  /// Emit a .spv file.
+  EmitSPIRV,
+
+  /// Emit a .spvc container file.
+  EmitSPIRVContainer,
+
+  /// Emit a .metallib file.
+  EmitMetalLib,
+
   /// Translate input source into HTML.
   EmitHTML,
 
diff --git a/clang/include/clang/Parse/Parser.h b/clang/include/clang/Parse/Parser.h
index 92a703b42173..6aa4dc12870b 100644
--- a/clang/include/clang/Parse/Parser.h
+++ b/clang/include/clang/Parse/Parser.h
@@ -2871,8 +2871,6 @@ private:
   SourceLocation SkipExtendedMicrosoftTypeAttributes();
   void ParseMicrosoftInheritanceClassAttributes(ParsedAttributes &attrs);
   void ParseBorlandTypeAttributes(ParsedAttributes &attrs);
-  void ParseOpenCLKernelAttributes(ParsedAttributes &attrs);
-  void ParseOpenCLQualifiers(ParsedAttributes &Attrs);
   void ParseNullabilityTypeSpecifiers(ParsedAttributes &attrs);
 
   VersionTuple ParseVersionTuple(SourceRange &Range);
diff --git a/clang/include/clang/Sema/DeclSpec.h b/clang/include/clang/Sema/DeclSpec.h
index ed5be2da3acd..24fdeacb6a3f 100644
--- a/clang/include/clang/Sema/DeclSpec.h
+++ b/clang/include/clang/Sema/DeclSpec.h
@@ -298,6 +298,11 @@ public:
 #define GENERIC_IMAGE_TYPE(ImgType, Id) \
   static const TST TST_##ImgType##_t = clang::TST_##ImgType##_t;
 #include "clang/Basic/OpenCLImageTypes.def"
+  static const TST TST_sampler_t = clang::TST_sampler_t;
+  static const TST TST_event_t = clang::TST_event_t;
+  static const TST TST_queue_t = clang::TST_queue_t;
+  static const TST TST_clk_event_t = clang::TST_clk_event_t;
+  static const TST TST_reserve_id_t = clang::TST_reserve_id_t;
   static const TST TST_error = clang::TST_error;
 
   // type-qualifiers
diff --git a/clang/include/clang/Sema/Initialization.h b/clang/include/clang/Sema/Initialization.h
index 8c1856f20827..7d712a5cd5da 100644
--- a/clang/include/clang/Sema/Initialization.h
+++ b/clang/include/clang/Sema/Initialization.h
@@ -924,7 +924,13 @@ public:
     SK_OCLSamplerInit,
 
     /// Initialize an opaque OpenCL type (event_t, queue_t, etc.) with zero
-    SK_OCLZeroOpaqueType
+    SK_OCLZeroOpaqueType,
+
+    /// Initialize queue_t from 0.
+    SK_OCLZeroQueue,
+
+    /// Passing zero to a function where OpenCL event_t is expected.
+    SK_OCLZeroEvent
   };
 
   /// A single step in the initialization sequence.
@@ -1358,6 +1364,9 @@ public:
   /// from a zero constant.
   void AddOCLZeroOpaqueTypeStep(QualType T);
 
+  /// Add a step to initialize an OpenCL queue_t from 0.
+  void AddOCLZeroQueueStep(QualType T);
+
   /// Add steps to unwrap a initializer list for a reference around a
   /// single element and rewrap it at the end.
   void RewrapReferenceInitList(QualType T, InitListExpr *Syntactic);
diff --git a/clang/include/clang/Sema/Overload.h b/clang/include/clang/Sema/Overload.h
index 7898a58c2796..63ef6b5f6186 100644
--- a/clang/include/clang/Sema/Overload.h
+++ b/clang/include/clang/Sema/Overload.h
@@ -184,6 +184,9 @@ class Sema;
     /// Zero constant to queue
     ICK_Zero_Queue_Conversion,
 
+    /// Integer constant to OpenCL sampler
+    ICK_Int_Sampler_Conversion,
+
     /// Conversions allowed in C, but not C++
     ICK_C_Only_Conversion,
 
@@ -767,6 +770,8 @@ class Sema;
     /// (CUDA) This candidate was not viable because the callee
     /// was not accessible from the caller's target (i.e. host->device,
     /// global->host, device->host).
+    /// (OpenCL) This candidate was not viable because the callee
+    /// uses extensions that are not enabled or supported.
     ovl_fail_bad_target,
 
     /// This candidate function was not viable because an enable_if
diff --git a/clang/include/clang/Sema/ParsedAttr.h b/clang/include/clang/Sema/ParsedAttr.h
index 52b2c0d963fc..f502e1e4f6e6 100644
--- a/clang/include/clang/Sema/ParsedAttr.h
+++ b/clang/include/clang/Sema/ParsedAttr.h
@@ -633,19 +633,15 @@ public:
   /// in LangAS, otherwise returns default address space.
   LangAS asOpenCLLangAS() const {
     switch (getParsedKind()) {
-    case ParsedAttr::AT_OpenCLConstantAddressSpace:
+    case ParsedAttr::AT_ConstantAddressSpace:
       return LangAS::opencl_constant;
-    case ParsedAttr::AT_OpenCLGlobalAddressSpace:
+    case ParsedAttr::AT_GlobalAddressSpace:
       return LangAS::opencl_global;
-    case ParsedAttr::AT_OpenCLGlobalDeviceAddressSpace:
-      return LangAS::opencl_global_device;
-    case ParsedAttr::AT_OpenCLGlobalHostAddressSpace:
-      return LangAS::opencl_global_host;
-    case ParsedAttr::AT_OpenCLLocalAddressSpace:
+    case ParsedAttr::AT_LocalAddressSpace:
       return LangAS::opencl_local;
-    case ParsedAttr::AT_OpenCLPrivateAddressSpace:
+    case ParsedAttr::AT_PrivateAddressSpace:
       return LangAS::opencl_private;
-    case ParsedAttr::AT_OpenCLGenericAddressSpace:
+    case ParsedAttr::AT_GenericAddressSpace:
       return LangAS::opencl_generic;
     default:
       return LangAS::Default;
@@ -656,17 +652,13 @@ public:
   /// representation in LangAS, otherwise returns default address space.
   LangAS asSYCLLangAS() const {
     switch (getKind()) {
-    case ParsedAttr::AT_OpenCLGlobalAddressSpace:
+    case ParsedAttr::AT_GlobalAddressSpace:
       return LangAS::sycl_global;
-    case ParsedAttr::AT_OpenCLGlobalDeviceAddressSpace:
-      return LangAS::sycl_global_device;
-    case ParsedAttr::AT_OpenCLGlobalHostAddressSpace:
-      return LangAS::sycl_global_host;
-    case ParsedAttr::AT_OpenCLLocalAddressSpace:
+    case ParsedAttr::AT_LocalAddressSpace:
       return LangAS::sycl_local;
-    case ParsedAttr::AT_OpenCLPrivateAddressSpace:
+    case ParsedAttr::AT_PrivateAddressSpace:
       return LangAS::sycl_private;
-    case ParsedAttr::AT_OpenCLGenericAddressSpace:
+    case ParsedAttr::AT_GenericAddressSpace:
     default:
       return LangAS::Default;
     }
diff --git a/clang/include/clang/Sema/Sema.h b/clang/include/clang/Sema/Sema.h
index 11e157bc7d73..094e52b470e6 100644
--- a/clang/include/clang/Sema/Sema.h
+++ b/clang/include/clang/Sema/Sema.h
@@ -403,7 +403,7 @@ public:
   typedef OpaquePtr<TemplateName> TemplateTy;
   typedef OpaquePtr<QualType> TypeTy;
 
-  OpenCLOptions OpenCLFeatures;
+  OpenCLOptions& OpenCLFeatures;
   FPOptions CurFPFeatures;
 
   const LangOptions &LangOpts;
@@ -10372,6 +10372,14 @@ private:
   SmallVector<AssumptionAttr *, 4> OMPAssumeGlobal;
 
 public:
+  /// Adds a color(location) attribute to a particular declaration.
+  void AddGraphicsFBOColorLocationAttr(SourceRange AttrRange, Decl *D, Expr *E,
+									   const AttributeCommonInfo &CI);
+
+  /// Adds a floor::image_flags(flags) attribute to a particular declaration.
+  void AddFloorImageFlagsAttr(SourceRange AttrRange, Decl *D, Expr *E,
+                              const AttributeCommonInfo &CI);
+
   /// The declarator \p D defines a function in the scope \p S which is nested
   /// in an `omp begin/end declare variant` scope. In this method we create a
   /// declaration for \p D and rename \p D according to the OpenMP context
diff --git a/clang/include/clang/Sema/SemaInternal.h b/clang/include/clang/Sema/SemaInternal.h
index 842eec099540..a3aefd6d1939 100644
--- a/clang/include/clang/Sema/SemaInternal.h
+++ b/clang/include/clang/Sema/SemaInternal.h
@@ -46,7 +46,7 @@ inline bool DeclAttrsMatchCUDAMode(const LangOptions &LangOpts, Decl *D) {
     return true;
   bool isDeviceSideDecl = D->hasAttr<CUDADeviceAttr>() ||
                           D->hasAttr<CUDASharedAttr>() ||
-                          D->hasAttr<CUDAGlobalAttr>();
+                          D->hasAttr<ComputeKernelAttr>();
   return isDeviceSideDecl == LangOpts.CUDAIsDevice;
 }
 
diff --git a/clang/lib/AST/ASTContext.cpp b/clang/lib/AST/ASTContext.cpp
index 2611295d21a8..8d49ad30eb63 100644
--- a/clang/lib/AST/ASTContext.cpp
+++ b/clang/lib/AST/ASTContext.cpp
@@ -937,8 +937,9 @@ static const LangASMap *getAddressSpaceMap(const TargetInfo &T,
         2,  // opencl_constant
         0,  // opencl_private
         4,  // opencl_generic
-        5,  // opencl_global_device
-        6,  // opencl_global_host
+        100,  // opencl_global_device
+        101,  // opencl_global_host
+        0,  // vulkan_input
         7,  // cuda_device
         8,  // cuda_constant
         9,  // cuda_shared
@@ -2176,7 +2177,18 @@ TypeInfo ASTContext::getTypeInfoImpl(const Type *T) const {
       Width = Target->getPointerWidth(0);
       Align = Target->getPointerAlign(0);
       break;
-    case BuiltinType::OCLSampler:
+    case BuiltinType::OCLSampler: {
+      // NOTE: samplers are modeled as integers for now.
+      Width = Target->getIntWidth();
+      Align = Target->getIntAlign();
+#if 0 // -> use this when treating samplers as pointers
+      AS = getTargetAddressSpace(
+          Target->getOpenCLTypeAddrSpace(getOpenCLTypeKind(T)));
+      Width = Target->getPointerWidth(AS);
+      Align = Target->getPointerAlign(AS);
+#endif
+      break;
+    }
     case BuiltinType::OCLEvent:
     case BuiltinType::OCLClkEvent:
     case BuiltinType::OCLQueue:
@@ -3012,8 +3024,14 @@ QualType ASTContext::getAddrSpaceQualType(QualType T,
 
   // If this type already has an address space specified, it cannot get
   // another one.
-  assert(!Quals.hasAddressSpace() &&
-         "Type cannot be in multiple addr spaces!");
+  if (Quals.hasAddressSpace()) {
+    if (Quals.getAddressSpace() == LangAS::opencl_private) {
+      Quals.removeAddressSpace(); // private can be replaced
+    } else {
+      assert(!Quals.hasAddressSpace() &&
+             "Type cannot be in multiple addr spaces!");
+    }
+  }
   Quals.addAddressSpace(AddressSpace);
 
   return getExtQualType(TypeNode, Quals);
@@ -9612,13 +9630,30 @@ QualType ASTContext::mergeFunctionTypes(QualType lhs, QualType rhs,
     assert((AllowCXX ||
             (!lproto->hasExceptionSpec() && !rproto->hasExceptionSpec())) &&
            "C++ shouldn't be here");
-    // Compatible functions must have the same number of parameters
-    if (lproto->getNumParams() != rproto->getNumParams())
-      return {};
 
-    // Variadic and non-variadic functions aren't compatible
-    if (lproto->isVariadic() != rproto->isVariadic())
-      return {};
+    auto lproto_nargs = lproto->getNumParams();
+    auto rproto_nargs = rproto->getNumParams();
+
+    if (LangOpts.OpenCLVersion < 200 || !lproto->isVariadic()) {
+      // Compatible functions must have the same number of parameters
+      if (lproto_nargs != rproto_nargs)
+        return {};
+
+      // Variadic and non-variadic functions aren't compatible
+      if (lproto->isVariadic() != rproto->isVariadic())
+        return {};
+    } else {
+      if (!lproto->isVariadic() && !lproto->isVariadic()) {
+        if (lproto_nargs != rproto_nargs)
+          return QualType();
+      } else if (lproto->isVariadic()) {
+        if (lproto_nargs > rproto_nargs)
+          return QualType();
+      } else if (rproto->isVariadic()) {
+        if (lproto_nargs < rproto_nargs)
+          return QualType();
+      }
+    }
 
     if (lproto->getMethodQuals() != rproto->getMethodQuals())
       return {};
@@ -10826,7 +10861,7 @@ static GVALinkage basicGVALinkageForFunction(const ASTContext &Context,
   if (!FD->isInlined())
     return External;
 
-  if ((!Context.getLangOpts().CPlusPlus &&
+  if ((!Context.getLangOpts().CPlusPlus && !Context.getLangOpts().OpenCL &&
        !Context.getTargetInfo().getCXXABI().isMicrosoft() &&
        !FD->hasAttr<DLLExportAttr>()) ||
       FD->hasAttr<GNUInlineAttr>()) {
@@ -10863,7 +10898,7 @@ static GVALinkage adjustGVALinkageForAttributes(const ASTContext &Context,
   } else if (Context.getLangOpts().CUDA && Context.getLangOpts().CUDAIsDevice) {
     // Device-side functions with __global__ attribute must always be
     // visible externally so they can be launched from host.
-    if (D->hasAttr<CUDAGlobalAttr>() &&
+    if (D->hasAttr<ComputeKernelAttr>() &&
         (L == GVA_DiscardableODR || L == GVA_Internal))
       return GVA_StrongODR;
     // Single source offloading languages like CUDA/HIP need to be able to
diff --git a/clang/lib/AST/Decl.cpp b/clang/lib/AST/Decl.cpp
index 32dae5ccad3a..ed5713a982e4 100644
--- a/clang/lib/AST/Decl.cpp
+++ b/clang/lib/AST/Decl.cpp
@@ -2347,10 +2347,12 @@ void VarDecl::setInit(Expr *I) {
 bool VarDecl::mightBeUsableInConstantExpressions(const ASTContext &C) const {
   const LangOptions &Lang = C.getLangOpts();
 
+#if 0 // we don't want this
   // OpenCL permits const integral variables to be used in constant
   // expressions, like in C++98.
   if (!Lang.CPlusPlus && !Lang.OpenCL)
     return false;
+#endif
 
   // Function parameters are never usable in constant expressions.
   if (isa<ParmVarDecl>(this))
@@ -3224,7 +3226,7 @@ bool FunctionDecl::isExternC() const {
 }
 
 bool FunctionDecl::isInExternCContext() const {
-  if (hasAttr<OpenCLKernelAttr>())
+  if (hasAttr<ComputeKernelAttr>())
     return true;
   return getLexicalDeclContext()->isExternCContext();
 }
diff --git a/clang/lib/AST/Expr.cpp b/clang/lib/AST/Expr.cpp
index 415b6e52b564..10eadbbf3e4b 100644
--- a/clang/lib/AST/Expr.cpp
+++ b/clang/lib/AST/Expr.cpp
@@ -636,7 +636,7 @@ std::string PredefinedExpr::ComputeName(IdentKind IK, const Decl *CurrentDecl) {
           GD = GlobalDecl(CD, Ctor_Base);
         else if (const CXXDestructorDecl *DD = dyn_cast<CXXDestructorDecl>(ND))
           GD = GlobalDecl(DD, Dtor_Base);
-        else if (ND->hasAttr<CUDAGlobalAttr>())
+        else if (ND->hasAttr<ComputeKernelAttr>())
           GD = GlobalDecl(cast<FunctionDecl>(ND));
         else
           GD = GlobalDecl(ND);
@@ -1841,6 +1841,8 @@ bool CastExpr::CastConsistency() const {
   case CK_ARCReclaimReturnedObject:
   case CK_ARCExtendBlockObject:
   case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
   case CK_IntToOCLSampler:
   case CK_FloatingToFixedPoint:
   case CK_FixedPointToFloating:
diff --git a/clang/lib/AST/ExprConstant.cpp b/clang/lib/AST/ExprConstant.cpp
index fa0d22064f0e..2c11247be0f9 100644
--- a/clang/lib/AST/ExprConstant.cpp
+++ b/clang/lib/AST/ExprConstant.cpp
@@ -10646,7 +10646,8 @@ public:
       : ExprEvaluatorBaseTy(info), Result(result) {}
 
   bool Success(const llvm::APSInt &SI, const Expr *E, APValue &Result) {
-    assert(E->getType()->isIntegralOrEnumerationType() &&
+    assert((E->getType()->isIntegralOrEnumerationType() ||
+            E->getType()->isSamplerT()) &&
            "Invalid evaluation result.");
     assert(SI.isSigned() == E->getType()->isSignedIntegerOrEnumerationType() &&
            "Invalid evaluation result.");
@@ -10660,7 +10661,8 @@ public:
   }
 
   bool Success(const llvm::APInt &I, const Expr *E, APValue &Result) {
-    assert(E->getType()->isIntegralOrEnumerationType() &&
+    assert((E->getType()->isIntegralOrEnumerationType() ||
+            E->getType()->isSamplerT()) &&
            "Invalid evaluation result.");
     assert(I.getBitWidth() == Info.Ctx.getIntWidth(E->getType()) &&
            "Invalid evaluation result.");
@@ -10674,7 +10676,8 @@ public:
   }
 
   bool Success(uint64_t Value, const Expr *E, APValue &Result) {
-    assert(E->getType()->isIntegralOrEnumerationType() &&
+    assert((E->getType()->isIntegralOrEnumerationType() ||
+            E->getType()->isSamplerT()) &&
            "Invalid evaluation result.");
     Result = APValue(Info.Ctx.MakeIntValue(Value, E->getType()));
     return true;
@@ -13180,9 +13183,10 @@ bool IntExprEvaluator::VisitCastExpr(const CastExpr *E) {
   case CK_IntegralComplexToFloatingComplex:
   case CK_BuiltinFnToFnPtr:
   case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
   case CK_NonAtomicToAtomic:
   case CK_AddressSpaceConversion:
-  case CK_IntToOCLSampler:
   case CK_FloatingToFixedPoint:
   case CK_FixedPointToFloating:
   case CK_FixedPointCast:
@@ -13190,6 +13194,14 @@ bool IntExprEvaluator::VisitCastExpr(const CastExpr *E) {
   case CK_MatrixCast:
     llvm_unreachable("invalid cast kind for integral value");
 
+  case CK_IntToOCLSampler: {
+    Expr::EvalResult ExprResult;
+    if(!SubExpr->EvaluateAsInt(ExprResult, Info.Ctx)) {
+      return false;
+    }
+    return Success(ExprResult.Val.getInt(), E);
+  }
+
   case CK_BitCast:
   case CK_Dependent:
   case CK_LValueBitCast:
@@ -13627,10 +13639,12 @@ bool FloatExprEvaluator::VisitCallExpr(const CallExpr *E) {
   case Builtin::BI__builtin_huge_valf:
   case Builtin::BI__builtin_huge_vall:
   case Builtin::BI__builtin_huge_valf128:
+  case Builtin::BI__builtin_huge_valh:
   case Builtin::BI__builtin_inf:
   case Builtin::BI__builtin_inff:
   case Builtin::BI__builtin_infl:
-  case Builtin::BI__builtin_inff128: {
+  case Builtin::BI__builtin_inff128:
+  case Builtin::BI__builtin_infh: {
     const llvm::fltSemantics &Sem =
       Info.Ctx.getFloatTypeSemantics(E->getType());
     Result = llvm::APFloat::getInf(Sem);
@@ -13641,6 +13655,7 @@ bool FloatExprEvaluator::VisitCallExpr(const CallExpr *E) {
   case Builtin::BI__builtin_nansf:
   case Builtin::BI__builtin_nansl:
   case Builtin::BI__builtin_nansf128:
+  case Builtin::BI__builtin_nansh:
     if (!TryEvaluateBuiltinNaN(Info.Ctx, E->getType(), E->getArg(0),
                                true, Result))
       return Error(E);
@@ -13650,6 +13665,7 @@ bool FloatExprEvaluator::VisitCallExpr(const CallExpr *E) {
   case Builtin::BI__builtin_nanf:
   case Builtin::BI__builtin_nanl:
   case Builtin::BI__builtin_nanf128:
+  case Builtin::BI__builtin_nanh:
     // If this is __builtin_nan() turn this into a nan, otherwise we
     // can't constant fold it.
     if (!TryEvaluateBuiltinNaN(Info.Ctx, E->getType(), E->getArg(0),
@@ -13922,6 +13938,8 @@ bool ComplexExprEvaluator::VisitCastExpr(const CastExpr *E) {
   case CK_CopyAndAutoreleaseBlockObject:
   case CK_BuiltinFnToFnPtr:
   case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
   case CK_NonAtomicToAtomic:
   case CK_AddressSpaceConversion:
   case CK_IntToOCLSampler:
@@ -14564,6 +14582,9 @@ static bool Evaluate(APValue &Result, EvalInfo &Info, const Expr *E) {
       if (!EvaluateAtomic(E, nullptr, Result, Info))
         return false;
     }
+  } else if (T->isSamplerT()) {
+    if (!IntExprEvaluator(Info, Result).Visit(E))
+      return false;
   } else if (Info.getLangOpts().CPlusPlus11) {
     Info.FFDiag(E, diag::note_constexpr_nonliteral) << E->getType();
     return false;
diff --git a/clang/lib/AST/ItaniumMangle.cpp b/clang/lib/AST/ItaniumMangle.cpp
index 799da1247cd3..38da3891617e 100644
--- a/clang/lib/AST/ItaniumMangle.cpp
+++ b/clang/lib/AST/ItaniumMangle.cpp
@@ -184,6 +184,9 @@ public:
 
   void mangleLambdaSig(const CXXRecordDecl *Lambda, raw_ostream &) override;
 
+  void mangleMetalFieldName(const FieldDecl *D, const CXXRecordDecl* RD, raw_ostream &) override;
+  void mangleMetalGeneric(const std::string& name, QualType Ty, const CXXRecordDecl* RD, raw_ostream &) override;
+
   bool getNextDiscriminator(const NamedDecl *ND, unsigned &disc) {
     // Lambda closure types are already numbered.
     if (isLambda(ND))
@@ -473,6 +476,7 @@ public:
   void mangleType(QualType T);
   void mangleNameOrStandardSubstitution(const NamedDecl *ND);
   void mangleLambdaSig(const CXXRecordDecl *Lambda);
+  void mangleMetalFieldName(const FieldDecl *D, const CXXRecordDecl* RD);
 
 private:
 
@@ -1436,7 +1440,7 @@ void CXXNameMangler::mangleUnqualifiedName(GlobalDecl GD,
                        FD->getType()->castAs<FunctionType>()->getCallConv() ==
                            clang::CC_X86RegCall;
       bool IsDeviceStub =
-          FD && FD->hasAttr<CUDAGlobalAttr>() &&
+          FD && FD->hasAttr<ComputeKernelAttr>() &&
           GD.getKernelReferenceKind() == KernelReferenceKind::Stub;
       if (IsDeviceStub)
         mangleDeviceStubName(II);
@@ -2714,7 +2718,7 @@ static bool isTypeSubstitutable(Qualifiers Quals, const Type *Ty,
   if (Ty->isSpecificBuiltinType(BuiltinType::ObjCSel))
     return true;
   if (Ty->isOpenCLSpecificType())
-    return true;
+    return false;
   if (Ty->isBuiltinType())
     return false;
   // Through to Clang 6.0, we accidentally treated undeduced auto types as
@@ -2793,7 +2797,8 @@ void CXXNameMangler::mangleType(QualType T) {
   const Type *ty = split.Ty;
 
   bool isSubstitutable =
-    isTypeSubstitutable(quals, ty, Context.getASTContext());
+    isTypeSubstitutable(quals, ty, Context.getASTContext()) &&
+    !(Context.getASTContext().getLangOpts().OpenCL && isa<ExtVectorType>(T));
   if (isSubstitutable && mangleSubstitution(T))
     return;
 
@@ -2845,6 +2850,30 @@ void CXXNameMangler::mangleNameOrStandardSubstitution(const NamedDecl *ND) {
     mangleName(ND);
 }
 
+void CXXNameMangler::mangleMetalFieldName(const FieldDecl *D, const CXXRecordDecl* RD) {
+	const DeclContext *DC = IgnoreLinkageSpecDecls(getEffectiveDeclContext(D));
+	const DeclContext *PDC = IgnoreLinkageSpecDecls(getEffectiveDeclContext(RD));
+	
+	if(const auto II = D->getIdentifier()) {
+		// TODO: need actual parent field entry for all nested types
+		if(DC->getParent()->Equals(PDC)) {
+			mangleSourceName(II);
+		}
+	}
+	
+	// top level: mangle directly (without enclosing record decl / PDC)
+	// all else: mangle nested type as well
+	if(!DC->getParent()->Equals(PDC)) {
+		if (GetLocalClassDecl(D)) {
+			mangleLocalName(D, nullptr);
+		}
+		else {
+			mangleNestedName(D, DC, nullptr);
+		}
+	}
+	mangleType(D->getType());
+}
+
 void CXXNameMangler::mangleType(const BuiltinType *T) {
   //  <type>         ::= <builtin-type>
   //  <builtin-type> ::= v  # void
@@ -3024,7 +3053,7 @@ void CXXNameMangler::mangleType(const BuiltinType *T) {
     break;
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
   case BuiltinType::Id: \
-    type_name = "ocl_" #ImgType "_" #Suffix; \
+    type_name = "ocl_" #ImgType; \
     Out << type_name.size() << type_name; \
     break;
 #include "clang/Basic/OpenCLImageTypes.def"
@@ -3094,8 +3123,10 @@ StringRef CXXNameMangler::getCallingConvQualifierName(CallingConv CC) {
   case CC_AAPCS_VFP:
   case CC_AArch64VectorCall:
   case CC_IntelOclBicc:
-  case CC_SpirFunction:
-  case CC_OpenCLKernel:
+  case CC_FloorFunction:
+  case CC_FloorKernel:
+  case CC_FloorVertex:
+  case CC_FloorFragment:
   case CC_PreserveMost:
   case CC_PreserveAll:
     // FIXME: we should be mangling all of the above.
@@ -6420,6 +6451,26 @@ void ItaniumMangleContextImpl::mangleLambdaSig(const CXXRecordDecl *Lambda,
   Mangler.mangleLambdaSig(Lambda);
 }
 
+void ItaniumMangleContextImpl::mangleMetalFieldName(const FieldDecl *D, const CXXRecordDecl* RD, raw_ostream &Out) {
+	assert(isa<FieldDecl>(D) &&
+		   "Invalid mangleName() call, argument is not a field decl!");
+	
+	PrettyStackTraceDecl CrashInfo(D, SourceLocation(),
+								   getASTContext().getSourceManager(),
+								   "Mangling declaration");
+	
+	CXXNameMangler Mangler(*this, Out, D);
+	Mangler.mangleMetalFieldName(D, RD);
+}
+
+void ItaniumMangleContextImpl::mangleMetalGeneric(const std::string& name, QualType Ty,
+												  const CXXRecordDecl* RD, raw_ostream &Out) {
+	CXXNameMangler Mangler(*this, Out, nullptr);
+	Out << name.size();
+	Out << name;
+	Mangler.mangleType(Ty);
+}
+
 ItaniumMangleContext *ItaniumMangleContext::create(ASTContext &Context,
                                                    DiagnosticsEngine &Diags) {
   return new ItaniumMangleContextImpl(
diff --git a/clang/lib/AST/Mangle.cpp b/clang/lib/AST/Mangle.cpp
index 54dbf484f377..a33c93cd99c1 100644
--- a/clang/lib/AST/Mangle.cpp
+++ b/clang/lib/AST/Mangle.cpp
@@ -523,7 +523,7 @@ private:
         GD = GlobalDecl(CtorD, Ctor_Complete);
       else if (const auto *DtorD = dyn_cast<CXXDestructorDecl>(D))
         GD = GlobalDecl(DtorD, Dtor_Complete);
-      else if (D->hasAttr<CUDAGlobalAttr>())
+      else if (D->hasAttr<ComputeKernelAttr>())
         GD = GlobalDecl(cast<FunctionDecl>(D));
       else
         GD = GlobalDecl(D);
diff --git a/clang/lib/AST/MicrosoftMangle.cpp b/clang/lib/AST/MicrosoftMangle.cpp
index 04baa85fe145..1984af679f4b 100644
--- a/clang/lib/AST/MicrosoftMangle.cpp
+++ b/clang/lib/AST/MicrosoftMangle.cpp
@@ -2392,7 +2392,7 @@ void MicrosoftCXXNameMangler::mangleType(const BuiltinType *T, Qualifiers,
 
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
   case BuiltinType::Id: \
-    Out << "PAUocl_" #ImgType "_" #Suffix "@@"; \
+    Out << "PAUocl_" #ImgType #Suffix "@@"; \
     break;
 #include "clang/Basic/OpenCLImageTypes.def"
   case BuiltinType::OCLSampler:
diff --git a/clang/lib/AST/Type.cpp b/clang/lib/AST/Type.cpp
index ead3944284ae..4dbebbc12ed7 100644
--- a/clang/lib/AST/Type.cpp
+++ b/clang/lib/AST/Type.cpp
@@ -2110,6 +2110,13 @@ bool Type::isFloatingType() const {
   return false;
 }
 
+bool Type::isDoubleType() const {
+  if (const BuiltinType *BT = dyn_cast<BuiltinType>(CanonicalType))
+    return BT->getKind() >= BuiltinType::Double &&
+           BT->getKind() <= BuiltinType::LongDouble;
+  return false;
+}
+
 bool Type::hasFloatingRepresentation() const {
   if (const auto *VT = dyn_cast<VectorType>(CanonicalType))
     return VT->getElementType()->isFloatingType();
@@ -2132,6 +2139,30 @@ bool Type::isRealType() const {
   return isExtIntType();
 }
 
+bool Type::isFloatingVecType() const {
+  if (const VectorType *VT = dyn_cast<VectorType>(CanonicalType))
+    return VT->getElementType()->isFloatingType();
+  return false;
+}
+
+bool Type::isDoubleVecType() const {
+  if (const VectorType *VT = dyn_cast<VectorType>(CanonicalType))
+    return VT->getElementType()->isDoubleType();
+  return false;
+}
+
+bool Type::isIntegerVecType() const {
+  if (const VectorType *VT = dyn_cast<VectorType>(CanonicalType))
+    return VT->getElementType()->isIntegerType();
+  return false;
+}
+
+bool Type::isRealVecType() const {
+  if (const VectorType *VT = dyn_cast<VectorType>(CanonicalType))
+    return VT->getElementType()->isRealType();
+  return false;
+}
+
 bool Type::isArithmeticType() const {
   if (const auto *BT = dyn_cast<BuiltinType>(CanonicalType))
     return BT->getKind() >= BuiltinType::Bool &&
@@ -2549,6 +2580,9 @@ bool Type::isLiteralType(const ASTContext &Ctx) const {
   if (isDependentType())
     return false;
 
+  if (Ctx.getLangOpts().OpenCL && isSamplerT())
+    return true;
+
   // C++1y [basic.types]p10:
   //   A type is a literal type if it is:
   //   -- cv void; or
@@ -3069,10 +3103,17 @@ StringRef BuiltinType::getName(const PrintingPolicy &Policy) const {
     return "Class";
   case ObjCSel:
     return "SEL";
+#if 0 // TODO: enable this again when using ro/wo/rw image types
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
   case Id: \
     return "__" #Access " " #ImgType "_t";
 #include "clang/Basic/OpenCLImageTypes.def"
+#else
+#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
+  case Id: \
+    return #ImgType "_t";
+#include "clang/Basic/OpenCLImageTypes.def"
+#endif
   case OCLSampler:
     return "sampler_t";
   case OCLEvent:
@@ -3150,8 +3191,10 @@ StringRef FunctionType::getNameForCallConv(CallingConv CC) {
   case CC_AAPCS_VFP: return "aapcs-vfp";
   case CC_AArch64VectorCall: return "aarch64_vector_pcs";
   case CC_IntelOclBicc: return "intel_ocl_bicc";
-  case CC_SpirFunction: return "spir_function";
-  case CC_OpenCLKernel: return "opencl_kernel";
+  case CC_FloorFunction: return "floor_function";
+  case CC_FloorKernel: return "floor_kernel";
+  case CC_FloorVertex: return "floor_vertex";
+  case CC_FloorFragment: return "floor_fragment";
   case CC_Swift: return "swiftcall";
   case CC_SwiftAsync: return "swiftasynccall";
   case CC_PreserveMost: return "preserve_most";
@@ -3579,6 +3622,9 @@ bool AttributedType::isCallingConv() const {
   case attr::IntelOclBicc:
   case attr::PreserveMost:
   case attr::PreserveAll:
+  case attr::GraphicsVertexShader:
+  case attr::GraphicsFragmentShader:
+  case attr::ComputeKernel:
     return true;
   }
   llvm_unreachable("invalid attr kind");
@@ -4429,3 +4475,112 @@ void AutoType::Profile(llvm::FoldingSetNodeID &ID, const ASTContext &Context,
   for (const TemplateArgument &Arg : Arguments)
     Arg.Profile(ID, Context);
 }
+
+static std::pair<bool, bool> isAggregateImageTypeRecurse(const CXXRecordDecl* decl) {
+	if(decl == nullptr) return { false, false };
+	
+	// union is not allowed
+	if(decl->isUnion()) return { false, false };
+	
+	// must have definition
+	if(!decl->hasDefinition()) return { false, false };
+	
+	// iterate over all fields/members and check if all are image types
+	bool has_any_image = false;
+	for(const auto* field : decl->fields()) {
+		// direct image type or array thereof
+		if (!field->getType()->isImageType() &&
+			!field->getType()->isArrayImageType(false)) {
+			// ignore zero-sized fields (if there are not an image)
+			if (field->isZeroSize(decl->getASTContext())) {
+				continue;
+			}
+			return { false, false };
+		}
+		has_any_image = true;
+	}
+	
+	// iterate over / recurse into all bases, check if they only consist of image types
+	for(const auto& base : decl->bases()) {
+		const auto base_ret = isAggregateImageTypeRecurse(base.getType()->getAsCXXRecordDecl());
+		if(!base_ret.first) {
+			return { false, false };
+		}
+		has_any_image |= base_ret.second;
+	}
+	
+	// all passed
+	return { true, has_any_image };
+}
+
+bool Type::isAggregateImageType() const {
+  // must be struct or class, union is not allowed
+  if(!isStructureOrClassType()) return false;
+
+  // check class/struct itself + all inherited base classes/structs
+  const auto valid_and_has_image = isAggregateImageTypeRecurse(getAsCXXRecordDecl());
+  return valid_and_has_image.first && valid_and_has_image.second;
+}
+
+bool Type::isArrayImageType(bool single_field_arr) const {
+  // only check/match this for C-style arrays of builtin images
+  if(!single_field_arr) {
+    if(isPointerType() && getPointeeType()->isArrayType()) {
+      return getPointeeType()->isArrayImageType(single_field_arr);
+    }
+
+    // simple C-style array that contains an image type
+    if(isArrayType()) {
+      return (getAsArrayTypeUnsafe()->getElementType()->isAggregateImageType() ||
+              getAsArrayTypeUnsafe()->getElementType()->isImageType());
+    }
+  }
+
+  // must be struct or class, union is not allowed
+  if(!isStructureOrClassType()) return false;
+
+  // must be a cxx rdecl
+  const auto decl = getAsCXXRecordDecl();
+  if(!decl) return false;
+
+  // must have definition
+  if(!decl->hasDefinition()) return false;
+
+  // must have at least one field
+  const auto field_count = std::distance(decl->field_begin(), decl->field_end());
+  if(field_count == 0) return false;
+
+  // handle "array of agg images" (single_field_arr == true) and "agg image with array of *images" (false)
+  if(single_field_arr) {
+    if(field_count != 1) return false;
+
+    // field must be an array
+    const QualType arr_field_type = decl->field_begin()->getType();
+    if(!arr_field_type->isArrayType()) return false;
+
+    // element type must be an aggregate image type
+    return arr_field_type->getArrayElementTypeNoTypeQual()->isAggregateImageType();
+  } else {
+    // must have an array field
+    bool has_array = false;
+    QualType arr_field_type;
+    for(const auto* field : decl->fields()) {
+      arr_field_type = field->getType();
+      has_array = arr_field_type->isArrayType();
+      if(has_array) break;
+    }
+    if(!has_array) return false;
+
+    // either the enclosing/parent type or the array field type must be an aggregate image type
+    if (arr_field_type->getArrayElementTypeNoTypeQual()->isAggregateImageType()) {
+      return true;
+    }
+    if (!arr_field_type->isStructureOrClassType()) {
+      return false;
+    }
+
+    // check if the aggregate contains an image type somewhere
+    auto st_arr_field_type = arr_field_type->getAsStructureType();
+    return st_arr_field_type->isAggregateImageType();
+  }
+}
diff --git a/clang/lib/AST/TypePrinter.cpp b/clang/lib/AST/TypePrinter.cpp
index cceed1ca26f3..4d896cc7bfe9 100644
--- a/clang/lib/AST/TypePrinter.cpp
+++ b/clang/lib/AST/TypePrinter.cpp
@@ -970,9 +970,17 @@ void TypePrinter::printFunctionAfter(const FunctionType::ExtInfo &Info,
     case CC_X86RegCall:
       OS << " __attribute__((regcall))";
       break;
-    case CC_SpirFunction:
-    case CC_OpenCLKernel:
-      // Do nothing. These CCs are not available as attributes.
+    case CC_FloorFunction:
+      OS << "floor_function";
+      break;
+    case CC_FloorKernel:
+      OS << "floor_kernel";
+      break;
+    case CC_FloorVertex:
+      OS << "floor_vertex";
+      break;
+    case CC_FloorFragment:
+      OS << "floor_fragment";
       break;
     case CC_Swift:
       OS << " __attribute__((swiftcall))";
@@ -1672,13 +1680,11 @@ void TypePrinter::printAttributedAfter(const AttributedType *T,
 #include "clang/Basic/AttrList.inc"
     llvm_unreachable("non-type attribute attached to type");
 
-  case attr::OpenCLPrivateAddressSpace:
-  case attr::OpenCLGlobalAddressSpace:
-  case attr::OpenCLGlobalDeviceAddressSpace:
-  case attr::OpenCLGlobalHostAddressSpace:
-  case attr::OpenCLLocalAddressSpace:
-  case attr::OpenCLConstantAddressSpace:
-  case attr::OpenCLGenericAddressSpace:
+  case attr::PrivateAddressSpace:
+  case attr::GlobalAddressSpace:
+  case attr::LocalAddressSpace:
+  case attr::ConstantAddressSpace:
+  case attr::GenericAddressSpace:
     // FIXME: Update printAttributedBefore to print these once we generate
     // AttributedType nodes for them.
     break;
@@ -1718,6 +1724,10 @@ void TypePrinter::printAttributedAfter(const AttributedType *T,
   case attr::MSABI: OS << "ms_abi"; break;
   case attr::SysVABI: OS << "sysv_abi"; break;
   case attr::RegCall: OS << "regcall"; break;
+  case attr::ComputeKernel: OS << "floor_kernel"; break;
+  case attr::GraphicsFragmentShader: OS << "floor_fragment"; break;
+  case attr::GraphicsVertexShader: OS << "floor_vertex"; break;
+  case attr::FloorArgBuffer: OS << "floor_arg_buffer"; break;
   case attr::Pcs: {
     OS << "pcs(";
    QualType t = T->getEquivalentType();
@@ -2131,29 +2141,29 @@ std::string Qualifiers::getAddrSpaceAsString(LangAS AS) {
     return "";
   case LangAS::opencl_global:
   case LangAS::sycl_global:
-    return "__global";
+    return "__global_as";
   case LangAS::opencl_local:
   case LangAS::sycl_local:
-    return "__local";
+    return "__local_as";
   case LangAS::opencl_private:
   case LangAS::sycl_private:
-    return "__private";
+    return "__private_as";
   case LangAS::opencl_constant:
-    return "__constant";
+    return "__constant_as";
   case LangAS::opencl_generic:
-    return "__generic";
+    return "__generic_as";
   case LangAS::opencl_global_device:
   case LangAS::sycl_global_device:
-    return "__global_device";
+    return "__global_device_as";
   case LangAS::opencl_global_host:
   case LangAS::sycl_global_host:
-    return "__global_host";
+    return "__global_host_as";
   case LangAS::cuda_device:
-    return "__device__";
+    return "__cuda_device__";
   case LangAS::cuda_constant:
-    return "__constant__";
+    return "__cuda_constant__";
   case LangAS::cuda_shared:
-    return "__shared__";
+    return "__cuda_shared__";
   case LangAS::ptr32_sptr:
     return "__sptr __ptr32";
   case LangAS::ptr32_uptr:
diff --git a/clang/lib/Basic/Builtins.cpp b/clang/lib/Basic/Builtins.cpp
index 2b0f4071662c..a297aee56185 100644
--- a/clang/lib/Basic/Builtins.cpp
+++ b/clang/lib/Basic/Builtins.cpp
@@ -72,7 +72,7 @@ bool Builtin::Context::builtinIsSupported(const Builtin::Info &BuiltinInfo,
   bool OclC1Unsupported = (LangOpts.OpenCLVersion / 100) != 1 &&
                           (BuiltinInfo.Langs & ALL_OCLC_LANGUAGES ) ==  OCLC1X_LANG;
   bool OclC2Unsupported =
-      (LangOpts.getOpenCLCompatibleVersion() != 200) &&
+      (LangOpts.getOpenCLCompatibleVersion() < 200) &&
       (BuiltinInfo.Langs & ALL_OCLC_LANGUAGES) == OCLC20_LANG;
   bool OclCUnsupported = !LangOpts.OpenCL &&
                          (BuiltinInfo.Langs & ALL_OCLC_LANGUAGES);
diff --git a/clang/lib/Basic/Cuda.cpp b/clang/lib/Basic/Cuda.cpp
index f5ee1fb4c228..21e4743d9230 100644
--- a/clang/lib/Basic/Cuda.cpp
+++ b/clang/lib/Basic/Cuda.cpp
@@ -40,6 +40,8 @@ const char *CudaVersionToString(CudaVersion V) {
     return "11.3";
   case CudaVersion::CUDA_114:
     return "11.4";
+  case CudaVersion::CUDA_115:
+    return "11.5";
   case CudaVersion::NEW:
     return "";
   }
@@ -62,6 +64,7 @@ CudaVersion CudaStringToVersion(const llvm::Twine &S) {
       .Case("11.2", CudaVersion::CUDA_112)
       .Case("11.3", CudaVersion::CUDA_113)
       .Case("11.4", CudaVersion::CUDA_114)
+      .Case("11.5", CudaVersion::CUDA_115)
       .Default(CudaVersion::UNKNOWN);
 }
 
@@ -85,9 +88,9 @@ static const CudaArchToStringMap arch_names[] = {
     SM(30), SM(32), SM(35), SM(37),  // Kepler
     SM(50), SM(52), SM(53),          // Maxwell
     SM(60), SM(61), SM(62),          // Pascal
-    SM(70), SM(72),                  // Volta
+    SM(70), SM(72), SM(73),          // Volta
     SM(75),                          // Turing
-    SM(80), SM(86),                  // Ampere
+    SM(80), SM(82), SM(86),          // Ampere
     GFX(600),  // gfx600
     GFX(601),  // gfx601
     GFX(602),  // gfx602
@@ -179,13 +182,18 @@ CudaVersion MinVersionForCudaArch(CudaArch A) {
   case CudaArch::SM_70:
     return CudaVersion::CUDA_90;
   case CudaArch::SM_72:
+  case CudaArch::SM_73:
     return CudaVersion::CUDA_91;
   case CudaArch::SM_75:
     return CudaVersion::CUDA_100;
   case CudaArch::SM_80:
     return CudaVersion::CUDA_110;
+  case CudaArch::SM_82:
   case CudaArch::SM_86:
     return CudaVersion::CUDA_111;
+  case CudaArch::SM_87:
+  case CudaArch::SM_88:
+    return CudaVersion::CUDA_115;
   default:
     llvm_unreachable("invalid enum");
   }
@@ -241,6 +249,8 @@ CudaVersion ToCudaVersion(llvm::VersionTuple Version) {
     return CudaVersion::CUDA_113;
   case 114:
     return CudaVersion::CUDA_114;
+  case 115:
+    return CudaVersion::CUDA_115;
   default:
     return CudaVersion::UNKNOWN;
   }
diff --git a/clang/lib/Basic/TargetInfo.cpp b/clang/lib/Basic/TargetInfo.cpp
index 514893b47b4f..86144388f781 100644
--- a/clang/lib/Basic/TargetInfo.cpp
+++ b/clang/lib/Basic/TargetInfo.cpp
@@ -374,7 +374,8 @@ void TargetInfo::adjust(DiagnosticsEngine &Diags, LangOptions &Opts) {
     // OpenCL standard only mentions these as "reserved".
     IntWidth = IntAlign = 32;
     LongWidth = LongAlign = 64;
-    LongLongWidth = LongLongAlign = 128;
+    //LongLongWidth = LongLongAlign = 128; // NOTE: there is no 128-bit type support in OpenCL!
+    LongLongWidth = LongLongAlign = LongWidth;
     HalfWidth = HalfAlign = 16;
     FloatWidth = FloatAlign = 32;
 
@@ -385,7 +386,8 @@ void TargetInfo::adjust(DiagnosticsEngine &Diags, LangOptions &Opts) {
       DoubleWidth = DoubleAlign = 64;
       DoubleFormat = &llvm::APFloat::IEEEdouble();
     }
-    LongDoubleWidth = LongDoubleAlign = 128;
+    //LongDoubleWidth = LongDoubleAlign = 128; // NOTE: there is no 128-bit type support in OpenCL!
+    LongDoubleWidth = LongDoubleAlign = DoubleWidth;
 
     unsigned MaxPointerWidth = getMaxPointerWidth();
     assert(MaxPointerWidth == 32 || MaxPointerWidth == 64);
@@ -433,7 +435,7 @@ void TargetInfo::adjust(DiagnosticsEngine &Diags, LangOptions &Opts) {
     }
   }
 
-  if (Opts.LongDoubleSize) {
+  if (Opts.LongDoubleSize && !Opts.OpenCL) {
     if (Opts.LongDoubleSize == DoubleWidth) {
       LongDoubleWidth = DoubleWidth;
       LongDoubleAlign = DoubleAlign;
diff --git a/clang/lib/Basic/Targets.cpp b/clang/lib/Basic/Targets.cpp
index 273eecbd09bf..ac558356d0e9 100644
--- a/clang/lib/Basic/Targets.cpp
+++ b/clang/lib/Basic/Targets.cpp
@@ -594,18 +594,12 @@ TargetInfo *AllocateTarget(const llvm::Triple &Triple,
       return new X86_64TargetInfo(Triple, Opts);
     }
 
-  case llvm::Triple::spir: {
-    if (os != llvm::Triple::UnknownOS ||
-        Triple.getEnvironment() != llvm::Triple::UnknownEnvironment)
-      return nullptr;
+  case llvm::Triple::spir:
     return new SPIR32TargetInfo(Triple, Opts);
-  }
-  case llvm::Triple::spir64: {
-    if (os != llvm::Triple::UnknownOS ||
-        Triple.getEnvironment() != llvm::Triple::UnknownEnvironment)
-      return nullptr;
+  case llvm::Triple::spir64:
     return new SPIR64TargetInfo(Triple, Opts);
-  }
+  case llvm::Triple::air64:
+    return new AIR64TargetInfo(Triple, Opts);
   case llvm::Triple::wasm32:
     if (Triple.getSubArch() != llvm::Triple::NoSubArch ||
         Triple.getVendor() != llvm::Triple::UnknownVendor ||
diff --git a/clang/lib/Basic/Targets/AArch64.cpp b/clang/lib/Basic/Targets/AArch64.cpp
index f75b8ffcb53d..9ced8c6326c3 100644
--- a/clang/lib/Basic/Targets/AArch64.cpp
+++ b/clang/lib/Basic/Targets/AArch64.cpp
@@ -645,7 +645,7 @@ AArch64TargetInfo::checkCallingConvention(CallingConv CC) const {
   case CC_SwiftAsync:
   case CC_PreserveMost:
   case CC_PreserveAll:
-  case CC_OpenCLKernel:
+  case CC_FloorKernel:
   case CC_AArch64VectorCall:
   case CC_Win64:
     return CCCR_OK;
@@ -916,7 +916,7 @@ WindowsARM64TargetInfo::checkCallingConvention(CallingConv CC) const {
   case CC_X86VectorCall:
     return CCCR_Ignore;
   case CC_C:
-  case CC_OpenCLKernel:
+  case CC_FloorKernel:
   case CC_PreserveMost:
   case CC_PreserveAll:
   case CC_Swift:
diff --git a/clang/lib/Basic/Targets/AMDGPU.cpp b/clang/lib/Basic/Targets/AMDGPU.cpp
index ba7ffa34c73e..4990bfa2c2d8 100644
--- a/clang/lib/Basic/Targets/AMDGPU.cpp
+++ b/clang/lib/Basic/Targets/AMDGPU.cpp
@@ -46,6 +46,7 @@ const LangASMap AMDGPUTargetInfo::AMDGPUDefIsGenMap = {
     Generic,  // opencl_generic
     Global,   // opencl_global_device
     Global,   // opencl_global_host
+    Generic,  // vulkan_input
     Global,   // cuda_device
     Constant, // cuda_constant
     Local,    // cuda_shared
@@ -68,6 +69,7 @@ const LangASMap AMDGPUTargetInfo::AMDGPUDefIsPrivMap = {
     Generic,  // opencl_generic
     Global,   // opencl_global_device
     Global,   // opencl_global_host
+    Generic,  // vulkan_input
     Global,   // cuda_device
     Constant, // cuda_constant
     Local,    // cuda_shared
diff --git a/clang/lib/Basic/Targets/AMDGPU.h b/clang/lib/Basic/Targets/AMDGPU.h
index 8b9d7ce79c16..ad6b80286534 100644
--- a/clang/lib/Basic/Targets/AMDGPU.h
+++ b/clang/lib/Basic/Targets/AMDGPU.h
@@ -410,7 +410,7 @@ public:
     default:
       return CCCR_Warning;
     case CC_C:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     }
   }
diff --git a/clang/lib/Basic/Targets/ARM.cpp b/clang/lib/Basic/Targets/ARM.cpp
index fc6b01c87fd2..694132f5dabf 100644
--- a/clang/lib/Basic/Targets/ARM.cpp
+++ b/clang/lib/Basic/Targets/ARM.cpp
@@ -1154,7 +1154,7 @@ ARMTargetInfo::checkCallingConvention(CallingConv CC) const {
   case CC_AAPCS_VFP:
   case CC_Swift:
   case CC_SwiftAsync:
-  case CC_OpenCLKernel:
+  case CC_FloorKernel:
     return CCCR_OK;
   default:
     return CCCR_Warning;
@@ -1229,7 +1229,7 @@ WindowsARMTargetInfo::checkCallingConvention(CallingConv CC) const {
   case CC_X86VectorCall:
     return CCCR_Ignore;
   case CC_C:
-  case CC_OpenCLKernel:
+  case CC_FloorKernel:
   case CC_PreserveMost:
   case CC_PreserveAll:
   case CC_Swift:
diff --git a/clang/lib/Basic/Targets/BPF.h b/clang/lib/Basic/Targets/BPF.h
index 393a91ff53a5..8fd4a75de4b3 100644
--- a/clang/lib/Basic/Targets/BPF.h
+++ b/clang/lib/Basic/Targets/BPF.h
@@ -95,7 +95,7 @@ public:
     default:
       return CCCR_Warning;
     case CC_C:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     }
   }
diff --git a/clang/lib/Basic/Targets/NVPTX.cpp b/clang/lib/Basic/Targets/NVPTX.cpp
index 0461a91d0add..d596ec4be7fb 100644
--- a/clang/lib/Basic/Targets/NVPTX.cpp
+++ b/clang/lib/Basic/Targets/NVPTX.cpp
@@ -44,6 +44,7 @@ NVPTXTargetInfo::NVPTXTargetInfo(const llvm::Triple &Triple,
     if (!Feature.startswith("+ptx"))
       continue;
     PTXVersion = llvm::StringSwitch<unsigned>(Feature)
+                     .Case("+ptx75", 75)
                      .Case("+ptx74", 74)
                      .Case("+ptx73", 73)
                      .Case("+ptx72", 72)
@@ -52,6 +53,7 @@ NVPTXTargetInfo::NVPTXTargetInfo(const llvm::Triple &Triple,
                      .Case("+ptx65", 65)
                      .Case("+ptx64", 64)
                      .Case("+ptx63", 63)
+                     .Case("+ptx62", 62)
                      .Case("+ptx61", 61)
                      .Case("+ptx60", 60)
                      .Case("+ptx50", 50)
@@ -248,12 +250,20 @@ void NVPTXTargetInfo::getTargetDefines(const LangOptions &Opts,
         return "700";
       case CudaArch::SM_72:
         return "720";
+      case CudaArch::SM_73:
+        return "730";
       case CudaArch::SM_75:
         return "750";
       case CudaArch::SM_80:
         return "800";
+      case CudaArch::SM_82:
+        return "820";
       case CudaArch::SM_86:
         return "860";
+      case CudaArch::SM_87:
+        return "870";
+      case CudaArch::SM_88:
+        return "880";
       }
       llvm_unreachable("unhandled CudaArch");
     }();
diff --git a/clang/lib/Basic/Targets/NVPTX.h b/clang/lib/Basic/Targets/NVPTX.h
index ef751b8e1a8d..84d7e27d887d 100644
--- a/clang/lib/Basic/Targets/NVPTX.h
+++ b/clang/lib/Basic/Targets/NVPTX.h
@@ -32,6 +32,7 @@ static const unsigned NVPTXAddrSpaceMap[] = {
     0, // opencl_generic
     1, // opencl_global_device
     1, // opencl_global_host
+    0, // vulkan_input
     1, // cuda_device
     4, // cuda_constant
     3, // cuda_shared
@@ -100,6 +101,7 @@ public:
     case 'l':
     case 'f':
     case 'd':
+    case 'b':
       Info.setAllowsRegister();
       return true;
     }
@@ -166,6 +168,13 @@ public:
   }
 
   CallingConvCheckResult checkCallingConvention(CallingConv CC) const override {
+    if (CC == CC_FloorFunction ||
+        CC == CC_FloorVertex ||
+        CC == CC_FloorFragment ||
+        CC == CC_FloorKernel) {
+        return CCCR_OK;
+    }
+
     // CUDA compilations support all of the host's calling conventions.
     //
     // TODO: We should warn if you apply a non-default CC to anything other than
@@ -175,6 +184,10 @@ public:
     return CCCR_Warning;
   }
 
+  CallingConv getDefaultCallingConv() const override {
+    return CC_FloorFunction;
+  }
+
   bool hasExtIntType() const override { return true; }
 };
 } // namespace targets
diff --git a/clang/lib/Basic/Targets/SPIR.cpp b/clang/lib/Basic/Targets/SPIR.cpp
index 9b7aab85314a..00fce09f3d81 100644
--- a/clang/lib/Basic/Targets/SPIR.cpp
+++ b/clang/lib/Basic/Targets/SPIR.cpp
@@ -32,3 +32,9 @@ void SPIR64TargetInfo::getTargetDefines(const LangOptions &Opts,
   SPIRTargetInfo::getTargetDefines(Opts, Builder);
   DefineStd(Builder, "SPIR64", Opts);
 }
+
+void AIR64TargetInfo::getTargetDefines(const LangOptions &Opts,
+                                       MacroBuilder &Builder) const {
+  // NOTE: don't define SPIR
+  DefineStd(Builder, "AIR64", Opts);
+}
diff --git a/clang/lib/Basic/Targets/SPIR.h b/clang/lib/Basic/Targets/SPIR.h
index 50f34abd6630..e2e501b2be1a 100644
--- a/clang/lib/Basic/Targets/SPIR.h
+++ b/clang/lib/Basic/Targets/SPIR.h
@@ -28,8 +28,9 @@ static const unsigned SPIRDefIsPrivMap[] = {
     2, // opencl_constant
     0, // opencl_private
     4, // opencl_generic
-    5, // opencl_global_device
-    6, // opencl_global_host
+    100, // opencl_global_device
+    101, // opencl_global_host
+    0, // vulkan_input
     0, // cuda_device
     0, // cuda_constant
     0, // cuda_shared
@@ -54,6 +55,7 @@ static const unsigned SPIRDefIsGenMap[] = {
     0, // opencl_generic
     0, // opencl_global_device
     0, // opencl_global_host
+    0, // vulkan_input
     0, // cuda_device
     0, // cuda_constant
     0, // cuda_shared
@@ -67,18 +69,44 @@ static const unsigned SPIRDefIsGenMap[] = {
     0  // ptr64
 };
 
+// Vulkan/SPIR-V uses its own storage classes
+static const unsigned VulkanAddrSpaceMap[] = {
+    0, // Default
+	12, // opencl_global == SPIRAS_StorageBuffer
+    3, // opencl_local
+    2, // opencl_constant
+    0, // opencl_private
+    4, // opencl_generic
+    0, // opencl_global_device
+    0, // opencl_global_host
+    6, // vulkan_input == SPIRAS_Input
+    0, // cuda_device
+    0, // cuda_constant
+    0, // cuda_shared
+    // SYCL address space values for this map are dummy
+    0, // sycl_global
+    0, // sycl_global_device
+    0, // sycl_global_host
+    0, // sycl_local
+    0, // sycl_private
+    0, // ptr32_sptr
+    0, // ptr32_uptr
+    0  // ptr64
+};
+
 class LLVM_LIBRARY_VISIBILITY SPIRTargetInfo : public TargetInfo {
+private:
+  // true for spir-unknown-* and spir64-unknown-* (-> false for AIR/Metal)
+  const bool is_pure_spir;
+  const bool is_vulkan;
 public:
   SPIRTargetInfo(const llvm::Triple &Triple, const TargetOptions &)
-      : TargetInfo(Triple) {
-    assert(getTriple().getOS() == llvm::Triple::UnknownOS &&
-           "SPIR target must use unknown OS");
-    assert(getTriple().getEnvironment() == llvm::Triple::UnknownEnvironment &&
-           "SPIR target must use unknown environment type");
+      : TargetInfo(Triple), is_pure_spir(Triple.getVendorName().str() == "unknown"),
+        is_vulkan(Triple.getEnvironment() == llvm::Triple::EnvironmentType::Vulkan) {
     TLSSupported = false;
     VLASupported = false;
     LongWidth = LongAlign = 64;
-    AddrSpaceMap = &SPIRDefIsPrivMap;
+    AddrSpaceMap = (!is_vulkan ? &SPIRDefIsPrivMap : &VulkanAddrSpaceMap);
     UseAddrSpaceMapMangling = true;
     HasLegalHalfType = true;
     HasFloat16 = true;
@@ -94,6 +122,9 @@ public:
     return Feature == "spir";
   }
 
+  bool isCLZForZeroUndef() const override { return false; }
+  bool isVulkan() const { return is_vulkan; }
+
   // SPIR supports the half type and the only llvm intrinsic allowed in SPIR is
   // memcpy as per section 3 of the SPIR spec.
   bool useFP16ConversionIntrinsics() const override { return false; }
@@ -123,16 +154,22 @@ public:
   }
 
   CallingConvCheckResult checkCallingConvention(CallingConv CC) const override {
-    return (CC == CC_SpirFunction || CC == CC_OpenCLKernel) ? CCCR_OK
-                                                            : CCCR_Warning;
+    if (!is_pure_spir) return CCCR_OK;
+    if (CC == CC_FloorFunction ||
+        CC == CC_FloorVertex ||
+        CC == CC_FloorFragment ||
+        CC == CC_FloorKernel) {
+        return CCCR_OK;
+    }
+    return CCCR_Warning;
   }
 
   CallingConv getDefaultCallingConv() const override {
-    return CC_SpirFunction;
+    return (is_pure_spir ? CC_FloorFunction : CC_C);
   }
 
   void setAddressSpaceMap(bool DefaultIsGeneric) {
-    AddrSpaceMap = DefaultIsGeneric ? &SPIRDefIsGenMap : &SPIRDefIsPrivMap;
+    AddrSpaceMap = (is_vulkan ? &VulkanAddrSpaceMap : (DefaultIsGeneric ? &SPIRDefIsGenMap : &SPIRDefIsPrivMap));
   }
 
   void adjust(DiagnosticsEngine &Diags, LangOptions &Opts) override {
@@ -155,7 +192,11 @@ public:
 
   bool hasExtIntType() const override { return true; }
 
-  bool hasInt128Type() const override { return false; }
+  bool hasInt128Type() const override { return true; }
+
+  llvm::Optional<LangAS> getConstantAddressSpace() const override {
+    return LangAS::opencl_constant;
+  }
 };
 
 class LLVM_LIBRARY_VISIBILITY SPIR32TargetInfo : public SPIRTargetInfo {
@@ -166,7 +207,8 @@ public:
     SizeType = TargetInfo::UnsignedInt;
     PtrDiffType = IntPtrType = TargetInfo::SignedInt;
     resetDataLayout("e-p:32:32-i64:64-v16:16-v24:32-v32:32-v48:64-"
-                    "v96:128-v192:256-v256:256-v512:512-v1024:1024");
+                    "v96:128-v192:256-v256:256-v512:512-v1024:1024"
+                    "-n8:16:32:64");
   }
 
   void getTargetDefines(const LangOptions &Opts,
@@ -180,13 +222,47 @@ public:
     PointerWidth = PointerAlign = 64;
     SizeType = TargetInfo::UnsignedLong;
     PtrDiffType = IntPtrType = TargetInfo::SignedLong;
-    resetDataLayout("e-i64:64-v16:16-v24:32-v32:32-v48:64-"
-                    "v96:128-v192:256-v256:256-v512:512-v1024:1024");
+    resetDataLayout("e-p:64:64-i64:64-v16:16-v24:32-v32:32-v48:64-"
+                    "v96:128-v192:256-v256:256-v512:512-v1024:1024"
+                    "-n8:16:32:64");
   }
 
   void getTargetDefines(const LangOptions &Opts,
                         MacroBuilder &Builder) const override;
 };
+
+// Metal/AIR target based on SPIR
+class LLVM_LIBRARY_VISIBILITY AIR64TargetInfo : public SPIRTargetInfo {
+public:
+  AIR64TargetInfo(const llvm::Triple &Triple, const TargetOptions &TO) : SPIRTargetInfo(Triple, TO) {
+    PointerWidth = PointerAlign = 64;
+    SizeType     = TargetInfo::UnsignedLong;
+    PtrDiffType = IntPtrType = TargetInfo::SignedLong;
+    if(Triple.getOS() == llvm::Triple::IOS) {
+      resetDataLayout("e-i64:64-f80:128-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-n8:16:32");
+    } else { // macOS, or default
+      resetDataLayout("e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-f80:128:128-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:1024-f80:128:128-n8:16:32");
+    }
+  }
+  
+  void getTargetDefines(const LangOptions &Opts,
+                        MacroBuilder &Builder) const override;
+
+  CallingConvCheckResult checkCallingConvention(CallingConv CC) const override {
+    if (CC == CC_FloorFunction ||
+        CC == CC_FloorVertex ||
+        CC == CC_FloorFragment ||
+        CC == CC_FloorKernel) {
+        return CCCR_OK;
+    }
+    return CCCR_Warning;
+  }
+
+  CallingConv getDefaultCallingConv() const override {
+    return CC_FloorFunction;
+  }
+};
+
 } // namespace targets
 } // namespace clang
 #endif // LLVM_CLANG_LIB_BASIC_TARGETS_SPIR_H
diff --git a/clang/lib/Basic/Targets/SystemZ.h b/clang/lib/Basic/Targets/SystemZ.h
index d3e3ed50dd47..0ba990811374 100644
--- a/clang/lib/Basic/Targets/SystemZ.h
+++ b/clang/lib/Basic/Targets/SystemZ.h
@@ -153,7 +153,7 @@ public:
     switch (CC) {
     case CC_C:
     case CC_Swift:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     case CC_SwiftAsync:
       return CCCR_Error;
diff --git a/clang/lib/Basic/Targets/TCE.h b/clang/lib/Basic/Targets/TCE.h
index 251b4d4b56f7..644cd626dccb 100644
--- a/clang/lib/Basic/Targets/TCE.h
+++ b/clang/lib/Basic/Targets/TCE.h
@@ -39,6 +39,7 @@ static const unsigned TCEOpenCLAddrSpaceMap[] = {
     1, // opencl_global_host
     // FIXME: generic has to be added to the target
     0, // opencl_generic
+    0, // vulkan_input
     0, // cuda_device
     0, // cuda_constant
     0, // cuda_shared
diff --git a/clang/lib/Basic/Targets/X86.h b/clang/lib/Basic/Targets/X86.h
index 2c4ab0494fbb..bd779847ee20 100644
--- a/clang/lib/Basic/Targets/X86.h
+++ b/clang/lib/Basic/Targets/X86.h
@@ -32,6 +32,7 @@ static const unsigned X86AddrSpaceMap[] = {
     0,   // opencl_generic
     0,   // opencl_global_device
     0,   // opencl_global_host
+    0,   // vulkan_input
     0,   // cuda_device
     0,   // cuda_constant
     0,   // cuda_shared
@@ -157,6 +158,10 @@ public:
     AddrSpaceMap = &X86AddrSpaceMap;
     HasStrictFP = true;
 
+    // enable fp16
+    HasLegalHalfType = false;
+    HasFloat16 = true;
+
     bool IsWinCOFF =
         getTriple().isOSWindows() && getTriple().isOSBinFormatCOFF();
     if (IsWinCOFF)
@@ -357,7 +362,7 @@ public:
     case CC_Swift:
     case CC_X86Pascal:
     case CC_IntelOclBicc:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     case CC_SwiftAsync:
       return CCCR_Error;
@@ -728,7 +733,7 @@ public:
     case CC_PreserveMost:
     case CC_PreserveAll:
     case CC_X86RegCall:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     default:
       return CCCR_Warning;
@@ -805,7 +810,7 @@ public:
     case CC_Swift:
     case CC_SwiftAsync:
     case CC_X86RegCall:
-    case CC_OpenCLKernel:
+    case CC_FloorKernel:
       return CCCR_OK;
     default:
       return CCCR_Warning;
diff --git a/clang/lib/CodeGen/BackendUtil.cpp b/clang/lib/CodeGen/BackendUtil.cpp
index 30f81b8ae473..4225b7e1df28 100644
--- a/clang/lib/CodeGen/BackendUtil.cpp
+++ b/clang/lib/CodeGen/BackendUtil.cpp
@@ -46,10 +46,13 @@
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Support/MemoryBuffer.h"
 #include "llvm/Support/PrettyStackTrace.h"
+#include "llvm/../../projects/spirv/include/LLVMSPIRVLib.h"
 #include "llvm/Support/TimeProfiler.h"
 #include "llvm/Support/Timer.h"
 #include "llvm/Support/ToolOutputFile.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/../../projects/spirv/lib/SPIRV/SPIRVWriterPass.h"
+#include "llvm/../../projects/spirv/lib/SPIRV/SPIRVContainerWriterPass.h"
 #include "llvm/Target/TargetMachine.h"
 #include "llvm/Target/TargetOptions.h"
 #include "llvm/Transforms/Coroutines.h"
@@ -75,6 +78,7 @@
 #include "llvm/Transforms/Instrumentation/MemorySanitizer.h"
 #include "llvm/Transforms/Instrumentation/SanitizerCoverage.h"
 #include "llvm/Transforms/Instrumentation/ThreadSanitizer.h"
+#include "llvm/Transforms/LibFloor/FloorGPUTTI.h"
 #include "llvm/Transforms/ObjCARC.h"
 #include "llvm/Transforms/Scalar.h"
 #include "llvm/Transforms/Scalar/EarlyCSE.h"
@@ -87,6 +91,7 @@
 #include "llvm/Transforms/Utils/NameAnonGlobals.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include <memory>
+#include <fstream>
 using namespace clang;
 using namespace llvm;
 
@@ -115,6 +120,13 @@ class EmitAssemblyHelper {
     if (TM)
       return TM->getTargetIRAnalysis();
 
+    if (LangOpts.OpenCL || LangOpts.Metal || LangOpts.Vulkan) {
+      // for OpenCL/Metal/Vulkan, provide our own TTI implementation
+      return TargetIRAnalysis([this](const Function &F) {
+        return TargetTransformInfo(LibFloorGPUTTIImpl(F, CodeGenOpts, TargetOpts, LangOpts, *TheModule));
+      });
+    }
+
     return TargetIRAnalysis();
   }
 
@@ -692,22 +704,26 @@ void EmitAssemblyHelper::CreatePasses(legacy::PassManager &MPM,
                                      /*DropTypeTests=*/true));
 
   PassManagerBuilderWrapper PMBuilder(TargetTriple, CodeGenOpts, LangOpts);
-
-  // At O0 and O1 we only run the always inliner which is more efficient. At
-  // higher optimization levels we run the normal inliner.
-  if (CodeGenOpts.OptimizationLevel <= 1) {
-    bool InsertLifetimeIntrinsics = ((CodeGenOpts.OptimizationLevel != 0 &&
-                                      !CodeGenOpts.DisableLifetimeMarkers) ||
-                                     LangOpts.Coroutines);
-    PMBuilder.Inliner = createAlwaysInlinerLegacyPass(InsertLifetimeIntrinsics);
+  // always inline everything for Metal/CUDA/OpenCL/Vulkan/Host-Compute, otherwise we run into trouble when fixing the IR
+  if (LangOpts.Metal || LangOpts.CUDA || LangOpts.OpenCL || LangOpts.Vulkan || LangOpts.FloorHostCompute) {
+    PMBuilder.Inliner = createEverythingInlinerPass();
   } else {
-    // We do not want to inline hot callsites for SamplePGO module-summary build
-    // because profile annotation will happen again in ThinLTO backend, and we
-    // want the IR of the hot path to match the profile.
-    PMBuilder.Inliner = createFunctionInliningPass(
-        CodeGenOpts.OptimizationLevel, CodeGenOpts.OptimizeSize,
-        (!CodeGenOpts.SampleProfileFile.empty() &&
-         CodeGenOpts.PrepareForThinLTO));
+    // At O0 and O1 we only run the always inliner which is more efficient. At
+    // higher optimization levels we run the normal inliner.
+    if (CodeGenOpts.OptimizationLevel <= 1) {
+      bool InsertLifetimeIntrinsics = ((CodeGenOpts.OptimizationLevel != 0 &&
+                                        !CodeGenOpts.DisableLifetimeMarkers) ||
+                                       LangOpts.Coroutines);
+      PMBuilder.Inliner = createAlwaysInlinerLegacyPass(InsertLifetimeIntrinsics);
+    } else {
+      // We do not want to inline hot callsites for SamplePGO module-summary build
+      // because profile annotation will happen again in ThinLTO backend, and we
+      // want the IR of the hot path to match the profile.
+      PMBuilder.Inliner = createFunctionInliningPass(
+          CodeGenOpts.OptimizationLevel, CodeGenOpts.OptimizeSize,
+          (!CodeGenOpts.SampleProfileFile.empty() &&
+           CodeGenOpts.PrepareForThinLTO));
+    }
   }
 
   PMBuilder.OptLevel = CodeGenOpts.OptimizationLevel;
@@ -729,6 +745,53 @@ void EmitAssemblyHelper::CreatePasses(legacy::PassManager &MPM,
 
   MPM.add(new TargetLibraryInfoWrapperPass(*TLII));
 
+  // close floor function info file, this is no longer needed
+  if ((LangOpts.Metal || LangOpts.CUDA || LangOpts.OpenCL || LangOpts.Vulkan || LangOpts.FloorHostCompute) &&
+      LangOpts.floor_function_info != nullptr) {
+    LangOpts.floor_function_info->close();
+    delete LangOpts.floor_function_info;
+  }
+  
+  PMBuilder.floor_image_capabilities = LangOpts.floor_image_capabilities;
+  
+  PMBuilder.EnableAddressSpaceFix = LangOpts.OpenCL;
+  if (PMBuilder.EnableAddressSpaceFix && CodeGenOpts.OptimizationLevel == 0) {
+    unsigned DiagID = Diags.getCustomDiagID(DiagnosticsEngine::Error, "compiling OpenCL/Metal/Vulkan with -O0 is not possible!");
+    Diags.Report(DiagID);
+    return;
+  }
+  
+  // only enable this for CUDA
+  PMBuilder.EnableCUDAPasses = LangOpts.CUDA;
+  
+  // only enable this for Metal/AIR
+  PMBuilder.EnableMetalPasses = LangOpts.Metal;
+  PMBuilder.EnableMetalIntelWorkarounds = CodeGenOpts.MetalIntelWorkarounds;
+  PMBuilder.EnableMetalNvidiaWorkarounds = CodeGenOpts.MetalNvidiaWorkarounds;
+  
+  // only enable this for OpenCL/SPIR and Vulkan/SPIR-V (don't want this for Metal)
+  PMBuilder.EnableSPIRPasses = (LangOpts.OpenCL &&
+                                (Triple(TheModule->getTargetTriple()).getArch() == Triple::spir64 ||
+                                 Triple(TheModule->getTargetTriple()).getArch() == Triple::spir));
+  PMBuilder.EnableSPIRIntelWorkarounds = CodeGenOpts.SPIRIntelWorkarounds;
+  PMBuilder.EnableVerifySPIR = PMBuilder.EnableSPIRPasses && LangOpts.CLVerifySPIR;
+  
+  // only enable this for Vulkan/SPIR-V
+  PMBuilder.EnableVulkanPasses = (PMBuilder.EnableSPIRPasses &&
+                                  Triple(TheModule->getTargetTriple()).getEnvironment() == Triple::Vulkan);
+  PMBuilder.EnableVulkanLLVMPreStructurizationPass = CodeGenOpts.VulkanLLVMPreStructurizationPass;
+
+  // don't enable any vectorization for Vulkan, this would lead to illegal pointer bitcasts and possibly unsupported vector dims
+  if (PMBuilder.EnableVulkanPasses) {
+    PMBuilder.SLPVectorize = false;
+    PMBuilder.LoopVectorize = false;
+  }
+  // for Metal: enable vectorization
+  if (PMBuilder.EnableMetalPasses) {
+    PMBuilder.SLPVectorize = true;
+    PMBuilder.LoopVectorize = true;
+  }
+
   if (TM)
     TM->adjustPassManager(PMBuilder);
 
@@ -850,6 +913,10 @@ void EmitAssemblyHelper::CreatePasses(legacy::PassManager &MPM,
   if (!CodeGenOpts.RewriteMapFiles.empty())
     addSymbolRewriterPass(CodeGenOpts, &MPM);
 
+  if (LangOpts.OpenCL || LangOpts.CUDA) {
+    MPM.add(createInternalizePass());
+  }
+
   if (Optional<GCOVOptions> Options = getGCOVOptions(CodeGenOpts, LangOpts)) {
     MPM.add(createGCOVProfilerPass(*Options));
     if (CodeGenOpts.getDebugInfo() == codegenoptions::NoDebugInfo)
@@ -979,6 +1046,11 @@ void EmitAssemblyHelper::EmitAssemblyWithLegacyPassManager(
 
   bool UsesCodeGen = (Action != Backend_EmitNothing &&
                       Action != Backend_EmitBC &&
+                      Action != Backend_EmitBC32 &&
+                      Action != Backend_EmitBC50 &&
+                      Action != Backend_EmitSPIRV &&
+                      Action != Backend_EmitSPIRVContainer &&
+                      Action != Backend_EmitMetalLib &&
                       Action != Backend_EmitLL);
   CreateTargetMachine(UsesCodeGen);
 
@@ -1047,6 +1119,26 @@ void EmitAssemblyHelper::EmitAssemblyWithLegacyPassManager(
     }
     break;
 
+  case Backend_EmitBC32:
+    PerModulePasses.add(createBitcode32WriterPass(*OS));
+    break;
+
+  case Backend_EmitBC50:
+    PerModulePasses.add(createBitcode50WriterPass(*OS));
+    break;
+
+  case Backend_EmitSPIRV:
+    PerModulePasses.add(createSPIRVWriterPass(*OS));
+    break;
+
+  case Backend_EmitSPIRVContainer:
+    PerModulePasses.add(createSPIRVContainerWriterPass(*OS));
+    break;
+
+  case Backend_EmitMetalLib:
+    PerModulePasses.add(createMetalLibWriterPass(*OS));
+    break;
+
   case Backend_EmitLL:
     PerModulePasses.add(
         createPrintModulePass(*OS, "", CodeGenOpts.EmitLLVMUseLists));
@@ -1460,6 +1552,33 @@ void EmitAssemblyHelper::RunOptimizationPipeline(
     }
     break;
 
+  case Backend_EmitBC32:
+    MPM.addPass(Bitcode32WriterPass(*OS));
+    break;
+
+  case Backend_EmitBC50: {
+    bool EmitLTOSummary =
+        (CodeGenOpts.PrepareForLTO &&
+         !CodeGenOpts.DisableLLVMPasses &&
+         llvm::Triple(TheModule->getTargetTriple()).getVendor() !=
+             llvm::Triple::Apple);
+    MPM.addPass(BitcodeWriterPass50(*OS, CodeGenOpts.EmitLLVMUseLists,
+                                    EmitLTOSummary));
+    break;
+  }
+
+  case Backend_EmitSPIRV:
+    MPM.addPass(SPIRVWriterPass(*OS));
+    break;
+
+  case Backend_EmitSPIRVContainer:
+    MPM.addPass(SPIRVContainerWriterPass(*OS));
+    break;
+
+  case Backend_EmitMetalLib:
+    MPM.addPass(MetalLibWriterPass(*OS));
+    break;
+
   case Backend_EmitLL:
     MPM.addPass(PrintModulePass(*OS, "", CodeGenOpts.EmitLLVMUseLists));
     break;
diff --git a/clang/lib/CodeGen/CGBuiltin.cpp b/clang/lib/CodeGen/CGBuiltin.cpp
index 501b0e3c3443..255fe9b0b4be 100644
--- a/clang/lib/CodeGen/CGBuiltin.cpp
+++ b/clang/lib/CodeGen/CGBuiltin.cpp
@@ -23,6 +23,7 @@
 #include "clang/AST/ASTContext.h"
 #include "clang/AST/Attr.h"
 #include "clang/AST/Decl.h"
+#include "clang/AST/Expr.h"
 #include "clang/AST/OSLog.h"
 #include "clang/Basic/TargetBuiltins.h"
 #include "clang/Basic/TargetInfo.h"
@@ -3967,26 +3968,25 @@ RValue CodeGenFunction::EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
       int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();
       AtomicRMWInst *Result = nullptr;
       switch (ord) {
-      case 0:  // memory_order_relaxed
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_relaxed:
       default: // invalid order
         Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,
                                          llvm::AtomicOrdering::Monotonic);
         break;
-      case 1: // memory_order_consume
-      case 2: // memory_order_acquire
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_consume:
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_acquire:
         Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,
                                          llvm::AtomicOrdering::Acquire);
         break;
-      case 3: // memory_order_release
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_release:
         Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,
                                          llvm::AtomicOrdering::Release);
         break;
-      case 4: // memory_order_acq_rel
-
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_acq_rel:
         Result = Builder.CreateAtomicRMW(llvm::AtomicRMWInst::Xchg, Ptr, NewVal,
                                          llvm::AtomicOrdering::AcquireRelease);
         break;
-      case 5: // memory_order_seq_cst
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_seq_cst:
         Result = Builder.CreateAtomicRMW(
             llvm::AtomicRMWInst::Xchg, Ptr, NewVal,
             llvm::AtomicOrdering::SequentiallyConsistent);
@@ -4050,14 +4050,14 @@ RValue CodeGenFunction::EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
       int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();
       StoreInst *Store = Builder.CreateStore(NewVal, Ptr, Volatile);
       switch (ord) {
-      case 0:  // memory_order_relaxed
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_relaxed:
       default: // invalid order
         Store->setOrdering(llvm::AtomicOrdering::Monotonic);
         break;
-      case 3:  // memory_order_release
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_release:
         Store->setOrdering(llvm::AtomicOrdering::Release);
         break;
-      case 5:  // memory_order_seq_cst
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_seq_cst:
         Store->setOrdering(llvm::AtomicOrdering::SequentiallyConsistent);
         break;
       }
@@ -4107,20 +4107,20 @@ RValue CodeGenFunction::EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
     if (isa<llvm::ConstantInt>(Order)) {
       int ord = cast<llvm::ConstantInt>(Order)->getZExtValue();
       switch (ord) {
-      case 0:  // memory_order_relaxed
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_relaxed:  // memory_order_relaxed
       default: // invalid order
         break;
-      case 1:  // memory_order_consume
-      case 2:  // memory_order_acquire
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_consume:  // memory_order_consume
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_acquire:  // memory_order_acquire
         Builder.CreateFence(llvm::AtomicOrdering::Acquire, SSID);
         break;
-      case 3:  // memory_order_release
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_release:  // memory_order_release
         Builder.CreateFence(llvm::AtomicOrdering::Release, SSID);
         break;
-      case 4:  // memory_order_acq_rel
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_acq_rel:  // memory_order_acq_rel
         Builder.CreateFence(llvm::AtomicOrdering::AcquireRelease, SSID);
         break;
-      case 5:  // memory_order_seq_cst
+      case AtomicExpr::AtomicOrderingKind::AO_ABI_memory_order_seq_cst:  // memory_order_seq_cst
         Builder.CreateFence(llvm::AtomicOrdering::SequentiallyConsistent, SSID);
         break;
       }
diff --git a/clang/lib/CodeGen/CGCall.cpp b/clang/lib/CodeGen/CGCall.cpp
index daea09be3e70..f65c5f986373 100644
--- a/clang/lib/CodeGen/CGCall.cpp
+++ b/clang/lib/CodeGen/CGCall.cpp
@@ -61,8 +61,10 @@ unsigned CodeGenTypes::ClangCallConvToLLVMCallConv(CallingConv CC) {
   // TODO: Add support for __vectorcall to LLVM.
   case CC_X86VectorCall: return llvm::CallingConv::X86_VectorCall;
   case CC_AArch64VectorCall: return llvm::CallingConv::AArch64_VectorCall;
-  case CC_SpirFunction: return llvm::CallingConv::SPIR_FUNC;
-  case CC_OpenCLKernel: return CGM.getTargetCodeGenInfo().getOpenCLKernelCallingConv();
+  case CC_FloorFunction: return llvm::CallingConv::FLOOR_FUNC;
+  case CC_FloorKernel: return llvm::CallingConv::FLOOR_KERNEL;
+  case CC_FloorVertex: return llvm::CallingConv::FLOOR_VERTEX;
+  case CC_FloorFragment: return llvm::CallingConv::FLOOR_FRAGMENT;
   case CC_PreserveMost: return llvm::CallingConv::PreserveMost;
   case CC_PreserveAll: return llvm::CallingConv::PreserveAll;
   case CC_Swift: return llvm::CallingConv::Swift;
@@ -82,8 +84,10 @@ CanQualType CodeGenTypes::DeriveThisType(const CXXRecordDecl *RD,
   else
     RecTy = Context.VoidTy;
 
+#if 0 // we don't want this
   if (MD)
     RecTy = Context.getAddrSpaceQualType(RecTy, MD->getMethodQualifiers().getAddressSpace());
+#endif
   return Context.getPointerType(CanQualType::CreateUnsafe(RecTy));
 }
 
@@ -236,6 +240,15 @@ static CallingConv getCallingConventionForDecl(const ObjCMethodDecl *D,
   if (D->hasAttr<SysVABIAttr>())
     return IsWindows ? CC_X86_64SysV : CC_C;
 
+  if (D->hasAttr<GraphicsVertexShaderAttr>())
+    return CC_FloorVertex;
+
+  if (D->hasAttr<GraphicsFragmentShaderAttr>())
+    return CC_FloorFragment;
+
+  if (D->hasAttr<ComputeKernelAttr>())
+    return CC_FloorKernel;
+
   if (D->hasAttr<PreserveMostAttr>())
     return CC_PreserveMost;
 
@@ -268,7 +281,7 @@ CodeGenTypes::arrangeCXXMethodType(const CXXRecordDecl *RD,
 /// Set calling convention for CUDA/HIP kernel.
 static void setCUDAKernelCallingConvention(CanQualType &FTy, CodeGenModule &CGM,
                                            const FunctionDecl *FD) {
-  if (FD->hasAttr<CUDAGlobalAttr>()) {
+  if (FD->hasAttr<ComputeKernelAttr>()) {
     const FunctionType *FT = FTy->getAs<FunctionType>();
     CGM.getTargetCodeGenInfo().setCUDAKernelCallingConvention(FT);
     FTy = FT->getCanonicalTypeUnqualified();
@@ -432,6 +445,184 @@ CodeGenTypes::arrangeCXXConstructorCall(const CallArgList &args,
                                  ParamInfos, Required);
 }
 
+uint32_t CodeGenTypes::getMetalVulkanImplicitArgCount(const FunctionDecl* FD) const {
+  if (!FD) return 0;
+
+  const auto& LangOpts = CGM.getLangOpts();
+  const auto& CodeGenOpts = CGM.getCodeGenOpts();
+
+  if (!LangOpts.Metal && !LangOpts.Vulkan) {
+    return 0;
+  }
+
+  if (LangOpts.Metal) {
+    const uint32_t printf_arg = (CodeGenOpts.MetalSoftPrintf > 0 ? 1 : 0);
+    if (FD->hasAttr<ComputeKernelAttr>()) {
+      if (CGM.getTriple().getOS() != llvm::Triple::OSType::MacOSX) {
+        return 6 + printf_arg;
+      } else {
+        return 10 + printf_arg;
+      }
+    } else if (FD->hasAttr<GraphicsVertexShaderAttr>()) {
+      return 2 + printf_arg;
+    } else if (FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+      return 1 + printf_arg + (CodeGenOpts.GraphicsPrimitiveID ? 1 : 0) + (CodeGenOpts.GraphicsBarycentricCoord ? 1 : 0);
+    }
+  } else if(LangOpts.Vulkan) {
+    const uint32_t printf_arg = (CodeGenOpts.VulkanSoftPrintf > 0 ? 1 : 0);
+    if (FD->hasAttr<ComputeKernelAttr>()) {
+      return 4 + printf_arg;
+    } else if (FD->hasAttr<GraphicsVertexShaderAttr>()) {
+      return 3 + printf_arg;
+    } else if (FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+      return 3 + printf_arg + (CodeGenOpts.GraphicsPrimitiveID ? 1 : 0) + (CodeGenOpts.GraphicsBarycentricCoord ? 1 : 0);
+    }
+  }
+
+  return 0;
+}
+
+// for all entry functions/points: handle the function type -> add implicit internal args
+void CodeGenTypes::handleMetalVulkanEntryFunction(CanQualType* FTy, FunctionArgList* ArgList, const FunctionDecl* FD) {
+  if (!FD) return; // just in case
+
+  const auto& LangOpts = CGM.getLangOpts();
+  const auto& CodeGenOpts = CGM.getCodeGenOpts();
+
+  if (!LangOpts.Metal && !LangOpts.Vulkan) {
+    return; // nothing to change here
+  }
+
+  if (!FD->hasAttr<ComputeKernelAttr>() &&
+      !FD->hasAttr<GraphicsVertexShaderAttr>() &&
+      !FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+    return; // no entry function/point
+  }
+
+  // if FTy is specified, we will update it with the new function type
+  const FunctionProtoType *FPT = nullptr;
+  if (FTy) {
+    FPT = (*FTy)->getAs<FunctionProtoType>();
+    if (!FPT) {
+      return; // just in case
+    }
+  }
+
+  // get/init original function type info
+  auto& Ctx = CGM.getContext();
+  SmallVector<QualType, 16> ft_args;
+  if (FPT) {
+    for (auto& param : FPT->param_types()) {
+      ft_args.emplace_back(param);
+    }
+  }
+
+  // adds an implicit argument
+  uint32_t added_args_count = 0; // for sanity checking
+  const auto add_arg = [&ft_args, &ArgList, &FD, &Ctx, &added_args_count](QualType type, const char* name) {
+    ++added_args_count;
+    ft_args.emplace_back(type);
+
+    if (ArgList) {
+      auto* arg_decl = ImplicitParamDecl::Create(Ctx, /*DC=*/nullptr, FD->getLocation(),
+                                                 &Ctx.Idents.get(name), type, ImplicitParamDecl::Other);
+      ArgList->emplace_back(arg_decl);
+    }
+  };
+
+  // add implicit internal args
+  // NOTE: Metal and Vulkan handle these differently: Metal uses direct params, Vulkan uses pointers in input AS
+  if (LangOpts.Metal) {
+    if (CodeGenOpts.MetalSoftPrintf > 0) {
+      add_arg(Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.IntTy, LangAS::opencl_global)), "__metal__printf_buffer__");
+    }
+
+    if (FD->hasAttr<ComputeKernelAttr>()) {
+      // id types, all int3:
+      auto int3_type = Ctx.getExtVectorType(Ctx.IntTy, 3);
+      add_arg(int3_type, "__metal__global_id__");
+      add_arg(int3_type, "__metal__global_size__");
+      add_arg(int3_type, "__metal__local_id__");
+      add_arg(int3_type, "__metal__local_size__");
+      add_arg(int3_type, "__metal__group_id__");
+      add_arg(int3_type, "__metal__group_size__");
+      if (CGM.getTriple().getOS() == llvm::Triple::OSType::MacOSX) {
+        // SIMD-group / sub-group ids
+        add_arg(Ctx.IntTy, "__metal__sub_group_id__");
+        add_arg(Ctx.IntTy, "__metal__sub_group_local_id__");
+        add_arg(Ctx.IntTy, "__metal__sub_group_size__");
+        add_arg(Ctx.IntTy, "__metal__num_sub_groups__");
+      }
+    } else if (FD->hasAttr<GraphicsVertexShaderAttr>()) {
+      // only vertex id and instance id for now:
+      add_arg(Ctx.IntTy, "__metal__vertex_id__");
+      add_arg(Ctx.IntTy, "__metal__instance_id__");
+    } else if (FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+      // optional: primitive id and barycentric coord
+      if (CodeGenOpts.GraphicsPrimitiveID) {
+        add_arg(Ctx.IntTy, "__metal__primitive_id__");
+      }
+      if (CodeGenOpts.GraphicsBarycentricCoord) {
+        auto float3_type = Ctx.getExtVectorType(Ctx.FloatTy, 3);
+        add_arg(float3_type, "__metal__barycentric_coord__");
+      }
+
+      // fixed: only point coord for now:
+      auto float2_type = Ctx.getExtVectorType(Ctx.FloatTy, 2);
+      add_arg(float2_type, "__metal__point_coord__");
+    }
+  } else if(LangOpts.Vulkan) {
+    if (CodeGenOpts.VulkanSoftPrintf > 0) {
+      add_arg(Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.IntTy, LangAS::opencl_global)), "vulkan.printf_buffer");
+    }
+
+    if (FD->hasAttr<ComputeKernelAttr>()) {
+      // id types, all int3*:
+      auto int3_type = Ctx.getExtVectorType(Ctx.IntTy, 3);
+      auto int3_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(int3_type, LangAS::vulkan_input));
+      add_arg(int3_ptr_type, "vulkan.global_invocation_id");
+      add_arg(int3_ptr_type, "vulkan.local_invocation_id");
+      add_arg(int3_ptr_type, "vulkan.workgroup_id");
+      add_arg(int3_ptr_type, "vulkan.num_workgroups");
+    } else if (FD->hasAttr<GraphicsVertexShaderAttr>()) {
+      // only vertex id + view index + instance id for now:
+      auto int_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.IntTy, LangAS::vulkan_input));
+      add_arg(int_ptr_type, "vulkan.vertex_index");
+      add_arg(int_ptr_type, "vulkan.view_index");
+      add_arg(int_ptr_type, "vulkan.instance_index");
+    } else if (FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+      auto int_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.IntTy, LangAS::vulkan_input));
+
+      // optional: primitive id and barycentric coord
+      if (CodeGenOpts.GraphicsPrimitiveID) {
+        add_arg(int_ptr_type, "vulkan.primitive_id");
+      }
+      if (CodeGenOpts.GraphicsBarycentricCoord) {
+        auto float3_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.getExtVectorType(Ctx.FloatTy, 3), LangAS::vulkan_input));
+        add_arg(float3_ptr_type, "vulkan.barycentric_coord");
+      }
+
+      // fixed: only point + frag coord + view index for now:
+      auto float2_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.getExtVectorType(Ctx.FloatTy, 2), LangAS::vulkan_input));
+      add_arg(float2_ptr_type, "vulkan.point_coord");
+      auto float4_ptr_type = Ctx.getPointerType(Context.getAddrSpaceQualType(Ctx.getExtVectorType(Ctx.FloatTy, 4), LangAS::vulkan_input));
+      add_arg(float4_ptr_type, "vulkan.frag_coord");
+      add_arg(int_ptr_type, "vulkan.view_index");
+    }
+  }
+
+  // sanity check
+  assert(added_args_count == getMetalVulkanImplicitArgCount(FD) &&
+         "invalid added implicit argument count");
+
+  // create new function type and update FTy
+  if (FTy) {
+    FunctionProtoType::ExtProtoInfo EPI = FPT->getExtProtoInfo();
+    QualType NewFT = Ctx.getFunctionType(FPT->getReturnType(), ft_args, EPI);
+    *FTy = NewFT.getTypePtr()->getCanonicalTypeUnqualified();
+  }
+}
+
 /// Arrange the argument and result information for the declaration or
 /// definition of the given function.
 const CGFunctionInfo &
@@ -445,6 +636,8 @@ CodeGenTypes::arrangeFunctionDeclaration(const FunctionDecl *FD) {
   assert(isa<FunctionType>(FTy));
   setCUDAKernelCallingConvention(FTy, CGM, FD);
 
+  handleMetalVulkanEntryFunction(&FTy, nullptr, FD);
+
   // When declaring a function without a prototype, always use a
   // non-variadic type.
   if (CanQual<FunctionNoProtoType> noProto = FTy.getAs<FunctionNoProtoType>()) {
@@ -770,11 +963,14 @@ CodeGenTypes::arrangeLLVMFunctionInfo(CanQualType resultType,
   assert(inserted && "Recursively being processed?");
 
   // Compute ABI information.
-  if (CC == llvm::CallingConv::SPIR_KERNEL) {
+#if 0 // this is stupid ... rather: use a proper ABI implementation as before, which does the correct thing
+  if (CC == llvm::CallingConv::FLOOR_KERNEL) {
     // Force target independent argument handling for the host visible
     // kernel functions.
     computeSPIRKernelABIInfo(CGM, *FI);
-  } else if (info.getCC() == CC_Swift || info.getCC() == CC_SwiftAsync) {
+  } else
+#endif
+  if (info.getCC() == CC_Swift || info.getCC() == CC_SwiftAsync) {
     swiftcall::computeABIInfo(CGM, *FI);
   } else {
     getABIInfo().computeInfo(*FI);
@@ -854,6 +1050,10 @@ struct TypeExpansion {
     TEK_Record,
     // For complex types, real and imaginary parts are expanded recursively.
     TEK_Complex,
+    // Special libfloor vector compat expansion (aggregate -> clang/llvm vector).
+    TEK_FloorVectorCompat,
+    // Special libfloor aggregate/record expansion.
+    TEK_FloorAggregate,
     // All other types are not expandable.
     TEK_None
   };
@@ -898,6 +1098,31 @@ struct ComplexExpansion : TypeExpansion {
   }
 };
 
+struct FloorVectorCompatExpansion : TypeExpansion {
+  QualType orig_type;
+  QualType vector_type;
+
+  FloorVectorCompatExpansion(QualType orig_type_, QualType vector_type_)
+      : TypeExpansion(TEK_FloorVectorCompat), orig_type(orig_type_), vector_type(vector_type_) {}
+  static bool classof(const TypeExpansion *TE) {
+    return TE->Kind == TEK_FloorVectorCompat;
+  }
+};
+
+struct FloorAggregateExpansion : TypeExpansion {
+  SmallVector<const CXXBaseSpecifier *, 1> bases;
+  SmallVector<const FieldDecl *, 1> field_decls;
+  std::vector<CodeGenTypes::aggregate_scalar_entry> fields;
+
+  FloorAggregateExpansion(SmallVector<const CXXBaseSpecifier *, 1> &&bases_,
+                          SmallVector<const FieldDecl *, 1> &&field_decls_,
+                          std::vector<CodeGenTypes::aggregate_scalar_entry> &&fields_)
+      : TypeExpansion(TEK_FloorAggregate), bases(bases_), field_decls(field_decls_), fields(fields_) {}
+  static bool classof(const TypeExpansion *TE) {
+    return TE->Kind == TEK_FloorAggregate;
+  }
+};
+
 struct NoExpansion : TypeExpansion {
   NoExpansion() : TypeExpansion(TEK_None) {}
   static bool classof(const TypeExpansion *TE) {
@@ -907,12 +1132,61 @@ struct NoExpansion : TypeExpansion {
 }  // namespace
 
 static std::unique_ptr<TypeExpansion>
-getTypeExpansion(QualType Ty, const ASTContext &Context) {
+getTypeExpansion(QualType Ty, const ASTContext &Context,
+                 const CodeGenTypes& CGT, const CallingConv CC) {
   if (const ConstantArrayType *AT = Context.getAsConstantArrayType(Ty)) {
     return std::make_unique<ConstantArrayExpansion>(
         AT->getElementType(), AT->getSize().getZExtValue());
   }
-  if (const RecordType *RT = Ty->getAs<RecordType>()) {
+  const RecordType *RT = Ty->getAs<RecordType>();
+  const CXXRecordDecl* cxx_rdecl = (RT != nullptr ? RT->getAsCXXRecordDecl() : nullptr);
+  if (cxx_rdecl) {
+    // libfloor vector compat expansion (metal/vulkan vertex/fragment shader only, or vulkan compute shader)
+    if (cxx_rdecl->hasAttr<VectorCompatAttr>() &&
+        ((Context.getLangOpts().Metal && (CC == CallingConv::CC_FloorVertex ||
+                                          CC == CallingConv::CC_FloorFragment)) ||
+         (Context.getLangOpts().Vulkan && (CC == CallingConv::CC_FloorKernel ||
+                                           CC == CallingConv::CC_FloorVertex ||
+                                           CC == CallingConv::CC_FloorFragment)))) {
+      const auto vec_type = CGT.get_compat_vector_type(cxx_rdecl);
+      return std::make_unique<FloorVectorCompatExpansion>(Ty, vec_type);
+    }
+    // libfloor aggregate expansion:
+    // * any aggregate image type
+    // * any aggregate if calling a metal vertex/fragment shader function
+    // similar to (non-union) record expansion below, but also stores some additional information
+    if ((Ty->isAggregateImageType() ||
+         ((Context.getLangOpts().Metal && (CC == CallingConv::CC_FloorVertex ||
+                                           CC == CallingConv::CC_FloorFragment)) ||
+          (Context.getLangOpts().Vulkan && (CC == CallingConv::CC_FloorKernel ||
+                                            CC == CallingConv::CC_FloorVertex ||
+                                            CC == CallingConv::CC_FloorFragment)))) &&
+        !cxx_rdecl->isUnion()) {
+      SmallVector<const CXXBaseSpecifier *, 1> bases;
+      SmallVector<const FieldDecl *, 1> field_decls;
+
+      assert(!cxx_rdecl->isDynamicClass() &&
+             "cannot expand vtable pointers in dynamic classes");
+      for (const CXXBaseSpecifier &BS : cxx_rdecl->bases()) {
+        bases.push_back(&BS);
+      }
+
+      for (const auto *FD : cxx_rdecl->fields()) {
+        // Skip zero length bitfields.
+        if (FD->isBitField() && FD->getBitWidthValue(Context) == 0)
+          continue;
+        assert(!FD->isBitField() &&
+               "Cannot expand structure with bit-field members.");
+        field_decls.push_back(FD);
+      }
+
+      auto fields = CGT.get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl, false, false,
+                                                    !(Context.getLangOpts().Vulkan ||
+                                                      Context.getLangOpts().Metal));
+      return std::make_unique<FloorAggregateExpansion>(std::move(bases), std::move(field_decls), std::move(fields));
+    }
+  }
+  if (RT) {
     SmallVector<const CXXBaseSpecifier *, 1> Bases;
     SmallVector<const FieldDecl *, 1> Fields;
     const RecordDecl *RD = RT->getDecl();
@@ -962,17 +1236,24 @@ getTypeExpansion(QualType Ty, const ASTContext &Context) {
   return std::make_unique<NoExpansion>();
 }
 
-static int getExpansionSize(QualType Ty, const ASTContext &Context) {
-  auto Exp = getTypeExpansion(Ty, Context);
+static int getExpansionSize(QualType Ty, const ASTContext &Context,
+                            const CodeGenTypes& CGT, const CallingConv CC) {
+  auto Exp = getTypeExpansion(Ty, Context, CGT, CC);
   if (auto CAExp = dyn_cast<ConstantArrayExpansion>(Exp.get())) {
-    return CAExp->NumElts * getExpansionSize(CAExp->EltTy, Context);
+    return CAExp->NumElts * getExpansionSize(CAExp->EltTy, Context, CGT, CC);
+  }
+  if (isa<FloorVectorCompatExpansion>(Exp.get())) {
+    return 1;
+  }
+  if (auto FAExp = dyn_cast<FloorAggregateExpansion>(Exp.get())) {
+    return FAExp->fields.size();
   }
   if (auto RExp = dyn_cast<RecordExpansion>(Exp.get())) {
     int Res = 0;
     for (auto BS : RExp->Bases)
-      Res += getExpansionSize(BS->getType(), Context);
+      Res += getExpansionSize(BS->getType(), Context, CGT, CC);
     for (auto FD : RExp->Fields)
-      Res += getExpansionSize(FD->getType(), Context);
+      Res += getExpansionSize(FD->getType(), Context, CGT, CC);
     return Res;
   }
   if (isa<ComplexExpansion>(Exp.get()))
@@ -983,17 +1264,30 @@ static int getExpansionSize(QualType Ty, const ASTContext &Context) {
 
 void
 CodeGenTypes::getExpandedTypes(QualType Ty,
-                               SmallVectorImpl<llvm::Type *>::iterator &TI) {
-  auto Exp = getTypeExpansion(Ty, Context);
+                               SmallVectorImpl<llvm::Type *>::iterator &TI,
+                               const CallingConv CC) {
+  auto Exp = getTypeExpansion(Ty, Context, *this, CC);
   if (auto CAExp = dyn_cast<ConstantArrayExpansion>(Exp.get())) {
     for (int i = 0, n = CAExp->NumElts; i < n; i++) {
-      getExpandedTypes(CAExp->EltTy, TI);
+      getExpandedTypes(CAExp->EltTy, TI, CC);
+    }
+  } else if (auto FVCExp = dyn_cast<FloorVectorCompatExpansion>(Exp.get())) {
+    *TI++ = ConvertType(FVCExp->vector_type);
+  } else if (auto FAExp = dyn_cast<FloorAggregateExpansion>(Exp.get())) {
+    for(const auto& field : FAExp->fields) {
+      auto conv_type = ConvertType(field.type);;
+      if (field.type->isArrayImageType(false)) {
+        *TI++ = (!conv_type->isPointerTy() ?
+                  llvm::PointerType::get(conv_type, 0) : conv_type);
+      } else {
+        *TI++ = conv_type;
+      }
     }
   } else if (auto RExp = dyn_cast<RecordExpansion>(Exp.get())) {
     for (auto BS : RExp->Bases)
-      getExpandedTypes(BS->getType(), TI);
+      getExpandedTypes(BS->getType(), TI, CC);
     for (auto FD : RExp->Fields)
-      getExpandedTypes(FD->getType(), TI);
+      getExpandedTypes(FD->getType(), TI, CC);
   } else if (auto CExp = dyn_cast<ComplexExpansion>(Exp.get())) {
     llvm::Type *EltTy = ConvertType(CExp->EltTy);
     *TI++ = EltTy;
@@ -1020,17 +1314,81 @@ static void forConstantArrayExpansion(CodeGenFunction &CGF,
 }
 
 void CodeGenFunction::ExpandTypeFromArgs(QualType Ty, LValue LV,
-                                         llvm::Function::arg_iterator &AI) {
+                                         llvm::Function::arg_iterator &AI,
+                                         const CallingConv CC) {
   assert(LV.isSimple() &&
          "Unexpected non-simple lvalue during struct expansion.");
 
-  auto Exp = getTypeExpansion(Ty, getContext());
+  auto Exp = getTypeExpansion(Ty, getContext(), getTypes(), CC);
   if (auto CAExp = dyn_cast<ConstantArrayExpansion>(Exp.get())) {
     forConstantArrayExpansion(
         *this, CAExp, LV.getAddress(*this), [&](Address EltAddr) {
           LValue LV = MakeAddrLValue(EltAddr, CAExp->EltTy);
-          ExpandTypeFromArgs(CAExp->EltTy, LV, AI);
+          ExpandTypeFromArgs(CAExp->EltTy, LV, AI, CC);
         });
+  } else if (auto FVCExp = dyn_cast<FloorVectorCompatExpansion>(Exp.get())) {
+    LValue VecLV = MakeAddrLValue(LV.getAddress(*this), FVCExp->vector_type);
+    ExpandTypeFromArgs(FVCExp->vector_type, VecLV, AI, CC);
+  } else if (auto FAExp = dyn_cast<FloorAggregateExpansion>(Exp.get())) {
+    // TODO: should this recurse into bases with ExpandTypeFromArgs or do this manually?
+    Address This = LV.getAddress(*this);
+    const auto cxx_rdecl = Ty->getAsCXXRecordDecl();
+    llvm::Type* flattened_rdecl = nullptr;
+    if (cxx_rdecl) {
+      flattened_rdecl = getTypes().getFlattenedRecordType(cxx_rdecl);
+    }
+    if (flattened_rdecl) {
+      // -> handle as flat struct/rdecl
+      for (const auto& field : FAExp->fields) {
+        // TODO: non-image arrays -> these have no FD
+        if (field.field_decl) {
+          // array of images
+          if (field.type->isArrayImageType(false)) {
+            LValue SubLV = EmitLValueForField(LV, field.field_decl);
+            Builder.CreateStore(&*AI++, SubLV.getAddress(*this));
+          }
+          // all else
+          else {
+            LValue SubLV = EmitLValueForFieldInitialization(LV, field.field_decl);
+            ExpandTypeFromArgs(SubLV.getType(), SubLV, AI, CC);
+          }
+        } else {
+          // will probably fail, but still try -> TODO above
+          EmitStoreThroughLValue(RValue::get(&*AI++), LV);
+        }
+      }
+    } else {
+      for (const CXXBaseSpecifier *BS : FAExp->bases) {
+        // Perform a single step derived-to-base conversion.
+        Address Base =
+        GetAddressOfBaseClass(This, Ty->getAsCXXRecordDecl(), &BS, &BS + 1,
+                  /*NullCheckValue=*/false, SourceLocation());
+        LValue SubLV = MakeAddrLValue(Base, BS->getType());
+        
+        // Recurse onto bases.
+        ExpandTypeFromArgs(BS->getType(), SubLV, AI, CC);
+      }
+      
+      for(const auto& field : FAExp->fields) {
+        if(field.is_in_base) continue; // already handled
+        // TODO: non-image arrays -> these have no FD
+        if (field.field_decl) {
+          // array of images
+          if (field.type->isArrayImageType(false)) {
+            LValue SubLV = EmitLValueForField(LV, field.field_decl);
+            Builder.CreateStore(&*AI++, SubLV.getAddress(*this));
+          }
+          // all else
+          else {
+            LValue SubLV = EmitLValueForFieldInitialization(LV, field.field_decl);
+            ExpandTypeFromArgs(SubLV.getType(), SubLV, AI, CC);
+          }
+        } else {
+          // will probably fail, but still try -> TODO above
+          EmitStoreThroughLValue(RValue::get(&*AI++), LV);
+        }
+      }
+    }
   } else if (auto RExp = dyn_cast<RecordExpansion>(Exp.get())) {
     Address This = LV.getAddress(*this);
     for (const CXXBaseSpecifier *BS : RExp->Bases) {
@@ -1041,12 +1399,12 @@ void CodeGenFunction::ExpandTypeFromArgs(QualType Ty, LValue LV,
       LValue SubLV = MakeAddrLValue(Base, BS->getType());
 
       // Recurse onto bases.
-      ExpandTypeFromArgs(BS->getType(), SubLV, AI);
+      ExpandTypeFromArgs(BS->getType(), SubLV, AI, CC);
     }
     for (auto FD : RExp->Fields) {
       // FIXME: What are the right qualifiers here?
       LValue SubLV = EmitLValueForFieldInitialization(LV, FD);
-      ExpandTypeFromArgs(FD->getType(), SubLV, AI);
+      ExpandTypeFromArgs(FD->getType(), SubLV, AI, CC);
     }
   } else if (isa<ComplexExpansion>(Exp.get())) {
     auto realValue = &*AI++;
@@ -1065,8 +1423,9 @@ void CodeGenFunction::ExpandTypeFromArgs(QualType Ty, LValue LV,
 
 void CodeGenFunction::ExpandTypeToArgs(
     QualType Ty, CallArg Arg, llvm::FunctionType *IRFuncTy,
-    SmallVectorImpl<llvm::Value *> &IRCallArgs, unsigned &IRCallArgPos) {
-  auto Exp = getTypeExpansion(Ty, getContext());
+    SmallVectorImpl<llvm::Value *> &IRCallArgs, unsigned &IRCallArgPos,
+    const CallingConv CC) {
+  auto Exp = getTypeExpansion(Ty, getContext(), getTypes(), CC);
   if (auto CAExp = dyn_cast<ConstantArrayExpansion>(Exp.get())) {
     Address Addr = Arg.hasLValue() ? Arg.getKnownLValue().getAddress(*this)
                                    : Arg.getKnownRValue().getAggregateAddress();
@@ -1076,8 +1435,41 @@ void CodeGenFunction::ExpandTypeToArgs(
               convertTempToRValue(EltAddr, CAExp->EltTy, SourceLocation()),
               CAExp->EltTy);
           ExpandTypeToArgs(CAExp->EltTy, EltArg, IRFuncTy, IRCallArgs,
-                           IRCallArgPos);
+                           IRCallArgPos, CC);
         });
+  } else if (auto FVCExp = dyn_cast<FloorVectorCompatExpansion>(Exp.get())) {
+    const auto llvm_vec_type = getTypes().ConvertType(FVCExp->vector_type);
+    Address This = Arg.hasLValue() ? Arg.getKnownLValue().getAddress(*this)
+                                   : Arg.getKnownRValue().getAggregateAddress();
+    auto vec_ptr = Builder.CreateBitCast(This.getPointer(),
+                                         llvm::PointerType::get(llvm_vec_type,
+                                                                getContext().getTargetAddressSpace(Ty.getAddressSpace())));
+    Address vec_ptr_addr(vec_ptr, This.getAlignment());
+    IRCallArgs[IRCallArgPos++] = Builder.CreateLoad(vec_ptr_addr);
+  } else if (auto FAExp = dyn_cast<FloorAggregateExpansion>(Exp.get())) {
+    // TODO: should this recurse into bases with ExpandTypeToArgs or do this manually?
+    Address This = Arg.hasLValue() ? Arg.getKnownLValue().getAddress(*this)
+                                   : Arg.getKnownRValue().getAggregateAddress();
+    for (const CXXBaseSpecifier *BS : FAExp->bases) {
+      // Perform a single step derived-to-base conversion.
+      Address Base =
+          GetAddressOfBaseClass(This, Ty->getAsCXXRecordDecl(), &BS, &BS + 1,
+                                /*NullCheckValue=*/false, SourceLocation());
+      CallArg BaseArg = CallArg(RValue::getAggregate(Base), BS->getType());
+
+      // Recurse onto bases.
+      ExpandTypeToArgs(BS->getType(), BaseArg, IRFuncTy, IRCallArgs,
+                       IRCallArgPos, CC);
+    }
+
+    LValue LV = MakeAddrLValue(This, Ty);
+    for(const auto& field : FAExp->fields) {
+      if(field.is_in_base) continue; // already handled
+      // TODO: arrays -> these have no FD
+      CallArg FldArg = CallArg(EmitRValueForField(LV, field.field_decl, SourceLocation()),
+                               field.field_decl->getType());
+      ExpandTypeToArgs(field.field_decl->getType(), FldArg, IRFuncTy, IRCallArgs, IRCallArgPos, CC);
+    }
   } else if (auto RExp = dyn_cast<RecordExpansion>(Exp.get())) {
     Address This = Arg.hasLValue() ? Arg.getKnownLValue().getAddress(*this)
                                    : Arg.getKnownRValue().getAggregateAddress();
@@ -1090,7 +1482,7 @@ void CodeGenFunction::ExpandTypeToArgs(
 
       // Recurse onto bases.
       ExpandTypeToArgs(BS->getType(), BaseArg, IRFuncTy, IRCallArgs,
-                       IRCallArgPos);
+                       IRCallArgPos, CC);
     }
 
     LValue LV = MakeAddrLValue(This, Ty);
@@ -1098,7 +1490,7 @@ void CodeGenFunction::ExpandTypeToArgs(
       CallArg FldArg =
           CallArg(EmitRValueForField(LV, FD, SourceLocation()), FD->getType());
       ExpandTypeToArgs(FD->getType(), FldArg, IRFuncTy, IRCallArgs,
-                       IRCallArgPos);
+                       IRCallArgPos, CC);
     }
   } else if (isa<ComplexExpansion>(Exp.get())) {
     ComplexPairTy CV = Arg.getKnownRValue().getComplexVal();
@@ -1305,6 +1697,11 @@ static llvm::Value *CreateCoercedLoad(Address Src, llvm::Type *Ty,
   return CGF.Builder.CreateLoad(Tmp);
 }
 
+static void CreateCoercedStore(llvm::Value *Src,
+                               Address Dst,
+                               bool DstIsVolatile,
+                               CodeGenFunction &CGF);
+
 // Function to store a first-class aggregate into memory.  We prefer to
 // store the elements rather than the aggregate to be more friendly to
 // fast-isel.
@@ -1316,6 +1713,13 @@ void CodeGenFunction::EmitAggregateStore(llvm::Value *Val, Address Dest,
     for (unsigned i = 0, e = STy->getNumElements(); i != e; ++i) {
       Address EltPtr = Builder.CreateStructGEP(Dest, i);
       llvm::Value *Elt = Builder.CreateExtractValue(Val, i);
+
+      // handle [[vector_compat]] stores from an aggregate to a vector type
+      if (EltPtr.getType()->getPointerElementType()->isVectorTy()) {
+        CreateCoercedStore(Elt, EltPtr, DestIsVolatile, *this);
+        continue;
+      }
+
       Builder.CreateStore(Elt, EltPtr, DestIsVolatile);
     }
   } else {
@@ -1329,10 +1733,10 @@ void CodeGenFunction::EmitAggregateStore(llvm::Value *Val, Address Dest,
 ///
 /// This safely handles the case when the src type is larger than the
 /// destination type; the upper bits of the src will be lost.
-static void CreateCoercedStore(llvm::Value *Src,
-                               Address Dst,
-                               bool DstIsVolatile,
-                               CodeGenFunction &CGF) {
+void CreateCoercedStore(llvm::Value *Src,
+                        Address Dst,
+                        bool DstIsVolatile,
+                        CodeGenFunction &CGF) {
   llvm::Type *SrcTy = Src->getType();
   llvm::Type *DstTy = Dst.getElementType();
   if (SrcTy == DstTy) {
@@ -1431,10 +1835,10 @@ class ClangToLLVMArgMapping {
 
 public:
   ClangToLLVMArgMapping(const ASTContext &Context, const CGFunctionInfo &FI,
-                        bool OnlyRequiredArgs = false)
+                        const CodeGenTypes& CGT, bool OnlyRequiredArgs = false)
       : InallocaArgNo(InvalidIndex), SRetArgNo(InvalidIndex), TotalIRArgs(0),
         ArgInfo(OnlyRequiredArgs ? FI.getNumRequiredArgs() : FI.arg_size()) {
-    construct(Context, FI, OnlyRequiredArgs);
+    construct(Context, FI, CGT, OnlyRequiredArgs);
   }
 
   bool hasInallocaArg() const { return InallocaArgNo != InvalidIndex; }
@@ -1470,11 +1874,12 @@ public:
 
 private:
   void construct(const ASTContext &Context, const CGFunctionInfo &FI,
-                 bool OnlyRequiredArgs);
+                 const CodeGenTypes& CGT, bool OnlyRequiredArgs);
 };
 
 void ClangToLLVMArgMapping::construct(const ASTContext &Context,
                                       const CGFunctionInfo &FI,
+                                      const CodeGenTypes& CGT,
                                       bool OnlyRequiredArgs) {
   unsigned IRArgNo = 0;
   bool SwapThisWithSRet = false;
@@ -1523,7 +1928,7 @@ void ClangToLLVMArgMapping::construct(const ASTContext &Context,
       IRArgs.NumberOfArgs = AI.getCoerceAndExpandTypeSequence().size();
       break;
     case ABIArgInfo::Expand:
-      IRArgs.NumberOfArgs = getExpansionSize(ArgType, Context);
+      IRArgs.NumberOfArgs = getExpansionSize(ArgType, Context, CGT, FI.getASTCallingConvention());
       break;
     }
 
@@ -1632,7 +2037,7 @@ CodeGenTypes::GetFunctionType(const CGFunctionInfo &FI) {
     break;
   }
 
-  ClangToLLVMArgMapping IRFunctionArgs(getContext(), FI, true);
+  ClangToLLVMArgMapping IRFunctionArgs(getContext(), FI, *this, true);
   SmallVector<llvm::Type*, 8> ArgTypes(IRFunctionArgs.totalIRArgs());
 
   // Add type for sret argument.
@@ -1714,7 +2119,7 @@ CodeGenTypes::GetFunctionType(const CGFunctionInfo &FI) {
 
     case ABIArgInfo::Expand:
       auto ArgTypesIter = ArgTypes.begin() + FirstIRArg;
-      getExpandedTypes(it->type, ArgTypesIter);
+      getExpandedTypes(it->type, ArgTypesIter, FI.getASTCallingConvention());
       assert(ArgTypesIter == ArgTypes.begin() + FirstIRArg + NumIRArgs);
       break;
     }
@@ -2132,7 +2537,7 @@ void CodeGenModule::ConstructAttributeList(
                                  NumElemsParam);
     }
 
-    if (TargetDecl->hasAttr<OpenCLKernelAttr>()) {
+    if (TargetDecl->hasAttr<ComputeKernelAttr>()) {
       if (getLangOpts().OpenCLVersion <= 120) {
         // OpenCL v1.2 Work groups are always uniform
         FuncAttrs.addAttribute("uniform-work-group-size", "true");
@@ -2229,7 +2634,7 @@ void CodeGenModule::ConstructAttributeList(
   }
 
   // Collect attributes from arguments and return values.
-  ClangToLLVMArgMapping IRFunctionArgs(getContext(), FI);
+  ClangToLLVMArgMapping IRFunctionArgs(getContext(), FI, getTypes());
 
   QualType RetTy = FI.getReturnType();
   const ABIArgInfo &RetAI = FI.getReturnInfo();
@@ -2625,7 +3030,7 @@ void CodeGenFunction::EmitFunctionProlog(const CGFunctionInfo &FI,
   // FIXME: We no longer need the types from FunctionArgList; lift up and
   // simplify.
 
-  ClangToLLVMArgMapping IRFunctionArgs(CGM.getContext(), FI);
+  ClangToLLVMArgMapping IRFunctionArgs(CGM.getContext(), FI, getTypes());
   assert(Fn->arg_size() == IRFunctionArgs.totalIRArgs());
 
   // If we're using inalloca, all the memory arguments are GEPs off of the last
@@ -2979,7 +3384,7 @@ void CodeGenFunction::EmitFunctionProlog(const CGFunctionInfo &FI,
       ArgVals.push_back(ParamValue::forIndirect(Alloca));
 
       auto FnArgIter = Fn->arg_begin() + FirstIRArg;
-      ExpandTypeFromArgs(Ty, LV, FnArgIter);
+      ExpandTypeFromArgs(Ty, LV, FnArgIter, FI.getASTCallingConvention());
       assert(FnArgIter == Fn->arg_begin() + FirstIRArg + NumIRArgs);
       for (unsigned i = 0, e = NumIRArgs; i != e; ++i) {
         auto AI = Fn->getArg(FirstIRArg + i);
@@ -4649,6 +5054,7 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
   }
 
 #ifndef NDEBUG
+#if 0 // we don't want this (address spaces may not match)
   if (!(CallInfo.isVariadic() && CallInfo.getArgStruct())) {
     // For an inalloca varargs function, we don't expect CallInfo to match the
     // function pointer's type, because the inalloca struct a will have extra
@@ -4665,6 +5071,7 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
           Callee.getFunctionPointer()->getType()->getPointerElementType();
     assert(IRFuncTy == TypeFromVal);
   }
+#endif
 #endif
 
   // 1. Set up the arguments.
@@ -4690,7 +5097,7 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
     ArgMemory = Address(AI, Align);
   }
 
-  ClangToLLVMArgMapping IRFunctionArgs(CGM.getContext(), CallInfo);
+  ClangToLLVMArgMapping IRFunctionArgs(CGM.getContext(), CallInfo, getTypes());
   SmallVector<llvm::Value *, 16> IRCallArgs(IRFunctionArgs.totalIRArgs());
 
   // If the call returns a temporary with struct return, create a temporary
@@ -4933,8 +5340,14 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
         // If the argument doesn't match, perform a bitcast to coerce it.  This
         // can happen due to trivial type mismatches.
         if (FirstIRArg < IRFuncTy->getNumParams() &&
-            V->getType() != IRFuncTy->getParamType(FirstIRArg))
-          V = Builder.CreateBitCast(V, IRFuncTy->getParamType(FirstIRArg));
+            V->getType() != IRFuncTy->getParamType(FirstIRArg)) {
+          const auto src_as = V->getType()->getPointerAddressSpace();
+          auto param_type = IRFuncTy->getParamType(FirstIRArg);
+          if(src_as > 0 && src_as != param_type->getPointerAddressSpace()) {
+            param_type = llvm::PointerType::get(cast<llvm::PointerType>(param_type->getScalarType())->getElementType(), src_as);
+          }
+          V = Builder.CreateBitCast(V, param_type);
+        }
 
         IRCallArgs[FirstIRArg] = V;
         break;
@@ -5055,7 +5468,8 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
 
     case ABIArgInfo::Expand: {
       unsigned IRArgPos = FirstIRArg;
-      ExpandTypeToArgs(I->Ty, *I, IRFuncTy, IRCallArgs, IRArgPos);
+      ExpandTypeToArgs(I->Ty, *I, IRFuncTy, IRCallArgs, IRArgPos,
+                       CallInfo.getASTCallingConvention());
       assert(IRArgPos == FirstIRArg + NumIRArgs);
       break;
     }
@@ -5161,8 +5575,17 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
     if (IRFunctionArgs.hasInallocaArg() &&
         i == IRFunctionArgs.getInallocaArgNo())
       continue;
-    if (i < IRFuncTy->getNumParams())
+    if (i < IRFuncTy->getNumParams()) {
+      if (getLangOpts().OpenCL &&
+          IRFuncTy->getParamType(i)->isPointerTy() &&
+          IRCallArgs[i]->getType()->isPointerTy() &&
+          llvm::PointerType::get(IRFuncTy->getParamType(i)->getPointerElementType(), 0) ==
+          llvm::PointerType::get(IRCallArgs[i]->getType()->getPointerElementType(), 0)) {
+        // ignore address space mismatches for OpenCL/Metal/Vulkan
+        continue;
+      }
       assert(IRCallArgs[i]->getType() == IRFuncTy->getParamType(i));
+    }
   }
 #endif
 
@@ -5458,6 +5881,13 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
             DestPtr = CreateMemTemp(RetTy, "agg.tmp");
             DestIsVolatile = false;
           }
+
+          // handle [[vector_compat]] stores from an aggregate to a vector type
+          if(DestPtr.getType()->getPointerElementType()->isVectorTy()) {
+            CreateCoercedStore(CI, DestPtr, DestIsVolatile, *this);
+            return RValue::get(DestPtr.getPointer());
+          }
+
           EmitAggregateStore(CI, DestPtr, DestIsVolatile);
           return RValue::getAggregate(DestPtr);
         }
@@ -5502,6 +5932,27 @@ RValue CodeGenFunction::EmitCall(const CGFunctionInfo &CallInfo,
     AllocAlignAttrEmitter.EmitAsAnAssumption(Loc, RetTy, Ret);
   }
 
+  // add LLVM [lower bound, upper bound] attribute if possible
+  if (llvm::CallInst *Call = dyn_cast<llvm::CallInst>(CI)) {
+    if (TargetDecl && Call->getType()->isIntegerTy()) {
+      RetRangeAttr* range_attr = TargetDecl->getAttr<RetRangeAttr>();
+      if (range_attr != nullptr) {
+        llvm::APSInt lower_bound(64), upper_bound(64);
+        lower_bound = range_attr->getLowerBound()->EvaluateKnownConstInt(getContext());
+        upper_bound = range_attr->getUpperBound()->EvaluateKnownConstInt(getContext());
+        llvm::Metadata* range_md[2] {
+          llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(Call->getType(),
+                                                               lower_bound.getZExtValue(),
+                                                               lower_bound.isSigned())),
+          llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(Call->getType(),
+                                                               upper_bound.getZExtValue(),
+                                                               upper_bound.isSigned()))
+        };
+        Call->setMetadata(llvm::LLVMContext::MD_range, llvm::MDNode::get(getLLVMContext(), range_md));
+      }
+    }
+  }
+
   // Explicitly call CallLifetimeEnd::Emit just to re-use the code even though
   // we can't use the full cleanup mechanism.
   for (CallLifetimeEnd &LifetimeEnd : CallLifetimeEndAfterCall)
diff --git a/clang/lib/CodeGen/CGClass.cpp b/clang/lib/CodeGen/CGClass.cpp
index 0df64d4d5d26..171e25b9acf1 100644
--- a/clang/lib/CodeGen/CGClass.cpp
+++ b/clang/lib/CodeGen/CGClass.cpp
@@ -220,7 +220,10 @@ CodeGenFunction::GetAddressOfDirectBaseInCompleteClass(Address This,
                                                    const CXXRecordDecl *Base,
                                                    bool BaseIsVirtual) {
   // 'this' must be a pointer (in some address space) to Derived.
-  assert(This.getElementType() == ConvertType(Derived));
+  // NOTE: element type might be flattend/packed, so if ConvertType() fails,
+  //       check if it is the corresponding flattened record type
+  assert(This.getElementType() == ConvertType(Derived) ||
+         This.getElementType() == CGM.getTypes().getFlattenedRecordType(Derived));
 
   // Compute the offset of the virtual base.
   CharUnits Offset;
@@ -270,8 +273,12 @@ ApplyNonVirtualAndVirtualOffset(CodeGenFunction &CGF, Address addr,
 
   // Apply the base offset.
   llvm::Value *ptr = addr.getPointer();
+#if 0 // we don't want this
   unsigned AddrSpace = ptr->getType()->getPointerAddressSpace();
   ptr = CGF.Builder.CreateBitCast(ptr, CGF.Int8Ty->getPointerTo(AddrSpace));
+#else
+  ptr = CGF.Builder.CreateBitCast(ptr, CGF.Int8PtrTy);
+#endif
   ptr = CGF.Builder.CreateInBoundsGEP(CGF.Int8Ty, ptr, baseOffset, "add.ptr");
 
   // If we have a virtual component, the alignment of the result will
@@ -325,7 +332,7 @@ Address CodeGenFunction::GetAddressOfBaseClass(
     VBase = nullptr; // we no longer have a virtual step
   }
 
-  // Get the base pointer type.
+  // Get the base pointer type, and keep the Values address space if it has one
   llvm::Type *BasePtrTy =
       ConvertType((PathEnd[-1])->getType())
           ->getPointerTo(Value.getType()->getPointerAddressSpace());
@@ -406,9 +413,13 @@ CodeGenFunction::GetAddressOfDerivedClass(Address BaseAddr,
 
   QualType DerivedTy =
     getContext().getCanonicalType(getContext().getTagDeclType(Derived));
+#if 0 // we don't want this
   unsigned AddrSpace =
     BaseAddr.getPointer()->getType()->getPointerAddressSpace();
   llvm::Type *DerivedPtrTy = ConvertType(DerivedTy)->getPointerTo(AddrSpace);
+#else
+  llvm::Type *DerivedPtrTy = ConvertType(DerivedTy)->getPointerTo();
+#endif
 
   llvm::Value *NonVirtualOffset =
     CGM.GetNonVirtualBaseClassOffset(Derived, PathBegin, PathEnd);
@@ -2060,6 +2071,7 @@ void CodeGenFunction::EmitCXXConstructorCall(const CXXConstructorDecl *D,
                                              AggValueSlot ThisAVS,
                                              const CXXConstructExpr *E) {
   CallArgList Args;
+#if 0 // we don't want this
   Address This = ThisAVS.getAddress();
   LangAS SlotAS = ThisAVS.getQualifiers().getAddressSpace();
   QualType ThisType = D->getThisType();
@@ -2073,6 +2085,10 @@ void CodeGenFunction::EmitCXXConstructorCall(const CXXConstructorDecl *D,
     ThisPtr = getTargetHooks().performAddrSpaceCast(*this, This.getPointer(),
                                                     ThisAS, SlotAS, NewType);
   }
+#else
+  Address This = ThisAVS.getAddress();
+  llvm::Value *ThisPtr = This.getPointer();
+#endif
 
   // Push the this ptr.
   Args.add(RValue::get(ThisPtr), D->getThisType());
@@ -2517,8 +2533,13 @@ void CodeGenFunction::InitializeVTablePointer(const VPtr &Vptr) {
 
   // Finally, store the address point. Use the same LLVM types as the field to
   // support optimization.
+#if 0 // we don't want this
   unsigned GlobalsAS = CGM.getDataLayout().getDefaultGlobalsAddressSpace();
   unsigned ProgAS = CGM.getDataLayout().getProgramAddressSpace();
+#else
+  constexpr unsigned GlobalsAS = 0;
+  constexpr unsigned ProgAS = 0;
+#endif
   llvm::Type *VTablePtrTy =
       llvm::FunctionType::get(CGM.Int32Ty, /*isVarArg=*/true)
           ->getPointerTo(ProgAS)
diff --git a/clang/lib/CodeGen/CGDebugInfo.cpp b/clang/lib/CodeGen/CGDebugInfo.cpp
index 6e477d40b83e..8547472bc49d 100644
--- a/clang/lib/CodeGen/CGDebugInfo.cpp
+++ b/clang/lib/CodeGen/CGDebugInfo.cpp
@@ -567,7 +567,17 @@ void CGDebugInfo::CreateCompileUnit() {
     LangTag = llvm::dwarf::DW_LANG_C89;
   }
 
-  std::string Producer = getClangFullVersion();
+  std::string Producer;
+  if (!CGM.getLangOpts().Metal) {
+    Producer = getClangFullVersion();
+  } else {
+    // override producer when targeting Metal
+    if (CGM.getLangOpts().MetalVersion < 240) {
+      Producer = "Apple LLVM version 31001.143 (metalfe-31001.143)";
+    } else {
+      Producer = "Apple metal version 31001.363 (metalfe-31001.363)";
+    }
+  }
 
   // Figure out which version of the ObjC runtime we have.
   unsigned RuntimeVers = 0;
@@ -600,11 +610,27 @@ void CGDebugInfo::CreateCompileUnit() {
   // file. Its directory part specifies what becomes the
   // DW_AT_comp_dir (the compilation directory), even if the source
   // file was specified with an absolute path.
+  // NOTE: however, for Metal we always need to specify the absolute path
   if (CSKind)
     CSInfo.emplace(*CSKind, Checksum);
+  std::string cu_file_name;
+  if (!CGM.getLangOpts().Metal) {
+    cu_file_name = remapDIPath(MainFileName);
+  } else {
+    cu_file_name = MainFileName;
+    SmallVector<char> src_file_name(cu_file_name.size());
+    src_file_name.assign(cu_file_name.begin(), cu_file_name.end());
+    llvm::sys::fs::make_absolute(src_file_name);
+    cu_file_name.resize(src_file_name.size(), '\0');
+    cu_file_name.assign(src_file_name.begin(), src_file_name.end());
+  }
   llvm::DIFile *CUFile = DBuilder.createFile(
-      remapDIPath(MainFileName), remapDIPath(getCurrentDirname()), CSInfo,
+      cu_file_name, remapDIPath(getCurrentDirname()), CSInfo,
       getSource(SM, SM.getMainFileID()));
+  if (CGM.getLangOpts().Metal) {
+    // cache DIFile, so we don't emit it twice
+    DIFileCache[CUFile->getFilename().data()].reset(CUFile);
+  }
 
   StringRef Sysroot, SDK;
   if (CGM.getCodeGenOpts().getDebuggerTuning() == llvm::DebuggerKind::LLDB) {
@@ -686,7 +712,7 @@ llvm::DIType *CGDebugInfo::CreateType(const BuiltinType *BT) {
 
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix)                   \
   case BuiltinType::Id:                                                        \
-    return getOrCreateStructPtrType("opencl_" #ImgType "_" #Suffix "_t",       \
+    return getOrCreateStructPtrType("opencl_" #ImgType #Suffix "_t",           \
                                     SingletonId);
 #include "clang/Basic/OpenCLImageTypes.def"
   case BuiltinType::OCLSampler:
@@ -1310,9 +1336,11 @@ static unsigned getDwarfCC(CallingConv CC) {
     return llvm::dwarf::DW_CC_LLVM_AAPCS_VFP;
   case CC_IntelOclBicc:
     return llvm::dwarf::DW_CC_LLVM_IntelOclBicc;
-  case CC_SpirFunction:
+  case CC_FloorFunction:
     return llvm::dwarf::DW_CC_LLVM_SpirFunction;
-  case CC_OpenCLKernel:
+  case CC_FloorKernel:
+  case CC_FloorVertex:
+  case CC_FloorFragment:
     return llvm::dwarf::DW_CC_LLVM_OpenCLKernel;
   case CC_Swift:
     return llvm::dwarf::DW_CC_LLVM_Swift;
@@ -3120,6 +3148,8 @@ llvm::DIType *CGDebugInfo::CreateType(const AtomicType *Ty, llvm::DIFile *U) {
 }
 
 llvm::DIType *CGDebugInfo::CreateType(const PipeType *Ty, llvm::DIFile *U) {
+  // Ignore the atomic wrapping
+  // FIXME: What is the correct representation?
   return getOrCreateType(Ty->getElementType(), U);
 }
 
diff --git a/clang/lib/CodeGen/CGDebugInfo.h b/clang/lib/CodeGen/CGDebugInfo.h
index d6b9bdd4839c..e5015ca50c0d 100644
--- a/clang/lib/CodeGen/CGDebugInfo.h
+++ b/clang/lib/CodeGen/CGDebugInfo.h
@@ -30,6 +30,7 @@
 #include "llvm/IR/DebugInfo.h"
 #include "llvm/IR/ValueHandle.h"
 #include "llvm/Support/Allocator.h"
+#include <unordered_map>
 
 namespace llvm {
 class MDNode;
@@ -150,7 +151,7 @@ class CGDebugInfo {
   llvm::BumpPtrAllocator DebugInfoNames;
   StringRef CWDName;
 
-  llvm::DenseMap<const char *, llvm::TrackingMDRef> DIFileCache;
+  std::unordered_map<std::string, llvm::TrackingMDRef> DIFileCache;
   llvm::DenseMap<const FunctionDecl *, llvm::TrackingMDRef> SPCache;
   /// Cache declarations relevant to DW_TAG_imported_declarations (C++
   /// using declarations) that aren't covered by other more specific caches.
diff --git a/clang/lib/CodeGen/CGDecl.cpp b/clang/lib/CodeGen/CGDecl.cpp
index dfb74a3fc654..be4819239210 100644
--- a/clang/lib/CodeGen/CGDecl.cpp
+++ b/clang/lib/CodeGen/CGDecl.cpp
@@ -214,8 +214,14 @@ void CodeGenFunction::EmitVarDecl(const VarDecl &D) {
 }
 
 static std::string getStaticDeclName(CodeGenModule &CGM, const VarDecl &D) {
-  if (CGM.getLangOpts().CPlusPlus)
+  // don't cxx mangle OpenCL "local" variables (only affects SPIR - Metal/AIR and SPIR-V/Vulkan uses cxx mangling)
+  if (CGM.getLangOpts().CPlusPlus &&
+      !(D.getType().getAddressSpace() == LangAS::opencl_local &&
+        CGM.getContext().getLangOpts().OpenCL &&
+        !CGM.getContext().getLangOpts().Metal &&
+        !CGM.getContext().getLangOpts().Vulkan)) {
     return CGM.getMangledName(&D).str();
+  }
 
   // If this isn't C++, we don't need a mangled name, just a pretty one.
   assert(!D.isExternallyVisible() && "name shouldn't matter");
diff --git a/clang/lib/CodeGen/CGDeclCXX.cpp b/clang/lib/CodeGen/CGDeclCXX.cpp
index d22f9dc3b68c..99709c86e653 100644
--- a/clang/lib/CodeGen/CGDeclCXX.cpp
+++ b/clang/lib/CodeGen/CGDeclCXX.cpp
@@ -118,7 +118,7 @@ static void EmitDeclDestroy(CodeGenFunction &CGF, const VarDecl &D,
     CXXDestructorDecl *Dtor = Record->getDestructor();
 
     Func = CGM.getAddrAndTypeOfCXXStructor(GlobalDecl(Dtor, Dtor_Complete));
-    if (CGF.getContext().getLangOpts().OpenCL) {
+    if (CGF.getContext().getLangOpts().OpenCL && !CGF.getContext().getLangOpts().CPlusPlus) {
       auto DestAS =
           CGM.getTargetCodeGenInfo().getAddrSpaceOfCxaAtexitPtrParam();
       auto DestTy = CGF.getTypes().ConvertType(Type)->getPointerTo(
@@ -685,6 +685,11 @@ CodeGenModule::EmitCXXGlobalInitFunc() {
   if (getCXXABI().useSinitAndSterm() && CXXGlobalInits.empty())
     return;
 
+  if (getLangOpts().OpenCL && !CXXGlobalInits.empty()) {
+    assert(false && "unsupported");
+    getDiags().getCustomDiagID(DiagnosticsEngine::Fatal, "global C++ init functions are not supported");
+  }
+
   // Include the filename in the symbol name. Including "sub_" matches gcc
   // and makes sure these symbols appear lexicographically behind the symbols
   // with priority emitted above.
@@ -703,10 +708,12 @@ CodeGenModule::EmitCXXGlobalInitFunc() {
   // However it seems global destruction has little meaning without any
   // dynamic resource allocation on the device and program scope variables are
   // destroyed by the runtime when program is released.
+#if 0 // nope
   if (getLangOpts().OpenCL) {
     GenOpenCLArgMetadata(Fn);
-    Fn->setCallingConv(llvm::CallingConv::SPIR_KERNEL);
+    Fn->setCallingConv(llvm::CallingConv::FLOOR_KERNEL);
   }
+#endif
 
   assert(!getLangOpts().CUDA || !getLangOpts().CUDAIsDevice ||
          getLangOpts().GPUAllowDeviceInit);
diff --git a/clang/lib/CodeGen/CGExpr.cpp b/clang/lib/CodeGen/CGExpr.cpp
index e73782f2d317..5f60a26c6406 100644
--- a/clang/lib/CodeGen/CGExpr.cpp
+++ b/clang/lib/CodeGen/CGExpr.cpp
@@ -140,7 +140,15 @@ Address CodeGenFunction::CreateMemTemp(QualType Ty, const Twine &Name,
 
 Address CodeGenFunction::CreateMemTemp(QualType Ty, CharUnits Align,
                                        const Twine &Name, Address *Alloca) {
-  Address Result = CreateTempAlloca(ConvertTypeForMem(Ty), Align, Name,
+  // if "Ty" has a flattened struct record, use that instead of the default LLVM type
+  llvm::Type* llvm_type = nullptr;
+  if (const auto cxx_rdecl = Ty->getAsCXXRecordDecl(); cxx_rdecl) {
+    llvm_type = getTypes().getFlattenedRecordType(cxx_rdecl);
+  }
+  if (!llvm_type) {
+    llvm_type = ConvertTypeForMem(Ty);
+  }
+  Address Result = CreateTempAlloca(llvm_type, Align, Name,
                                     /*ArraySize=*/nullptr, Alloca);
 
   if (Ty->isConstantMatrixType()) {
@@ -1699,6 +1707,7 @@ llvm::Value *CodeGenFunction::EmitLoadOfScalar(Address Addr, bool Volatile,
                                                LValueBaseInfo BaseInfo,
                                                TBAAAccessInfo TBAAInfo,
                                                bool isNontemporal) {
+#if 0 // incorrect and not at all beneficial for compute backends
   if (!CGM.getCodeGenOpts().PreserveVec3Type) {
     // For better performance, handle vector loads differently.
     if (Ty->isVectorType()) {
@@ -1722,6 +1731,7 @@ llvm::Value *CodeGenFunction::EmitLoadOfScalar(Address Addr, bool Volatile,
       }
     }
   }
+#endif
 
   // Atomic operations have to be done on integral types.
   LValue AtomicLValue =
@@ -1820,6 +1830,7 @@ void CodeGenFunction::EmitStoreOfScalar(llvm::Value *Value, Address Addr,
     // Handle vectors differently to get better performance.
     if (Ty->isVectorType()) {
       llvm::Type *SrcTy = Value->getType();
+#if 0 // incorrect and not at all beneficial for compute backends
       auto *VecTy = dyn_cast<llvm::VectorType>(SrcTy);
       // Handle vec3 special.
       if (VecTy && cast<llvm::FixedVectorType>(VecTy)->getNumElements() == 3) {
@@ -1828,6 +1839,7 @@ void CodeGenFunction::EmitStoreOfScalar(llvm::Value *Value, Address Addr,
                                             "extractVec");
         SrcTy = llvm::FixedVectorType::get(VecTy->getElementType(), 4);
       }
+#endif
       if (Addr.getElementType() != SrcTy) {
         Addr = Builder.CreateElementBitCast(Addr, SrcTy, "storetmp");
       }
@@ -4228,14 +4240,39 @@ static Address emitAddrOfZeroSizeField(CodeGenFunction &CGF, Address Base,
 ///
 /// The resulting address doesn't necessarily have the right type.
 static Address emitAddrOfFieldStorage(CodeGenFunction &CGF, Address base,
-                                      const FieldDecl *field) {
+                                      const FieldDecl *field, llvm::Type* elem_type) {
   if (field->isZeroSize(CGF.getContext()))
     return emitAddrOfZeroSizeField(CGF, base, field);
 
   const RecordDecl *rec = field->getParent();
 
   unsigned idx =
-    CGF.CGM.getTypes().getCGRecordLayout(rec).getLLVMFieldNo(field);
+    CGF.CGM.getTypes().getCGRecordLayout(rec, elem_type).getLLVMFieldNo(field);
+
+  // deal with internal libfloor vector class -> vector type conversion which
+  // can lead to a struct element load / GEP like this
+  // -> simply return the vector itself here (we can't drill down further)
+  if (elem_type->isVectorTy()) {
+    return base;
+  }
+
+  // deal with array of opaque (struct) types (e.g. used for array of images)
+  if (elem_type->isArrayTy() &&
+      elem_type->getArrayElementType()->isPointerTy()) {
+    if (elem_type->getArrayElementType()->getPointerElementType()->isStructTy() &&
+        !elem_type->getArrayElementType()->getPointerElementType()->isSized()) {
+      // -> 1D
+      return CGF.Builder.CreateConstArrayGEP(base, idx, field->getName());
+    } else if (elem_type->getArrayElementType()->getPointerElementType()->isArrayTy()) {
+      // -> 2D
+      auto arr_type = dyn_cast<llvm::ArrayType>(elem_type->getArrayElementType()->getPointerElementType());
+      if (arr_type && arr_type->getElementType()->isPointerTy() &&
+          arr_type->getElementType()->getPointerElementType()->isStructTy() &&
+          !arr_type->getElementType()->getPointerElementType()->isSized()) {
+        return CGF.Builder.CreateConstArrayGEP(base, idx, field->getName());
+      }
+    }
+  }
 
   return CGF.Builder.CreateStructGEP(base, idx, field->getName());
 }
@@ -4275,10 +4312,13 @@ static bool hasAnyVptr(const QualType Type, const ASTContext &Context) {
 LValue CodeGenFunction::EmitLValueForField(LValue base,
                                            const FieldDecl *field) {
   LValueBaseInfo BaseInfo = base.getBaseInfo();
+  Address Addr = base.getAddress(*this);
+  llvm::Type* elem_type = Addr.getType()->getPointerElementType();
+  const RecordDecl *rec = field->getParent();
+  const CGRecordLayout &RL = CGM.getTypes().getCGRecordLayout(rec, elem_type);
+  const auto is_flattened_struct = CGM.getTypes().is_flattened_struct_type(elem_type);
 
   if (field->isBitField()) {
-    const CGRecordLayout &RL =
-        CGM.getTypes().getCGRecordLayout(field->getParent());
     const CGBitFieldInfo &Info = RL.getBitFieldInfo(field);
     const bool UseVolatile = isAAPCS(CGM.getTarget()) &&
                              CGM.getCodeGenOpts().AAPCSBitfieldWidth &&
@@ -4286,7 +4326,6 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
                              field->getType()
                                  .withCVRQualifiers(base.getVRQualifiers())
                                  .isVolatileQualified();
-    Address Addr = base.getAddress(*this);
     unsigned Idx = RL.getLLVMFieldNo(field);
     const RecordDecl *rec = field->getParent();
     if (!UseVolatile) {
@@ -4309,6 +4348,7 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
     llvm::Type *FieldIntTy = llvm::Type::getIntNTy(getLLVMContext(), SS);
     if (Addr.getElementType() != FieldIntTy)
       Addr = Builder.CreateElementBitCast(Addr, FieldIntTy);
+    // TODO: check if address space is correct
     if (UseVolatile) {
       const unsigned VolatileOffset = Info.VolatileStorageOffset.getQuantity();
       if (VolatileOffset)
@@ -4327,7 +4367,6 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
   // FIXME: this should get propagated down through anonymous structs
   // and unions.
   QualType FieldType = field->getType();
-  const RecordDecl *rec = field->getParent();
   AlignmentSource BaseAlignSource = BaseInfo.getAlignmentSource();
   LValueBaseInfo FieldBaseInfo(getFieldAlignmentSource(BaseAlignSource));
   TBAAAccessInfo FieldTBAAInfo;
@@ -4341,7 +4380,7 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
     // If no base type been assigned for the base access, then try to generate
     // one for this base lvalue.
     FieldTBAAInfo = base.getTBAAInfo();
-    if (!FieldTBAAInfo.BaseType) {
+    if (!FieldTBAAInfo.BaseType && !is_flattened_struct /* do not use invalid TBAA info when this is flattened */) {
         FieldTBAAInfo.BaseType = CGM.getTBAABaseTypeInfo(base.getType());
         assert(!FieldTBAAInfo.Offset &&
                "Nonzero offset for an access with no base type!");
@@ -4361,7 +4400,7 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
         getContext().getTypeSizeInChars(FieldType).getQuantity();
   }
 
-  Address addr = base.getAddress(*this);
+  auto& addr = Addr;
   if (auto *ClassDef = dyn_cast<CXXRecordDecl>(rec)) {
     if (CGM.getCodeGenOpts().StrictVTablePointers &&
         ClassDef->isDynamicClass()) {
@@ -4402,7 +4441,7 @@ LValue CodeGenFunction::EmitLValueForField(LValue base,
     if (!IsInPreservedAIRegion &&
         (!getDebugInfo() || !rec->hasAttr<BPFPreserveAccessIndexAttr>()))
       // For structs, we GEP to the field that the record layout suggests.
-      addr = emitAddrOfFieldStorage(*this, addr, field);
+      addr = emitAddrOfFieldStorage(*this, addr, field, elem_type);
     else
       // Remember the original struct field index
       addr = emitPreserveStructAccess(*this, base, addr, field);
@@ -4449,7 +4488,8 @@ CodeGenFunction::EmitLValueForFieldInitialization(LValue Base,
   if (!FieldType->isReferenceType())
     return EmitLValueForField(Base, Field);
 
-  Address V = emitAddrOfFieldStorage(*this, Base.getAddress(*this), Field);
+  llvm::Type* elem_type = Base.getAddress(*this).getType()->getPointerElementType();
+  Address V = emitAddrOfFieldStorage(*this, Base.getAddress(*this), Field, elem_type);
 
   // Make sure that the address is pointing to the right type.
   llvm::Type *llvmType = ConvertTypeForMem(FieldType);
@@ -4651,7 +4691,6 @@ LValue CodeGenFunction::EmitCastLValue(const CastExpr *E) {
   case CK_ARCReclaimReturnedObject:
   case CK_ARCExtendBlockObject:
   case CK_CopyAndAutoreleaseBlockObject:
-  case CK_IntToOCLSampler:
   case CK_FloatingToFixedPoint:
   case CK_FixedPointToFloating:
   case CK_FixedPointCast:
@@ -4788,6 +4827,12 @@ LValue CodeGenFunction::EmitCastLValue(const CastExpr *E) {
   }
   case CK_ZeroToOCLOpaqueType:
     llvm_unreachable("NULL to OpenCL opaque type lvalue cast is not valid");
+  case CK_ZeroToOCLEvent:
+    llvm_unreachable("NULL to OpenCL event lvalue cast is not valid");
+  case CK_ZeroToOCLQueue:
+    llvm_unreachable("NULL to OpenCL queue lvalue cast is not valid");
+  case CK_IntToOCLSampler:
+    llvm_unreachable("int to OpenCL sampler lvalue cast is not valid");
   }
 
   llvm_unreachable("Unhandled lvalue cast kind?");
@@ -4923,7 +4968,7 @@ static CGCallee EmitDirectCallee(CodeGenFunction &CGF, GlobalDecl GD) {
 
   llvm::Constant *CalleePtr = EmitFunctionDeclPointer(CGF.CGM, GD);
   if (CGF.CGM.getLangOpts().CUDA && !CGF.CGM.getLangOpts().CUDAIsDevice &&
-      FD->hasAttr<CUDAGlobalAttr>())
+      FD->hasAttr<ComputeKernelAttr>())
     CalleePtr = CGF.CGM.getCUDARuntime().getKernelStub(
         cast<llvm::GlobalValue>(CalleePtr->stripPointerCasts()));
 
diff --git a/clang/lib/CodeGen/CGExprAgg.cpp b/clang/lib/CodeGen/CGExprAgg.cpp
index 1e81ad9f2dc7..90663c3627f9 100644
--- a/clang/lib/CodeGen/CGExprAgg.cpp
+++ b/clang/lib/CodeGen/CGExprAgg.cpp
@@ -905,6 +905,8 @@ void AggExprEmitter::VisitCastExpr(CastExpr *E) {
   case CK_CopyAndAutoreleaseBlockObject:
   case CK_BuiltinFnToFnPtr:
   case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
   case CK_MatrixCast:
 
   case CK_IntToOCLSampler:
@@ -1444,10 +1446,13 @@ static bool castPreservesZero(const CastExpr *CE) {
   case CK_BlockPointerToObjCPointerCast:
   case CK_CPointerToObjCPointerCast:
   case CK_ObjCObjectLValueCast:
-  case CK_IntToOCLSampler:
-  case CK_ZeroToOCLOpaqueType:
     // FIXME: Check these.
     return false;
+  case CK_IntToOCLSampler:
+  case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
+    return true;
 
   case CK_FixedPointCast:
   case CK_FixedPointToBoolean:
@@ -2060,6 +2065,7 @@ void CodeGenFunction::EmitAggregateCopy(LValue Dest, LValue Src, QualType Ty,
   if (getLangOpts().CPlusPlus) {
     if (const RecordType *RT = Ty->getAs<RecordType>()) {
       CXXRecordDecl *Record = cast<CXXRecordDecl>(RT->getDecl());
+#if 0 // TODO: fix this!
       assert((Record->hasTrivialCopyConstructor() ||
               Record->hasTrivialCopyAssignment() ||
               Record->hasTrivialMoveConstructor() ||
@@ -2067,6 +2073,7 @@ void CodeGenFunction::EmitAggregateCopy(LValue Dest, LValue Src, QualType Ty,
               Record->hasAttr<TrivialABIAttr>() || Record->isUnion()) &&
              "Trying to aggregate-copy a type without a trivial copy/move "
              "constructor or assignment operator");
+#endif
       // Ignore empty classes in C++.
       if (Record->isEmpty())
         return;
diff --git a/clang/lib/CodeGen/CGExprCXX.cpp b/clang/lib/CodeGen/CGExprCXX.cpp
index cc838bf38c6c..1c6d5b206f62 100644
--- a/clang/lib/CodeGen/CGExprCXX.cpp
+++ b/clang/lib/CodeGen/CGExprCXX.cpp
@@ -24,6 +24,8 @@
 using namespace clang;
 using namespace CodeGen;
 
+// TODO: fix other This uses?
+
 namespace {
 struct MemberCallInfo {
   RequiredArgs ReqArgs;
@@ -45,7 +47,12 @@ commonEmitCXXMemberOrOperatorCall(CodeGenFunction &CGF, const CXXMethodDecl *MD,
   // Push the this ptr.
   const CXXRecordDecl *RD =
       CGF.CGM.getCXXABI().getThisArgumentTypeForMethod(MD);
-  Args.add(RValue::get(This), CGF.getTypes().DeriveThisType(RD, MD));
+  ASTContext &C = CGF.getContext();
+  QualType this_type = C.VoidPtrTy;
+  if (RD) {
+    this_type = C.getPointerType(C.getAddrSpaceQualType(C.getTypeDeclType(RD), (LangAS)This->getType()->getPointerAddressSpace()));
+  }
+  Args.add(RValue::get(This), this_type);
 
   // If there is an implicit parameter (e.g. VTT), emit it.
   if (ImplicitParam) {
@@ -100,6 +107,7 @@ RValue CodeGenFunction::EmitCXXDestructorCall(
   assert(ThisTy->getAsCXXRecordDecl() == DtorDecl->getParent() &&
          "Pointer/Object mixup");
 
+#if 0 // we don't want this
   LangAS SrcAS = ThisTy.getAddressSpace();
   LangAS DstAS = DtorDecl->getMethodQualifiers().getAddressSpace();
   if (SrcAS != DstAS) {
@@ -108,6 +116,7 @@ RValue CodeGenFunction::EmitCXXDestructorCall(
     This = getTargetHooks().performAddrSpaceCast(*this, This, SrcAS, DstAS,
                                                  NewType);
   }
+#endif
 
   CallArgList Args;
   commonEmitCXXMemberOrOperatorCall(*this, DtorDecl, This, ImplicitParam,
diff --git a/clang/lib/CodeGen/CGExprComplex.cpp b/clang/lib/CodeGen/CGExprComplex.cpp
index 5409e82d437e..2ea7234545ca 100644
--- a/clang/lib/CodeGen/CGExprComplex.cpp
+++ b/clang/lib/CodeGen/CGExprComplex.cpp
@@ -525,6 +525,8 @@ ComplexPairTy ComplexExprEmitter::EmitCast(CastKind CK, Expr *Op,
   case CK_CopyAndAutoreleaseBlockObject:
   case CK_BuiltinFnToFnPtr:
   case CK_ZeroToOCLOpaqueType:
+  case CK_ZeroToOCLEvent:
+  case CK_ZeroToOCLQueue:
   case CK_AddressSpaceConversion:
   case CK_IntToOCLSampler:
   case CK_FloatingToFixedPoint:
diff --git a/clang/lib/CodeGen/CGExprConstant.cpp b/clang/lib/CodeGen/CGExprConstant.cpp
index ff900ed077e6..fad03d8f2810 100644
--- a/clang/lib/CodeGen/CGExprConstant.cpp
+++ b/clang/lib/CodeGen/CGExprConstant.cpp
@@ -1098,8 +1098,14 @@ public:
     case CK_ConstructorConversion:
       return Visit(subExpr, destType);
 
-    case CK_IntToOCLSampler:
-      llvm_unreachable("global sampler variables are not generated");
+    case CK_IntToOCLSampler: {
+      auto C = Emitter.tryEmitPrivateForMemory(subExpr, subExpr->getType());
+      if (!C)
+        return nullptr;
+      if (!CGM.getLangOpts().CLSamplerOpaque)
+        return C;
+      return CGM.createIntToSamplerConversion(subExpr, Emitter.CGF);
+    }
 
     case CK_Dependent: llvm_unreachable("saw dependent cast!");
 
@@ -1170,6 +1176,8 @@ public:
     case CK_FixedPointToIntegral:
     case CK_IntegralToFixedPoint:
     case CK_ZeroToOCLOpaqueType:
+    case CK_ZeroToOCLEvent:
+    case CK_ZeroToOCLQueue:
     case CK_MatrixCast:
       return nullptr;
     }
diff --git a/clang/lib/CodeGen/CGExprScalar.cpp b/clang/lib/CodeGen/CGExprScalar.cpp
index ae9434f96529..e95011ea1a0e 100644
--- a/clang/lib/CodeGen/CGExprScalar.cpp
+++ b/clang/lib/CodeGen/CGExprScalar.cpp
@@ -2028,8 +2028,14 @@ Value *ScalarExprEmitter::VisitCastExpr(CastExpr *CE) {
     llvm::Type *DstTy = ConvertType(DestTy);
     if (SrcTy->isPtrOrPtrVectorTy() && DstTy->isPtrOrPtrVectorTy() &&
         SrcTy->getPointerAddressSpace() != DstTy->getPointerAddressSpace()) {
-      llvm_unreachable("wrong cast for pointers in different address spaces"
-                       "(must be an address space cast)!");
+      // allow this with OpenCL/Metal/Vulkan
+      if (CGF.getLangOpts().OpenCL) {
+        llvm::Type *MidTy = CGF.CGM.getDataLayout().getIntPtrType(SrcTy);
+        return Builder.CreateIntToPtr(Builder.CreatePtrToInt(Src, MidTy), DstTy);
+      } else {
+        llvm_unreachable("wrong cast for pointers in different address spaces"
+                         "(must be an address space cast)!");
+      }
     }
 
     if (CGF.SanOpts.has(SanitizerKind::CFIUnrelatedCast)) {
@@ -2402,6 +2408,7 @@ Value *ScalarExprEmitter::VisitCastExpr(CastExpr *CE) {
                                          CE->getExprLoc());
   }
 
+  case CK_ZeroToOCLEvent:
   case CK_ZeroToOCLOpaqueType: {
     assert((DestTy->isEventT() || DestTy->isQueueT() ||
             DestTy->isOCLIntelSubgroupAVCType()) &&
@@ -2409,8 +2416,25 @@ Value *ScalarExprEmitter::VisitCastExpr(CastExpr *CE) {
     return llvm::Constant::getNullValue(ConvertType(DestTy));
   }
 
-  case CK_IntToOCLSampler:
-    return CGF.CGM.createOpenCLIntToSamplerConversion(E, CGF);
+  case CK_ZeroToOCLQueue: {
+    assert(DestTy->isQueueT() && "CK_ZeroToOCLQueue cast on non queue_t type");
+    return llvm::Constant::getNullValue(ConvertType(DestTy));
+  }
+
+  case CK_IntToOCLSampler: {
+    assert(DestTy->isSamplerT() && "CK_IntToOCLSampler cast to non sampler type");
+    if (!CGF.CGM.getLangOpts().CLSamplerOpaque)
+      return Visit(E);
+    if (const CastExpr* SCE = dyn_cast<CastExpr>(E)) {
+      if (const DeclRefExpr *DRE = cast<DeclRefExpr>(SCE->getSubExpr())) {
+        if (const VarDecl *VD = cast<VarDecl>(DRE->getDecl())) {
+          assert(VD->getInit() && "Invalid sampler initializer");
+          E = const_cast<Expr*>(VD->getInit());
+        }
+      }
+    }
+    return CGF.CGM.createIntToSamplerConversion(E, &CGF);
+  }
 
   } // end of switch
 
@@ -3526,9 +3550,9 @@ static Value *emitPointerArithmetic(CodeGenFunction &CGF,
   // GNU void* casts amount to no-ops since our void* type is i8*, but this is
   // future proof.
   if (elementType->isVoidType() || elementType->isFunctionType()) {
-    Value *result = CGF.EmitCastToVoidPtr(pointer);
+    Value *result = CGF.Builder.CreatePointerCast(pointer, CGF.VoidPtrTy);
     result = CGF.Builder.CreateGEP(CGF.Int8Ty, result, index, "add.ptr");
-    return CGF.Builder.CreateBitCast(result, pointer->getType());
+    return CGF.Builder.CreatePointerCast(result, pointer->getType());
   }
 
   if (CGF.getLangOpts().isSignedOverflowDefined())
@@ -4597,33 +4621,9 @@ VisitAbstractConditionalOperator(const AbstractConditionalOperator *E) {
     llvm::Type *condType = ConvertType(condExpr->getType());
     auto *vecTy = cast<llvm::FixedVectorType>(condType);
 
-    unsigned numElem = vecTy->getNumElements();
-    llvm::Type *elemType = vecTy->getElementType();
-
     llvm::Value *zeroVec = llvm::Constant::getNullValue(vecTy);
     llvm::Value *TestMSB = Builder.CreateICmpSLT(CondV, zeroVec);
-    llvm::Value *tmp = Builder.CreateSExt(
-        TestMSB, llvm::FixedVectorType::get(elemType, numElem), "sext");
-    llvm::Value *tmp2 = Builder.CreateNot(tmp);
-
-    // Cast float to int to perform ANDs if necessary.
-    llvm::Value *RHSTmp = RHS;
-    llvm::Value *LHSTmp = LHS;
-    bool wasCast = false;
-    llvm::VectorType *rhsVTy = cast<llvm::VectorType>(RHS->getType());
-    if (rhsVTy->getElementType()->isFloatingPointTy()) {
-      RHSTmp = Builder.CreateBitCast(RHS, tmp2->getType());
-      LHSTmp = Builder.CreateBitCast(LHS, tmp->getType());
-      wasCast = true;
-    }
-
-    llvm::Value *tmp3 = Builder.CreateAnd(RHSTmp, tmp2);
-    llvm::Value *tmp4 = Builder.CreateAnd(LHSTmp, tmp);
-    llvm::Value *tmp5 = Builder.CreateOr(tmp3, tmp4, "cond");
-    if (wasCast)
-      tmp5 = Builder.CreateBitCast(tmp5, RHS->getType());
-
-    return tmp5;
+    return Builder.CreateSelect(TestMSB, LHS, RHS);
   }
 
   if (condExpr->getType()->isVectorType()) {
@@ -4831,6 +4831,9 @@ Value *ScalarExprEmitter::VisitAsTypeExpr(AsTypeExpr *E) {
     return Src;
   }
 
+  if (SrcTy->isPointerTy() || DstTy->isPointerTy())
+    return Builder.CreatePointerCast(Src, DstTy, "astype");
+
   return createCastsForTypeOfSameSize(Builder, CGF.CGM.getDataLayout(),
                                       Src, DstTy, "astype");
 }
diff --git a/clang/lib/CodeGen/CGOpenCLRuntime.cpp b/clang/lib/CodeGen/CGOpenCLRuntime.cpp
index dbe375294d17..286781c11338 100644
--- a/clang/lib/CodeGen/CGOpenCLRuntime.cpp
+++ b/clang/lib/CodeGen/CGOpenCLRuntime.cpp
@@ -37,36 +37,90 @@ llvm::Type *CGOpenCLRuntime::convertOpenCLSpecificType(const Type *T) {
   llvm::LLVMContext& Ctx = CGM.getLLVMContext();
   uint32_t AddrSpc = CGM.getContext().getTargetAddressSpace(
       CGM.getContext().getOpenCLTypeAddrSpace(T));
-  switch (cast<BuiltinType>(T)->getKind()) {
-  default:
-    llvm_unreachable("Unexpected opencl builtin type!");
-    return nullptr;
+  if (!CGM.getLangOpts().Metal) { // OpenCL/SPIR and SPIR-V/Vulkan
+    switch (cast<BuiltinType>(T)->getKind()) {
+    default:
+      llvm_unreachable("Unexpected opencl builtin type!");
+      return nullptr;
 #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
-  case BuiltinType::Id: \
-    return llvm::PointerType::get( \
-        llvm::StructType::create(Ctx, "opencl." #ImgType "_" #Suffix "_t"), \
-        AddrSpc);
+    case BuiltinType::Id: \
+      return llvm::PointerType::get( \
+          llvm::StructType::create(Ctx, "opencl." #ImgType #Suffix "_t"), \
+          AddrSpc);
 #include "clang/Basic/OpenCLImageTypes.def"
-  case BuiltinType::OCLSampler:
-    return getSamplerType(T);
-  case BuiltinType::OCLEvent:
-    return llvm::PointerType::get(
-        llvm::StructType::create(Ctx, "opencl.event_t"), AddrSpc);
-  case BuiltinType::OCLClkEvent:
-    return llvm::PointerType::get(
-        llvm::StructType::create(Ctx, "opencl.clk_event_t"), AddrSpc);
-  case BuiltinType::OCLQueue:
-    return llvm::PointerType::get(
-        llvm::StructType::create(Ctx, "opencl.queue_t"), AddrSpc);
-  case BuiltinType::OCLReserveID:
-    return llvm::PointerType::get(
-        llvm::StructType::create(Ctx, "opencl.reserve_id_t"), AddrSpc);
+    case BuiltinType::OCLSampler:
+      return CGM.getLangOpts().CLSamplerOpaque ? (llvm::Type*)getSamplerType(T) : (llvm::Type*)llvm::IntegerType::get(Ctx, 32);
+    case BuiltinType::OCLEvent:
+      return llvm::PointerType::get(
+          llvm::StructType::create(Ctx, "opencl.event_t"), AddrSpc);
+    case BuiltinType::OCLClkEvent:
+      return llvm::PointerType::get(
+          llvm::StructType::create(Ctx, "opencl.clk_event_t"), AddrSpc);
+    case BuiltinType::OCLQueue:
+      return llvm::PointerType::get(
+          llvm::StructType::create(Ctx, "opencl.queue_t"), AddrSpc);
+    case BuiltinType::OCLReserveID:
+      return llvm::PointerType::get(
+          llvm::StructType::create(Ctx, "opencl.reserve_id_t"), AddrSpc);
 #define EXT_OPAQUE_TYPE(ExtType, Id, Ext) \
-  case BuiltinType::Id: \
-    return llvm::PointerType::get( \
-        llvm::StructType::create(Ctx, "opencl." #ExtType), AddrSpc);
+    case BuiltinType::Id: \
+      return llvm::PointerType::get( \
+          llvm::StructType::create(Ctx, "opencl." #ExtType), AddrSpc);
 #include "clang/Basic/OpenCLExtensionTypes.def"
+    }
+  } else if (CGM.getLangOpts().Metal) { // Metal/AIR
+    const auto get_or_create_opaque_ptr_type = [&Ctx](const char* name, uint32_t address_space) {
+      auto opaque_type = llvm::StructType::getTypeByName(Ctx, name);
+      if (!opaque_type) {
+        opaque_type = llvm::StructType::create(Ctx, name);
+      }
+      return llvm::PointerType::get(opaque_type, address_space);
+    };
+    switch (cast<BuiltinType>(T)->getKind()) {
+      default:
+        llvm_unreachable("Unexpected metal builtin type!");
+        return nullptr;
+      case BuiltinType::OCLImage1d:
+        return get_or_create_opaque_ptr_type("struct._texture_1d_t", AddrSpc);
+      case BuiltinType::OCLImage1dArray:
+                return get_or_create_opaque_ptr_type("struct._texture_1d_array_t", AddrSpc);
+      case BuiltinType::OCLImage1dBuffer:
+        llvm_unreachable("Unsupported image type (1D-buffer is not supported by metal)!");
+        return nullptr;
+      case BuiltinType::OCLImage2d:
+        return get_or_create_opaque_ptr_type("struct._texture_2d_t", AddrSpc);
+      case BuiltinType::OCLImage2dArray:
+        return get_or_create_opaque_ptr_type("struct._texture_2d_array_t", AddrSpc);
+      case BuiltinType::OCLImage2dDepth:
+        return get_or_create_opaque_ptr_type("struct._depth_2d_t", AddrSpc);
+      case BuiltinType::OCLImage2dArrayDepth:
+        return get_or_create_opaque_ptr_type("struct._depth_2d_array_t", AddrSpc);
+      case BuiltinType::OCLImage2dMSAA:
+        return get_or_create_opaque_ptr_type("struct._texture_2d_ms_t", AddrSpc);
+      case BuiltinType::OCLImage2dArrayMSAA:
+        return get_or_create_opaque_ptr_type("struct._texture_2d_ms_array_t", AddrSpc);
+      case BuiltinType::OCLImage2dMSAADepth:
+        return get_or_create_opaque_ptr_type("struct._depth_2d_ms_t", AddrSpc);
+      case BuiltinType::OCLImage2dArrayMSAADepth:
+        return get_or_create_opaque_ptr_type("struct._depth_2d_ms_array_t", AddrSpc);
+      case BuiltinType::OCLImageCube:
+        return get_or_create_opaque_ptr_type("struct._texture_cube_t", AddrSpc);
+      case BuiltinType::OCLImageCubeArray:
+        return get_or_create_opaque_ptr_type("struct._texture_cube_array_t", AddrSpc);
+      case BuiltinType::OCLImageCubeDepth:
+        return get_or_create_opaque_ptr_type("struct._depth_cube_t", AddrSpc);
+      case BuiltinType::OCLImageCubeArrayDepth:
+        return get_or_create_opaque_ptr_type("struct._depth_cube_array_t", AddrSpc);
+      case BuiltinType::OCLImage3d:
+        return get_or_create_opaque_ptr_type("struct._texture_3d_t", AddrSpc);
+      case BuiltinType::OCLSampler:
+        return get_or_create_opaque_ptr_type("struct._sampler_t", CGM.getContext().getTargetAddressSpace(LangAS::opencl_constant));
+      case BuiltinType::OCLEvent:
+        return get_or_create_opaque_ptr_type("struct._event_t", 0);
+    }
   }
+  llvm_unreachable("Unexpected builtin type!");
+  return nullptr;
 }
 
 llvm::Type *CGOpenCLRuntime::getPipeType(const PipeType *T) {
@@ -180,7 +234,58 @@ CGOpenCLRuntime::emitOpenCLEnqueuedBlock(CodeGenFunction &CGF, const Expr *E) {
   // The common part of the post-processing of the kernel goes here.
   F->addFnAttr(llvm::Attribute::NoUnwind);
   F->setCallingConv(
-      CGF.getTypes().ClangCallConvToLLVMCallConv(CallingConv::CC_OpenCLKernel));
+      CGF.getTypes().ClangCallConvToLLVMCallConv(CallingConv::CC_FloorKernel));
   EnqueuedBlockMap[Block].Kernel = F;
   return EnqueuedBlockMap[Block];
 }
+
+//
+// Ocl20Mangler
+//
+Ocl20Mangler::Ocl20Mangler(llvm::SmallVectorImpl<char>& SS): MangledString(&SS) {}
+
+Ocl20Mangler& Ocl20Mangler::appendReservedId() {
+  this->appendString("13ocl_reserveid");
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendPipe() {
+  this->appendString("8ocl_pipe");
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendInt() {
+  MangledString->push_back('i');
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendUint() {
+  MangledString->push_back('j');
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendVoid() {
+  MangledString->push_back('v');
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendPointer() {
+  this->appendString("P");
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendPointer(int addressSpace) {
+  assert(addressSpace >=0 && addressSpace <= 4 &&
+         "Illegal address space for OpenCL");
+  if (!addressSpace)
+    return appendPointer();
+
+  this->appendString("PU3AS");
+  MangledString->push_back('0' + addressSpace);
+  return *this;
+}
+
+Ocl20Mangler& Ocl20Mangler::appendString(llvm::StringRef S) {
+  MangledString->append(S.begin(), S.end());
+  return *this;
+}
diff --git a/clang/lib/CodeGen/CGOpenCLRuntime.h b/clang/lib/CodeGen/CGOpenCLRuntime.h
index 3f7aa9b0d8dc..080ffb6d229d 100644
--- a/clang/lib/CodeGen/CGOpenCLRuntime.h
+++ b/clang/lib/CodeGen/CGOpenCLRuntime.h
@@ -97,6 +97,46 @@ public:
   llvm::Function *getInvokeFunction(const Expr *E);
 };
 
+class Ocl20Mangler {
+public:
+  Ocl20Mangler(llvm::SmallVectorImpl<char>&);
+
+  // \brief Appends the mangled representation of reserve_id_t parameter to the
+  //  mangled string.
+  Ocl20Mangler& appendReservedId();
+
+  // \brief Appends the mangled representation of pipe_t parameter to the
+  //  mangled string.
+  Ocl20Mangler& appendPipe();
+
+  // \brief Appends the mangled representation of 'int' parameter to the
+  //  mangled string.
+  Ocl20Mangler& appendInt();
+
+  // \brief Appends the mangled representation of 'unsigned int' parameter to the
+  // mangled string.
+  Ocl20Mangler& appendUint();
+
+  // \brief Appends the mangled representation of a pointer.
+  Ocl20Mangler& appendPointer();
+
+  // \brief Appends the mangled representation of void.
+  Ocl20Mangler& appendVoid();
+
+  // \brief Appends the mangled representation of a pointer with a given address
+  // space.
+  // \param addressSapace The address space of the pointer. Valid values are
+  // [0,4].
+  Ocl20Mangler& appendPointer(int addressSapace);
+
+private:
+
+  // \brief Appends the given string to the mangled prototype.
+  Ocl20Mangler& appendString(llvm::StringRef);
+
+  llvm::SmallVectorImpl<char> *MangledString;
+};
+
 }
 }
 
diff --git a/clang/lib/CodeGen/CGOpenMPRuntimeGPU.cpp b/clang/lib/CodeGen/CGOpenMPRuntimeGPU.cpp
index 56428cacfe7c..5b1c9decc99f 100644
--- a/clang/lib/CodeGen/CGOpenMPRuntimeGPU.cpp
+++ b/clang/lib/CodeGen/CGOpenMPRuntimeGPU.cpp
@@ -3866,9 +3866,13 @@ void CGOpenMPRuntimeGPU::processRequiresDirective(
       case CudaArch::SM_62:
       case CudaArch::SM_70:
       case CudaArch::SM_72:
+      case CudaArch::SM_73:
       case CudaArch::SM_75:
       case CudaArch::SM_80:
+      case CudaArch::SM_82:
       case CudaArch::SM_86:
+      case CudaArch::SM_87:
+      case CudaArch::SM_88:
       case CudaArch::GFX600:
       case CudaArch::GFX601:
       case CudaArch::GFX602:
diff --git a/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp b/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp
index cf8313f92587..9e5b84b656b2 100644
--- a/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp
+++ b/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp
@@ -1005,6 +1005,43 @@ CodeGenTypes::ComputeRecordLayout(const RecordDecl *D, llvm::StructType *Ty) {
   return RL;
 }
 
+void CodeGenTypes::create_flattened_cg_layout(const CXXRecordDecl* D, llvm::StructType* Ty,
+											  const std::vector<CodeGenTypes::aggregate_scalar_entry>& fields) {
+	bool zero_init = true;
+	for(const auto& field : fields) {
+		// vector types (or replaced vector types) are always zero initializable
+		if(field.type->isExtVectorType() ||
+		   field.type->isVectorType()) {
+			continue;
+		}
+		
+		// else: need to make some calls based on the field decl type
+		const Type *Type = field.field_decl->getType()->getBaseElementTypeUnsafe();
+		if (const MemberPointerType *MPT = Type->getAs<MemberPointerType>()) {
+			if(!TheCXXABI.isZeroInitializable(MPT)) {
+				zero_init = false;
+				break;
+			}
+		}
+		else if (const CXXRecordDecl* cxx_rdecl = Type->getAsCXXRecordDecl()) {
+			if(!isZeroInitializable(cxx_rdecl)) {
+				zero_init = false;
+				break;
+			}
+		}
+		// else: it is zero initializable
+	}
+	
+	CGRecordLayout *RL = new CGRecordLayout(Ty, Ty, zero_init, zero_init);
+	uint32_t field_idx = 0;
+	for(const auto& field : fields) {
+		RL->FieldInfo.insert({ field.field_decl, field_idx++ });
+	}
+	
+	FlattenedCGRecordLayouts.insert({ Ty, RL });
+	FlattenedRecords.insert({ D, Ty });
+}
+
 void CGRecordLayout::print(raw_ostream &OS) const {
   OS << "<CGRecordLayout\n";
   OS << "  LLVMType:" << *CompleteObjectType << "\n";
diff --git a/clang/lib/CodeGen/CGSPIRMetadataAdder.cpp b/clang/lib/CodeGen/CGSPIRMetadataAdder.cpp
new file mode 100644
index 000000000000..c429f459f70a
--- /dev/null
+++ b/clang/lib/CodeGen/CGSPIRMetadataAdder.cpp
@@ -0,0 +1,327 @@
+//===- SPIRMetadataAdder.cpp - Add SPIR related module scope metadata -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+//
+//===----------------------------------------------------------------------===//
+
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallString.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/IR/TypeFinder.h"
+#include "CGSPIRMetadataAdder.h"
+#include <set>
+
+using namespace llvm;
+using namespace clang;
+using namespace CodeGen;
+
+static const char *ImageTypeNames[] = {
+  "opencl.image1d_t", "opencl.image1d_array_t", "opencl.image1d_buffer_t",
+  "opencl.image2d_t", "opencl.image2d_array_t",
+  "opencl.image2d_depth_t", "opencl.image2d_array_depth_t",
+  "opencl.image2d_msaa_t", "opencl.image2d_array_msaa_t",
+  "opencl.image2d_msaa_depth_t", "opencl.image2d_array_msaa_depth_t",
+  "opencl.image3d_t",
+  "opencl.imagecube_t", "opencl.imagecube_array_t",
+  "opencl.imagecube_depth_t", "opencl.imagecube_array_depth_t",
+};
+
+static const char *ImageDepthTypeNames[] = {
+  "opencl.image2d_depth_t", "opencl.image2d_array_depth_t"
+};
+
+static const char *ImageMSAATypeNames[] = {
+  "opencl.image2d_msaa_t", "opencl.image2d_array_msaa_t",
+  "opencl.image2d_msaa_depth_t", "opencl.image2d_array_msaa_depth_t"
+};
+
+struct OCLExtensionsTy {
+#define OPENCLEXTNAME(ext)  unsigned _##ext : 1;
+#include "clang/Basic/OpenCLExtensions.def"
+#undef OPENCLEXTNAME
+
+  OCLExtensionsTy() {
+#define OPENCLEXTNAME(ext)   _##ext = 0;
+#include "clang/Basic/OpenCLExtensions.def"
+#undef OPENCLEXTNAME
+  }
+};
+
+typedef void (*func_call_handler)(CallInst *callInstr, OCLExtensionsTy &exts);
+
+void baseAtomics64(CallInst *callInstr, OCLExtensionsTy &exts) {
+  PointerType *firstArgType = dyn_cast<PointerType>(callInstr->getArgOperand(0)->getType());
+
+  if (firstArgType &&
+      firstArgType->getPointerElementType()->isIntegerTy() &&
+      firstArgType->getPointerElementType()->getScalarSizeInBits() == 64)
+    exts._cl_khr_int64_base_atomics = 1;
+}
+
+void extAtomics64(CallInst *callInstr, OCLExtensionsTy &exts) {
+  PointerType *firstArgType = dyn_cast<PointerType>(callInstr->getArgOperand(0)->getType());
+
+  if (firstArgType &&
+      firstArgType->getPointerElementType()->isIntegerTy() &&
+      firstArgType->getPointerElementType()->getScalarSizeInBits() == 64)
+    exts._cl_khr_int64_extended_atomics = 1;
+}
+
+void image3DWrite(CallInst *callInstr, OCLExtensionsTy &exts) {
+  PointerType *firstArgType = dyn_cast<PointerType>(callInstr->getArgOperand(0)->getType());
+
+  if (firstArgType &&
+      firstArgType->getPointerElementType()->isStructTy() &&
+      !firstArgType->getPointerElementType()->getStructName().compare("opencl.image3d_t"))
+    exts._cl_khr_3d_image_writes = 1;
+}
+
+typedef struct {
+  const char *funcName;
+  func_call_handler handler;
+} funcCallHandlersTy;
+
+static const funcCallHandlersTy funcCallHandlers[] = {
+  {"_Z8atom_add", baseAtomics64},
+  {"_Z8atom_sub", baseAtomics64},
+  {"_Z9atom_xchg", baseAtomics64},
+  {"_Z8atom_inc", baseAtomics64},
+  {"_Z8atom_dec", baseAtomics64},
+  {"_Z12atom_cmpxchg", baseAtomics64},
+  {"_Z8atom_min", extAtomics64},
+  {"_Z8atom_max", extAtomics64},
+  {"_Z8atom_and", extAtomics64},
+  {"_Z7atom_or", extAtomics64},
+  {"_Z8atom_xor", extAtomics64},
+  {"_Z12write_imagef", image3DWrite},
+  {"_Z12write_imagei", image3DWrite},
+  {"_Z13write_imageui", image3DWrite}
+};
+
+static bool searchTypeInType (llvm::Type *ty1, llvm::Type *ty2, bool ignorePtrs);
+
+static bool searchTypeInType (llvm::Type *ty1, llvm::Type *ty2, bool ignorePtrs, std::set<llvm::Type*> &typesList) {
+  if (ty1 == ty2)
+    return true;
+
+  if (ty1->isVectorTy())
+    if (auto ty1_vec_type = dyn_cast_or_null<FixedVectorType>(ty1))
+      return searchTypeInType(ty1_vec_type->getElementType(), ty2, ignorePtrs, typesList);
+
+  if (ty1->isArrayTy())
+    return searchTypeInType(ty1->getArrayElementType(), ty2, ignorePtrs, typesList);
+
+  if (!ignorePtrs && ty1->isPointerTy()) {
+    // prevent infinte loop (such a struct that conatinc pointer to itself)
+    std::set<llvm::Type*>::iterator itr = typesList.find(ty1->getPointerElementType());
+    if ( itr != typesList.end() ) {
+      return false;
+    }
+    return searchTypeInType(ty1->getPointerElementType(), ty2, ignorePtrs, typesList);
+  }
+
+  if (ty1->isStructTy()) {
+    typesList.insert( ty1 );
+    llvm::StructType *strTy = dyn_cast<llvm::StructType>(ty1);
+
+    for (StructType::element_iterator EI = strTy->element_begin(),
+         EE = strTy->element_end(); EI != EE; ++EI)
+      if (searchTypeInType((*EI), ty2, ignorePtrs, typesList))
+        return true;
+  }
+
+  if (ty1->isFunctionTy()) {
+    typesList.insert( ty1 );
+    FunctionType *FuncTy = dyn_cast<llvm::FunctionType>(ty1);
+
+    if (searchTypeInType(FuncTy->getReturnType(), ty2, ignorePtrs))
+      return true;
+
+    for (FunctionType::param_iterator PI = FuncTy->param_begin(),
+         PE = FuncTy->param_end(); PI != PE; ++PI)
+      if (searchTypeInType((*PI), ty2, ignorePtrs))
+        return true;
+  }
+
+  return false;
+}
+
+static bool searchTypeInType (llvm::Type *ty1, llvm::Type *ty2, bool ignorePtrs) {
+  std::set<llvm::Type*> typesList;
+  return searchTypeInType( ty1, ty2, ignorePtrs, typesList);
+}
+
+static void FunctionAddSPIRMetadata(Function &F, bool &bUseDoubles, OCLExtensionsTy &sUsedExts);
+
+void clang::CodeGen::AddSPIRMetadata(Module &M, int OCLVersion, std::list<std::string> sBuildOptions, const LangOptions& LangOpts, const OpenCLOptions& cl_options) {
+  Type *pDoubleType = Type::getDoubleTy(M.getContext());
+  Type *pHalfType = Type::getHalfTy(M.getContext());
+
+  OCLExtensionsTy sUsedExts;
+
+  bool bUseDoubles = false;
+  bool bUseImages  = false;
+
+  for (Module::global_iterator GI = M.global_begin(), GE = M.global_end();
+       GI != GE; ++GI) {
+    if (searchTypeInType(GI->getType(), pDoubleType, false))
+      bUseDoubles = true;
+    if (searchTypeInType(GI->getType(), pHalfType, true))
+      sUsedExts._cl_khr_fp16 = true;
+  }
+
+  //check if image types are defined
+  for (size_t i = 0; i < sizeof(ImageTypeNames)/sizeof(ImageTypeNames[0]); i++) {
+    if (llvm::StructType::getTypeByName(M.getContext(), ImageTypeNames[i])) {
+      bUseImages = true;
+      break;
+    }
+  }
+
+  //check if depth image types are defined
+  for (size_t i = 0; i < sizeof(ImageDepthTypeNames)/sizeof(ImageDepthTypeNames[0]); i++) {
+    if (llvm::StructType::getTypeByName(M.getContext(), ImageDepthTypeNames[i])) {
+      sUsedExts._cl_khr_depth_images = true;
+      break;
+    }
+  }
+
+  //check if msaa image types are defined
+  for (size_t i = 0; i < sizeof(ImageMSAATypeNames)/sizeof(ImageMSAATypeNames[0]); i++) {
+    if (llvm::StructType::getTypeByName(M.getContext(), ImageMSAATypeNames[i])) {
+      sUsedExts._cl_khr_gl_msaa_sharing = true;
+      break;
+    }
+  }
+
+  // scan all functions
+  for (Module::iterator FI = M.begin(), FE = M.end();
+       FI != FE; ++FI) {
+    FunctionAddSPIRMetadata(*FI, bUseDoubles, sUsedExts);
+  }
+
+  // enable/add explicitly enabled pragma extensions
+#define OPENCLEXTNAME(nm) if (cl_options.isAvailableOption(#nm, LangOpts)) sUsedExts._##nm = true;
+#include "clang/Basic/OpenCLExtensions.def"
+
+#if 0 // already emitted by CodeGenModule
+  // Add SPIR version (1.2)
+  llvm::Metadata *SPIRVerElts[] = {
+    llvm::ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M.getContext()), 1)),
+    llvm::ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M.getContext()), 2))
+  };
+  llvm::NamedMDNode *SPIRVerMD =
+    M.getOrInsertNamedMetadata("opencl.spir.version");
+  SPIRVerMD->addOperand(llvm::MDNode::get(M.getContext(), SPIRVerElts));
+
+  // Add OpenCL version
+  llvm::Metadata *OCLVerElts[] = {
+    llvm::ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M.getContext()), OCLVersion / 100)),
+    llvm::ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M.getContext()), (OCLVersion % 100) / 10))
+  };
+  llvm::NamedMDNode *OCLVerMD =
+    M.getOrInsertNamedMetadata("opencl.ocl.version");
+  OCLVerMD->addOperand(llvm::MDNode::get(M.getContext(), OCLVerElts));
+#endif
+
+  // add kernels metadata node (this is usually also done by CodeGenModule,
+  // but if no kernels exist, this will not be emitted -> add empty node here)
+  M.getOrInsertNamedMetadata("opencl.kernels");
+
+  // Add used extensions
+  llvm::SmallVector<llvm::Metadata*, 5> OCLExtElts;
+
+#define OPENCLEXTNAME(nm)  if (sUsedExts._##nm) \
+  OCLExtElts.push_back(llvm::MDString::get(M.getContext(), #nm));
+#include "clang/Basic/OpenCLExtensions.def"
+
+  llvm::NamedMDNode *OCLExtMD =
+    M.getOrInsertNamedMetadata("opencl.used.extensions");
+
+  OCLExtMD->addOperand(llvm::MDNode::get(M.getContext(), OCLExtElts));
+
+  // Add used optional core features
+  llvm::SmallVector<llvm::Metadata*, 5> OCLOptCoreElts;
+
+  // TODO: flag for this?
+  if (bUseDoubles)
+    OCLOptCoreElts.push_back(llvm::MDString::get(M.getContext(), "cl_doubles"));
+
+  if (bUseImages)
+    OCLOptCoreElts.push_back(llvm::MDString::get(M.getContext(), "cl_images"));
+
+  llvm::NamedMDNode *OptCoreMD =
+    M.getOrInsertNamedMetadata("opencl.used.optional.core.features");
+  OptCoreMD->addOperand(llvm::MDNode::get(M.getContext(), OCLOptCoreElts));
+
+  // Add build options
+  llvm::NamedMDNode *OCLCompOptsMD =
+    M.getOrInsertNamedMetadata("opencl.compiler.options");
+      llvm::SmallVector<llvm::Metadata*,5> OCLBuildOptions;
+  // TODO: should probably parse clang args, -cl-spir-compile-options doesn't seem to work?
+  sBuildOptions.push_back("-cl-kernel-arg-info");
+  sBuildOptions.push_back("-cl-mad-enable");
+  sBuildOptions.push_back("-cl-denorms-are-zero");
+  sBuildOptions.push_back("-cl-unsafe-math-optimizations");
+  for (std::list<std::string>::const_iterator it = sBuildOptions.begin(),
+       e = sBuildOptions.end(); it != e ; ++it) {
+    OCLBuildOptions.push_back(llvm::MDString::get(M.getContext(), *it));
+  }
+  OCLCompOptsMD->addOperand(llvm::MDNode::get(M.getContext(), OCLBuildOptions));
+}
+
+static void FunctionAddSPIRMetadata(Function &F, bool &bUseDoubles, OCLExtensionsTy &sUsedExts) {
+  Type *pDoubleType = Type::getDoubleTy(F.getParent()->getContext());
+  Type *pHalfType = Type::getHalfTy(F.getParent()->getContext());
+
+  for (Function::arg_iterator AI = F.arg_begin(), AE = F.arg_end();
+       AI != AE; ++AI) {
+    if (searchTypeInType(AI->getType(), pDoubleType, false))
+      bUseDoubles = true;
+    if (searchTypeInType(AI->getType(), pHalfType, true))
+      sUsedExts._cl_khr_fp16 = true;
+  }
+
+  for (Function::iterator BB = F.begin(), E = F.end(); BB != E; ++BB)
+    for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I) {
+      if (searchTypeInType(I->getType(), pDoubleType, false))
+        if (!(dyn_cast<FPExtInst>(I)))
+          bUseDoubles = true;
+      if (searchTypeInType(I->getType(), pHalfType, true))
+        sUsedExts._cl_khr_fp16 = true;
+
+      for (Instruction::op_iterator OI = (*I).op_begin(), OE = (*I).op_end();
+           OI != OE; ++OI) {
+        if (searchTypeInType((*OI)->getType(), pDoubleType, false))
+          if (!(dyn_cast<CallInst>(I) &&
+                dyn_cast<CallInst>(I)->getCalledFunction() &&
+                dyn_cast<CallInst>(I)->getCalledFunction()->isVarArg()))
+            bUseDoubles = true;
+        if (searchTypeInType((*OI)->getType(), pHalfType, true))
+          sUsedExts._cl_khr_fp16 = true;
+      }
+
+      CallInst* pCallInst = dyn_cast<CallInst>(I);
+      if (pCallInst && pCallInst->getCalledFunction()) {
+        std::string funcName = pCallInst->getCalledFunction()->getName().str();
+
+        for (size_t i = 0; i < sizeof(funcCallHandlers)/sizeof(funcCallHandlers[0]); i++) {
+          if (funcName.find(funcCallHandlers[i].funcName) == 0)
+            funcCallHandlers[i].handler(pCallInst, sUsedExts);
+        }
+      }
+    }
+}
diff --git a/clang/lib/CodeGen/CGSPIRMetadataAdder.h b/clang/lib/CodeGen/CGSPIRMetadataAdder.h
new file mode 100644
index 000000000000..b7808265b3bb
--- /dev/null
+++ b/clang/lib/CodeGen/CGSPIRMetadataAdder.h
@@ -0,0 +1,30 @@
+//===- SPIRMetadataAdder.h - Add SPIR related module scope metadata -------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/IR/Module.h"
+#include "clang/Basic/LangOptions.h"
+#include "clang/Basic/OpenCLOptions.h"
+#include <list>
+#include <string>
+
+#ifndef CLANG_CODEGEN_SPIRMETADATAADDER_H
+#define CLANG_CODEGEN_SPIRMETADATAADDER_H
+
+namespace clang {
+
+namespace CodeGen {
+
+  void AddSPIRMetadata(llvm::Module &M, int OCLVersion, std::list<std::string> sBuildOptions, const LangOptions& LangOpts, const OpenCLOptions& cl_options);
+
+} // end namespace CodeGen
+} // end namespace clang
+#endif
diff --git a/clang/lib/CodeGen/CGStmt.cpp b/clang/lib/CodeGen/CGStmt.cpp
index 6768a72319b0..6268c57b7540 100644
--- a/clang/lib/CodeGen/CGStmt.cpp
+++ b/clang/lib/CodeGen/CGStmt.cpp
@@ -2252,12 +2252,15 @@ static void UpdateAsmCallInst(llvm::CallBase &Result, bool HasSideEffect,
                                          llvm::ConstantAsMetadata::get(Loc)));
   }
 
+#if 0 // TODO: don't do this pessimistically, check asm code for convergent instructions instead
   if (CGF.getLangOpts().assumeFunctionsAreConvergent())
     // Conservatively, mark all inline asm blocks in CUDA or OpenCL as
     // convergent (meaning, they may call an intrinsically convergent op, such
     // as bar.sync, and so can't have certain optimizations applied around
     // them).
     Result.addFnAttr(llvm::Attribute::Convergent);
+#endif
+
   // Extract all of the register value results from the asm.
   if (ResultRegTypes.size() == 1) {
     RegResults.push_back(&Result);
diff --git a/clang/lib/CodeGen/CMakeLists.txt b/clang/lib/CodeGen/CMakeLists.txt
index 5d493595dcd5..166e6c08e507 100644
--- a/clang/lib/CodeGen/CMakeLists.txt
+++ b/clang/lib/CodeGen/CMakeLists.txt
@@ -2,6 +2,9 @@ set(LLVM_LINK_COMPONENTS
   Analysis
   BitReader
   BitWriter
+  BitWriter32
+  BitWriter50
+  MetalLib
   Core
   Coroutines
   Coverage
@@ -14,6 +17,7 @@ set(LLVM_LINK_COMPONENTS
   InstCombine
   Instrumentation
   LTO
+  LibFloor
   Linker
   MC
   ObjCARCOpts
@@ -22,6 +26,7 @@ set(LLVM_LINK_COMPONENTS
   ProfileData
   Remarks
   ScalarOpts
+  SPIRVLib
   Support
   Target
   TransformUtils
@@ -63,6 +68,7 @@ add_clang_library(clangCodeGen
   CGOpenMPRuntimeGPU.cpp
   CGOpenMPRuntimeNVPTX.cpp
   CGRecordLayoutBuilder.cpp
+  CGSPIRMetadataAdder.cpp
   CGStmt.cpp
   CGStmtOpenMP.cpp
   CGVTT.cpp
diff --git a/clang/lib/CodeGen/CodeGenAction.cpp b/clang/lib/CodeGen/CodeGenAction.cpp
index 52c54d3c7a72..a172e1c8cba1 100644
--- a/clang/lib/CodeGen/CodeGenAction.cpp
+++ b/clang/lib/CodeGen/CodeGenAction.cpp
@@ -593,6 +593,12 @@ BackendConsumer::InlineAsmDiagHandler(const llvm::DiagnosticInfoInlineAsm &D) {
     // If Loc is invalid, we still need to report the diagnostic, it just gets
     // no location info.
     FullSourceLoc Loc;
+    if (const auto instr = D.getInstruction(); instr) {
+      Message += ": ";
+      llvm::raw_string_ostream instr_stream(Message);
+      instr->print(instr_stream);
+      instr_stream.flush();
+    }
     Diags.Report(Loc, DiagID).AddString(Message);
   }
   // We handled all the possible severities.
@@ -962,7 +968,15 @@ GetOutputStream(CompilerInstance &CI, StringRef InFile, BackendAction Action) {
   case Backend_EmitLL:
     return CI.createDefaultOutputFile(false, InFile, "ll");
   case Backend_EmitBC:
+  case Backend_EmitBC32:
+  case Backend_EmitBC50:
     return CI.createDefaultOutputFile(true, InFile, "bc");
+  case Backend_EmitSPIRV:
+    return CI.createDefaultOutputFile(true, InFile, "spv");
+  case Backend_EmitSPIRVContainer:
+    return CI.createDefaultOutputFile(true, InFile, "spvc");
+  case Backend_EmitMetalLib:
+    return CI.createDefaultOutputFile(true, InFile, "metallib");
   case Backend_EmitNothing:
     return nullptr;
   case Backend_EmitMCNull:
@@ -1190,6 +1204,26 @@ void EmitBCAction::anchor() { }
 EmitBCAction::EmitBCAction(llvm::LLVMContext *_VMContext)
   : CodeGenAction(Backend_EmitBC, _VMContext) {}
 
+void EmitBC32Action::anchor() { }
+EmitBC32Action::EmitBC32Action(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitBC32, _VMContext) {}
+
+void EmitBC50Action::anchor() { }
+EmitBC50Action::EmitBC50Action(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitBC50, _VMContext) {}
+
+void EmitSPIRVAction::anchor() { }
+EmitSPIRVAction::EmitSPIRVAction(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitSPIRV, _VMContext) {}
+
+void EmitSPIRVContainerAction::anchor() { }
+EmitSPIRVContainerAction::EmitSPIRVContainerAction(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitSPIRVContainer, _VMContext) {}
+
+void EmitMetalLibAction::anchor() { }
+EmitMetalLibAction::EmitMetalLibAction(llvm::LLVMContext *_VMContext)
+  : CodeGenAction(Backend_EmitMetalLib, _VMContext) {}
+
 void EmitLLVMAction::anchor() { }
 EmitLLVMAction::EmitLLVMAction(llvm::LLVMContext *_VMContext)
   : CodeGenAction(Backend_EmitLL, _VMContext) {}
diff --git a/clang/lib/CodeGen/CodeGenFunction.cpp b/clang/lib/CodeGen/CodeGenFunction.cpp
index fc4540f26aab..991325c2c2de 100644
--- a/clang/lib/CodeGen/CodeGenFunction.cpp
+++ b/clang/lib/CodeGen/CodeGenFunction.cpp
@@ -30,6 +30,8 @@
 #include "clang/AST/StmtObjC.h"
 #include "clang/Basic/Builtins.h"
 #include "clang/Basic/CodeGenOptions.h"
+#include "clang/Basic/FileManager.h"
+#include "clang/Basic/SourceManager.h"
 #include "clang/Basic/TargetInfo.h"
 #include "clang/CodeGen/CGFunctionInfo.h"
 #include "clang/Frontend/FrontendDiagnostic.h"
@@ -46,6 +48,10 @@
 #include "llvm/Transforms/Scalar/LowerExpectIntrinsic.h"
 #include "llvm/Transforms/Utils/PromoteMemToReg.h"
 
+#include <sstream>
+#include <unordered_set>
+#include <fstream>
+
 using namespace clang;
 using namespace CodeGen;
 
@@ -589,14 +595,28 @@ CodeGenFunction::DecodeAddrUsedInPrologue(llvm::Value *F,
 }
 
 void CodeGenFunction::EmitOpenCLKernelMetadata(const FunctionDecl *FD,
-                                               llvm::Function *Fn)
+                                               llvm::Function *Fn,
+                                               const CGFunctionInfo &FnInfo)
 {
-  if (!FD->hasAttr<OpenCLKernelAttr>())
+  if (!FD->hasAttr<ComputeKernelAttr>() &&
+      !FD->hasAttr<GraphicsVertexShaderAttr>() &&
+	  !FD->hasAttr<GraphicsFragmentShaderAttr>()) {
     return;
+  }
 
   llvm::LLVMContext &Context = getLLVMContext();
 
-  CGM.GenOpenCLArgMetadata(Fn, FD, this);
+  SmallVector<llvm::Metadata *, 5> kernelMDArgs;
+  kernelMDArgs.push_back(llvm::ConstantAsMetadata::get(Fn));
+
+  if (CGM.getCodeGenOpts().EmitOpenCLArgMetadata)
+    CGM.GenOpenCLArgMetadata(Fn, FD, this, kernelMDArgs);
+
+  if (CGM.getLangOpts().Metal)
+    CGM.GenAIRMetadata(FD, Fn, FnInfo, kernelMDArgs, Builder);
+
+  if (CGM.getLangOpts().Vulkan)
+    CGM.GenVulkanMetadata(FD, Fn, Builder);
 
   if (const VecTypeHintAttr *A = FD->getAttr<VecTypeHintAttr>()) {
     QualType HintQTy = A->getTypeHint();
@@ -636,6 +656,142 @@ void CodeGenFunction::EmitOpenCLKernelMetadata(const FunctionDecl *FD,
     Fn->setMetadata("intel_reqd_sub_group_size",
                     llvm::MDNode::get(Context, AttrMDArgs));
   }
+
+  llvm::MDNode *kernelMDNode = llvm::MDNode::get(Context, kernelMDArgs);
+  llvm::NamedMDNode *MainMetadataNode;
+  if (!CGM.getLangOpts().Metal) {
+    MainMetadataNode = CGM.getModule().getOrInsertNamedMetadata("opencl.kernels");
+  } else {
+    MainMetadataNode = CGM.getModule().getOrInsertNamedMetadata(
+      (FD->hasAttr<GraphicsVertexShaderAttr>() ? "air.vertex" :
+       (FD->hasAttr<GraphicsFragmentShaderAttr>() ? "air.fragment" : "air.kernel")));
+  }
+  MainMetadataNode->addOperand(kernelMDNode);
+
+  // add soft-printf info
+  if (CGM.getCodeGenOpts().MetalSoftPrintf > 0 || CGM.getCodeGenOpts().VulkanSoftPrintf > 0) {
+    CGM.getModule().getOrInsertNamedMetadata("floor.soft_printf");
+  }
+
+  // add primitive id and barycentric coord info
+  if (CGM.getCodeGenOpts().GraphicsPrimitiveID) {
+    CGM.getModule().getOrInsertNamedMetadata("floor.primitive_id");
+  }
+  if (CGM.getCodeGenOpts().GraphicsBarycentricCoord) {
+    CGM.getModule().getOrInsertNamedMetadata("floor.barycentric_coord");
+  }
+
+  // additional air info
+  if (CGM.getLangOpts().Metal) {
+	  // only do this once
+	  llvm::NamedMDNode *AIRVersion = CGM.getModule().getOrInsertNamedMetadata("air.version");
+	  if (AIRVersion->getNumOperands() > 0) return;
+	  
+	  // insert empty sampler state, this will be filled in by MetalImage later on
+	  CGM.getModule().getOrInsertNamedMetadata("air.sampler_states");
+	  
+	  // figure out which metal versions we should emit
+	  std::array<uint32_t, 3> metal_version;
+	  std::array<uint32_t, 3> metal_language_version;
+	  const auto full_version = CGM.getLangOpts().MetalVersion;
+	  metal_version = {{ full_version / 100u, (full_version % 100u) / 10u, full_version % 10u }};
+	  metal_language_version = metal_version;
+	  
+	  SmallVector <llvm::Metadata*, 3> air_version;
+	  air_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_version[0])));
+	  air_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_version[1])));
+	  air_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_version[2])));
+	  AIRVersion->addOperand(llvm::MDNode::get(Context, air_version));
+	  
+	  llvm::NamedMDNode *AIRLangVersion = CGM.getModule().getOrInsertNamedMetadata("air.language_version");
+	  SmallVector <llvm::Metadata*, 4> air_lang_version;
+	  air_lang_version.push_back(llvm::MDString::get(Context, "Metal"));
+	  air_lang_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_language_version[0])));
+	  air_lang_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_language_version[1])));
+	  air_lang_version.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(metal_language_version[2])));
+	  AIRLangVersion->addOperand(llvm::MDNode::get(Context, air_lang_version));
+	  
+	  llvm::NamedMDNode *AIRCompOpts = CGM.getModule().getOrInsertNamedMetadata("air.compile_options");
+	  AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.denorms_disable")));
+	  AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.fast_math_enable")));
+	  if (CGM.getLangOpts().MetalVersion < 230) {
+	    AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.framebuffer_fetch_disable")));
+	    AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.native_double_disable")));
+	    if (CGM.getLangOpts().MetalVersion >= 220) {
+	      AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.native_long_long_enable")));
+	      AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.native_wide_vectors_disable")));
+	    }
+	  } else {
+	    AIRCompOpts->addOperand(llvm::MDNode::get(Context, llvm::MDString::get(Context, "air.compile.framebuffer_fetch_enable")));
+	  }
+
+	  // emit debug info
+	  if (CGM.getLangOpts().MetalVersion >= 240 &&
+	      CGM.getCodeGenOpts().getDebugInfo() != codegenoptions::NoDebugInfo) {
+	    // emit "air.source_file_name"
+	    llvm::NamedMDNode *AIRSourceFile = CGM.getModule().getOrInsertNamedMetadata("air.source_file_name");
+	    SmallVector <llvm::Metadata*, 1> air_source_file;
+	    std::string src_file_name_str = CGM.getModule().getSourceFileName();
+	    SmallVector<char> src_file_name(src_file_name_str.size());
+	    src_file_name.assign(src_file_name_str.begin(), src_file_name_str.end());
+	    CGM.getContext().getSourceManager().getFileManager().makeAbsolutePath(src_file_name);
+	    src_file_name_str.resize(src_file_name.size(), '\0');
+	    src_file_name_str.assign(src_file_name.begin(), src_file_name.end());
+	    air_source_file.push_back(llvm::MDString::get(Context, src_file_name_str.data()));
+	    AIRSourceFile->addOperand(llvm::MDNode::get(Context, air_source_file));
+
+	    // emit "llvm_utils.workingdir"
+	    std::string_view src_file_name_view(src_file_name.data(), src_file_name.size());
+	    const auto last_slash_pos = src_file_name_view.rfind('/');
+	    if (last_slash_pos != std::string::npos) {
+	      llvm::NamedMDNode *workingdir_md = CGM.getModule().getOrInsertNamedMetadata("llvm_utils.workingdir");
+	      SmallVector <llvm::Metadata*, 1> workingdir;
+	      const std::string working_dir_str(src_file_name.data(), last_slash_pos);
+	      workingdir.push_back(llvm::MDString::get(Context, working_dir_str));
+	      workingdir_md->addOperand(llvm::MDNode::get(Context, workingdir));
+	    }
+	  }
+	  
+	  // AIR limits (since Metal 2.3)
+	  if (CGM.getLangOpts().MetalVersion >= 230) {
+	    llvm::NamedMDNode *ModuleFlags = CGM.getModule().getOrInsertNamedMetadata("llvm.module.flags");
+	    static const std::vector<std::pair<std::string, int>> limits {
+	      { "air.max_device_buffers", 31 },
+	      { "air.max_constant_buffers", 31 },
+	      { "air.max_threadgroup_buffers", 31 },
+	      { "air.max_textures", 128 },
+	      { "air.max_read_write_textures", 8 },
+	      { "air.max_samplers", 16 },
+	    };
+	    for (const auto& limit : limits) {
+	      SmallVector <llvm::Metadata*, 3> air_limit;
+	      air_limit.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(7)));
+	      air_limit.push_back(llvm::MDString::get(Context, limit.first));
+	      air_limit.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(limit.second)));
+	      ModuleFlags->addOperand(llvm::MDNode::get(Context, air_limit));
+	    }
+      }
+  }
+
+  // additional vulkan info
+  if (CGM.getLangOpts().Vulkan) {
+	  // only do this once
+	  llvm::NamedMDNode *VulkanVersion = CGM.getModule().getOrInsertNamedMetadata("vulkan.version");
+	  if (VulkanVersion->getNumOperands() > 0) return;
+	  
+	  // figure out which vulkan versions we should emit
+	  std::array<uint32_t, 2> vulkan_version {{
+		  CGM.getLangOpts().VulkanVersion / 100,
+		  (CGM.getLangOpts().VulkanVersion % 100) / 10
+	  }};
+	  
+	  SmallVector <llvm::Metadata*, 2> vulkan_version_md;
+	  vulkan_version_md.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(vulkan_version[0])));
+	  vulkan_version_md.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(vulkan_version[1])));
+	  VulkanVersion->addOperand(llvm::MDNode::get(Context, vulkan_version_md));
+  }
+
+  // NOTE: additional/global opencl metadata is handled in CGSPIRMetadataAdder
 }
 
 /// Determine whether the function F ends with a return stmt.
@@ -895,9 +1051,16 @@ void CodeGenFunction::StartFunction(GlobalDecl GD, QualType RetTy,
   if (D && D->hasAttr<NoProfileFunctionAttr>())
     Fn->addFnAttr(llvm::Attribute::NoProfile);
 
-  if (FD && getLangOpts().OpenCL) {
-    // Add metadata for a kernel function.
-    EmitOpenCLKernelMetadata(FD, Fn);
+  // emit compute metadata
+  if (FD && (getLangOpts().OpenCL || getLangOpts().CUDA || getLangOpts().FloorHostCompute)) {
+    // add floor specific metadata for kernel functions
+    EmitFloorKernelMetadata(FD, Fn, Args, FnInfo, CGM);
+    
+    // OpenCL/SPIR, Metal and Vulkan specific metadata
+    if (getLangOpts().OpenCL) {
+      // Add metadata for a kernel function.
+      EmitOpenCLKernelMetadata(FD, Fn, FnInfo);
+    }
   }
 
   // If we are checking function types, emit a function type signature as
@@ -943,8 +1106,7 @@ void CodeGenFunction::StartFunction(GlobalDecl GD, QualType RetTy,
   //     recursive code, virtual functions or make use of C++ libraries that
   //     are not compiled for the device.
   if (FD && ((getLangOpts().CPlusPlus && FD->isMain()) ||
-             getLangOpts().OpenCL || getLangOpts().SYCLIsDevice ||
-             (getLangOpts().CUDA && FD->hasAttr<CUDAGlobalAttr>())))
+             getLangOpts().SYCLIsDevice))
     Fn->addFnAttr(llvm::Attribute::NoRecurse);
 
   llvm::RoundingMode RM = getLangOpts().getFPRoundingMode();
@@ -1084,7 +1246,18 @@ void CodeGenFunction::StartFunction(GlobalDecl GD, QualType RetTy,
     Addr = Builder.CreateAlignedLoad(Ty, Addr, getPointerAlign(), "agg.result");
     ReturnValue = Address(Addr, CGM.getNaturalTypeAlignment(RetTy));
   } else {
-    ReturnValue = CreateIRTemp(RetTy, "retval");
+    // fix retval allocation for vertex/fragment shader return values (use the computed coerce type)
+    const FunctionDecl *FD = dyn_cast_or_null<FunctionDecl>(D);
+    if (FD &&
+        (FD->hasAttr<GraphicsVertexShaderAttr>() ||
+         FD->hasAttr<GraphicsFragmentShaderAttr>())) {
+      auto RetAlloc = CreateTempAlloca(FnInfo.getReturnInfo().getCoerceToType(), "retval");
+      CharUnits Align = getContext().getTypeAlignInChars(RetTy);
+      RetAlloc->setAlignment(Align.getAsAlign());
+      ReturnValue = Address(RetAlloc, Align);
+	} else {
+      ReturnValue = CreateIRTemp(RetTy, "retval");
+	}
 
     // Tell the epilog emitter to autorelease the result.  We do this
     // now so that various specialized functions can suppress it
@@ -1285,6 +1458,9 @@ QualType CodeGenFunction::BuildFunctionArgList(GlobalDecl GD,
   if (MD && (isa<CXXConstructorDecl>(MD) || isa<CXXDestructorDecl>(MD)))
     CGM.getCXXABI().addImplicitStructorParams(*this, ResTy, Args);
 
+  // add additional implicit internal args if necessary
+  CGM.getTypes().handleMetalVulkanEntryFunction(nullptr, &Args, FD);
+
   return ResTy;
 }
 
@@ -1374,7 +1550,7 @@ void CodeGenFunction::GenerateCode(GlobalDecl GD, llvm::Function *Fn,
     EmitConstructorBody(Args);
   else if (getLangOpts().CUDA &&
            !getLangOpts().CUDAIsDevice &&
-           FD->hasAttr<CUDAGlobalAttr>())
+           FD->hasAttr<ComputeKernelAttr>())
     CGM.getCUDARuntime().emitDeviceStub(*this, Args);
   else if (isa<CXXMethodDecl>(FD) &&
            cast<CXXMethodDecl>(FD)->isLambdaStaticInvoker()) {
diff --git a/clang/lib/CodeGen/CodeGenFunction.h b/clang/lib/CodeGen/CodeGenFunction.h
index dd60e21b0ce1..12324cc99fcf 100644
--- a/clang/lib/CodeGen/CodeGenFunction.h
+++ b/clang/lib/CodeGen/CodeGenFunction.h
@@ -1937,7 +1937,14 @@ private:
   /// Add OpenCL kernel arg metadata and the kernel attribute metadata to
   /// the function metadata.
   void EmitOpenCLKernelMetadata(const FunctionDecl *FD,
-                                llvm::Function *Fn);
+                                llvm::Function *Fn,
+                               const CGFunctionInfo &FnInfo);
+
+  void EmitFloorKernelMetadata(const FunctionDecl *FD,
+                               llvm::Function *Fn,
+                               const FunctionArgList &Args,
+                               const CGFunctionInfo &FnInfo,
+                               CodeGenModule &CGM);
 
 public:
   CodeGenFunction(CodeGenModule &cgm, bool suppressNewContext=false);
@@ -4631,14 +4638,16 @@ private:
   ///
   /// \param AI - The first function argument of the expansion.
   void ExpandTypeFromArgs(QualType Ty, LValue Dst,
-                          llvm::Function::arg_iterator &AI);
+                          llvm::Function::arg_iterator &AI,
+                          const CallingConv CC);
 
   /// ExpandTypeToArgs - Expand an CallArg \arg Arg, with the LLVM type for \arg
   /// Ty, into individual arguments on the provided vector \arg IRCallArgs,
   /// starting at index \arg IRCallArgPos. See ABIArgInfo::Expand.
   void ExpandTypeToArgs(QualType Ty, CallArg Arg, llvm::FunctionType *IRFuncTy,
                         SmallVectorImpl<llvm::Value *> &IRCallArgs,
-                        unsigned &IRCallArgPos);
+                        unsigned &IRCallArgPos,
+                        const CallingConv CC);
 
   llvm::Value* EmitAsmInput(const TargetInfo::ConstraintInfo &Info,
                             const Expr *InputExpr, std::string &ConstraintStr);
diff --git a/clang/lib/CodeGen/CodeGenModule.cpp b/clang/lib/CodeGen/CodeGenModule.cpp
index 74c4490d5422..489f37b1e628 100644
--- a/clang/lib/CodeGen/CodeGenModule.cpp
+++ b/clang/lib/CodeGen/CodeGenModule.cpp
@@ -18,6 +18,7 @@
 #include "CGDebugInfo.h"
 #include "CGObjCRuntime.h"
 #include "CGOpenCLRuntime.h"
+#include "CGSPIRMetadataAdder.h"
 #include "CGOpenMPRuntime.h"
 #include "CGOpenMPRuntimeAMDGCN.h"
 #include "CGOpenMPRuntimeNVPTX.h"
@@ -39,6 +40,7 @@
 #include "clang/Basic/CharInfo.h"
 #include "clang/Basic/CodeGenOptions.h"
 #include "clang/Basic/Diagnostic.h"
+#include "clang/Basic/DiagnosticSema.h"
 #include "clang/Basic/FileManager.h"
 #include "clang/Basic/Module.h"
 #include "clang/Basic/SourceManager.h"
@@ -65,6 +67,11 @@
 #include "llvm/Support/TimeProfiler.h"
 #include "llvm/Support/X86TargetParser.h"
 
+#include <sstream>
+#include <unordered_set>
+#include <fstream>
+#include <optional>
+
 using namespace clang;
 using namespace CodeGen;
 
@@ -629,7 +636,10 @@ void CodeGenModule::Release() {
     // parser will drop debug info with a different version number
     // (and warn about it, too).
     getModule().addModuleFlag(llvm::Module::Warning, "Debug Info Version",
-                              llvm::DEBUG_METADATA_VERSION);
+                              Context.getTargetInfo().getTriple().getOS() != llvm::Triple::IOS ?
+                              llvm::DEBUG_METADATA_VERSION :
+                              // Metal/iOS uses/requires a very specific metadata version number
+                              llvm::IOS_METAL_DEBUG_METADATA_VERSION);
 
   // We need to record the widths of enums and wchar_t, so that we can generate
   // the correct build attributes in the ARM backend. wchar_size is also used by
@@ -733,9 +743,10 @@ void CodeGenModule::Release() {
 
   // Emit OpenCL specific module metadata: OpenCL/SPIR version.
   if (LangOpts.OpenCL) {
-    EmitOpenCLMetadata();
     // Emit SPIR version.
     if (getTriple().isSPIR()) {
+      EmitOpenCLMetadata();
+
       // SPIR v2.0 s2.12 - The SPIR version used by the module is stored in the
       // opencl.spir.version named metadata.
       // C++ for OpenCL has a distinct mapping for version compatibility with
@@ -793,6 +804,9 @@ void CodeGenModule::Release() {
 
   SimplifyPersonality();
 
+  if (getLangOpts().OpenCL && !getLangOpts().Metal)
+    EmitOCLAnnotations();
+
   if (getCodeGenOpts().EmitDeclMetadata)
     EmitDeclMetadata();
 
@@ -802,6 +816,34 @@ void CodeGenModule::Release() {
   if (CGDebugInfo *DI = getModuleDebugInfo())
     DI->finalize();
 
+  const llvm::Triple TT(TheModule.getTargetTriple());
+  if (TT.getArch() == llvm::Triple::ArchType::spir ||
+      TT.getArch() == llvm::Triple::ArchType::spir64) {
+    std::list<std::string> sBuildOptions;
+    std::string tmp = getCodeGenOpts().SPIRCompileOptions;
+    while (!tmp.empty()) {
+      auto first = tmp.find_first_not_of(' ');
+      auto last = tmp.find_first_of(' ', first);
+
+      std::string s;
+      if (last != std::string::npos)
+        s = tmp.substr(first, last-first);
+      else if (first != std::string::npos)
+        s = tmp.substr(first);
+      else
+        s = "";
+
+      if (!s.empty())
+        sBuildOptions.push_back(s);
+
+      if (last != std::string::npos)
+        tmp = tmp.substr(last);
+      else
+        tmp = "";
+    }
+    AddSPIRMetadata(TheModule, getLangOpts().OpenCLVersion, sBuildOptions, LangOpts, getContext().getOpenCLFeatures());
+  }
+
   if (getCodeGenOpts().EmitVersionIdentMetadata)
     EmitVersionIdentMetadata();
 
@@ -1261,7 +1303,7 @@ static std::string getMangledNameImpl(CodeGenModule &CGM, GlobalDecl GD,
     if (FD &&
         FD->getType()->castAs<FunctionType>()->getCallConv() == CC_X86RegCall) {
       Out << "__regcall3__" << II->getName();
-    } else if (FD && FD->hasAttr<CUDAGlobalAttr>() &&
+    } else if (FD && FD->hasAttr<ComputeKernelAttr>() &&
                GD.getKernelReferenceKind() == KernelReferenceKind::Stub) {
       Out << "__device_stub__" << II->getName();
     } else {
@@ -1377,6 +1419,7 @@ StringRef CodeGenModule::getMangledName(GlobalDecl GD) {
   const auto *ND = cast<NamedDecl>(GD.getDecl());
   std::string MangledName = getMangledNameImpl(*this, GD, ND);
 
+#if 0 // we don't want this
   // Ensure either we have different ABIs between host and device compilations,
   // says host compilation following MSVC ABI but device compilation follows
   // Itanium C++ ABI or, if they follow the same ABI, kernel names after
@@ -1386,7 +1429,7 @@ StringRef CodeGenModule::getMangledName(GlobalDecl GD) {
   // result in undefined behavior. Even though we cannot check that naming
   // directly between host- and device-compilations, the host- and
   // device-mangling in host compilation could help catching certain ones.
-  assert(!isa<FunctionDecl>(ND) || !ND->hasAttr<CUDAGlobalAttr>() ||
+  assert(!isa<FunctionDecl>(ND) || !ND->hasAttr<ComputeKernelAttr>() ||
          getLangOpts().CUDAIsDevice ||
          (getContext().getAuxTargetInfo() &&
           (getContext().getAuxTargetInfo()->getCXXABI() !=
@@ -1396,6 +1439,7 @@ StringRef CodeGenModule::getMangledName(GlobalDecl GD) {
                  *this,
                  GD.getWithKernelReferenceKind(KernelReferenceKind::Kernel),
                  ND));
+#endif
 
   auto Result = Manglings.insert(std::make_pair(MangledName, GD));
   return MangledDeclNames[CanonicalGD] = Result.first->first();
@@ -1525,6 +1569,7 @@ void CodeGenModule::SetLLVMFunctionAttributes(GlobalDecl GD,
   F->setCallingConv(static_cast<llvm::CallingConv::ID>(CallingConv));
 }
 
+#if 0 // unused
 static void removeImageAccessQualifier(std::string& TyName) {
   std::string ReadOnlyQual("__read_only");
   std::string::size_type ReadOnlyPos = TyName.find(ReadOnlyQual);
@@ -1544,6 +1589,7 @@ static void removeImageAccessQualifier(std::string& TyName) {
     }
   }
 }
+#endif
 
 // Returns the address space id that should be produced to the
 // kernel_arg_addr_space metadata. This is always fixed to the ids
@@ -1551,6 +1597,7 @@ static void removeImageAccessQualifier(std::string& TyName) {
 // for example in clGetKernelArgInfo() implementation between the address
 // spaces with targets without unique mapping to the OpenCL address spaces
 // (basically all single AS CPUs).
+#if 0 // unused
 static unsigned ArgInfoAddressSpace(LangAS AS) {
   switch (AS) {
   case LangAS::opencl_global:
@@ -1562,152 +1609,2182 @@ static unsigned ArgInfoAddressSpace(LangAS AS) {
   case LangAS::opencl_generic:
     return 4; // Not in SPIR 2.0 specs.
   case LangAS::opencl_global_device:
-    return 5;
+    return 100;
   case LangAS::opencl_global_host:
-    return 6;
+    return 101;
   default:
     return 0; // Assume private.
   }
 }
+#endif
+
+// will recurse through the specified class/struct decl and its base classes,
+// returning a vector containing all iterators to all contained fields
+static std::vector<RecordDecl::field_iterator> get_aggregate_fields(const CXXRecordDecl* decl) {
+	if (decl == nullptr) return {};
+	
+	// must have definition
+	if (!decl->hasDefinition()) return {};
+	
+	//
+	std::vector<RecordDecl::field_iterator> ret;
+	
+	// iterate over / recurse into all bases
+	for (const auto& base : decl->bases()) {
+		const auto base_ret = get_aggregate_fields(base.getType()->getAsCXXRecordDecl());
+		if (!base_ret.empty()) {
+			ret.insert(ret.end(), base_ret.begin(), base_ret.end());
+		}
+	}
+	
+	// iterate over all fields/members
+	for (auto iter = decl->field_begin(); iter != decl->field_end(); ++iter) {
+		ret.push_back(iter);
+	}
+	
+	return ret;
+}
+
+// will recurse through the specified class/struct decl and its base classes,
+// returning a vector containing all iterators to all contained image types
+// NOTE: will return an empty vector if not a proper aggregate image
+static std::vector<RecordDecl::field_iterator> get_aggregate_image_fields(const CXXRecordDecl* decl) {
+	// extract all fields, then check if all are image types (if one isn't, fail)
+	auto ret = get_aggregate_fields(decl);
+	for (auto iter = ret.begin(); iter != ret.end(); ) {
+		if (!(*iter)->getType()->isImageType() &&
+			!(*iter)->getType()->isArrayImageType(false)) {
+			// ignore zero-sized fields (if there are not an image)
+			if ((*iter)->isZeroSize(decl->getASTContext())) {
+				iter = ret.erase(iter);
+				continue;
+			}
+			return {};
+		}
+		++iter;
+	}
+	return ret;
+}
+
+struct array_image_info_t {
+	QualType image_type;
+	FloorImageFlagsAttr* flags { nullptr };
+	FloorImageDataTypeAttr* data_type { nullptr };
+	uint32_t element_count { 0u };
+};
+static std::optional<array_image_info_t> get_array_image_info(const CXXRecordDecl* decl, const ASTContext& ASTCtx) {
+	const auto ret = get_aggregate_fields(decl);
+	if (ret.size() != 1) return {};
+	
+	FieldDecl* arr_field_decl = *ret[0];
+	const ConstantArrayType *CAT = ASTCtx.getAsConstantArrayType(arr_field_decl->getType());
+	if (!CAT) return {};
+	
+	auto img_cxx_rdecl = CAT->getElementType()->getAsCXXRecordDecl();
+	if (!img_cxx_rdecl) return {};
+	
+	// handle nested/2D image arrays (Vulkan)
+	// NOTE: expecting a single field (write-only) or two fields (read-write) here,
+	// with the writable image consisting of a pointer to another array
+	const auto inner = get_aggregate_image_fields(img_cxx_rdecl);
+	if (inner.size() == 1 || inner.size() == 2) {
+		const auto writable_img_idx = inner.size() - 1;
+		if ((*inner[writable_img_idx])->getType()->isPointerType()) {
+			FieldDecl* inner_field_decl = *inner[0];
+			const ConstantArrayType *inner_CAT = ASTCtx.getAsConstantArrayType(inner_field_decl->getType()->getPointeeType());
+			// if this is not an array, something is wrong (field isn't an image either)
+			if (!inner_CAT) return {};
+			
+			// NOTE: the outer array element count is reported here (which is user-specified),
+			//       the inner array element count is an implementation detail
+			return array_image_info_t {
+				inner_CAT->getElementType(),
+				inner[0]->getAttr<FloorImageFlagsAttr>(),
+				inner[0]->getAttr<FloorImageDataTypeAttr>(),
+				uint32_t(CAT->getSize().getZExtValue()),
+			};
+		}
+	}
+	
+	auto img_fields = get_aggregate_image_fields(img_cxx_rdecl);
+	if (img_fields.size() != 1) return {};
+	
+	return array_image_info_t {
+		img_fields[0]->getType(),
+		img_fields[0]->getAttr<FloorImageFlagsAttr>(),
+		img_fields[0]->getAttr<FloorImageDataTypeAttr>(),
+		uint32_t(CAT->getSize().getZExtValue()),
+	};
+}
+
+// will recurse through the specified class/struct decl and its base classes,
+// returning the first image flags attribute that it encounters (or nullptr if none)
+static const FloorImageFlagsAttr* get_aggregate_image_flags_attr(const CXXRecordDecl* decl) {
+	if (decl == nullptr) return nullptr;
+	
+	// must have definition
+	if (!decl->hasDefinition()) return nullptr;
+	
+	// iterate over / recurse into all bases
+	for (const auto& base : decl->bases()) {
+		const auto base_ret = get_aggregate_image_flags_attr(base.getTypeSourceInfo()->getType()->getAsCXXRecordDecl());
+		if (base_ret != nullptr) {
+			return base_ret;
+		}
+	}
+	
+	// iterate over all fields/members and return the first access attr
+	for (auto iter = decl->field_begin(); iter != decl->field_end(); ++iter) {
+		if (iter->isZeroSize(decl->getASTContext())) {
+			continue;
+		}
+		
+		// try direct attr first
+		const FloorImageFlagsAttr* flags_attr = iter->getAttr<FloorImageFlagsAttr>();
+		if (flags_attr != nullptr) {
+			return flags_attr;
+		}
+		
+		// then check if this is a c++ decl (struct/union/class) and check if it has the attr
+		const auto as_decl = iter->getType()->getAsCXXRecordDecl();
+		if (as_decl != nullptr) {
+			flags_attr = as_decl->getAttr<FloorImageFlagsAttr>();
+			if (flags_attr != nullptr) {
+				return flags_attr;
+			}
+		}
+	}
+	
+	return nullptr;
+}
+
+// Metadata values extractors.
+static std::string getScalarMetadataValue(const clang::Type *Ty,
+										  const PrintingPolicy &Policy) {
+	assert(Ty && "NULL type");
+	
+	if (Ty->isHalfType()) return "half";
+	
+	if (!Ty->isUnsignedIntegerType()) {
+		return QualType(Ty, 0).getAsString(Policy);
+	}
+	
+	std::string TyName = QualType(Ty, 0).getAsString();
+	if (llvm::StringRef(TyName).startswith("unsigned")) {
+		// Replace unsigned <ty> with u<ty>
+		TyName.erase(1, 8);
+	}
+	
+	return TyName;
+}
+
+static std::string getVectorMetadataValue(const clang::ExtVectorType *Ty,
+										  const PrintingPolicy &Policy) {
+	assert(Ty && "NULL type");
+	
+	const clang::VectorType *VTy = llvm::dyn_cast<clang::VectorType>(Ty);
+	assert(VTy && "Cast to vector failed");
+	
+	auto Ret = getScalarMetadataValue(VTy->getElementType().getTypePtr(), Policy);
+	Ret += std::to_string(VTy->getNumElements());
+	
+	return Ret;
+}
+
+// Returns true if the given module has SPIR (32/64) target
+static bool isSpirTarget(const llvm::Module *M) {
+	assert (M && "NULL module given");
+	return llvm::StringRef(M->getTargetTriple()).startswith("spir");
+}
+
+static std::string getPipeMetadataValue(const clang::PipeType *Ty,
+										const PrintingPolicy &Policy) {
+	assert(Ty && "Null type");
+	
+	const clang::QualType ElemTy = Ty->getElementType();
+	if (const clang::ExtVectorType *VTy = ElemTy->getAs<ExtVectorType>()) {
+		return getVectorMetadataValue(VTy, Policy);
+	}
+	
+	return getScalarMetadataValue(ElemTy.getTypePtr(), Policy);
+}
+
+static FloorImageFlagsAttr* getFloorImageFlagsAttribute(const NamedDecl* decl_, const QualType* type_, llvm::LLVMContext &Context) {
+	const Decl* decl = decl_;
+	if (type_) {
+		if (const TypedefType *QTT = dyn_cast<const TypedefType>(*type_)) {
+			decl = QTT->getDecl();
+		}
+	}
+	
+	if (decl->hasAttr<FloorImageFlagsAttr>()) {
+		return decl->getAttr<FloorImageFlagsAttr>();
+	}
+	return nullptr;
+}
 
 void CodeGenModule::GenOpenCLArgMetadata(llvm::Function *Fn,
-                                         const FunctionDecl *FD,
-                                         CodeGenFunction *CGF) {
-  assert(((FD && CGF) || (!FD && !CGF)) &&
-         "Incorrect use - FD and CGF should either be both null or not!");
-  // Create MDNodes that represent the kernel arg metadata.
-  // Each MDNode is a list in the form of "key", N number of values which is
-  // the same number of values as their are kernel arguments.
-
-  const PrintingPolicy &Policy = Context.getPrintingPolicy();
-
-  // MDNode for the kernel argument address space qualifiers.
-  SmallVector<llvm::Metadata *, 8> addressQuals;
-
-  // MDNode for the kernel argument access qualifiers (images only).
-  SmallVector<llvm::Metadata *, 8> accessQuals;
-
-  // MDNode for the kernel argument type names.
-  SmallVector<llvm::Metadata *, 8> argTypeNames;
-
-  // MDNode for the kernel argument base type names.
-  SmallVector<llvm::Metadata *, 8> argBaseTypeNames;
-
-  // MDNode for the kernel argument type qualifiers.
-  SmallVector<llvm::Metadata *, 8> argTypeQuals;
-
-  // MDNode for the kernel argument names.
-  SmallVector<llvm::Metadata *, 8> argNames;
-
-  if (FD && CGF)
-    for (unsigned i = 0, e = FD->getNumParams(); i != e; ++i) {
-      const ParmVarDecl *parm = FD->getParamDecl(i);
-      QualType ty = parm->getType();
-      std::string typeQuals;
-
-      // Get image and pipe access qualifier:
-      if (ty->isImageType() || ty->isPipeType()) {
-        const Decl *PDecl = parm;
-        if (auto *TD = dyn_cast<TypedefType>(ty))
-          PDecl = TD->getDecl();
-        const OpenCLAccessAttr *A = PDecl->getAttr<OpenCLAccessAttr>();
-        if (A && A->isWriteOnly())
-          accessQuals.push_back(llvm::MDString::get(VMContext, "write_only"));
-        else if (A && A->isReadWrite())
-          accessQuals.push_back(llvm::MDString::get(VMContext, "read_write"));
-        else
-          accessQuals.push_back(llvm::MDString::get(VMContext, "read_only"));
-      } else
-        accessQuals.push_back(llvm::MDString::get(VMContext, "none"));
-
-      // Get argument name.
-      argNames.push_back(llvm::MDString::get(VMContext, parm->getName()));
-
-      auto getTypeSpelling = [&](QualType Ty) {
-        auto typeName = Ty.getUnqualifiedType().getAsString(Policy);
-
-        if (Ty.isCanonical()) {
-          StringRef typeNameRef = typeName;
-          // Turn "unsigned type" to "utype"
-          if (typeNameRef.consume_front("unsigned "))
-            return std::string("u") + typeNameRef.str();
-          if (typeNameRef.consume_front("signed "))
-            return typeNameRef.str();
-        }
-
-        return typeName;
-      };
-
-      if (ty->isPointerType()) {
-        QualType pointeeTy = ty->getPointeeType();
-
-        // Get address qualifier.
-        addressQuals.push_back(
-            llvm::ConstantAsMetadata::get(CGF->Builder.getInt32(
-                ArgInfoAddressSpace(pointeeTy.getAddressSpace()))));
-
-        // Get argument type name.
-        std::string typeName = getTypeSpelling(pointeeTy) + "*";
-        std::string baseTypeName =
-            getTypeSpelling(pointeeTy.getCanonicalType()) + "*";
-        argTypeNames.push_back(llvm::MDString::get(VMContext, typeName));
-        argBaseTypeNames.push_back(
-            llvm::MDString::get(VMContext, baseTypeName));
-
-        // Get argument type qualifiers:
-        if (ty.isRestrictQualified())
-          typeQuals = "restrict";
-        if (pointeeTy.isConstQualified() ||
-            (pointeeTy.getAddressSpace() == LangAS::opencl_constant))
-          typeQuals += typeQuals.empty() ? "const" : " const";
-        if (pointeeTy.isVolatileQualified())
-          typeQuals += typeQuals.empty() ? "volatile" : " volatile";
-      } else {
-        uint32_t AddrSpc = 0;
-        bool isPipe = ty->isPipeType();
-        if (ty->isImageType() || isPipe)
-          AddrSpc = ArgInfoAddressSpace(LangAS::opencl_global);
-
-        addressQuals.push_back(
-            llvm::ConstantAsMetadata::get(CGF->Builder.getInt32(AddrSpc)));
-
-        // Get argument type name.
-        ty = isPipe ? ty->castAs<PipeType>()->getElementType() : ty;
-        std::string typeName = getTypeSpelling(ty);
-        std::string baseTypeName = getTypeSpelling(ty.getCanonicalType());
-
-        // Remove access qualifiers on images
-        // (as they are inseparable from type in clang implementation,
-        // but OpenCL spec provides a special query to get access qualifier
-        // via clGetKernelArgInfo with CL_KERNEL_ARG_ACCESS_QUALIFIER):
-        if (ty->isImageType()) {
-          removeImageAccessQualifier(typeName);
-          removeImageAccessQualifier(baseTypeName);
-        }
-
-        argTypeNames.push_back(llvm::MDString::get(VMContext, typeName));
-        argBaseTypeNames.push_back(
-            llvm::MDString::get(VMContext, baseTypeName));
-
-        if (isPipe)
-          typeQuals = "pipe";
-      }
-      argTypeQuals.push_back(llvm::MDString::get(VMContext, typeQuals));
-    }
-
-  Fn->setMetadata("kernel_arg_addr_space",
-                  llvm::MDNode::get(VMContext, addressQuals));
-  Fn->setMetadata("kernel_arg_access_qual",
-                  llvm::MDNode::get(VMContext, accessQuals));
-  Fn->setMetadata("kernel_arg_type",
-                  llvm::MDNode::get(VMContext, argTypeNames));
-  Fn->setMetadata("kernel_arg_base_type",
-                  llvm::MDNode::get(VMContext, argBaseTypeNames));
-  Fn->setMetadata("kernel_arg_type_qual",
-                  llvm::MDNode::get(VMContext, argTypeQuals));
-  if (getCodeGenOpts().EmitOpenCLArgMetadata)
-    Fn->setMetadata("kernel_arg_name",
-                    llvm::MDNode::get(VMContext, argNames));
+										 const FunctionDecl *FD,
+										 CodeGenFunction *CGF,
+										 SmallVector<llvm::Metadata *, 5> &kernelMDArgs) {
+	if (!FD || !CGF) {
+		assert(false && "should not be here");
+	}
+	
+	// Create MDNodes that represent the kernel arg metadata.
+	// Each MDNode is a list in the form of "key", N number of values which is
+	// the same number of values as their are kernel arguments.
+	
+	// TODO: proper handling of EmitVerbose + kernel arg names
+	const PrintingPolicy &Policy = Context.getPrintingPolicy();
+	const bool EmitVerbose = getCodeGenOpts().EmitOpenCLArgMetadata;
+	auto& Builder = CGF->Builder;
+	
+	if (!isSpirTarget(Fn->getParent()) && !EmitVerbose) {
+		return;
+	}
+	
+	// MDNode for the kernel argument address space qualifiers.
+	SmallVector<llvm::Metadata *, 8> addressQuals;
+	addressQuals.push_back(llvm::MDString::get(VMContext, "kernel_arg_addr_space"));
+	
+	// MDNode for the kernel argument access qualifiers (images only).
+	SmallVector<llvm::Metadata *, 8> accessQuals;
+	accessQuals.push_back(llvm::MDString::get(VMContext, "kernel_arg_access_qual"));
+	
+	// MDNode for the kernel argument type names.
+	SmallVector<llvm::Metadata *, 8> argTypeNames;
+	argTypeNames.push_back(llvm::MDString::get(VMContext, "kernel_arg_type"));
+	
+	// MDNode for the kernel argument base type names.
+	SmallVector<llvm::Metadata *, 8> argBaseTypeNames;
+	argBaseTypeNames.push_back(llvm::MDString::get(VMContext, "kernel_arg_base_type"));
+	
+	// MDNode for the kernel argument type qualifiers.
+	SmallVector<llvm::Metadata *, 8> argTypeQuals;
+	argTypeQuals.push_back(llvm::MDString::get(VMContext, "kernel_arg_type_qual"));
+	
+	// MDNode for the kernel argument names.
+	SmallVector<llvm::Metadata *, 8> argNames;
+	if (EmitVerbose) {
+		argNames.push_back(llvm::MDString::get(VMContext, "kernel_arg_name"));
+	}
+	
+	// Creates a canonical name for complex types. In case of anonymous types, the
+	// function appends the meta-type name as prefix: e.g., in case the type is
+	// defined as: typedef struct {...} S, the method returns struct S.
+	static const auto canonicalName = [](const std::string &TyName,
+										 const std::string &MetaTyName) {
+		if (StringRef(TyName).startswith(MetaTyName)) {
+			return TyName;
+		}
+		return std::string(MetaTyName) + " __" + TyName;
+	};
+	
+	static const auto getComplexMetadataValue = [](const clang::Type *Ty,
+												   const PrintingPolicy &Policy) {
+		std::string TyName = QualType(Ty, 0).getCanonicalType().getAsString();
+		if (Ty->isStructureOrClassType()) {
+			return canonicalName(TyName, "struct");
+		} else if (Ty->isUnionType()) {
+			return canonicalName(TyName, "union");
+		} else if (Ty->isEnumeralType()) {
+			return canonicalName(TyName, "enum");
+		}
+		return getScalarMetadataValue(Ty, Policy);
+	};
+	
+	static const auto getPointerOrRefMetadataValue = [](const clang::Type *PTy,
+														bool CanTy,
+														const PrintingPolicy &Policy) {
+		assert(PTy && "Null type");
+		std::string Ret;
+		if (const ExtVectorType *VTy = llvm::dyn_cast<ExtVectorType>(PTy)) {
+			Ret = getVectorMetadataValue(VTy, Policy);
+		} else {
+			Ret = CanTy ? getComplexMetadataValue(PTy, Policy) : getScalarMetadataValue(PTy, Policy);
+		}
+		return Ret + "*";
+	};
+	
+	const auto add_image_arg = [&Builder, this, &Policy, &addressQuals, &accessQuals, &argTypeNames, &argBaseTypeNames, &argNames, &argTypeQuals](const clang::QualType& type,
+																																				  const FloorImageFlagsAttr* flags_attr,
+																																				  const std::string& name) {
+		// image is always in global address space
+		addressQuals.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(getContext().getTargetAddressSpace(LangAS::opencl_global))));
+		
+		// set access qualifier
+		if (flags_attr && flags_attr->isWriteOnly()) {
+			accessQuals.push_back(llvm::MDString::get(VMContext, "write_only"));
+		} else if (flags_attr && flags_attr->isReadWrite()) {
+			accessQuals.push_back(llvm::MDString::get(VMContext, "read_write"));
+		} else {
+			accessQuals.push_back(llvm::MDString::get(VMContext, "read_only"));
+		}
+		
+		// image type / base type
+		// NOTE: always set base type, because types in image aggregates might be "weird", but should be considered normal
+		const QualType baseTy = type.isCanonical() ? type : type.getCanonicalType();
+		const auto type_name = getComplexMetadataValue(baseTy.getTypePtr(), Policy);
+		argTypeNames.push_back(llvm::MDString::get(VMContext, type_name));
+		argBaseTypeNames.push_back(llvm::MDString::get(VMContext, type_name));
+		
+		// set arg name
+		argNames.push_back(llvm::MDString::get(VMContext, name));
+		
+		// type quals is always empty for images
+		argTypeQuals.push_back(llvm::MDString::get(VMContext, ""));
+	};
+	
+	for (const auto& parm : FD->parameters()) {
+		const auto clang_type = parm->getType();
+		const bool IsCanonical = clang_type.isCanonical();
+		
+		// pointer / buffer
+		if (clang_type->isPointerType() || clang_type->isReferenceType()) {
+			// Get argument type name.
+			std::string tyName;
+			if (const PointerType *PTy = dyn_cast<PointerType>(clang_type.getTypePtr())) {
+				tyName = getPointerOrRefMetadataValue(PTy->getPointeeType().getTypePtr(), false, Policy);
+			} else if (const ReferenceType *RTy = dyn_cast<ReferenceType>(clang_type.getTypePtr())) {
+				tyName = getPointerOrRefMetadataValue(RTy->getPointeeType().getTypePtr(), false, Policy);
+			} else if (const DecayedType *DTy = dyn_cast<DecayedType>(clang_type.getTypePtr())) {
+				tyName = getPointerOrRefMetadataValue(DTy->getPointeeType().getTypePtr(), false, Policy);
+			} else {
+				tyName = getScalarMetadataValue(clang_type.getTypePtr(), Policy);
+			}
+			argTypeNames.push_back(llvm::MDString::get(VMContext, tyName));
+			
+			// Acquiring the base type of the parameter.
+			std::string baseTyName;
+			if (IsCanonical) {
+				baseTyName = tyName;
+			} else {
+				QualType can_pointee_type;
+				if (clang_type->isPointerType()) {
+					can_pointee_type = clang_type.getCanonicalType()->getAs<PointerType>()->getPointeeType();
+				} else { // ref
+					can_pointee_type = clang_type.getCanonicalType()->getAs<ReferenceType>()->getPointeeType();
+				}
+				baseTyName = getPointerOrRefMetadataValue(can_pointee_type.getTypePtr(), true, Policy);
+			}
+			argBaseTypeNames.push_back(llvm::MDString::get(VMContext, baseTyName));
+			
+			// Get address qualifier.
+			QualType pointeeTy = clang_type->getPointeeType();
+			addressQuals.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(Context.getTargetAddressSpace(pointeeTy.getAddressSpace()))));
+			
+			// Get argument type qualifiers:
+			std::string typeQuals;
+			if (clang_type.isRestrictQualified()) {
+				typeQuals = "restrict";
+			}
+			if (pointeeTy.isConstQualified() ||
+				(pointeeTy.getAddressSpace() == LangAS::opencl_constant)) {
+				typeQuals += typeQuals.empty() ? "const" : " const";
+			}
+			if (pointeeTy.isVolatileQualified()) {
+				typeQuals += typeQuals.empty() ? "volatile" : " volatile";
+			}
+			argTypeQuals.push_back(llvm::MDString::get(VMContext, typeQuals));
+			
+			accessQuals.push_back(llvm::MDString::get(VMContext, "none"));
+			argNames.push_back(llvm::MDString::get(VMContext, parm->getName()));
+		} else if (clang_type->isImageType()) { // normal image
+			add_image_arg(clang_type, getFloorImageFlagsAttribute(parm, &clang_type, VMContext), parm->getName().str());
+		} else if (clang_type->isAggregateImageType()) { // aggregate image
+			const auto decl = clang_type->getAsCXXRecordDecl();
+			const auto agg_images = get_aggregate_image_fields(decl);
+			
+			const std::string base_name = parm->getName().str() + ".";
+			unsigned int img_idx = 0;
+			for (const auto& img : agg_images) {
+				const auto img_type = img->getType();
+				
+				add_image_arg(img_type, getFloorImageFlagsAttribute(*img, &img_type, VMContext),
+							  base_name + std::to_string(img_idx));
+				++img_idx;
+			}
+		} else if (clang_type->isPipeType()) { // pipe
+			// Get argument type name.
+			std::string tyName = getPipeMetadataValue(clang_type->getAs<PipeType>(), Policy);
+			
+			// Acquiring the base type of the parameter.
+			std::string baseTyName;
+			if (IsCanonical) {
+				baseTyName = tyName;
+			} else {
+				baseTyName = getPipeMetadataValue(clang_type.getCanonicalType()->getAs<PipeType>(), Policy);
+			}
+			
+			// Get address qualifier.
+			addressQuals.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(Context.getTargetAddressSpace(LangAS::opencl_global))));
+			
+			// Get argument type qualifiers.
+			std::string typeQuals = "pipe";
+			
+			// Adding the type and base type to the metadata.
+			assert(!tyName.empty() && "Empty type name");
+			argTypeNames.push_back(llvm::MDString::get(VMContext, tyName));
+			assert(!baseTyName.empty() && "Empty base type name");
+			argBaseTypeNames.push_back(llvm::MDString::get(VMContext, baseTyName));
+			
+			argTypeQuals.push_back(llvm::MDString::get(VMContext, typeQuals));
+			
+			// TODO: Get image access qualifier: (also for pipe?)
+			//accessQuals.push_back(getAccessAttribute(parm, Context));
+			
+			if (EmitVerbose) {
+				// Get argument name.
+				argNames.push_back(llvm::MDString::get(VMContext, parm->getName()));
+			}
+		} else { // kernel parameter
+			// TODO: merge pipe functionality
+			
+			addressQuals.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(0 /* private address space*/)));
+			
+			// Get argument type name.
+			std::string tyName = getScalarMetadataValue(clang_type.getTypePtr(), Policy);
+			argTypeNames.push_back(llvm::MDString::get(VMContext, tyName));
+			
+			// Acquiring the base type of the parameter.
+			QualType baseTy = IsCanonical ? clang_type : clang_type.getCanonicalType();
+			std::string baseTyName;
+			if (clang_type->isVectorType()) {
+				baseTyName = getVectorMetadataValue(llvm::dyn_cast<clang::ExtVectorType>(baseTy.getTypePtr()), Policy);
+			} else {
+				baseTyName = getComplexMetadataValue(baseTy.getTypePtr(), Policy);
+			}
+			argBaseTypeNames.push_back(llvm::MDString::get(VMContext, baseTyName));
+			
+			// Get argument type qualifiers:
+			std::string typeQuals;
+			if (clang_type.isConstQualified()) {
+				typeQuals = "const";
+			}
+			if (clang_type.isVolatileQualified()) {
+				typeQuals += typeQuals.empty() ? "volatile" : " volatile";
+			}
+			argTypeQuals.push_back(llvm::MDString::get(VMContext, typeQuals));
+			
+			accessQuals.push_back(llvm::MDString::get(VMContext, "none"));
+			argNames.push_back(llvm::MDString::get(VMContext, parm->getName()));
+		}
+	}
+	
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, addressQuals));
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, accessQuals));
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, argTypeNames));
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, argBaseTypeNames));
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, argTypeQuals));
+	if (EmitVerbose) {
+		kernelMDArgs.push_back(llvm::MDNode::get(VMContext, argNames));
+	}
+}
+
+void CodeGenModule::GenVulkanMetadata(const FunctionDecl *FD, llvm::Function *Fn, CGBuilderTy& Builder) {
+	const bool is_kernel = FD->hasAttr<ComputeKernelAttr>();
+	const bool is_vertex = FD->hasAttr<GraphicsVertexShaderAttr>();
+	const bool is_fragment = FD->hasAttr<GraphicsFragmentShaderAttr>();
+	
+	SmallVector<llvm::Metadata*, 8> stage_infos;
+	stage_infos.push_back(llvm::MDString::get(VMContext, FD->getName()));
+	
+	static const std::string prefix_builtin = "builtin:";
+	static const std::string prefix_stage = "stage:";
+	static const std::string prefix_arg = "arg:";
+	static const std::string prefix_iub = "iub:";
+	
+	//
+	const auto handle_stage_input_output = [this, &stage_infos, &is_vertex, &is_fragment, &Fn](const QualType& clang_type,
+																							   llvm::Type* llvm_type,
+																							   const bool is_return,
+																							   uint32_t* arg_idx) {
+		assert((arg_idx != nullptr && !is_return) || (arg_idx == nullptr && is_return) && "invalid args");
+		
+		const bool is_vertex_io = (is_return && is_vertex) || (!is_return && is_fragment);
+		const bool is_fragment_io = (is_return && is_fragment);
+		
+		const auto add_fbo_output = [this, &stage_infos](const QualType& type, const uint32_t location) {
+			const auto canon_data_type = type.getCanonicalType();
+			std::string output_type_str = "float";
+			if (canon_data_type->isIntegerType()) output_type_str = "int";
+			if (canon_data_type->isUnsignedIntegerType()) output_type_str = "uint";
+			stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "fbo_output:" + output_type_str + ":" + std::to_string(location)));
+		};
+		
+		const auto cxx_rdecl = clang_type->getAsCXXRecordDecl();
+		if (cxx_rdecl) {
+			if (!cxx_rdecl->hasAttr<VectorCompatAttr>()) {
+				// must handle each field individually
+				const auto fields = getTypes().get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+				
+				// fbo output location resolve/computation needs to happen in two passes:
+				// * gather all fixed/attr locations, make sure none conflict
+				// * used fixed locations + generate automatic location for non-fixed outputs
+				std::unordered_set<uint32_t> fbo_locations;
+				if (is_fragment_io) {
+					for (const auto& field : fields) {
+						if (field.hasAttr<GraphicsFBOColorLocationAttr>()) {
+							const auto loc_attr = field.getAttr<GraphicsFBOColorLocationAttr>();
+							if (!fbo_locations.insert(loc_attr->getEvalLocation()).second) {
+								// TODO: should have been detected earlier ...
+								Error(loc_attr->getLocation(), StringRef("location already in use"));
+								// TODO: add note of prev location?
+								return;
+							}
+						}
+					}
+				}
+				
+				uint32_t struct_arg_idx = 0, fbo_location = 0;
+				for (const auto& field : fields) {
+					llvm::Type* field_llvm_type = nullptr;
+					// get llvm type from function args (if !return), else get it from the struct type itself
+					if (arg_idx) {
+						field_llvm_type = std::next(Fn->arg_begin(), *arg_idx)->getType();
+					} else {
+						field_llvm_type = llvm_type->getStructElementType(struct_arg_idx++);
+					}
+					
+					if (is_vertex_io) {
+						if (field.hasAttr<GraphicsVertexPositionAttr>()) {
+							stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "position"));
+						} else if (field.hasAttr<GraphicsPointSizeAttr>()) {
+							stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "point_size"));
+						} else {
+							stage_infos.push_back(llvm::MDString::get(VMContext, "none"));
+						}
+					} else if (is_fragment_io) {
+						if (field.hasAttr<GraphicsFBOColorLocationAttr>()) {
+							add_fbo_output(field.type, field.getAttr<GraphicsFBOColorLocationAttr>()->getEvalLocation());
+						} else if (field.hasAttr<GraphicsFBODepthTypeAttr>()) {
+							const auto depth_attr = field.getAttr<GraphicsFBODepthTypeAttr>();
+							if (!field.type->isFloatingType()) {
+								// TODO: should have been detected earlier ...
+								Error(depth_attr->getLocation(),
+									  StringRef("depth attribute can only be applied to floating point types"));
+								return;
+							}
+							
+							std::string depth_qual;
+							switch (depth_attr->getDepthQualifier()) {
+								case clang::GraphicsFBODepthTypeAttr::FBODepthTypeAny: depth_qual = "any"; break;
+								case clang::GraphicsFBODepthTypeAttr::FBODepthTypeLess: depth_qual = "less"; break;
+								case clang::GraphicsFBODepthTypeAttr::FBODepthTypeGreater: depth_qual = "greater"; break;
+							}
+							stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "fbo_depth:" + depth_qual));
+						} else {
+							for (;;) {
+								if (fbo_locations.count(fbo_location) > 0) {
+									++fbo_location;
+								} else {
+									break;
+								}
+							}
+							add_fbo_output(field.type, fbo_location);
+							++fbo_location;
+						}
+					}
+					
+					// next
+					if (arg_idx) ++*arg_idx;
+				}
+				if (arg_idx) --*arg_idx; // fixup, b/c of inc later
+			} else {
+				// stage defaults (can only be those)
+				if (is_vertex_io) {
+					stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "position"));
+				} else if (is_fragment_io) {
+					add_fbo_output(clang_type, 0);
+				}
+			}
+		} else if (!clang_type->isVoidType()) {
+			// TODO: anything else?
+			// stage defaults (can only be those)
+			if (is_vertex_io) {
+				stage_infos.push_back(llvm::MDString::get(VMContext, prefix_stage + "position"));
+			} else if (is_fragment_io) {
+				add_fbo_output(clang_type, 0);
+			}
+		} else {
+			return;
+		}
+	};
+	
+	//
+	stage_infos.push_back(llvm::MDString::get(VMContext, "stage_input"));
+	
+	const auto add_image_arg = [this, &stage_infos](const FloorImageFlagsAttr* flags_attr,
+													const FloorImageDataTypeAttr* data_type,
+													const std::string& name,
+													const uint32_t elem_count = 1,
+													const bool is_array = false) {
+		std::string access_str = "read";
+		if (flags_attr && flags_attr->isWriteOnly()) {
+			access_str = "write";
+		} else if (flags_attr && flags_attr->isReadWrite()) {
+			assert(false && "read/write is not supported");
+		}
+		
+		std::string sample_type_str = "float";
+		if (data_type) {
+			const auto canon_data_type = data_type->getImageDataType().getCanonicalType();
+			if (canon_data_type->isIntegerType()) sample_type_str = "int";
+			if (canon_data_type->isUnsignedIntegerType()) sample_type_str = "uint";
+			// else: just assume float
+		}
+		
+		stage_infos.push_back(llvm::MDString::get(VMContext, (prefix_arg + access_str +
+															  (is_array ? ":array" : ":scalar") +
+															  ":" + std::to_string(elem_count) +
+															  ":" + sample_type_str)));
+	};
+	
+	uint32_t arg_idx = 0;
+	uint32_t iub_count = 0;
+	for (const auto& parm : FD->parameters()) {
+		const auto clang_type = parm->getType();
+		auto arg_iter = std::next(Fn->arg_begin(), arg_idx);
+		const auto llvm_type = arg_iter->getType();
+		const auto cxx_rdecl = clang_type->getAsCXXRecordDecl();
+		
+		// TODO: put into static func
+		const auto compute_type_size = [this, &parm, &Fn](llvm::Type* type) {
+			if (!type->isSized()) {
+				auto err_diagID = getDiags().getCustomDiagID(DiagnosticsEngine::Fatal, "%0");
+				getDiags().Report(parm->getSourceRange().getBegin(), err_diagID) << "parameter uses a type with an unknown size (NOTE: this can happen when internal vector/array type conversion/replacement has failed)";
+				
+				auto param_note_diagID = getDiags().getCustomDiagID(DiagnosticsEngine::Note, "LLVM type: %0");
+				std::string param_type = "";
+				llvm::raw_string_ostream param_type_stream(param_type);
+				type->print(param_type_stream);
+				getDiags().Report(parm->getSourceRange().getBegin(), param_note_diagID) << param_type_stream.str();
+				
+				auto func_note_diagID = getDiags().getCustomDiagID(DiagnosticsEngine::Note, "LLVM function type: %0");
+				std::string fun_type = "";
+				llvm::raw_string_ostream fun_type_stream(fun_type);
+				Fn->getFunctionType()->print(fun_type_stream);
+				getDiags().Report(parm->getSourceRange().getBegin(), func_note_diagID) << fun_type_stream.str();
+				
+				return uint64_t(0);
+			}
+			return getDataLayout().getTypeStoreSize(type).getFixedValue();
+		};
+		
+		// stage input
+		if (parm->hasAttr<GraphicsStageInputAttr>()) {
+			handle_stage_input_output(clang_type, llvm_type, false, &arg_idx);
+		} else if (clang_type->isArrayImageType(true)) { // image array
+			const auto array_image_info = get_array_image_info(cxx_rdecl, Context);
+			if (array_image_info) {
+				add_image_arg(array_image_info->flags,
+							  array_image_info->data_type,
+							  parm->getName().str(),
+							  array_image_info->element_count,
+							  true /* always an array */);
+			}
+		} else if (clang_type->isAggregateImageType()) { // aggregate image
+			const auto agg_images = get_aggregate_image_fields(clang_type->getAsCXXRecordDecl());
+			for (const auto& img : agg_images) {
+				uint32_t elem_count = 1;
+				bool is_array = false;
+				if (img->getType()->isPointerType()) {
+					elem_count = Context.getAsConstantArrayType(img->getType()->getPointeeType())->getSize().getZExtValue();
+					is_array = true;
+				}
+				add_image_arg(img->getAttr<FloorImageFlagsAttr>(),
+							  img->getAttr<FloorImageDataTypeAttr>(),
+							  parm->getName().str(),
+							  elem_count,
+							  is_array);
+				
+				// next llvm arg
+				++arg_idx;
+			}
+			// fix up llvm arg count (will inc again after this)
+			--arg_idx;
+		}
+		// make parameters a IUB if their size is <= the size limit and we are still below the IUB count limit
+		else if (clang_type->isReferenceType() &&
+				 clang_type->getPointeeType().getAddressSpace() == LangAS::opencl_constant &&
+				 parm->getAttr<FloorArgBufferAttr>() == nullptr /* must not be an argument buffer */ &&
+				 compute_type_size(llvm_type->getPointerElementType()) <= getCodeGenOpts().VulkanIUBSize &&
+				 iub_count < getCodeGenOpts().VulkanIUBCount) {
+			stage_infos.push_back(llvm::MDString::get(VMContext, prefix_iub + std::to_string(iub_count)));
+			++iub_count;
+			
+			// tag as IUB
+			arg_iter->addAttr(llvm::Attribute::get(getLLVMContext(), "vulkan_iub"));
+		} else { // anything else
+			stage_infos.push_back(llvm::MDString::get(VMContext, "none"));
+			
+			if (parm->getAttr<FloorArgBufferAttr>() != nullptr) {
+				// tag as argument buffer
+				arg_iter->addAttr(llvm::Attribute::get(getLLVMContext(), "vulkan_arg_buffer"));
+			}
+		}
+		
+		// next llvm arg
+		++arg_idx;
+	}
+	
+	// add fixed input
+	if (getCodeGenOpts().VulkanSoftPrintf > 0) {
+		// soft-printf buffer metadata (plain parameter/buffer)
+		stage_infos.push_back(llvm::MDString::get(VMContext, "none"));
+	}
+	if (is_kernel) {
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "global_invocation_id"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "local_invocation_id"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "workgroup_id"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "num_workgroups"));
+	} else if (is_vertex) {
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "vertex_index"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "view_index"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "instance_index"));
+	} else if (is_fragment) {
+		if (getCodeGenOpts().GraphicsPrimitiveID) {
+			stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "primitive_id"));
+		}
+		if (getCodeGenOpts().GraphicsBarycentricCoord) {
+			stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "barycentric_coord"));
+		}
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "point_coord"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "frag_coord"));
+		stage_infos.push_back(llvm::MDString::get(VMContext, prefix_builtin + "view_index"));
+	}
+	
+	// handle return value
+	stage_infos.push_back(llvm::MDString::get(VMContext, "stage_output"));
+	if (is_vertex || is_fragment) {
+		handle_stage_input_output(FD->getReturnType(), Fn->getReturnType(), true, nullptr);
+	}
+	
+	// add to global stage_io node
+	auto stage_infos_node = llvm::MDNode::get(VMContext, stage_infos);
+	auto vk_stage_io = getModule().getOrInsertNamedMetadata("vulkan.stage_io");
+	vk_stage_io->addOperand(stage_infos_node);
+}
+
+static bool is_indirect_buffer(const clang::QualType& type, CodeGenModule &CGM,
+							   const bool has_arg_buffer_attr) {
+	if (type.isNull()) {
+		return false;
+	}
+	
+	const auto clang_pointee_type = type->getPointeeType();
+	const auto llvm_type = CGM.getTypes().ConvertTypeForMem(type);
+	const auto llvm_pointee_type = llvm_type->getPointerElementType();
+	
+	// CUDA/Host-Compute: must have had floor arg buffer attribute on the original decl
+	// otherwise: check if in constant address space
+	const auto is_arg_buffer = (!CGM.getLangOpts().CUDA && !CGM.getLangOpts().FloorHostCompute ?
+								(clang_pointee_type.getAddressSpace() == LangAS::opencl_constant) :
+								has_arg_buffer_attr);
+	
+	if (type->isReferenceType() && llvm_pointee_type->isStructTy() && is_arg_buffer) {
+		// initial requirements are fulfilled
+		// -> need to recursively check for any pointers/buffers/images now (if none are found, this is a normal struct/param)
+		const std::function<bool(const clang::QualType&)> indirect_checker = [&indirect_checker](const clang::QualType& type) {
+			if (type->isPointerType() ||
+				type->isReferenceType() ||
+				type->isImageType() ||
+				type->isArrayImageType(true) ||
+				type->isAggregateImageType()) {
+				return true;
+			} else if (const auto type_rdecl = type->getAsCXXRecordDecl()) {
+				// struct -> recursively check fields
+				const auto fields = get_aggregate_fields(type_rdecl);
+				for (const auto& field : fields) {
+					auto field_type = field->getType();
+					if (field_type->isArrayType()) {
+						field_type = field_type->getAsArrayTypeUnsafe()->getElementType();
+					}
+					if (indirect_checker(field_type)) {
+						return true;
+					}
+				}
+			}
+			// normal arg
+			return false;
+		};
+		return indirect_checker(clang_pointee_type);
+	}
+	return false;
+}
+
+void CodeGenModule::GenAIRMetadata(const FunctionDecl *FD, llvm::Function *Fn,
+								   const CGFunctionInfo &FnInfo,
+								   SmallVector <llvm::Metadata*, 5> &kernelMDArgs,
+								   CGBuilderTy& Builder) {
+	const bool is_kernel = FD->hasAttr<ComputeKernelAttr>();
+	const bool is_vertex = FD->hasAttr<GraphicsVertexShaderAttr>();
+	const bool is_fragment = FD->hasAttr<GraphicsFragmentShaderAttr>();
+	
+	//
+	SmallVector<llvm::Metadata*, 4> stage_infos;
+	SmallVector<llvm::Metadata*, 8> arg_infos;
+	
+	//
+	const PrintingPolicy &Policy = Context.getPrintingPolicy();
+	const auto make_type_name = [&Policy](const clang::QualType& type) {
+		// NOTE: air wants the type w/o qualifiers
+		const auto base_unq_type = type.getTypePtr()->getBaseElementTypeUnsafe();
+		const auto unqualified_type = base_unq_type->getCanonicalTypeInternal();
+		
+		// strips "const", "volatile", "restrict" and "__restrict" from the type name
+		const auto strip_cvr = [](std::string in_str) {
+			if (const auto const_pos = in_str.find("const "); const_pos != std::string::npos) {
+				in_str.erase(const_pos, 6);
+			}
+			if (const auto volatile_pos = in_str.find("volatile "); volatile_pos != std::string::npos) {
+				in_str.erase(volatile_pos, 9);
+			}
+			if (const auto restrict_pos = in_str.find("restrict "); restrict_pos != std::string::npos) {
+				in_str.erase(restrict_pos, 9);
+			}
+			if (const auto restrict2_pos = in_str.find("__restrict "); restrict2_pos != std::string::npos) {
+				in_str.erase(restrict2_pos, 11);
+			}
+			return in_str;
+		};
+		
+		// convert floor "vectorN<type>" to "typeN"
+		if (const auto cxx_rdecl = unqualified_type->getAsCXXRecordDecl();
+			cxx_rdecl && cxx_rdecl->hasAttr<VectorCompatAttr>()) {
+			const auto floor_vec_name = unqualified_type.getAsString(Policy);
+			const auto type_param_start = floor_vec_name.find('<');
+			const auto type_param_end = floor_vec_name.rfind('>');
+			if (type_param_start != std::string::npos && type_param_start > 0 &&
+				type_param_end != std::string::npos && type_param_end > type_param_start) {
+				const auto vec_elem_type = floor_vec_name.substr(type_param_start + 1, type_param_end - type_param_start - 1);
+				const auto vec_size = floor_vec_name.substr(type_param_start - 1, 1);
+				return strip_cvr(vec_elem_type + vec_size);
+			}
+		}
+		
+		std::string type_name = "";
+		if (type->isVectorType()) {
+			type_name = getVectorMetadataValue(llvm::dyn_cast<clang::ExtVectorType>(unqualified_type.getTypePtr()), Policy);
+		} else if (type->isHalfType()) {
+			type_name = "half";
+		} else {
+			type_name = unqualified_type.getAsString(Policy);
+		}
+		// Turn "unsigned type" to "utype"
+		const auto pos = type_name.find("unsigned");
+		if (pos != std::string::npos) type_name.erase(pos + 1, 8);
+		return strip_cvr(type_name);
+	};
+	
+	//
+	auto abi_arg_info_iter = FnInfo.arg_begin();
+	unsigned int arg_idx = 0, buffer_idx = 0, tex_idx = 0;
+	for (const auto& parm : FD->parameters()) {
+		const auto clang_type = parm->getType();
+		const auto cxx_rdecl = clang_type->getAsCXXRecordDecl();
+		
+		//
+		const auto add_image_arg = [this, &Builder, &tex_idx, &arg_infos, &arg_idx, &parm](const clang::QualType& type,
+																						   const FloorImageFlagsAttr* flags_attr,
+																						   const FloorImageDataTypeAttr* data_type,
+																						   const std::string& name,
+																						   const uint32_t elem_count = 1) {
+			SmallVector<llvm::Metadata*, 16> arg_info;
+			
+			// #0: param index
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			// #1: storage type
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.texture"));
+			// #2/#3: location_index (note: separate for buffers and textures)
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.location_index"));
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(tex_idx)));
+			tex_idx += elem_count;
+			// #4: index count/range
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(elem_count)));
+			// #5: access type (sample = 0, read = 1 or write = 2)
+			// note that "read" is essentially a subset of "sample" -> use "sample" for r/o
+			if (flags_attr && flags_attr->isWriteOnly()) {
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.write"));
+			} else if (flags_attr && flags_attr->isReadWrite()) {
+				// TODO: this isn't really supported
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.write"));
+			} else {
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.sample"));
+			}
+			
+			// #6/#7: texture type
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			// proper type is necessary for metal debugging purposes
+			std::string tex_type_name = "";
+			if (elem_count > 1) {
+				tex_type_name += "array<";
+			}
+			const auto builtin_type = type->getAs<BuiltinType>();
+			if (!builtin_type) {
+				Error(parm->getSourceRange().getBegin(), StringRef("invalid image type (not a builtin type)!"));
+				return;
+			}
+			switch (builtin_type->getKind()) {
+				case BuiltinType::OCLImage1d:
+					tex_type_name += "texture1d";
+					break;
+				case BuiltinType::OCLImage1dArray:
+					tex_type_name += "texture1d_array";
+					break;
+				case BuiltinType::OCLImage2d:
+					tex_type_name += "texture2d";
+					break;
+				case BuiltinType::OCLImage2dArray:
+					tex_type_name += "texture2d_array";
+					break;
+				case BuiltinType::OCLImage2dDepth:
+					tex_type_name += "depth2d";
+					break;
+				case BuiltinType::OCLImage2dArrayDepth:
+					tex_type_name += "depth2d_array";
+					break;
+				case BuiltinType::OCLImage2dMSAA:
+					tex_type_name += "texture2d_ms";
+					break;
+				case BuiltinType::OCLImage2dMSAADepth:
+					tex_type_name += "depth2d_ms";
+					break;
+				case BuiltinType::OCLImage3d:
+					tex_type_name += "texture3d";
+					break;
+				case BuiltinType::OCLImageCube:
+					tex_type_name += "texturecube";
+					break;
+				case BuiltinType::OCLImageCubeArray:
+					tex_type_name += "texturecube_array";
+					break;
+				case BuiltinType::OCLImageCubeDepth:
+					tex_type_name += "depthcube";
+					break;
+				case BuiltinType::OCLImageCubeArrayDepth:
+					tex_type_name += "depthcube_array";
+					break;
+				default:
+					Error(parm->getSourceRange().getBegin(), StringRef("invalid image type!"));
+					return;
+			}
+			
+			tex_type_name += "<";
+			std::string sample_type_str = "float";
+			if (data_type) {
+				const auto canon_data_type = data_type->getImageDataType().getCanonicalType();
+				if (canon_data_type->isIntegerType()) sample_type_str = "int";
+				if (canon_data_type->isUnsignedIntegerType()) sample_type_str = "uint";
+				// else: just assume float
+			}
+			tex_type_name += sample_type_str;
+			tex_type_name += ", ";
+			if (flags_attr && flags_attr->isReadOnly()) {
+				tex_type_name += "sample";
+			} else {
+				tex_type_name += "write";
+			}
+			tex_type_name += ">";
+			
+			if (elem_count > 1) {
+				tex_type_name += ", " + std::to_string(elem_count) + ">";
+			}
+			
+			arg_info.push_back(llvm::MDString::get(VMContext, tex_type_name));
+			
+			// #8/#9: arg name
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, StringRef(name)));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+		};
+		
+		//
+		const auto add_indirect_constant = [this, &Builder, &make_type_name](const clang::QualType& type,
+																			 const NamedDecl& decl,
+																			 const bool is_top_level,
+																			 uint32_t& arg_idx_at_level,
+																			 uint32_t& buffer_idx_at_level,
+																			 const uint32_t buffer_array_size) -> SmallVector<llvm::Metadata*, 16> {
+			const auto clang_pointee_type = type->getPointeeType();
+			
+			SmallVector<llvm::Metadata*, 16> arg_info;
+			
+			// #0: param index
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx_at_level)));
+			if (!is_top_level) {
+				++arg_idx_at_level;
+			}
+			// #1: storage type
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.indirect_constant"));
+			// #2/#3: location_index (note: even though this is a constant, it uses the same idx as buffers)
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.location_index"));
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(buffer_idx_at_level)));
+			buffer_idx_at_level += std::max(buffer_array_size, 1u);
+			// #4: index count/range
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(1)));
+			// #5/#6: type name
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			// NOTE: air wants the pointed-to/pointee type here
+			arg_info.push_back(llvm::MDString::get(VMContext, make_type_name(!clang_pointee_type.isNull() ? clang_pointee_type : type)));
+			// #7/#8: arg name
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, decl.getName()));
+			
+			return arg_info;
+		};
+		
+		// "forward decl"
+		std::function<SmallVector<llvm::Metadata*, 16>(const clang::QualType&, const NamedDecl&, const bool, const bool, uint32_t&, uint32_t&, const uint32_t)> add_buffer_arg;
+		
+		// handle "air.struct_type_info" metadata
+		const std::function<SmallVector<llvm::Metadata*, 16>(const CXXRecordDecl&, const Decl&, const bool, const bool, uint32_t&, uint32_t&)> add_struct_type_info =
+		[this, &add_struct_type_info, &add_buffer_arg, &make_type_name, &add_indirect_constant, &Builder](const CXXRecordDecl& struct_rdecl,
+																										  const Decl& parent_decl,
+																										  const bool is_indirect, // specifies if we're within an indirect buffer
+																										  const bool indirect_buffer,
+																										  uint32_t& arg_idx_child,
+																										  uint32_t& buffer_idx_child) -> SmallVector<llvm::Metadata*, 16> {
+			// TODO: this is not ideal and doesn't properly handle unions
+			const auto fields = get_aggregate_fields(&struct_rdecl);
+			bool ignore = false;
+			for (const auto& field : fields) {
+				if (field->isAnonymousStructOrUnion() ||
+					field->isBitField()) {
+					ignore = true;
+					break;
+				}
+			}
+			// TODO/NOTE: ignore anonymous structs/unions and bitfields for now
+			if (ignore) {
+				return {};
+			}
+			
+			SmallVector<llvm::Metadata*, 16> struct_info;
+			uint32_t offset = 0;
+			for (const auto& field : fields) {
+				auto field_type = field->getType();
+				auto llvm_field_type = getTypes().ConvertTypeForMem(field_type); // TODO: should use this _everywhere_ instead of llvm type tracking/matching!
+				uint32_t array_size = 0;
+				if (field_type->isArrayType()) {
+					field_type = field_type->getAsArrayTypeUnsafe()->getElementType();
+					array_size = (uint32_t)llvm_field_type->getArrayNumElements();
+					llvm_field_type = getTypes().ConvertTypeForMem(field_type);
+				}
+				auto field_pointee_type = field_type->getPointeeType();
+				
+				bool is_inline_struct = false;
+				const auto buffer_idx_offset = buffer_idx_child;
+				if (indirect_buffer || is_indirect) {
+					if (!field_type->isPointerType() &&
+						!field_type->isReferenceType() &&
+						!field_type->isImageType() &&
+						!field_type->isArrayImageType(true) &&
+						!field_type->isAggregateImageType() &&
+						!parent_decl.hasAttr<GraphicsStageInputAttr>() &&
+						field_type->isStructureOrClassType()) {
+						if (const auto inline_struct_rdecl = field_type->getAsCXXRecordDecl();
+							inline_struct_rdecl && !inline_struct_rdecl->hasAttr<VectorCompatAttr>()) {
+							is_inline_struct = true;
+							// #-2: inline struct type
+							struct_info.push_back(llvm::MDString::get(VMContext, "air.struct_type_info"));
+							// #-1: metadata of struct type
+							uint32_t struct_arg_idx_child = 0, struct_buf_idx_child = 0;
+							auto struct_type_info = add_struct_type_info(*inline_struct_rdecl, struct_rdecl, is_indirect, indirect_buffer, struct_arg_idx_child, struct_buf_idx_child);
+							assert(!struct_type_info.empty());
+							struct_info.push_back(llvm::MDNode::get(VMContext, struct_type_info));
+							
+							// next param
+							++arg_idx_child;
+							// adjust buffer index at this level
+							buffer_idx_child += std::max(array_size, 1u) * struct_buf_idx_child;
+						}
+					}
+				}
+				
+				// #0: offset
+				struct_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(offset)));
+				// #1: sizeof
+				const auto size = (uint32_t)getDataLayout().getTypeStoreSize(llvm_field_type);
+				offset += size * uint32_t(std::max(array_size, 1u));
+				struct_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(size)));
+				// #2: array size (0 signals "no array")
+				struct_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(array_size)));
+				// #3: type name
+				struct_info.push_back(llvm::MDString::get(VMContext, make_type_name(!field_pointee_type.isNull() ? field_pointee_type : field_type)));
+				// #4: name/identifier
+				struct_info.push_back(llvm::MDString::get(VMContext, field->getName()));
+				// #5/#6: indirect argument (aka "argument buffer")
+				if (indirect_buffer || is_indirect) {
+					struct_info.push_back(llvm::MDString::get(VMContext, "air.indirect_argument"));
+					if (is_inline_struct) {
+						// idx offset to this struct (struct itself is encoded above)
+						struct_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(buffer_idx_offset)));
+					} else if (field_type->isPointerType() || field_type->isReferenceType()) {
+						// buffer field
+						auto field_arg_info = add_buffer_arg(field_type, **field, false, true, arg_idx_child, buffer_idx_child, array_size);
+						if (field_arg_info.empty()) {
+							return {};
+						}
+						struct_info.push_back(llvm::MDNode::get(VMContext, field_arg_info));
+					} else if (field_type->isImageType()) {
+						// TODO: support this
+						Error(struct_rdecl.getSourceRange().getBegin(), StringRef("images are not yet supported in indirect/argument buffers"));
+						return {};
+					} else if (field_type->isArrayImageType(true)) {
+						// TODO: support this
+						Error(struct_rdecl.getSourceRange().getBegin(), StringRef("image arrays are not yet supported in indirect/argument buffers"));
+						return {};
+					} else if (field_type->isAggregateImageType()) {
+						// TODO: support this
+						Error(struct_rdecl.getSourceRange().getBegin(), StringRef("aggregate images are not yet supported in indirect/argument buffers"));
+						return {};
+					} else if (parent_decl.hasAttr<GraphicsStageInputAttr>()) {
+						// hard error
+						Error(struct_rdecl.getSourceRange().getBegin(), StringRef("stage input is not supported in indirect/argument buffers"));
+						return {};
+					} else {
+						// value / "indirect constant"
+						auto field_arg_info = add_indirect_constant(field_type, **field, false, arg_idx_child, buffer_idx_child, array_size);
+						if (field_arg_info.empty()) {
+							return {};
+						}
+						struct_info.push_back(llvm::MDNode::get(VMContext, field_arg_info));
+					}
+				}
+			}
+			return struct_info;
+		};
+		
+		// handle general buffer metadata
+		add_buffer_arg = [this, &Builder, &make_type_name, &add_struct_type_info](const clang::QualType& type,
+																				  const NamedDecl& decl,
+																				  const bool is_top_level,
+																				  const bool is_indirect, // specifies if we're within an indirect buffer
+																				  uint32_t& arg_idx_at_level,
+																				  uint32_t& buffer_idx_at_level,
+																				  const uint32_t buffer_array_size) -> SmallVector<llvm::Metadata*, 16> {
+			const auto clang_pointee_type = type->getPointeeType();
+			const auto llvm_type = getTypes().ConvertTypeForMem(type);
+			const auto llvm_pointee_type = llvm_type->getPointerElementType();
+			
+			// check if this is an indirect buffer
+			const auto indirect_buffer = is_indirect_buffer(type, *this, false);
+			
+			SmallVector<llvm::Metadata*, 16> arg_info;
+			
+			// #0: param index
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx_at_level)));
+			if (!is_top_level) {
+				++arg_idx_at_level;
+			}
+			// #1: storage type
+			arg_info.push_back(llvm::MDString::get(VMContext, (!indirect_buffer ? "air.buffer" : "air.indirect_buffer")));
+			
+			// references / single-object parameters also store/require the buffer_size
+			if (type->isReferenceType()) {
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.buffer_size"));
+				arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(getDataLayout().getTypeStoreSize(llvm_pointee_type))));
+			}
+			
+			// #2/#3: location_index (note: separate for buffers and textures)
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.location_index"));
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(buffer_idx_at_level)));
+			buffer_idx_at_level += std::max(buffer_array_size, 1u);
+			// #4: index count/range
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(1)));
+			// #5: access (read/read_write, TODO: write?)
+			if (clang_pointee_type.isConstQualified() ||
+				(clang_pointee_type.getAddressSpace() == LangAS::opencl_constant)) {
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.read"));
+			} else {
+				arg_info.push_back(llvm::MDString::get(VMContext, "air.read_write"));
+			}
+			
+			// #6/#7: struct info
+			if (const auto pointee_rdecl = clang_pointee_type->getAsCXXRecordDecl()) {
+				uint32_t arg_idx_child = 0, buffer_idx_child = 0; // for indirect/arg buffers
+				auto struct_type_info = add_struct_type_info(*pointee_rdecl, decl, is_indirect, indirect_buffer, arg_idx_child, buffer_idx_child);
+				if (!struct_type_info.empty()) {
+					arg_info.push_back(llvm::MDString::get(VMContext, "air.struct_type_info"));
+					arg_info.push_back(llvm::MDNode::get(VMContext, struct_type_info));
+				}
+			}
+			
+			// #8/#9: type size
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_size"));
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(getDataLayout().getTypeStoreSize(llvm_pointee_type))));
+			// #10/#11: type alignment
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_align_size"));
+			// max out at 16, anything higher is unreasonable
+			// TODO: make sure this is POT
+			const auto align_size = std::min(getDataLayout().getTypeAllocSize(llvm_pointee_type).getFixedValue(), uint64_t(16));
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(align_size)));
+			//getPrimitiveSizeInBits
+			// #12/#13: type name
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			// NOTE: air wants the pointed-to/pointee type here
+			arg_info.push_back(llvm::MDString::get(VMContext, make_type_name(clang_pointee_type)));
+			// #14/#15: arg name
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, decl.getName()));
+			return arg_info;
+		};
+		
+		if (clang_type->isPointerType() || clang_type->isReferenceType()) { // pointer / buffer
+			auto arg_info = add_buffer_arg(clang_type, *parm, true, false, arg_idx, buffer_idx, 0);
+			if (arg_info.empty()) {
+				return;
+			}
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+		} else if (clang_type->isImageType()) { // image
+			add_image_arg(clang_type, parm->getAttr<FloorImageFlagsAttr>(),
+						  parm->getAttr<FloorImageDataTypeAttr>(), parm->getName().str());
+		} else if (clang_type->isArrayImageType(true)) { // image array
+			const auto array_image_info = get_array_image_info(cxx_rdecl, Context);
+			if (array_image_info) {
+				add_image_arg(array_image_info->image_type,
+							  array_image_info->flags,
+							  array_image_info->data_type,
+							  parm->getName().str(),
+							  array_image_info->element_count);
+			} else {
+				Error(parm->getSourceRange().getBegin(), StringRef("invalid image array!"));
+				return;
+			}
+		} else if (clang_type->isAggregateImageType()) { // aggregate image
+			const auto agg_images = get_aggregate_image_fields(cxx_rdecl);
+			
+			const std::string base_name = parm->getName().str() + ".";
+			unsigned int img_idx = 0;
+			for (const auto& img : agg_images) {
+				add_image_arg(img->getType(),
+							  img->getAttr<FloorImageFlagsAttr>(),
+							  img->getAttr<FloorImageDataTypeAttr>(),
+							  base_name + std::to_string(img_idx));
+				++img_idx;
+				
+				// next llvm arg
+				++arg_idx;
+			}
+			// fix up llvm arg count (will inc again after this)
+			--arg_idx;
+		} else if (parm->hasAttr<GraphicsStageInputAttr>()) { // stage input
+			if (cxx_rdecl) {
+				// must handle each field individually
+				const auto fields = getTypes().get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+				for (const auto& field : fields) {
+					SmallVector<llvm::Metadata*, 16> arg_info;
+					
+					// #0: param index
+					arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+					
+					// #1: type
+					// TODO: handle perspective/center correctly
+					const auto type_name = make_type_name(field.type);
+					if (field.hasAttr<GraphicsVertexPositionAttr>()) {
+						arg_info.push_back(llvm::MDString::get(VMContext, "air.position"));
+						arg_info.push_back(llvm::MDString::get(VMContext, "air.center"));
+						arg_info.push_back(llvm::MDString::get(VMContext, "air.no_perspective"));
+					} else {
+						arg_info.push_back(llvm::MDString::get(VMContext, "air.fragment_input"));
+						arg_info.push_back(llvm::MDString::get(VMContext, StringRef(field.mangled_name)));
+						bool is_int_type = field.type->isIntegerType();
+						if (!is_int_type && field.type->isVectorType()) {
+							auto vec_type = llvm::dyn_cast<clang::ExtVectorType>(field.type.getTypePtr()->getBaseElementTypeUnsafe()->getCanonicalTypeInternal().getTypePtr());
+							if (vec_type != nullptr) {
+								is_int_type = vec_type->getElementType()->isIntegerType();
+							}
+						}
+						if (is_int_type) {
+							// use flat "interpolation" for uint* and int* types
+							arg_info.push_back(llvm::MDString::get(VMContext, "air.flat"));
+						} else {
+							arg_info.push_back(llvm::MDString::get(VMContext, "air.center"));
+							arg_info.push_back(llvm::MDString::get(VMContext, "air.perspective"));
+						}
+					}
+					
+					// type name
+					arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+					arg_info.push_back(llvm::MDString::get(VMContext, type_name));
+					
+					// arg name
+					arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+					arg_info.push_back(llvm::MDString::get(VMContext, field.name));
+					arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+					
+					// next
+					++arg_idx;
+				}
+				--arg_idx; // fixup, b/c of inc later
+			} else {
+				// TODO: add as-is
+			}
+		} else { // unsupported simple kernel parameter
+			Error(parm->getSourceRange().getBegin(),
+				  StringRef("metal kernel parameter must be a pointer or an image type!"));
+			return;
+		}
+		
+		// next llvm arg
+		++abi_arg_info_iter;
+		++arg_idx;
+	}
+	
+	// soft-printf buffer metadata
+	if (getCodeGenOpts().MetalSoftPrintf > 0) {
+		SmallVector<llvm::Metadata*, 16> arg_info;
+		
+		// #0: param index
+		arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+		// #1: storage type
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.buffer"));
+		// #2/#3: location_index (note: separate for buffers and textures)
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.location_index"));
+		arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(buffer_idx)));
+		++buffer_idx;
+		// #4: index count/range
+		arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(1)));
+		// #5: access
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.read_write"));
+		// #8/#9: type size
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_size"));
+		arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(4)));
+		// #10/#11: type alignment
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_align_size"));
+		arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(4)));
+		// #12/#13: type name
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+		arg_info.push_back(llvm::MDString::get(VMContext, "uint"));
+		// #14/#15: arg name
+		arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+		arg_info.push_back(llvm::MDString::get(VMContext, "__metal__printf_buffer__"));
+		arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+		
+		++arg_idx;
+	}
+	
+	// limits check
+	// https://developer.apple.com/metal/limits/
+	const bool is_osx = (getModule().getTargetTriple().find("macosx") != std::string::npos);
+	const uint32_t buf_limit = 31;
+	const uint32_t tex_limit = (is_osx ? 128 : 31);
+	if (buffer_idx > buf_limit) {
+		Error(FD->getSourceRange().getBegin(),
+			  StringRef("can't use more than " + std::to_string(buf_limit) + " buffers per function"));
+		return;
+	}
+	if (tex_idx > tex_limit) {
+		Error(FD->getSourceRange().getBegin(),
+			  StringRef("can't use more than " + std::to_string(tex_limit) + " images per function"));
+		return;
+	}
+	
+	//
+	if (is_kernel) {
+		// add id handling arg metadata
+		// NOTE: the actual args are added by handleMetalVulkanEntryFunction + the order in here must match the order in there
+		const auto add_id_arg = [this, &arg_idx, &arg_infos, &Builder](const char* name, const char* air_name, const char* air_type) {
+			SmallVector<llvm::Metadata*, 6> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, air_name));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, air_type));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, name));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			
+			// next llvm arg
+			++arg_idx;
+		};
+		
+		add_id_arg("__metal__global_id__", "air.thread_position_in_grid", "uint3");
+		add_id_arg("__metal__global_size__", "air.threads_per_grid", "uint3");
+		add_id_arg("__metal__local_id__", "air.thread_position_in_threadgroup", "uint3");
+		add_id_arg("__metal__local_size__", "air.threads_per_threadgroup", "uint3");
+		add_id_arg("__metal__group_id__", "air.threadgroup_position_in_grid", "uint3");
+		add_id_arg("__metal__group_size__", "air.threadgroups_per_grid", "uint3");
+		
+		if (getTriple().getOS() == llvm::Triple::OSType::MacOSX) {
+			add_id_arg("__metal__sub_group_id__", "air.simdgroup_index_in_threadgroup", "uint");
+			add_id_arg("__metal__sub_group_local_id__", "air.thread_index_in_simdgroup", "uint");
+			add_id_arg("__metal__sub_group_size__", "air.threads_per_simdgroup", "uint");
+			add_id_arg("__metal__num_sub_groups__", "air.simdgroups_per_threadgroup", "uint");
+		}
+	} else if (is_vertex) {
+		{
+			SmallVector<llvm::Metadata*, 6> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.vertex_id"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "uint"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "__metal__vertex_id__"));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			++arg_idx; // next llvm arg
+		}
+		
+		{
+			SmallVector<llvm::Metadata*, 6> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.instance_id"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "uint"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "__metal__instance_id__"));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			++arg_idx; // next llvm arg
+		}
+		
+		const auto add_vs_output = [this, &stage_infos, &make_type_name](const CodeGenTypes::aggregate_scalar_entry& entry,
+																		 const bool force_position = false) {
+			SmallVector<llvm::Metadata*, 6> ret_info;
+			
+			if (entry.hasAttr<GraphicsVertexPositionAttr>() || force_position) {
+				ret_info.push_back(llvm::MDString::get(VMContext, "air.position"));
+			} else if (entry.hasAttr<GraphicsPointSizeAttr>()) {
+				ret_info.push_back(llvm::MDString::get(VMContext, "air.point_size"));
+			} else {
+				ret_info.push_back(llvm::MDString::get(VMContext, "air.vertex_output"));
+				ret_info.push_back(llvm::MDString::get(VMContext, StringRef(entry.mangled_name)));
+			}
+			
+			ret_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			ret_info.push_back(llvm::MDString::get(VMContext, make_type_name(entry.type)));
+			
+			ret_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			ret_info.push_back(llvm::MDString::get(VMContext, entry.name));
+			
+			stage_infos.push_back(llvm::MDNode::get(VMContext, ret_info));
+		};
+		
+		// vertex output
+		const auto ret_type = FD->getReturnType();
+		const auto cxx_rdecl = ret_type->getAsCXXRecordDecl();
+		if (cxx_rdecl && !cxx_rdecl->hasAttr<VectorCompatAttr>()) {
+			const auto fields = getTypes().get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+			for (const auto& field : fields) {
+				add_vs_output(field);
+			}
+		} else if (!ret_type->isVoidType()) {
+			// direct output: always vertex position, no mangled name
+			add_vs_output(CodeGenTypes::aggregate_scalar_entry {
+				(cxx_rdecl && cxx_rdecl->hasAttr<VectorCompatAttr>() ?
+				 getTypes().get_compat_vector_type(cxx_rdecl) : ret_type),
+				FD->getName().str(), // func name if direct
+				"",
+				nullptr,
+				nullptr,
+				{},
+				(cxx_rdecl && cxx_rdecl->hasAttr<VectorCompatAttr>()),
+				false
+			}, true);
+		}
+	} else if (is_fragment) {
+		if (getCodeGenOpts().GraphicsPrimitiveID) {
+			SmallVector<llvm::Metadata*, 6> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.primitive_id"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "uint"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "__metal__primitive_id__"));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			++arg_idx; // next llvm arg
+		}
+		
+		if (getCodeGenOpts().GraphicsBarycentricCoord) {
+			SmallVector<llvm::Metadata*, 8> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.barycentric_coord"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.center"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.perspective"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "float3"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "__metal__barycentric_coord__"));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			++arg_idx; // next llvm arg
+		}
+		
+		{
+			SmallVector<llvm::Metadata*, 6> arg_info;
+			arg_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(arg_idx)));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.point_coord"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "float2"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			arg_info.push_back(llvm::MDString::get(VMContext, "__metal__point_coord__"));
+			arg_infos.push_back(llvm::MDNode::get(VMContext, arg_info));
+			++arg_idx; // next llvm arg
+		}
+		
+		const auto add_fs_output = [this, &Builder, &stage_infos, &make_type_name](const CodeGenTypes::aggregate_scalar_entry& entry,
+																				   const unsigned int& location) {
+			SmallVector<llvm::Metadata*, 6> rtt_info;
+			
+			// #0/1: render target location index
+			rtt_info.push_back(llvm::MDString::get(VMContext, "air.render_target"));
+			rtt_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(location)));
+			// NOTE: this isn't handled yet, so just always set it to 0
+			rtt_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(0)));
+			
+			// #2/3: type name
+			rtt_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+			rtt_info.push_back(llvm::MDString::get(VMContext, make_type_name(entry.type)));
+			
+			// #4/#5: name/identifier
+			rtt_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+			rtt_info.push_back(llvm::MDString::get(VMContext, entry.name));
+			
+			stage_infos.push_back(llvm::MDNode::get(VMContext, rtt_info));
+		};
+		
+		// render targets / return types
+		const auto ret_type = FD->getReturnType();
+		const auto cxx_rdecl = ret_type->getAsCXXRecordDecl();
+		if (cxx_rdecl && !cxx_rdecl->hasAttr<VectorCompatAttr>()) {
+			const auto fields = getTypes().get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+			
+			// fbo output location resolve/computation needs to happen in two passes:
+			// * gather all fixed/attr locations, make sure none conflict
+			// * used fixed locations + generate automatic location for non-fixed outputs
+			std::unordered_set<unsigned int> fbo_locations;
+			for (const auto& field : fields) {
+				if (field.hasAttr<GraphicsFBOColorLocationAttr>()) {
+					const auto loc_attr = field.getAttr<GraphicsFBOColorLocationAttr>();
+					if (!fbo_locations.insert(loc_attr->getEvalLocation()).second) {
+						// TODO: should have been detected earlier ...
+						Error(loc_attr->getLocation(), StringRef("location already in use"));
+						// TODO: add note of prev location?
+						return;
+					}
+				}
+			}
+			
+			unsigned int location = 0;
+			for (const auto& field : fields) {
+				if (field.hasAttr<GraphicsFBOColorLocationAttr>()) {
+					add_fs_output(field, field.getAttr<GraphicsFBOColorLocationAttr>()->getEvalLocation());
+				} else if (field.hasAttr<GraphicsFBODepthTypeAttr>()) {
+					const auto depth_attr = field.getAttr<GraphicsFBODepthTypeAttr>();
+					if (!field.type->isFloatingType()) {
+						// TODO: should have been detected earlier ...
+						Error(depth_attr->getLocation(),
+							  StringRef("depth attribute can only be applied to floating point types"));
+						return;
+					}
+					
+					SmallVector<llvm::Metadata*, 7> depth_info;
+					
+					depth_info.push_back(llvm::MDString::get(VMContext, "air.depth"));
+					
+					// #1/2: depth qualifier
+					depth_info.push_back(llvm::MDString::get(VMContext, "air.depth_qualifier"));
+					std::string depth_qual = "air.";
+					switch (depth_attr->getDepthQualifier()) {
+						case clang::GraphicsFBODepthTypeAttr::FBODepthTypeAny: depth_qual += "any"; break;
+						case clang::GraphicsFBODepthTypeAttr::FBODepthTypeLess: depth_qual += "less"; break;
+						case clang::GraphicsFBODepthTypeAttr::FBODepthTypeGreater: depth_qual += "greater"; break;
+					}
+					depth_info.push_back(llvm::MDString::get(VMContext, depth_qual));
+					
+					// #3/4: type name
+					depth_info.push_back(llvm::MDString::get(VMContext, "air.arg_type_name"));
+					depth_info.push_back(llvm::MDString::get(VMContext, make_type_name(field.type)));
+					
+					// #5/#6: name/identifier
+					depth_info.push_back(llvm::MDString::get(VMContext, "air.arg_name"));
+					depth_info.push_back(llvm::MDString::get(VMContext, field.name));
+					
+					stage_infos.push_back(llvm::MDNode::get(VMContext, depth_info));
+				} else {
+					for (;;) {
+						if (fbo_locations.count(location) > 0) {
+							++location;
+						} else {
+							break;
+						}
+					}
+					add_fs_output(field, location);
+					++location;
+				}
+			}
+		} else if (!ret_type->isVoidType()) {
+			add_fs_output(CodeGenTypes::aggregate_scalar_entry {
+				(cxx_rdecl && cxx_rdecl->hasAttr<VectorCompatAttr>() ?
+				 getTypes().get_compat_vector_type(cxx_rdecl) : ret_type),
+				FD->getName().str(), // func name if direct
+				"",
+				nullptr,
+				nullptr,
+				{},
+				(cxx_rdecl && cxx_rdecl->hasAttr<VectorCompatAttr>()),
+				false
+			}, 0);
+		}
+	}
+	
+	// insert into kernel metadata
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, stage_infos));
+	kernelMDArgs.push_back(llvm::MDNode::get(VMContext, arg_infos));
+	
+	// Metal 2.1+ supports defining a max work-group size via max_total_threads_per_threadgroup/air.max_work_group_size
+	if (const ReqdWorkGroupSizeAttr *reg_local_size = FD->getAttr<ReqdWorkGroupSizeAttr>()) {
+		if (getLangOpts().MetalVersion >= 210) {
+			// NOTE: this is a 1D extent, not a 3D size
+			const uint32_t max_work_group_size {
+				std::max(1u, reg_local_size->getXDim()) *
+				std::max(1u, reg_local_size->getYDim()) *
+				std::max(1u, reg_local_size->getZDim())
+			};
+			
+			SmallVector<llvm::Metadata*, 7> max_work_group_size_info;
+			max_work_group_size_info.push_back(llvm::MDString::get(VMContext, "air.max_work_group_size"));
+			max_work_group_size_info.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(max_work_group_size)));
+			
+			kernelMDArgs.push_back(llvm::MDNode::get(VMContext, max_work_group_size_info));
+		}
+	}
+}
+
+void CodeGenFunction::EmitFloorKernelMetadata(const FunctionDecl *FD,
+											  llvm::Function *Fn,
+											  const FunctionArgList &Args,
+											  const CGFunctionInfo &FnInfo,
+											  CodeGenModule &CGM) {
+	const bool is_kernel = FD->hasAttr<ComputeKernelAttr>();
+	const bool is_vertex = FD->hasAttr<GraphicsVertexShaderAttr>();
+	const bool is_fragment = FD->hasAttr<GraphicsFragmentShaderAttr>();
+	if (!is_kernel && !is_vertex && !is_fragment) {
+		return;
+	}
+	
+	if (getLangOpts().floor_function_info == nullptr) {
+		return;
+	}
+	std::fstream& file = *getLangOpts().floor_function_info;
+	std::stringstream info;
+	std::stringstream arg_buf_info;
+	
+	const PrintingPolicy &Policy = getContext().getPrintingPolicy();
+	
+	// #0: info version
+	constexpr const uint32_t floor_info_version { 4u };
+	info << floor_info_version << ",";
+	// #1: function name
+	info << Fn->getName().str() << ",";
+	// #2: function type
+	info << (is_kernel ? "1" : (is_vertex ? "2" : "3")) << ",";
+	// #3: function flags
+	if ((getLangOpts().Metal && CGM.getCodeGenOpts().MetalSoftPrintf > 0) ||
+		(getLangOpts().Vulkan && CGM.getCodeGenOpts().VulkanSoftPrintf > 0)) {
+		info << "1,";
+	} else {
+		info << "0,";
+	}
+	// #4,5,6: local size/dim
+	if (const ReqdWorkGroupSizeAttr *reg_local_size = FD->getAttr<ReqdWorkGroupSizeAttr>()) {
+		info << reg_local_size->getXDim() << ",";
+		info << reg_local_size->getYDim() << ",";
+		info << reg_local_size->getZDim() << ",";
+	} else {
+		info << "0,0,0,";
+	}
+	
+	// iterate over clang function decl parameters
+	// NOTE: in case of struct expansion, this doesn't match the llvm parameters
+	// (which is why it iterates over the original clang list!)
+	unsigned int arg_idx = 0;
+	auto abi_arg_info_iter = FnInfo.arg_begin();
+	uint32_t vulkan_iub_count = 0;
+	for (const auto& parm : FD->parameters()) {
+		const auto clang_type = parm->getType();
+		const auto llvm_type = std::next(Fn->arg_begin(), arg_idx)->getType();
+		const auto cxx_rdecl = clang_type->getAsCXXRecordDecl();
+		
+		enum class FLOOR_ARG_INFO : uint64_t {
+			// 0 == invalid!
+			NONE						= (0ull),
+			
+			// sets: -------- 000000-- -------- 00000xxx 00000000 00000000 00000000 00000000
+			__AS_SHIFT					= (32ull),
+			__AS_MASK					= (0x0000000700000000ull),
+			AS_NONE						= NONE,
+			AS_GLOBAL					= (1ull << __AS_SHIFT),
+			AS_LOCAL					= (2ull << __AS_SHIFT),
+			AS_CONSTANT					= (3ull << __AS_SHIFT),
+			AS_IMAGE					= (4ull << __AS_SHIFT),
+			
+			// sets: -------- 000000-- xxxxxxxx 00000--- 00000000 00000000 00000000 00000000
+			__IMG_TYPE_SHIFT			= (40ull),
+			__IMG_TYPE_MASK				= (0x0000FF0000000000ull),
+			IMG_1D						= (1ull << __IMG_TYPE_SHIFT),
+			IMG_1D_ARRAY				= (2ull << __IMG_TYPE_SHIFT),
+			IMG_1D_BUFFER				= (3ull << __IMG_TYPE_SHIFT),
+			IMG_2D						= (4ull << __IMG_TYPE_SHIFT),
+			IMG_2D_ARRAY				= (5ull << __IMG_TYPE_SHIFT),
+			IMG_2D_DEPTH				= (6ull << __IMG_TYPE_SHIFT),
+			IMG_2D_ARRAY_DEPTH			= (7ull << __IMG_TYPE_SHIFT),
+			IMG_2D_MSAA					= (8ull << __IMG_TYPE_SHIFT),
+			IMG_2D_ARRAY_MSAA			= (9ull << __IMG_TYPE_SHIFT),
+			IMG_2D_MSAA_DEPTH			= (10ull << __IMG_TYPE_SHIFT),
+			IMG_2D_ARRAY_MSAA_DEPTH		= (11ull << __IMG_TYPE_SHIFT),
+			IMG_3D						= (12ull << __IMG_TYPE_SHIFT),
+			IMG_CUBE					= (13ull << __IMG_TYPE_SHIFT),
+			IMG_CUBE_ARRAY				= (14ull << __IMG_TYPE_SHIFT),
+			IMG_CUBE_DEPTH				= (15ull << __IMG_TYPE_SHIFT),
+			IMG_CUBE_ARRAY_DEPTH		= (16ull << __IMG_TYPE_SHIFT),
+			
+			// sets: -------- 000000xx -------- 00000--- 00000000 00000000 00000000 00000000
+			__IMG_ACCESS_SHIFT			= (48ull),
+			__IMG_ACCESS_MASK			= (0x0003000000000000ull),
+			IMG_ACCESS_READ				= (1ull << __IMG_ACCESS_SHIFT),
+			IMG_ACCESS_WRITE			= (2ull << __IMG_ACCESS_SHIFT),
+			IMG_ACCESS_READ_WRITE		= (IMG_ACCESS_READ | IMG_ACCESS_WRITE),
+			
+			// sets: xxxxxxxx 000000-- -------- 00000--- 00000000 00000000 00000000 00000000
+			__SPECIAL_TYPE_SHIFT		= (56ull),
+			__SPECIAL_TYPE_MASK			= (0xFF00000000000000ull),
+			STAGE_INPUT					= (1ull << __SPECIAL_TYPE_SHIFT),
+			PUSH_CONSTANT				= (2ull << __SPECIAL_TYPE_SHIFT),
+			SSBO						= (3ull << __SPECIAL_TYPE_SHIFT),
+			IMAGE_ARRAY					= (4ull << __SPECIAL_TYPE_SHIFT),
+			IUB							= (5ull << __SPECIAL_TYPE_SHIFT),
+			ARGUMENT_BUFFER				= (6ull << __SPECIAL_TYPE_SHIFT),
+		};
+		static const auto to_fas = [](const LangAS& addr_space) {
+			if (addr_space == LangAS::opencl_global) {
+				return FLOOR_ARG_INFO::AS_GLOBAL;
+			} else if (addr_space == LangAS::opencl_local) {
+				return FLOOR_ARG_INFO::AS_LOCAL;
+			} else if (addr_space == LangAS::opencl_constant) {
+				return FLOOR_ARG_INFO::AS_CONSTANT;
+			}
+			return FLOOR_ARG_INFO::AS_NONE;
+		};
+		
+		const auto compute_type_size = [&CGM, &parm, &Fn](llvm::Type* type) {
+			if (!type->isSized()) {
+				auto err_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Fatal, "%0");
+				CGM.getDiags().Report(parm->getSourceRange().getBegin(), err_diagID) << "parameter uses a type with an unknown size (NOTE: this can happen when internal vector/array type conversion/replacement has failed)";
+				
+				auto param_note_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Note, "LLVM type: %0");
+				std::string param_type = "";
+				llvm::raw_string_ostream param_type_stream(param_type);
+				type->print(param_type_stream);
+				CGM.getDiags().Report(parm->getSourceRange().getBegin(), param_note_diagID) << param_type_stream.str();
+				
+				auto func_note_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Note, "LLVM function type: %0");
+				std::string fun_type = "";
+				llvm::raw_string_ostream fun_type_stream(fun_type);
+				Fn->getFunctionType()->print(fun_type_stream);
+				CGM.getDiags().Report(parm->getSourceRange().getBegin(), func_note_diagID) << fun_type_stream.str();
+				
+				return uint64_t(0);
+			}
+			return CGM.getDataLayout().getTypeStoreSize(type).getFixedValue();
+		};
+		
+		static const auto get_image_access = [](const FloorImageFlagsAttr* flags_attr) {
+			if (flags_attr != nullptr) {
+				if (flags_attr->isWriteOnly()) {
+					return FLOOR_ARG_INFO::IMG_ACCESS_WRITE;
+				} else if (flags_attr->isReadWrite()) {
+					return FLOOR_ARG_INFO::IMG_ACCESS_READ_WRITE;
+				}
+			}
+			return FLOOR_ARG_INFO::IMG_ACCESS_READ;
+		};
+		static const auto img_type_to_floor_type = [](const clang::Type* type) {
+			const auto builtin_type = type->getAs<BuiltinType>();
+			if (!builtin_type) {
+				return (FLOOR_ARG_INFO)~0ull;
+			}
+			switch (builtin_type->getKind()) {
+				case BuiltinType::OCLImage1d:
+					return FLOOR_ARG_INFO::IMG_1D;
+				case BuiltinType::OCLImage1dArray:
+					return FLOOR_ARG_INFO::IMG_1D_ARRAY;
+				case BuiltinType::OCLImage1dBuffer:
+					return FLOOR_ARG_INFO::IMG_1D_BUFFER;
+				case BuiltinType::OCLImage2d:
+					return FLOOR_ARG_INFO::IMG_2D;
+				case BuiltinType::OCLImage2dArray:
+					return FLOOR_ARG_INFO::IMG_2D_ARRAY;
+				case BuiltinType::OCLImage2dDepth:
+					return FLOOR_ARG_INFO::IMG_2D_DEPTH;
+				case BuiltinType::OCLImage2dArrayDepth:
+					return FLOOR_ARG_INFO::IMG_2D_ARRAY_DEPTH;
+				case BuiltinType::OCLImage2dMSAA:
+					return FLOOR_ARG_INFO::IMG_2D_MSAA;
+				case BuiltinType::OCLImage2dArrayMSAA:
+					return FLOOR_ARG_INFO::IMG_2D_ARRAY_MSAA;
+				case BuiltinType::OCLImage2dMSAADepth:
+					return FLOOR_ARG_INFO::IMG_2D_MSAA_DEPTH;
+				case BuiltinType::OCLImage2dArrayMSAADepth:
+					return FLOOR_ARG_INFO::IMG_2D_ARRAY_MSAA_DEPTH;
+				case BuiltinType::OCLImage3d:
+					return FLOOR_ARG_INFO::IMG_3D;
+				case BuiltinType::OCLImageCube:
+					return FLOOR_ARG_INFO::IMG_CUBE;
+				case BuiltinType::OCLImageCubeArray:
+					return FLOOR_ARG_INFO::IMG_CUBE_ARRAY;
+				case BuiltinType::OCLImageCubeDepth:
+					return FLOOR_ARG_INFO::IMG_CUBE_DEPTH;
+				case BuiltinType::OCLImageCubeArrayDepth:
+					return FLOOR_ARG_INFO::IMG_CUBE_ARRAY_DEPTH;
+				default: break;
+			}
+			return (FLOOR_ARG_INFO)~0ull;
+		};
+		const auto add_image_arg = [](const FLOOR_ARG_INFO& floor_img_type,
+									  const FLOOR_ARG_INFO& access,
+									  const uint32_t elem_count = 1) -> uint64_t /* arg info */ {
+			uint64_t arg_info = uint64_t(FLOOR_ARG_INFO::AS_IMAGE);
+			arg_info |= uint64_t(floor_img_type);
+			arg_info |= uint64_t(access);
+			if (elem_count > 1) {
+				arg_info |= elem_count;
+				arg_info |= uint64_t(FLOOR_ARG_INFO::IMAGE_ARRAY);
+			}
+			return arg_info;
+		};
+		// anything that isn't a pointer or special type
+		const auto add_normal_arg = [&compute_type_size](llvm::Type* llvm_type,
+														 const clang::QualType& clang_type,
+														 const FLOOR_ARG_INFO init_info = FLOOR_ARG_INFO::NONE) -> uint64_t /* arg info */ {
+			// for now: just use the direct type size + no address space
+			uint64_t arg_info = (uint64_t)init_info;
+			// handle some llvm weirdness? why can this be a pointer still?
+			if (llvm_type->isPointerTy()) {
+				arg_info |= compute_type_size(llvm_type->getPointerElementType());
+			} else {
+				arg_info |= compute_type_size(llvm_type);
+			}
+			arg_info |= (uint64_t)to_fas(clang_type.getAddressSpace());
+			return arg_info;
+		};
+		//
+		const std::function<uint64_t(const clang::QualType&, const NamedDecl&, const bool, const bool, const bool, const uint32_t)>
+		add_buffer_arg = [&CGM, &vulkan_iub_count, &compute_type_size, &arg_buf_info, &Fn, &add_buffer_arg, &add_normal_arg](const clang::QualType& type,
+																															 const NamedDecl& decl,
+																															 const bool is_top_level,
+																															 const bool is_indirect, // specifies if we're within an indirect buffer
+																															 const bool has_arg_buffer_attr,
+																															 const uint32_t arg_idx) -> uint64_t /* arg info */ {
+			const auto clang_pointee_type = type->getPointeeType();
+			const auto llvm_type = CGM.getTypes().ConvertTypeForMem(type);
+			const auto llvm_pointee_type = llvm_type->getPointerElementType();
+			
+			const auto indirect_buffer = is_indirect_buffer(type, CGM, has_arg_buffer_attr);
+			
+			const uint64_t arg_size = compute_type_size(llvm_pointee_type);
+			uint64_t arg_info = arg_size;
+			if (CGM.getLangOpts().OpenCL) {
+				arg_info |= (uint64_t)to_fas(clang_pointee_type.getAddressSpace());
+			} else if (CGM.getLangOpts().CUDA || CGM.getLangOpts().FloorHostCompute) {
+				// always pretend this is global
+				arg_info |= (uint64_t)FLOOR_ARG_INFO::AS_GLOBAL;
+			}
+			if (indirect_buffer) {
+				arg_info |= (uint64_t)FLOOR_ARG_INFO::ARGUMENT_BUFFER;
+			} else if (CGM.getLangOpts().Vulkan) {
+				// NOTE: only using global& for const parameters (aka uniforms) right now,
+				//       if this should change, this must also be modified
+				//       -> global pointer must always be a SSBO
+				if (!type->isReferenceType() &&
+					clang_pointee_type.getAddressSpace() == LangAS::opencl_global) {
+					arg_info |= (uint64_t)FLOOR_ARG_INFO::SSBO;
+				}
+				
+				// make parameters a IUB if their size is <= the size limit and we are still below the IUB count limit
+				if (is_top_level) {
+					if (type->isReferenceType() &&
+						clang_pointee_type.getAddressSpace() == LangAS::opencl_constant &&
+						decl.getAttr<FloorArgBufferAttr>() == nullptr /* must not be an argument buffer */ &&
+						arg_size <= CGM.getCodeGenOpts().VulkanIUBSize &&
+						vulkan_iub_count < CGM.getCodeGenOpts().VulkanIUBCount) {
+						arg_info |= (uint64_t)FLOOR_ARG_INFO::IUB;
+						++vulkan_iub_count;
+					}
+				}
+			}
+			
+			//
+			if (indirect_buffer || is_indirect) {
+				if (const auto pointee_rdecl = clang_pointee_type->getAsCXXRecordDecl()) {
+					// TODO: this is not ideal and doesn't handle properly handle unions
+					const auto fields = get_aggregate_fields(pointee_rdecl);
+					bool ignore = false;
+					for (const auto& field : fields) {
+						if (field->isAnonymousStructOrUnion() ||
+							field->isBitField()) {
+							ignore = true;
+							break;
+						}
+					}
+					
+					if (!ignore && indirect_buffer && is_indirect) {
+						// TODO/NOTE: for now, recursive argument buffers are not supported -> implement support for this
+						CGM.Error(decl.getSourceRange().getBegin(), StringRef("recursive argument buffers are currently not supported"));
+						return {};
+					}
+					
+					if (!ignore) {
+						std::stringstream this_arg_buf_info;
+						// #0: info version
+						this_arg_buf_info << floor_info_version << ",";
+						// #1: function name
+						this_arg_buf_info << Fn->getName().str() << ",";
+						// #2: function type
+						this_arg_buf_info << "100,";
+						// #3: function flags
+						this_arg_buf_info << "0,"; // none for argument buffers
+						// #4: argument index in function
+						this_arg_buf_info << arg_idx << ",";
+						// #5/6: 0 / unused for argument buffers
+						this_arg_buf_info << "0,0,";
+						
+						uint32_t field_arg_idx = 0;
+						for (const auto& field : fields) {
+							auto field_type = field->getType();
+							auto llvm_field_type = CGM.getTypes().ConvertTypeForMem(field_type);
+							
+							// TODO: do we need array size handling?
+							if (indirect_buffer || is_indirect) {
+								if (field_type->isPointerType() || field_type->isReferenceType()) {
+									// buffer field
+									// TODO: handle recursive arg buffers
+									const auto field_arg_info = add_buffer_arg(field_type, **field, false, true, false, field_arg_idx);
+									this_arg_buf_info << field_arg_info << ",";
+								} else if (field_type->isImageType()) {
+									// TODO: support this
+									CGM.Error(decl.getSourceRange().getBegin(), StringRef("images are not yet supported in indirect/argument buffers"));
+									return {};
+								} else if (field_type->isArrayImageType(true)) {
+									// TODO: support this
+									CGM.Error(decl.getSourceRange().getBegin(), StringRef("image arrays are not yet supported in indirect/argument buffers"));
+									return {};
+								} else if (field_type->isAggregateImageType()) {
+									// TODO: support this
+									CGM.Error(decl.getSourceRange().getBegin(), StringRef("aggregate images are not yet supported in indirect/argument buffers"));
+									return {};
+								} else if (decl.hasAttr<GraphicsStageInputAttr>()) {
+									// hard error
+									CGM.Error(decl.getSourceRange().getBegin(), StringRef("stage input is not supported in indirect/argument buffers"));
+									return {};
+								} else {
+									// value / "indirect constant"
+									const auto field_arg_info = add_normal_arg(llvm_field_type, field_type);
+									this_arg_buf_info << field_arg_info << ",";
+								}
+							}
+							++field_arg_idx;
+						}
+						arg_buf_info << this_arg_buf_info.str() << "\n";
+					}
+				}
+			}
+			
+			return arg_info;
+		};
+		
+		// #2+: argument sizes + types
+		if (clang_type->isPointerType() || clang_type->isReferenceType()) {
+			const auto arg_info = add_buffer_arg(clang_type, *parm, true, false, (parm->getAttr<FloorArgBufferAttr>() != nullptr), arg_idx);
+			info << arg_info << ",";
+		} else if (clang_type->isImageType()) { // handle image types
+			const auto arg_info = add_image_arg(img_type_to_floor_type(clang_type.getTypePtr()),
+												get_image_access(parm->getAttr<FloorImageFlagsAttr>()));
+			info << arg_info << ",";
+		}
+		// image array (std::array<*image_*<type>, extent>)
+		// NOTE: check before "isAggregateImageType()", because this is essentially a sub-type of it
+		else if (clang_type->isArrayImageType(true)) {
+			const auto array_image_info = get_array_image_info(cxx_rdecl, getContext());
+			if (array_image_info) {
+				const auto arg_info = add_image_arg(img_type_to_floor_type(array_image_info->image_type.getTypePtr()),
+													get_image_access(array_image_info->flags),
+													array_image_info->element_count);
+				info << arg_info << ",";
+			} else {
+				CGM.Error(parm->getSourceRange().getBegin(), StringRef("invalid image array!"));
+				return;
+			}
+		} else if (!getLangOpts().CUDA && clang_type->isArrayImageType(false)) { // image array (image*_t[N])
+			CGM.Error(parm->getSourceRange().getBegin(), StringRef("C array of images not supported yet"));
+			return;
+		} else if (clang_type->isAggregateImageType()) { // aggregate of image types (used with OpenCL/Metal/Vulkan)
+			const auto agg_images = get_aggregate_image_fields(cxx_rdecl);
+			
+			// image count must either be 1 (for single read or write images) or 2 (one read, one write image)
+			const auto field_count = agg_images.size();
+			if (field_count == 0) {
+				CGM.Error(cxx_rdecl->getSourceRange().getBegin(),
+						  StringRef("no images in aggregate-image (min: 1)"));
+				return;
+			} else if (field_count > 2) {
+				CGM.Error(cxx_rdecl->getSourceRange().getBegin(),
+						  StringRef("too many images in aggregate-image (max: 2)"));
+				return;
+			}
+			
+			// sanity check that all field types are actually images, have proper access attributes and image types match
+			// (should probably put this somewhere else, since it is sema-checking, but then I'd need to duplicate code)
+			FLOOR_ARG_INFO floor_img_type = FLOOR_ARG_INFO::NONE;
+			uint64_t floor_img_access = 0;
+			for (const auto& img : agg_images) {
+				const auto access_attr = img->getAttr<FloorImageFlagsAttr>();
+				if (access_attr == nullptr) {
+					CGM.Error(img->getSourceRange().getBegin(),
+							  StringRef("image type in an aggregate-image must have an access qualifier"));
+					return;
+				}
+				floor_img_access |= uint64_t(get_image_access(access_attr));
+				
+				// first field initializes this
+				auto img_type = img->getType();
+				if (img_type->isArrayImageType(false)) {
+					// get element image type if this is an array
+					if (img_type->isPointerType()) {
+						img_type = img_type->getPointeeType();
+					}
+					img_type = img_type->getAsArrayTypeUnsafe()->getElementType();
+				}
+				if (floor_img_type == FLOOR_ARG_INFO::NONE) {
+					floor_img_type = img_type_to_floor_type(img_type.getTypePtr());
+				} else {
+					// second field must have the same type!
+					if (floor_img_type != img_type_to_floor_type(img_type.getTypePtr())) {
+						CGM.Error(img->getSourceRange().getBegin(),
+								  StringRef("second image in aggregate-image does not have the same type as the first"));
+						return;
+					}
+				}
+			}
+			
+			// if the aggregate has two image objects, one must be read, one must be write -> read/write
+			if (field_count == 2 && floor_img_access != uint64_t(FLOOR_ARG_INFO::IMG_ACCESS_READ_WRITE)) {
+				CGM.Error(cxx_rdecl->getSourceRange().getBegin(),
+						  StringRef("aggregate-image has 2 image fields, but joint access is not read-write"));
+				return;
+			}
+			
+			// everything works out, add this as a single kernel argument (floor backends will handle r/w images as necessary)
+			// NOTE: for aggregate images that contain an array of images we still only count this as one image,
+			//       since this is behind-the-scenes stuff and not part of the user interface!
+			const auto arg_info = add_image_arg(floor_img_type, (FLOOR_ARG_INFO)floor_img_access);
+			info << arg_info << ",";
+			
+			// 1 clang aggregate-image == 2 llvm image types -> inc index once more
+			if (field_count == 2) {
+				++arg_idx;
+			}
+		} else if (getLangOpts().CUDA || getLangOpts().FloorHostCompute) { // handle non-pointer parameters
+			// is this an aggregate that is expanded into multiple llvm arguments?
+			if (cxx_rdecl &&
+				(abi_arg_info_iter->info.isDirect() || abi_arg_info_iter->info.isIndirect()) &&
+				TargetCodeGenInfo::TCGIisAggregateTypeForABI(abi_arg_info_iter->type)) {
+				// check if this is an aggregate image (must have image access qualifiers)
+				const FloorImageFlagsAttr* flags_attr = get_aggregate_image_flags_attr(cxx_rdecl);
+				if (flags_attr != nullptr) {
+					uint64_t arg_info = 0; // size is irrelevant for cuda images
+					arg_info |= uint64_t(get_image_access(flags_attr));
+					arg_info |= (uint64_t)FLOOR_ARG_INFO::AS_IMAGE;
+					info << arg_info << ",";
+				} else if (abi_arg_info_iter->info.isDirect()) {
+					// simple aggregate, all constant -> must handle each field individually
+					// note that we're only interested in the first expanded layer, not multiple expansion
+					// (i.e. fully scalarized), as this is identical to what cuda / nvptx / the abi do
+					uint64_t arg_info = 0; // sizes will be accumulated
+					const auto fields = get_aggregate_fields(cxx_rdecl);
+					for (size_t i = 0; i < fields.size(); ++i) {
+						const auto field_llvm_type = std::next(Fn->arg_begin(), arg_idx)->getType();
+						arg_info += compute_type_size(field_llvm_type);
+						++arg_idx;
+					}
+					arg_info |= (uint64_t)FLOOR_ARG_INFO::AS_CONSTANT;
+					info << arg_info << ",";
+					--arg_idx; // fixup, b/c of inc later
+				} else { // -> indirect
+					// simple aggregate, all constant -> single pointer on either side of clang/llvm
+					uint64_t arg_info = 0;
+					arg_info |= compute_type_size(llvm_type->getPointerElementType());
+					arg_info |= (uint64_t)FLOOR_ARG_INFO::AS_CONSTANT;
+					info << arg_info << ",";
+				}
+			} else {
+				// -> this is a simple constant (scalar or aggregate with scalar eval)
+				// store the parameter size
+				uint64_t arg_info = compute_type_size(llvm_type);
+				arg_info |= (uint64_t)FLOOR_ARG_INFO::AS_CONSTANT;
+				info << arg_info << ",";
+			}
+		} else if (parm->hasAttr<GraphicsStageInputAttr>()) { // stage input
+			if (!is_vertex && !is_fragment) {
+				// TODO: should check this in sema
+				// TODO: should also make sure that only 1 exists
+				CGM.Error(FD->getSourceRange().getBegin(), "[[stage_input]] only allowed on vertex and fragment functions");
+			}
+			
+			if (cxx_rdecl) {
+				// must handle each field individually
+				const auto fields = CGM.getTypes().get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+				for (const auto& field : fields) {
+					// TODO: check if field type is int or float!
+					const auto field_llvm_type = std::next(Fn->arg_begin(), arg_idx)->getType();
+					const auto arg_info = add_normal_arg(field_llvm_type, field.type, FLOOR_ARG_INFO::STAGE_INPUT);
+					info << arg_info << ",";
+					++arg_idx;
+				}
+				--arg_idx; // fixup, b/c of inc later
+			} else {
+				// add as-is
+				// TODO: check if type is int or float!
+				const auto arg_info = add_normal_arg(llvm_type, clang_type, FLOOR_ARG_INFO::STAGE_INPUT);
+				info << arg_info << ",";
+			}
+		} else {
+			const auto arg_info = add_normal_arg(llvm_type, clang_type);
+			info << arg_info << ",";
+		}
+		
+		// next arg
+		++arg_idx;
+		++abi_arg_info_iter;
+	}
+	
+	info << "\n";
+	file << info.str();
+	file << arg_buf_info.str();
+	
+#if 0 // for debugging purposes
+	printf("floor function info: %s", info.str().c_str()); fflush(stdout);
+#endif
+	
+	// if this is wrong, the kernel will almost certainly not be usable
+	arg_idx += CGM.getTypes().getMetalVulkanImplicitArgCount(FD); // account for implicit args
+	if (arg_idx != Fn->arg_size()) {
+		// signal that this is _very_ bad
+		auto err_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Fatal, "kernel function parameter count mismatch: %0 (clang), %1 (llvm)");
+		CGM.getDiags().Report(FD->getSourceRange().getBegin(), err_diagID) << std::to_string(arg_idx) << std::to_string(Fn->arg_size());
+		
+		auto llvm_note_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Note, "LLVM function type: %0");
+		std::string llvm_fun_type = "";
+		llvm::raw_string_ostream llvm_fun_type_stream(llvm_fun_type);
+		Fn->getFunctionType()->print(llvm_fun_type_stream);
+		CGM.getDiags().Report(FD->getSourceRange().getBegin(), llvm_note_diagID) << llvm_fun_type_stream.str();
+		
+		auto clang_note_diagID = CGM.getDiags().getCustomDiagID(DiagnosticsEngine::Note, "clang function type: %0");
+		std::string clang_fun_type = "";
+		llvm::raw_string_ostream clang_fun_type_stream(clang_fun_type);
+		FD->getType().print(clang_fun_type_stream, Policy);
+		CGM.getDiags().Report(FD->getSourceRange().getBegin(), clang_note_diagID) << clang_fun_type_stream.str();
+		return;
+	}
 }
 
 /// Determines whether the language options require us to model
@@ -2508,6 +4585,113 @@ void CodeGenModule::EmitGlobalAnnotations() {
   gv->setSection(AnnotationSection);
 }
 
+void CodeGenModule::EmitOCLAnnotations() {
+  // For SPIR, we generate this metadata in a seperate pass
+  if (getTarget().getTriple().getArch() != llvm::Triple::spir &&
+      getTarget().getTriple().getArch() != llvm::Triple::spir64)
+    EmitOCLBuildOptions();
+
+  if (!Context.isFPContractDisabled() && getLangOpts().getDefaultFPContractMode() != LangOptions::FPModeKind::FPM_Off) {
+    TheModule.getOrInsertNamedMetadata("opencl.enable.FP_CONTRACT");
+  }
+}
+
+llvm::SmallVector<llvm::Metadata *, 5> CodeGenModule::getBuildOptions() {
+  llvm::SmallVector<llvm::Metadata *, 5> BuildOption;
+
+  if(!getLangOpts().OpenCL)
+    return BuildOption;
+
+  // get language level fp options
+  const FPOptions fp_opts(getLangOpts());
+
+  // Math Intrinsics Options
+  if(getLangOpts().SinglePrecisionConstants)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-single-precision-constant")));
+
+  if(getCodeGenOpts().FPDenormalMode == llvm::DenormalMode::getPositiveZero() ||
+     getCodeGenOpts().FP32DenormalMode == llvm::DenormalMode::getPositiveZero())
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-denorms-are-zero")));
+
+  if(getCodeGenOpts().OpenCLCorrectlyRoundedDivSqrt)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-fp32-correctly-rounded-divide-sqrt")));
+
+  //Optimization Options
+#if 0 // we don't have that
+  if(getCodeGenOpts().OptDisable)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-opt-disable")));
+#endif
+
+  if(getCodeGenOpts().LessPreciseFPMAD)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-mad-enable")));
+
+  if(fp_opts.getNoSignedZero() || getLangOpts().CLNoSignedZero)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-no-signed-zeros")));
+
+  if(getLangOpts().CLUnsafeMath)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-unsafe-math-optimizations")));
+
+  if(getLangOpts().CLFiniteMathOnly)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-finite-math-only")));
+
+  if(getLangOpts().FastRelaxedMath)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-fast-relaxed-math")));
+
+  if(getCodeGenOpts().getDebugInfo() != codegenoptions::NoDebugInfo)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-g")));
+
+  //Options Controlling the OpenCL C version
+  if(110 == getLangOpts().OpenCLVersion)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-std=CL1.1")));
+
+  if(120 == getLangOpts().OpenCLVersion)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-std=CL1.2")));
+
+  if(200 == getLangOpts().OpenCLVersion)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-std=CL2.0")));
+
+  if(210 == getLangOpts().OpenCLVersion)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-std=CL2.1")));
+
+  if(220 == getLangOpts().OpenCLVersion)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-std=CL2.2")));
+
+  // Options for Querying Kernel Argument Information
+  if(getCodeGenOpts().EmitOpenCLArgMetadata)
+    BuildOption.push_back(llvm::MDString::get(
+    VMContext, llvm::StringRef("-cl-kernel-arg-info")));
+
+  return BuildOption;
+}
+
+void CodeGenModule::EmitOCLBuildOptions()
+{
+  llvm::SmallVector<llvm::Metadata *, 5> BuildOptions = getBuildOptions();
+
+  if (BuildOptions.empty())
+    return;
+
+  llvm::NamedMDNode *OpenCLMetadata =
+    TheModule.getOrInsertNamedMetadata("opencl.compiler.options");
+
+  OpenCLMetadata->addOperand(llvm::MDNode::get(VMContext, BuildOptions));
+}
+
 llvm::Constant *CodeGenModule::EmitAnnotationString(StringRef Str) {
   llvm::Constant *&AStr = AnnotationStrings[Str];
   if (AStr)
@@ -2897,11 +5081,12 @@ void CodeGenModule::EmitGlobal(GlobalDecl GD) {
   if (Global->hasAttr<CPUDispatchAttr>())
     return emitCPUDispatchDefinition(GD);
 
+#if 0 // we need to emit everything (whatever is not actually needed will be removed in LLVM)
   // If this is CUDA, be selective about which declarations we emit.
   if (LangOpts.CUDA) {
     if (LangOpts.CUDAIsDevice) {
       if (!Global->hasAttr<CUDADeviceAttr>() &&
-          !Global->hasAttr<CUDAGlobalAttr>() &&
+          !Global->hasAttr<ComputeKernelAttr>() &&
           !Global->hasAttr<CUDAConstantAttr>() &&
           !Global->hasAttr<CUDASharedAttr>() &&
           !Global->getType()->isCUDADeviceBuiltinSurfaceType() &&
@@ -2922,6 +5107,7 @@ void CodeGenModule::EmitGlobal(GlobalDecl GD) {
              "Expected Variable or Function");
     }
   }
+#endif
 
   if (LangOpts.OpenMP) {
     // If this is OpenMP, check if it is legal to emit this global normally.
@@ -3738,7 +5924,7 @@ llvm::Constant *CodeGenModule::GetAddrOfFunction(GlobalDecl GD,
                                     IsForDefinition);
   // Returns kernel handle for HIP kernel stub function.
   if (LangOpts.CUDA && !LangOpts.CUDAIsDevice &&
-      cast<FunctionDecl>(GD.getDecl())->hasAttr<CUDAGlobalAttr>()) {
+      cast<FunctionDecl>(GD.getDecl())->hasAttr<ComputeKernelAttr>()) {
     auto *Handle = getCUDARuntime().getKernelHandle(
         cast<llvm::Function>(F->stripPointerCasts()), GD);
     if (IsForDefinition)
@@ -4291,7 +6477,7 @@ static bool shouldBeInCOMDAT(CodeGenModule &CGM, const Decl &D) {
 
   // Do not set COMDAT attribute for CUDA/HIP stub functions to prevent
   // them being "merged" by the COMDAT Folding linker optimization.
-  if (D.hasAttr<CUDAGlobalAttr>())
+  if (D.hasAttr<ComputeKernelAttr>())
     return false;
 
   if (D.hasAttr<SelectAnyAttr>())
@@ -4693,7 +6879,7 @@ llvm::GlobalValue::LinkageTypes CodeGenModule::getLLVMLinkageForDeclarator(
       return llvm::Function::ExternalLinkage;
     if (getLangOpts().CUDA && getLangOpts().CUDAIsDevice &&
         !getLangOpts().GPURelocatableDeviceCode)
-      return D->hasAttr<CUDAGlobalAttr>() ? llvm::Function::ExternalLinkage
+      return D->hasAttr<ComputeKernelAttr>() ? llvm::Function::ExternalLinkage
                                           : llvm::Function::InternalLinkage;
     return llvm::Function::WeakODRLinkage;
   }
@@ -4853,12 +7039,73 @@ void CodeGenModule::HandleCXXStaticMemberVarInstantiation(VarDecl *VD) {
   EmitTopLevelDecl(VD);
 }
 
+static llvm::Type* GraphicsExpandIOType(const QualType& type,
+										llvm::Type* llvm_type,
+										CodeGenTypes& CGT,
+										const bool is_metal_2_3) {
+	const llvm::StructType* ST = dyn_cast<llvm::StructType>(llvm_type);
+	if(!ST) return llvm_type;
+	
+	const auto cxx_rdecl = type->getAsCXXRecordDecl();
+	
+	// if the top decl already is a compat vector, return it directly
+	if(cxx_rdecl->hasAttr<VectorCompatAttr>()) {
+		return CGT.ConvertType(CGT.get_compat_vector_type(cxx_rdecl));
+	}
+	
+	// check if we already handled this
+	const auto existing_flattened_type = CGT.getFlattenedRecordType(cxx_rdecl);
+	if (existing_flattened_type) {
+		return existing_flattened_type;
+	}
+	
+	// else: extract all fields and create a flat llvm struct from them
+	const auto fields = CGT.get_aggregate_scalar_fields(cxx_rdecl, cxx_rdecl);
+	std::vector<llvm::Type*> llvm_fields;
+	for(const auto& field : fields) {
+		llvm_fields.push_back(CGT.ConvertType(field.type));
+	}
+	
+	llvm::StructType* ret = nullptr;
+	if (!is_metal_2_3) {
+		const std::string name = "struct.floor.flat." + cxx_rdecl->getName().str() + ".packed";
+		ret = llvm::StructType::create(llvm_fields, name, true); // always make this packed
+	} else {
+		// TODO/NOTE: this is disabled for now, since it can't handle complex types (but note that this isn't required anyways)
+		// Metal 2.3+: make this an unnamed struct
+		ret = llvm::StructType::get(CGT.getLLVMContext(), llvm_fields, true); // always make this packed
+	}
+	ret->setGraphicsIOType(); // fix up alignment/sizes/offsets
+	CGT.create_flattened_cg_layout(cxx_rdecl, ret, fields); // create corresponding flattend CGRecordLayout
+	return ret;
+}
+
 void CodeGenModule::EmitGlobalFunctionDefinition(GlobalDecl GD,
                                                  llvm::GlobalValue *GV) {
   const auto *D = cast<FunctionDecl>(GD.getDecl());
 
   // Compute the function info and LLVM type.
   const CGFunctionInfo &FI = getTypes().arrangeGlobalDeclaration(GD);
+  if (D && (getLangOpts().Metal || getLangOpts().Vulkan)) {
+    // TODO: do this properly in CGCall
+    // if this is a vertex/fragment shader function and we have an I/O type that is a struct/aggregate,
+    // fully expand/flatten all types within (i.e. structs and arrays to scalars, keep existing scalars)
+    if (D->hasAttr<GraphicsVertexShaderAttr>() || D->hasAttr<GraphicsFragmentShaderAttr>()) {
+		//const bool is_metal_2_3 = (getLangOpts().MetalVersion >= 230); // TODO/NOTE: disabled for now, see above
+		const bool is_metal_2_3 = false;
+		if (FI.getReturnType()->isStructureOrClassType()) {
+			auto& retInfo = const_cast<ABIArgInfo&>(FI.getReturnInfo());
+			retInfo.setCoerceToType(GraphicsExpandIOType(FI.getReturnType(), retInfo.getCoerceToType(), getTypes(), is_metal_2_3));
+		}
+		for (const auto& param : D->parameters()) {
+			const auto param_type = param->getType();
+			if (param_type->isStructureOrClassType() && param->hasAttr<GraphicsStageInputAttr>()) {
+				auto llvm_param_type = getTypes().ConvertType(param_type);
+				(void)GraphicsExpandIOType(param_type, llvm_param_type, getTypes(), is_metal_2_3);
+			}
+		}
+    }
+  }
   llvm::FunctionType *Ty = getTypes().GetFunctionType(FI);
 
   // Get or create the prototype for the function.
@@ -4962,8 +7209,7 @@ void CodeGenModule::EmitAliasDefinition(GlobalDecl GD) {
     // Remove it and replace uses of it with the alias.
     GA->takeName(Entry);
 
-    Entry->replaceAllUsesWith(llvm::ConstantExpr::getBitCast(GA,
-                                                          Entry->getType()));
+    Entry->replaceAllUsesWith(llvm::ConstantExpr::getPointerBitCastOrAddrSpaceCast(GA, Entry->getType()));
     Entry->eraseFromParent();
   } else {
     GA->setName(MangledName);
@@ -6261,6 +8507,71 @@ void CodeGenModule::EmitOMPThreadPrivateDecl(const OMPThreadPrivateDecl *D) {
   }
 }
 
+llvm::Constant*
+CodeGenModule::createIntToSamplerConversion(const Expr *E,
+                                            CodeGenFunction *CGF,
+                                            llvm::GlobalVariable *InsertBefore,
+                                            StringRef Name) {
+  ConstantEmitter emitter(*this, CGF);
+  llvm::Constant *C = emitter.tryEmitForInitializer(E, E->getType().getAddressSpace(),
+                                                    E->getType());
+  assert(C && "Sampler must be initialized by constant");
+  assert(isa<llvm::ConstantInt>(C) && "Sampler must be initialized by integer");
+  if (!getLangOpts().CLSamplerOpaque)
+    return C;
+
+  llvm::StructType* ConstSamplerTy = llvm::StructType::getTypeByName(TheModule.getContext(), "spirv.ConstantSampler");
+  if (!ConstSamplerTy ) {
+    llvm::Type* Elements[] = {Int32Ty, Int32Ty, Int32Ty};
+    ConstSamplerTy = llvm::StructType::create(VMContext, Elements,
+                                              "spirv.ConstantSampler");
+  }
+  const llvm::ConstantInt *CI = static_cast<llvm::ConstantInt*>(C);
+  const uint64_t SamplerValue = CI->getValue().getZExtValue();
+  // 32-bit value of sampler's initializer is interpreted as
+  // bit-field with the following structure:
+  // |unspecified|Filter|Addressing Mode| Normalized Coords|
+  // |31        6|5    4|3             1|                 0|
+  // This structure corresponds to values of sampler properties from opencl.h
+  // Mapping these bits to values defined by SPIR-V specification.
+  unsigned NormalizedCoords = 0x01 & SamplerValue;
+  unsigned AddressingMode  = (0x0E & SamplerValue) >> 1;
+  unsigned FilterMode      = (0x30 & SamplerValue) >> 4;
+  // In SPIR sampler's filter bits are defined as the following
+  // #define CLK_FILTER_NEAREST 0x10
+  // #define CLK_FILTER_LINEAR 0x20
+  // corresponding to 1 and 2 in bits 4-5.
+  // SPIR-V defines sampler filter mode enum as: nearest=0, linear=1,
+  // Therefore, to convert FilterMode from SPIR to SPIR-V,
+  // FilterMode value must be decremented
+  if (FilterMode == 1 || FilterMode == 2)
+    --FilterMode;
+   else
+    getDiags().Report(Context.getFullLoc(E->getBeginLoc()),
+      diag::warn_sampler_initializer_invalid_bits) << "Filter Mode";
+  if (AddressingMode > 4)
+    getDiags().Report(Context.getFullLoc(E->getBeginLoc()),
+      diag::warn_sampler_initializer_invalid_bits) << "Addressing Mode";
+
+  llvm::Constant *Initializer = llvm::ConstantStruct::get(ConstSamplerTy,
+    llvm::ConstantInt::get(Int32Ty, AddressingMode),
+    llvm::ConstantInt::get(Int32Ty, NormalizedCoords),
+    llvm::ConstantInt::get(Int32Ty, FilterMode));
+  llvm::StructType* SamplerTy = llvm::StructType::getTypeByName(TheModule.getContext(), "spirv.Sampler");
+  if(!SamplerTy)
+    SamplerTy = llvm::StructType::create(VMContext, "spirv.Sampler");
+
+  unsigned AS = Context.getTargetAddressSpace(LangAS::opencl_constant);
+  llvm::GlobalVariable *GV =
+    new llvm::GlobalVariable(TheModule, ConstSamplerTy, true,
+                             llvm::GlobalVariable::InternalLinkage,
+                             Initializer, Name + ".sampler.init", InsertBefore,
+                             llvm::GlobalVariable::NotThreadLocal, AS);
+
+  return llvm::ConstantExpr::getBitCast(GV,
+                                        llvm::PointerType::get(SamplerTy, AS));
+}
+
 llvm::Metadata *
 CodeGenModule::CreateMetadataIdentifierImpl(QualType T, MetadataTypeMap &Map,
                                             StringRef Suffix) {
@@ -6368,17 +8679,6 @@ llvm::SanitizerStatReport &CodeGenModule::getSanStats() {
   return *SanStats;
 }
 
-llvm::Value *
-CodeGenModule::createOpenCLIntToSamplerConversion(const Expr *E,
-                                                  CodeGenFunction &CGF) {
-  llvm::Constant *C = ConstantEmitter(CGF).emitAbstract(E, E->getType());
-  auto *SamplerT = getOpenCLRuntime().getSamplerType(E->getType().getTypePtr());
-  auto *FTy = llvm::FunctionType::get(SamplerT, {C->getType()}, false);
-  auto *Call = CGF.EmitRuntimeCall(
-      CreateRuntimeFunction(FTy, "__translate_sampler_initializer"), {C});
-  return Call;
-}
-
 CharUnits CodeGenModule::getNaturalPointeeTypeAlignment(
     QualType T, LValueBaseInfo *BaseInfo, TBAAAccessInfo *TBAAInfo) {
   return getNaturalTypeAlignment(T->getPointeeType(), BaseInfo, TBAAInfo,
diff --git a/clang/lib/CodeGen/CodeGenModule.h b/clang/lib/CodeGen/CodeGenModule.h
index 1c5c3ff6aab8..4e2804c77a49 100644
--- a/clang/lib/CodeGen/CodeGenModule.h
+++ b/clang/lib/CodeGen/CodeGenModule.h
@@ -87,6 +87,7 @@ namespace CodeGen {
 class CallArgList;
 class CodeGenFunction;
 class CodeGenTBAA;
+class CGBuilderTy;
 class CGCXXABI;
 class CGDebugInfo;
 class CGObjCRuntime;
@@ -1272,6 +1273,12 @@ public:
   /// Emit all the global annotations.
   void EmitGlobalAnnotations();
 
+  /// Emit OpenCL related annotations.
+  void EmitOCLAnnotations();
+
+  /// Emit OCL compiler options
+  void EmitOCLBuildOptions();
+
   /// Emit an annotation string.
   llvm::Constant *EmitAnnotationString(StringRef Str);
 
@@ -1431,8 +1438,11 @@ public:
 
   llvm::SanitizerStatReport &getSanStats();
 
-  llvm::Value *
-  createOpenCLIntToSamplerConversion(const Expr *E, CodeGenFunction &CGF);
+  llvm::Constant*
+  createIntToSamplerConversion(const Expr *E,
+                               CodeGenFunction *CGF,
+                               llvm::GlobalVariable *InsertBefore = nullptr,
+                               StringRef Name = "");
 
   /// OpenCL v1.2 s5.6.4.6 allows the compiler to store kernel argument
   /// information in the program executable. The argument information stored
@@ -1445,8 +1455,17 @@ public:
   /// \param FD is a pointer to function declaration if any.
   /// \param CGF is a pointer to CodeGenFunction that generates this function.
   void GenOpenCLArgMetadata(llvm::Function *FN,
-                            const FunctionDecl *FD = nullptr,
-                            CodeGenFunction *CGF = nullptr);
+                            const FunctionDecl *FD,
+                            CodeGenFunction *CGF,
+                            SmallVector<llvm::Metadata *, 5> &kernelMDArgs);
+
+  void GenAIRMetadata(const FunctionDecl *FD, llvm::Function *Fn,
+                      const CGFunctionInfo &FnInfo,
+                      SmallVector <llvm::Metadata*, 5> &kernelMDArgs,
+                      CGBuilderTy& Builder);
+
+  void GenVulkanMetadata(const FunctionDecl *FD, llvm::Function *Fn,
+                         CGBuilderTy& Builder);
 
   /// Get target specific null pointer.
   /// \param T is the LLVM type of the null pointer.
@@ -1627,6 +1646,10 @@ private:
 
   llvm::Metadata *CreateMetadataIdentifierImpl(QualType T, MetadataTypeMap &Map,
                                                StringRef Suffix);
+
+  // Get a metadata vector containing the build options
+  llvm::SmallVector<llvm::Metadata *, 5> getBuildOptions();
+
 };
 
 }  // end namespace CodeGen
diff --git a/clang/lib/CodeGen/CodeGenPGO.cpp b/clang/lib/CodeGen/CodeGenPGO.cpp
index ab953c2c7d52..85fa79e76d1e 100644
--- a/clang/lib/CodeGen/CodeGenPGO.cpp
+++ b/clang/lib/CodeGen/CodeGenPGO.cpp
@@ -798,7 +798,7 @@ void CodeGenPGO::assignRegionCounters(GlobalDecl GD, llvm::Function *Fn) {
 
   // Skip CUDA/HIP kernel launch stub functions.
   if (CGM.getLangOpts().CUDA && !CGM.getLangOpts().CUDAIsDevice &&
-      D->hasAttr<CUDAGlobalAttr>())
+      D->hasAttr<ComputeKernelAttr>())
     return;
 
   bool InstrumentRegions = CGM.getCodeGenOpts().hasProfileClangInstr();
@@ -871,9 +871,9 @@ bool CodeGenPGO::skipRegionMappingForDecl(const Decl *D) {
   // ones, their coverage mapping may still be generated.
   if (CGM.getLangOpts().CUDA &&
       ((CGM.getLangOpts().CUDAIsDevice && !D->hasAttr<CUDADeviceAttr>() &&
-        !D->hasAttr<CUDAGlobalAttr>()) ||
+        !D->hasAttr<ComputeKernelAttr>()) ||
        (!CGM.getLangOpts().CUDAIsDevice &&
-        (D->hasAttr<CUDAGlobalAttr>() ||
+        (D->hasAttr<ComputeKernelAttr>() ||
          (!D->hasAttr<CUDAHostAttr>() && D->hasAttr<CUDADeviceAttr>())))))
     return true;
 
diff --git a/clang/lib/CodeGen/CodeGenTypes.cpp b/clang/lib/CodeGen/CodeGenTypes.cpp
index fb05475a4e8c..a12862c05f11 100644
--- a/clang/lib/CodeGen/CodeGenTypes.cpp
+++ b/clang/lib/CodeGen/CodeGenTypes.cpp
@@ -25,6 +25,7 @@
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DerivedTypes.h"
 #include "llvm/IR/Module.h"
+#include "llvm/Transforms/LibFloor/FloorImageType.h"
 using namespace clang;
 using namespace CodeGen;
 
@@ -397,6 +398,10 @@ llvm::Type *CodeGenTypes::ConvertType(QualType T) {
 
   const Type *Ty = T.getTypePtr();
 
+  // intercept image arrays before RT conversion
+  if (Ty->isArrayImageType(true))
+    return ConvertArrayImageType(Ty);
+
   // For the device-side compilation, CUDA device builtin surface/texture types
   // may be represented in different types.
   if (Context.getLangOpts().CUDAIsDevice) {
@@ -807,6 +812,87 @@ bool CodeGenModule::isPaddedAtomicType(const AtomicType *type) {
   return Context.getTypeSize(type) != Context.getTypeSize(type->getValueType());
 }
 
+/// image type cache for structs with 1 image field and 2 image fields (used below)
+static std::unordered_map<COMPUTE_IMAGE_TYPE, llvm::StructType*> image_type_cache_1if;
+static std::unordered_map<COMPUTE_IMAGE_TYPE, llvm::StructType*> image_type_cache_2if;
+
+/// we need to ensure that image-based types are always unique (for Metal)
+/// -> if this RecordDecl contains an opaque (image) type, check the existing cache of converted image types and
+///    set Entry to a matching StructType if there is one + return { true-if-new, cache-func } if this is an image type
+static std::pair<bool, std::function<void(llvm::StructType*)>>
+handle_image_rdecl(ASTContext& Context, const RecordDecl *RD, llvm::StructType *&Entry) {
+	if (Entry || !Context.getLangOpts().Metal) {
+		return { false, {} };
+	}
+	
+	auto def = RD->getDefinition();
+	if (!def || !def->isCompleteDefinition()) {
+		return { false, {} };
+	}
+	
+	auto pot_img_def = dyn_cast_or_null<ClassTemplateSpecializationDecl>(def);
+	if (!pot_img_def) {
+		return { false, {} };
+	}
+	
+	bool is_all_image_fields = true;
+	uint32_t active_field_count = 0;
+	for (auto field : def->fields()) {
+		if (field->isZeroSize(RD->getASTContext())) {
+			continue;
+		}
+		++active_field_count;
+		if (!field->getType()->isImageType() &&
+			!field->getType()->isArrayImageType(true)) {
+			is_all_image_fields = false;
+			break;
+		}
+	}
+	if (!is_all_image_fields || active_field_count < 1 || active_field_count > 2) {
+		// must have either 1 or 2 image fields, and all fields must be image types
+		return { false, {} };
+	}
+	
+	const auto& templ_args = pot_img_def->getTemplateArgs();
+	if (templ_args.size() == 0) {
+		return { false, {} };
+	}
+	const auto& arg_0 = templ_args.get(0);
+	if (arg_0.getKind() != TemplateArgument::Integral) {
+		return { false, {} };
+	}
+	
+	// mask/bits that we need to identify an opaque image type
+	static constexpr const COMPUTE_IMAGE_TYPE opaque_image_mask {
+		COMPUTE_IMAGE_TYPE::__DIM_MASK |
+		COMPUTE_IMAGE_TYPE::FLAG_DEPTH |
+		COMPUTE_IMAGE_TYPE::FLAG_ARRAY |
+		COMPUTE_IMAGE_TYPE::FLAG_BUFFER |
+		COMPUTE_IMAGE_TYPE::FLAG_CUBE |
+		COMPUTE_IMAGE_TYPE::FLAG_MSAA
+	};
+	const auto image_type = (COMPUTE_IMAGE_TYPE)arg_0.getAsIntegral().getZExtValue();
+	const auto masked_image_type = image_type & opaque_image_mask;
+	
+	// check if we have a cached entry, return it if so
+	std::unordered_map<COMPUTE_IMAGE_TYPE, llvm::StructType*>& cache =
+		(active_field_count == 1 ? image_type_cache_1if : image_type_cache_2if);
+	auto iter = cache.find(masked_image_type);
+	if (iter != cache.end()) {
+		Entry = iter->second;
+		return { true, {} };
+	}
+	
+	// this is a new image type, provide cache function
+	std::function<void(llvm::StructType*)> cache_func = [masked_image_type, active_field_count](llvm::StructType* type) {
+		std::unordered_map<COMPUTE_IMAGE_TYPE, llvm::StructType*>& cache =
+			(active_field_count == 1 ? image_type_cache_1if : image_type_cache_2if);
+		cache.emplace(masked_image_type, type);
+	};
+	
+	return { true, std::move(cache_func) };
+}
+
 /// ConvertRecordDeclType - Lay out a tagged decl type like struct or union.
 llvm::StructType *CodeGenTypes::ConvertRecordDeclType(const RecordDecl *RD) {
   // TagDecl's are not necessarily unique, instead use the (clang)
@@ -815,6 +901,9 @@ llvm::StructType *CodeGenTypes::ConvertRecordDeclType(const RecordDecl *RD) {
 
   llvm::StructType *&Entry = RecordDeclTypes[Key];
 
+  // we need to ensure that image-based types are always unique (for Metal)
+  auto [is_new_image_rdecl, cache_func] = handle_image_rdecl(Context, RD, Entry);
+
   // If we don't have a StructType at all yet, create the forward declaration.
   if (!Entry) {
     Entry = llvm::StructType::create(getLLVMContext());
@@ -825,7 +914,7 @@ llvm::StructType *CodeGenTypes::ConvertRecordDeclType(const RecordDecl *RD) {
   // If this is still a forward declaration, or the LLVM type is already
   // complete, there's nothing more to do.
   RD = RD->getDefinition();
-  if (!RD || !RD->isCompleteDefinition() || !Ty->isOpaque())
+  if (!RD || !RD->isCompleteDefinition() || (!Ty->isOpaque() && !is_new_image_rdecl))
     return Ty;
 
   // If converting this type would cause us to infinitely loop, don't do it!
@@ -867,12 +956,80 @@ llvm::StructType *CodeGenTypes::ConvertRecordDeclType(const RecordDecl *RD) {
     while (!DeferredRecords.empty())
       ConvertRecordDeclType(DeferredRecords.pop_back_val());
 
+  // cache the image type
+  if (is_new_image_rdecl && cache_func) {
+    cache_func(Ty);
+  }
+
   return Ty;
 }
 
+llvm::Type *CodeGenTypes::ConvertArrayImageType(const Type* Ty) {
+  // ptr to array of images
+  if(Ty->isPointerType() &&
+     Ty->getPointeeType()->isArrayType() &&
+     Ty->getPointeeType()->getArrayElementTypeNoTypeQual()->isImageType()) {
+    return llvm::PointerType::get(ConvertArrayImageType(Ty->getPointeeType().getTypePtr()), 0);
+  }
+	
+  // simple C-style array that contains an image type
+  if(Ty->isArrayType() &&
+     Ty->getArrayElementTypeNoTypeQual()->isImageType()) {
+    const ConstantArrayType *CAT = Context.getAsConstantArrayType(QualType(Ty, 0));
+    const auto elem_type = CAT->getElementType();
+    if(elem_type->isImageType()) {
+      return llvm::ArrayType::get(ConvertType(elem_type), CAT->getSize().getZExtValue());
+    } else if(elem_type->isAggregateImageType()) {
+      // must be an aggregate image with exactly one image
+      const auto agg_img_type = elem_type->getAsCXXRecordDecl();
+      auto agg_img_fields = get_aggregate_scalar_fields(agg_img_type, agg_img_type, false, false,
+                                                        true /* TODO */);
+      if(agg_img_fields.size() != 1) return nullptr;
+      return llvm::ArrayType::get(ConvertType(agg_img_fields[0].type), CAT->getSize().getZExtValue());
+    }
+    assert(false && "invalid array of images type");
+  }
+
+  // must be struct or class, union is not allowed
+  if(!Ty->isStructureOrClassType()) return nullptr;
+
+  // must be a cxx rdecl
+  const auto decl = Ty->getAsCXXRecordDecl();
+  if(!decl) return nullptr;
+
+  // must have definition
+  if(!decl->hasDefinition()) return nullptr;
+
+  // must have exactly one field
+  const auto field_count = std::distance(decl->field_begin(), decl->field_end());
+  if(field_count != 1) return nullptr;
+
+  // field must be an array
+  const QualType arr_field_type = decl->field_begin()->getType();
+  const ConstantArrayType *CAT = Context.getAsConstantArrayType(arr_field_type);
+  if(!CAT) return nullptr;
+
+  // must be an aggregate image with exactly one image
+  const auto agg_img_type = CAT->getElementType()->getAsCXXRecordDecl();
+  auto agg_img_fields = get_aggregate_scalar_fields(agg_img_type, agg_img_type, false, false,
+                                                    true /* TODO */);
+  if(agg_img_fields.size() != 1) return nullptr;
+
+  // got everything we need
+  return llvm::ArrayType::get(ConvertType(agg_img_fields[0].type), CAT->getSize().getZExtValue());
+}
+
+
 /// getCGRecordLayout - Return record layout info for the given record decl.
 const CGRecordLayout &
-CodeGenTypes::getCGRecordLayout(const RecordDecl *RD) {
+CodeGenTypes::getCGRecordLayout(const RecordDecl *RD, llvm::Type* struct_type) {
+  // check if there is a flattened layout for this llvm struct type,
+  // return it if so, otherwise continue as usual
+  if (struct_type != nullptr) {
+    const auto flat_layout = FlattenedCGRecordLayouts.lookup(struct_type);
+    if(flat_layout) return *flat_layout;
+  }
+
   const Type *Key = Context.getTagDeclType(RD).getTypePtr();
 
   auto I = CGRecordLayouts.find(Key);
@@ -889,6 +1046,11 @@ CodeGenTypes::getCGRecordLayout(const RecordDecl *RD) {
   return *I->second;
 }
 
+llvm::Type* CodeGenTypes::getFlattenedRecordType(const CXXRecordDecl* D) const {
+  const auto iter = FlattenedRecords.find_as(D);
+  return (iter != FlattenedRecords.end() ? iter->second : nullptr);
+}
+
 bool CodeGenTypes::isPointerZeroInitializable(QualType T) {
   assert((T->isAnyPointerType() || T->isBlockPointerType()) && "Invalid type");
   return isZeroInitializable(T);
@@ -925,3 +1087,230 @@ bool CodeGenTypes::isZeroInitializable(QualType T) {
 bool CodeGenTypes::isZeroInitializable(const RecordDecl *RD) {
   return getCGRecordLayout(RD).isZeroInitializable();
 }
+
+//
+static std::string aggregate_scalar_fields_mangle(const CXXRecordDecl* root_decl,
+												  MangleContext& MC,
+												  RecordDecl::field_iterator field_iter) {
+	std::string gen_type_name = "";
+	llvm::raw_string_ostream gen_type_name_stream(gen_type_name);
+	MC.mangleMetalFieldName(*field_iter, root_decl, gen_type_name_stream);
+	return "generated(" + gen_type_name_stream.str() + ")";
+}
+static std::string aggregate_scalar_fields_mangle(const CXXRecordDecl* root_decl,
+												  MangleContext& MC,
+												  const std::string& name,
+												  const clang::QualType type) {
+	std::string gen_type_name = "";
+	llvm::raw_string_ostream gen_type_name_stream(gen_type_name);
+	MC.mangleMetalGeneric(name, type, root_decl, gen_type_name_stream);
+	return "generated(" + gen_type_name_stream.str() + ")";
+}
+
+clang::QualType CodeGenTypes::get_compat_vector_type(const CXXRecordDecl* decl) const {
+	const auto fields = get_aggregate_scalar_fields(decl, decl, true, false, true);
+	
+	const auto vec_size = fields.size();
+	if(vec_size < 1 || vec_size > 4) {
+		assert(false && "invalid vector size (must be >= 1 && <= 4)");
+		return Context.VoidTy;
+	}
+	
+	const auto elem_type = fields[0].type.getUnqualifiedType();
+	for(size_t i = 1; i < vec_size; ++i) {
+		if(fields[i].type.getUnqualifiedType() != elem_type) {
+			assert(false && "all vector-compat element types must be equal");
+			return Context.VoidTy;
+		}
+	}
+	
+	return Context.getExtVectorType(elem_type, vec_size);
+}
+
+void CodeGenTypes::aggregate_scalar_fields_add_array(const CXXRecordDecl* root_decl,
+													 const CXXRecordDecl* parent_decl,
+													 const ConstantArrayType* CAT,
+													 const AttrVec* attrs,
+													 const FieldDecl* parent_field_decl,
+													 const std::string& name,
+													 const bool expand_array_image,
+													 std::vector<CodeGenTypes::aggregate_scalar_entry>& ret) const {
+	if(expand_array_image ||
+	   !(CAT->getElementType()->isAggregateImageType() ||
+		 CAT->getElementType()->isImageType())) {
+		const auto count = CAT->getSize().getZExtValue();
+		const auto ET = CAT->getElementType();
+		if(const auto arr_rdecl = ET->getAsCXXRecordDecl()) {
+			auto contained_ret = get_aggregate_scalar_fields(root_decl, arr_rdecl, false, false, expand_array_image);
+			for(auto& entry : contained_ret) {
+				entry.parents.push_back(parent_decl);
+			}
+			for(uint64_t i = 0; i < count; ++i) {
+				ret.insert(ret.end(), contained_ret.begin(), contained_ret.end());
+			}
+		}
+		else if(ET->isArrayType()) {
+			const auto aoa_decl = dyn_cast<ConstantArrayType>(ET->getAsArrayTypeUnsafe());
+			if(aoa_decl) {
+				for(uint64_t i = 0; i < count; ++i) {
+					const auto idx_str = "_" + std::to_string(i);
+					aggregate_scalar_fields_add_array(root_decl, parent_decl, aoa_decl, attrs, parent_field_decl,
+													  name + idx_str, expand_array_image, ret);
+				}
+			}
+			else {
+				// TODO: error
+			}
+		}
+		else {
+			for(uint64_t i = 0; i < count; ++i) {
+				const auto idx_str = "_" + std::to_string(i);
+				ret.push_back(aggregate_scalar_entry {
+					ET,
+					name + idx_str,
+					aggregate_scalar_fields_mangle(root_decl, TheCXXABI.getMangleContext(), name + idx_str, ET),
+					attrs,
+					parent_field_decl,
+					{ parent_decl },
+					false,
+					false
+				});
+			}
+		}
+	}
+	else {
+		// directly add an array entry
+		ret.push_back(aggregate_scalar_entry {
+			clang::QualType(CAT, 0),
+			name,
+			aggregate_scalar_fields_mangle(root_decl, TheCXXABI.getMangleContext(), name, clang::QualType(CAT, 0)),
+			attrs,
+			parent_field_decl,
+			{ parent_decl },
+			false,
+			false
+		});
+	}
+}
+
+std::vector<CodeGenTypes::aggregate_scalar_entry>
+CodeGenTypes::get_aggregate_scalar_fields(const CXXRecordDecl* root_decl,
+										  const CXXRecordDecl* decl,
+										  const bool ignore_root_vec_compat,
+										  const bool ignore_bases,
+										  const bool expand_array_image) const {
+	if(decl == nullptr) return {};
+	
+	// must have definition
+	if(!decl->hasDefinition()) return {};
+	
+	// if the root decl is a direct compat vector, return it directly
+	if(!ignore_root_vec_compat &&
+	   decl->hasAttr<VectorCompatAttr>()) {
+		return {
+			aggregate_scalar_entry {
+				get_compat_vector_type(decl),
+				"",
+				"",
+				&decl->getAttrs(),
+				nullptr,
+				{},
+				true,
+				false
+			}
+		};
+	}
+	
+	//
+	std::vector<aggregate_scalar_entry> ret;
+	
+	// iterate over / recurse into all bases
+	if(!ignore_bases) {
+		for(const auto& base : decl->bases()) {
+			auto base_ret = get_aggregate_scalar_fields(root_decl, base.getType()->getAsCXXRecordDecl(),
+														false, false, expand_array_image);
+			for(auto& elem : base_ret) {
+				elem.is_in_base = true;
+			}
+			if(!base_ret.empty()) {
+				ret.insert(ret.end(), base_ret.begin(), base_ret.end());
+			}
+		}
+	}
+	
+	// TODO/NOTE: make sure attrs are correctly forwarded/inherited/passed-through
+	const auto add_field = [this, &root_decl, &decl, &expand_array_image, &ret](RecordDecl::field_iterator field_iter) {
+		if(const auto rdecl = field_iter->getType()->getAsCXXRecordDecl()) {
+			if(rdecl->hasAttr<VectorCompatAttr>() ||
+			   field_iter->hasAttr<GraphicsVertexPositionAttr>()) {
+				const auto vec_type = get_compat_vector_type(rdecl);
+				
+				if(field_iter->hasAttr<GraphicsVertexPositionAttr>()) {
+					const auto as_vec_type = vec_type->getAs<ExtVectorType>();
+					if(as_vec_type->getNumElements() != 4 ||
+					   !as_vec_type->getElementType()->isFloatingType()) {
+						// TODO: error!
+					}
+				}
+				
+				ret.push_back(aggregate_scalar_entry {
+					vec_type,
+					field_iter->getName().str(),
+					aggregate_scalar_fields_mangle(root_decl, TheCXXABI.getMangleContext(),
+												   field_iter->getName().str(), vec_type),
+					field_iter->hasAttrs() ? &field_iter->getAttrs() : nullptr,
+					*field_iter,
+					{ decl },
+					true,
+					false
+				});
+			}
+			else {
+				auto contained_ret = get_aggregate_scalar_fields(root_decl, rdecl,
+																 false, false, expand_array_image);
+				for(auto& entry : contained_ret) {
+					entry.parents.push_back(decl);
+				}
+				if(!contained_ret.empty()) {
+					ret.insert(ret.end(), contained_ret.begin(), contained_ret.end());
+				}
+			}
+		}
+		else if(field_iter->getType()->isArrayType()) {
+			const auto arr_decl = dyn_cast<ConstantArrayType>(field_iter->getType()->getAsArrayTypeUnsafe());
+			if(arr_decl) {
+				aggregate_scalar_fields_add_array(root_decl, decl, arr_decl,
+												  field_iter->hasAttrs() ? &field_iter->getAttrs() : nullptr,
+												  *field_iter, field_iter->getName().str(), expand_array_image, ret);
+			}
+			else {
+				// TODO: error
+			}
+		}
+		else {
+			ret.push_back(aggregate_scalar_entry {
+				field_iter->getType(),
+				field_iter->getName().str(),
+				aggregate_scalar_fields_mangle(root_decl, TheCXXABI.getMangleContext(), field_iter),
+				field_iter->hasAttrs() ? &field_iter->getAttrs() : nullptr,
+				*field_iter,
+				{ decl },
+				false,
+				false
+			});
+		}
+	};
+	
+	if(!decl->isUnion()) {
+		// iterate over all fields/members
+		for(auto iter = decl->field_begin(); iter != decl->field_end(); ++iter) {
+			add_field(iter);
+		}
+	}
+	else {
+		// for unions: only use the first field
+		add_field(decl->field_begin());
+	}
+	
+	return ret;
+}
diff --git a/clang/lib/CodeGen/CodeGenTypes.h b/clang/lib/CodeGen/CodeGenTypes.h
index f8f7542e4c83..1095f581dad2 100644
--- a/clang/lib/CodeGen/CodeGenTypes.h
+++ b/clang/lib/CodeGen/CodeGenTypes.h
@@ -77,6 +77,13 @@ class CodeGenTypes {
   /// Maps clang struct type with corresponding record layout info.
   llvm::DenseMap<const Type*, std::unique_ptr<CGRecordLayout>> CGRecordLayouts;
 
+  /// This maps special flattened llvm struct types
+  /// with the corresponding record layout info.
+  llvm::DenseMap<const llvm::Type*, CGRecordLayout *> FlattenedCGRecordLayouts;
+
+  /// This maps CXX record decls to their special flattend llvm struct types
+  llvm::DenseMap<const CXXRecordDecl*, llvm::Type*> FlattenedRecords;
+
   /// Contains the LLVM IR type for any converted RecordDecl.
   llvm::DenseMap<const Type*, llvm::StructType *> RecordDeclTypes;
 
@@ -157,7 +164,12 @@ public:
   /// and/or incomplete argument types, this will return the opaque type.
   llvm::Type *GetFunctionTypeForVTable(GlobalDecl GD);
 
-  const CGRecordLayout &getCGRecordLayout(const RecordDecl*);
+  const CGRecordLayout &getCGRecordLayout(const RecordDecl*,
+										  llvm::Type* struct_type = nullptr);
+
+  /// Returns the flattend LLVM type of the specified CXX record decl,
+  /// or nullptr if no flattened type exists.
+  llvm::Type *getFlattenedRecordType(const CXXRecordDecl* D) const;
 
   /// UpdateCompletedType - When we find the full definition for a TagDecl,
   /// replace the 'opaque' type we previously made for it if applicable.
@@ -280,15 +292,85 @@ public:
   void addRecordTypeName(const RecordDecl *RD, llvm::StructType *Ty,
                          StringRef suffix);
 
+  //
+  struct aggregate_scalar_entry {
+	clang::QualType type;
+	std::string name;
+	std::string mangled_name;
+    const AttrVec* attrs;
+	// NOTE: this is nullptr for non-fields!
+    const FieldDecl* field_decl;
+    std::vector<const CXXRecordDecl*> parents;
+    bool compat_vector;
+    bool is_in_base;
+	
+	template <typename SpecificAttr>
+	bool hasAttr() const {
+		if(attrs == nullptr) return false;
+		return hasSpecificAttr<SpecificAttr>(*attrs);
+	}
+	
+	template <typename SpecificAttr>
+	SpecificAttr* getAttr() const {
+		if(attrs == nullptr) return nullptr;
+		return getSpecificAttr<SpecificAttr>(*attrs);
+	}
+  };
+
+  // will recurse through the specified class/struct decl, its base classes,
+  // all its contained class/struct/union decls, all its contained arrays,
+  // returning a vector of all contained/scalarized fields + info
+  // NOTE: for unions, only the first field will be considered
+  // NOTE: this also transform/converts [[vector_compat]] types to clang vector types
+  std::vector<aggregate_scalar_entry> get_aggregate_scalar_fields(const CXXRecordDecl* root_decl,
+                                                                  const CXXRecordDecl* decl,
+																  const bool ignore_root_vec_compat = false,
+																  const bool ignore_bases = false,
+																  const bool expand_array_image = true) const;
+
+  // returns the corresponding clang vector type for a [[vector_compat]] aggregate
+  clang::QualType get_compat_vector_type(const CXXRecordDecl* decl) const;
+
+  //
+  void create_flattened_cg_layout(const CXXRecordDecl* decl, llvm::StructType* type,
+								  const std::vector<aggregate_scalar_entry>& fields);
+
+  // for all entry functions/points: handle the function type -> add implicit internal args
+  // "FTy" is optional and if specified will update the function type
+  // "Args" is optional and if specified, will add all additional args to this the specified list
+  void handleMetalVulkanEntryFunction(CanQualType* FTy, FunctionArgList* ArgList, const FunctionDecl* FD);
+
+  // returns the amount of implicit internal args that are added for the specified FunctionDecl
+  uint32_t getMetalVulkanImplicitArgCount(const FunctionDecl* FD) const;
+
+  // returns true if the specified LLVM type is a flattened type
+  bool is_flattened_struct_type(llvm::Type* Ty) const {
+    return FlattenedCGRecordLayouts.count(Ty) > 0;
+  }
+
+private:
+  // helper function for get_aggregate_scalar_fields
+  void aggregate_scalar_fields_add_array(const CXXRecordDecl* root_decl,
+										 const CXXRecordDecl* parent_decl,
+                                         const ConstantArrayType* CAT,
+                                         const AttrVec* attrs,
+										 const FieldDecl* parent_field_decl,
+                                         const std::string& name,
+										 const bool expand_array_image,
+                                         std::vector<CodeGenTypes::aggregate_scalar_entry>& ret) const;
+
 
 public:  // These are internal details of CGT that shouldn't be used externally.
   /// ConvertRecordDeclType - Lay out a tagged decl type like struct or union.
   llvm::StructType *ConvertRecordDeclType(const RecordDecl *TD);
 
+  llvm::Type *ConvertArrayImageType(const Type* Ty);
+
   /// getExpandedTypes - Expand the type \arg Ty into the LLVM
   /// argument types it would be passed as. See ABIArgInfo::Expand.
   void getExpandedTypes(QualType Ty,
-                        SmallVectorImpl<llvm::Type *>::iterator &TI);
+                        SmallVectorImpl<llvm::Type *>::iterator &TI,
+                        const CallingConv CC);
 
   /// IsZeroInitializable - Return whether a type can be
   /// zero-initialized (in the C++ sense) with an LLVM zeroinitializer.
diff --git a/clang/lib/CodeGen/TargetInfo.cpp b/clang/lib/CodeGen/TargetInfo.cpp
index c76346337dc9..5ef4e9f1a731 100644
--- a/clang/lib/CodeGen/TargetInfo.cpp
+++ b/clang/lib/CodeGen/TargetInfo.cpp
@@ -77,10 +77,17 @@ static void AssignToArrayRange(CodeGen::CGBuilderTy &Builder,
   }
 }
 
-static bool isAggregateTypeForABI(QualType T) {
+bool TargetCodeGenInfo::TCGIisAggregateTypeForABI(QualType T) {
   return !CodeGenFunction::hasScalarEvaluationKind(T) ||
          T->isMemberFunctionPointerType();
 }
+static inline bool isAggregateTypeForABI(QualType T) {
+	return TargetCodeGenInfo::TCGIisAggregateTypeForABI(T);
+}
+
+static bool isAggregateImageType(QualType T) {
+  return CodeGenFunction::hasAggregateEvaluationKind(T) && T->isAggregateImageType();
+}
 
 ABIArgInfo ABIInfo::getNaturalAlignIndirect(QualType Ty, bool ByVal,
                                             bool Realign,
@@ -111,6 +118,22 @@ bool ABIInfo::isPromotableIntegerTypeForABI(QualType Ty) const {
   return false;
 }
 
+static ABIArgInfo classifyOpenCL(QualType Ty) {
+  if (Ty->isVoidType())
+    return ABIArgInfo::getIgnore();
+
+  if (const EnumType *EnumTy = Ty->getAs<EnumType>())
+    Ty = EnumTy->getDecl()->getIntegerType();
+
+  if (Ty->isRecordType())
+    return ABIArgInfo::getIndirect(CharUnits::Zero(), /*ByVal=*/false);
+
+  if (Ty->isPromotableIntegerType())
+    return ABIArgInfo::getExtend(Ty);
+
+  return ABIArgInfo::getDirect();
+}
+
 ABIInfo::~ABIInfo() {}
 
 /// Does the given lowering require more than the given number of
@@ -470,7 +493,7 @@ TargetCodeGenInfo::getDependentLibraryOption(llvm::StringRef Lib,
 unsigned TargetCodeGenInfo::getOpenCLKernelCallingConv() const {
   // OpenCL kernels are called via an explicit runtime API with arguments
   // set with clSetKernelArg(), not as normal sub-functions.
-  // Return SPIR_KERNEL by default as the kernel calling convention to
+  // Return FLOOR_KERNEL by default as the kernel calling convention to
   // ensure the fingerprint is fixed such way that each OpenCL argument
   // gets one matching argument in the produced kernel function argument
   // list to enable feasible implementation of clSetKernelArg() with
@@ -478,7 +501,7 @@ unsigned TargetCodeGenInfo::getOpenCLKernelCallingConv() const {
   // clSetKernelArg() might break depending on the target-specific
   // conventions; different targets might split structs passed as values
   // to multiple function arguments etc.
-  return llvm::CallingConv::SPIR_KERNEL;
+  return llvm::CallingConv::FLOOR_KERNEL;
 }
 
 llvm::Constant *TargetCodeGenInfo::getNullPointer(const CodeGen::CodeGenModule &CGM,
@@ -746,6 +769,9 @@ public:
 };
 
 ABIArgInfo DefaultABIInfo::classifyArgumentType(QualType Ty) const {
+  if (isAggregateImageType(Ty))
+    return ABIArgInfo::getExpand();
+
   Ty = useFirstFieldIfTransparentUnion(Ty);
 
   if (isAggregateTypeForABI(Ty)) {
@@ -1941,6 +1967,19 @@ ABIArgInfo X86_32ABIInfo::classifyArgumentType(QualType Ty,
 }
 
 void X86_32ABIInfo::computeInfo(CGFunctionInfo &FI) const {
+  QualType RetTy = FI.getReturnType();
+
+  if (getContext().getLangOpts().OpenCL) {
+    // Use OpenCL classify to prevent coercing
+    FI.getReturnInfo() = classifyOpenCL(RetTy);
+
+    for (CGFunctionInfo::arg_iterator it = FI.arg_begin(), ie = FI.arg_end();
+         it != ie; ++it)
+      it->info= classifyOpenCL(it->type);
+
+    return;
+  }
+
   CCState State(FI);
   if (IsMCUABI)
     State.FreeRegs = 3;
@@ -3920,6 +3959,18 @@ ABIArgInfo X86_64ABIInfo::classifyRegCallStructType(QualType Ty,
 }
 
 void X86_64ABIInfo::computeInfo(CGFunctionInfo &FI) const {
+  QualType RetTy = FI.getReturnType();
+
+  if (getContext().getLangOpts().OpenCL) {
+    // Use OpenCL classify to prevent coercing
+    FI.getReturnInfo() = classifyOpenCL(RetTy);
+
+    for (CGFunctionInfo::arg_iterator it = FI.arg_begin(), ie = FI.arg_end();
+         it != ie; ++it)
+      it->info= classifyOpenCL(it->type);
+
+    return;
+  }
 
   const unsigned CallingConv = FI.getCallingConvention();
   // It is possible to force Win64 calling convention on any x86_64 target by
@@ -4397,6 +4448,17 @@ void WinX86_64ABIInfo::computeInfo(CGFunctionInfo &FI) const {
     FI.getReturnInfo() = classify(FI.getReturnType(), FreeSSERegs, true,
                                   IsVectorCall, IsRegCall);
 
+  if (getContext().getLangOpts().OpenCL) {
+    // Use OpenCL classify to prevent coercing
+    FI.getReturnInfo() = classifyOpenCL(FI.getReturnType());
+
+    for (CGFunctionInfo::arg_iterator it = FI.arg_begin(), ie = FI.arg_end();
+         it != ie; ++it)
+      it->info= classifyOpenCL(it->type);
+
+    return;
+  }
+
   if (IsVectorCall) {
     // We can use up to 6 SSE register parameters with vectorcall.
     FreeSSERegs = 6;
@@ -7212,7 +7274,7 @@ void NVPTXTargetCodeGenInfo::setTargetAttributes(
   if (M.getLangOpts().OpenCL) {
     // Use OpenCL function attributes to check for kernel functions
     // By default, all functions are device functions
-    if (FD->hasAttr<OpenCLKernelAttr>()) {
+    if (FD->hasAttr<ComputeKernelAttr>()) {
       // OpenCL __kernel functions get kernel metadata
       // Create !{<func-ref>, metadata !"kernel", i32 1} node
       addNVVMMetadata(F, "kernel", 1);
@@ -7226,7 +7288,7 @@ void NVPTXTargetCodeGenInfo::setTargetAttributes(
     // CUDA __global__ functions get a kernel metadata entry.  Since
     // __global__ functions cannot be called from the device, we do not
     // need to set the noinline attribute.
-    if (FD->hasAttr<CUDAGlobalAttr>()) {
+    if (FD->hasAttr<ComputeKernelAttr>()) {
       // Create !{<func-ref>, metadata !"kernel", i32 1} node
       addNVVMMetadata(F, "kernel", 1);
     }
@@ -7247,7 +7309,11 @@ void NVPTXTargetCodeGenInfo::setTargetAttributes(
           // Create !{<func-ref>, metadata !"minctasm", i32 <val>} node
           addNVVMMetadata(F, "minctasm", MinBlocks.getExtValue());
       }
-    }
+    } else if (const ReqdWorkGroupSizeAttr *reg_local_size = FD->getAttr<ReqdWorkGroupSizeAttr>()) {
+      addNVVMMetadata(F, "reqntidx", reg_local_size->getXDim());
+      addNVVMMetadata(F, "reqntidy", reg_local_size->getYDim());
+      addNVVMMetadata(F, "reqntidz", reg_local_size->getZDim());
+	}
   }
 }
 
@@ -8313,7 +8379,7 @@ void TCETargetCodeGenInfo::setTargetAttributes(
   llvm::Function *F = cast<llvm::Function>(GV);
 
   if (M.getLangOpts().OpenCL) {
-    if (FD->hasAttr<OpenCLKernelAttr>()) {
+    if (FD->hasAttr<ComputeKernelAttr>()) {
       // OpenCL C Kernel functions are not subject to inlining
       F->addFnAttr(llvm::Attribute::NoInline);
       const ReqdWorkGroupSizeAttr *Attr = FD->getAttr<ReqdWorkGroupSizeAttr>();
@@ -9174,8 +9240,8 @@ static bool requiresAMDGPUProtectedVisibility(const Decl *D,
   if (GV->getVisibility() != llvm::GlobalValue::HiddenVisibility)
     return false;
 
-  return D->hasAttr<OpenCLKernelAttr>() ||
-         (isa<FunctionDecl>(D) && D->hasAttr<CUDAGlobalAttr>()) ||
+  return D->hasAttr<ComputeKernelAttr>() ||
+         (isa<FunctionDecl>(D) && D->hasAttr<ComputeKernelAttr>()) ||
          (isa<VarDecl>(D) &&
           (D->hasAttr<CUDADeviceAttr>() || D->hasAttr<CUDAConstantAttr>() ||
            cast<VarDecl>(D)->getType()->isCUDADeviceBuiltinSurfaceType() ||
@@ -9202,9 +9268,9 @@ void AMDGPUTargetCodeGenInfo::setTargetAttributes(
 
 
   const bool IsOpenCLKernel = M.getLangOpts().OpenCL &&
-                              FD->hasAttr<OpenCLKernelAttr>();
+                              FD->hasAttr<ComputeKernelAttr>();
   const bool IsHIPKernel = M.getLangOpts().HIP &&
-                           FD->hasAttr<CUDAGlobalAttr>();
+                           FD->hasAttr<ComputeKernelAttr>();
   if ((IsOpenCLKernel || IsHIPKernel) &&
       (M.getTriple().getOS() == llvm::Triple::AMDHSA))
     F->addFnAttr("amdgpu-implicitarg-num-bytes", "56");
@@ -9369,7 +9435,7 @@ bool AMDGPUTargetCodeGenInfo::shouldEmitStaticExternCAliases() const {
 void AMDGPUTargetCodeGenInfo::setCUDAKernelCallingConvention(
     const FunctionType *&FT) const {
   FT = getABIInfo().getContext().adjustFunctionType(
-      FT, FT->getExtInfo().withCallingConv(CC_OpenCLKernel));
+      FT, FT->getExtInfo().withCallingConv(CC_FloorKernel));
 }
 
 //===----------------------------------------------------------------------===//
@@ -10169,6 +10235,7 @@ void XCoreTargetCodeGenInfo::emitTargetMetadata(
     }
   }
 }
+
 //===----------------------------------------------------------------------===//
 // SPIR ABI Implementation
 //===----------------------------------------------------------------------===//
@@ -10199,7 +10266,7 @@ public:
 } // End anonymous namespace.
 void SPIRABIInfo::setCCs() {
   assert(getRuntimeCC() == llvm::CallingConv::C);
-  RuntimeCC = llvm::CallingConv::SPIR_FUNC;
+  RuntimeCC = llvm::CallingConv::FLOOR_FUNC;
 }
 
 namespace clang {
@@ -10212,9 +10279,186 @@ void computeSPIRKernelABIInfo(CodeGenModule &CGM, CGFunctionInfo &FI) {
 }
 
 unsigned SPIRTargetCodeGenInfo::getOpenCLKernelCallingConv() const {
-  return llvm::CallingConv::SPIR_KERNEL;
+  return llvm::CallingConv::FLOOR_KERNEL;
+}
+
+//===----------------------------------------------------------------------===//
+// Metal/AIR ABI Implementation
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+class AIRABIInfo : public ABIInfo {
+public:
+  AIRABIInfo(CodeGenTypes &CGT) : ABIInfo(CGT) {}
+
+  ABIArgInfo classifyReturnType(QualType RetTy) const;
+  ABIArgInfo classifyArgumentType(QualType Ty) const;
+
+  void computeInfo(CGFunctionInfo &FI) const override;
+  Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                    QualType Ty) const override;
+};
+
+class AIRTargetCodeGenInfo : public TargetCodeGenInfo {
+public:
+  AIRTargetCodeGenInfo(CodeGenTypes &CGT)
+    : TargetCodeGenInfo(std::make_unique<AIRABIInfo>(CGT)) {}
+};
+
+ABIArgInfo AIRABIInfo::classifyReturnType(QualType RetTy) const {
+  if (RetTy->isVoidType())
+    return ABIArgInfo::getIgnore();
+
+  // note: this is different from default ABI
+  if (!RetTy->isScalarType())
+    return ABIArgInfo::getDirect();
+
+  // Treat an enum type as its underlying type.
+  if (const EnumType *EnumTy = RetTy->getAs<EnumType>())
+    RetTy = EnumTy->getDecl()->getIntegerType();
+
+  return (RetTy->isPromotableIntegerType() ?
+          ABIArgInfo::getExtend(RetTy) : ABIArgInfo::getDirect());
+}
+
+ABIArgInfo AIRABIInfo::classifyArgumentType(QualType Ty) const {
+  // CGT.getTarget() // TODO: native array image test
+  // direct array of images (not writable)
+  if (Ty->isArrayImageType(true))
+    return getNaturalAlignIndirect(Ty);
+
+  if (CodeGenFunction::hasAggregateEvaluationKind(Ty) &&
+      Ty->isStructureOrClassType()) {
+    return ABIArgInfo::getExpand();
+  }
+
+  // Treat an enum type as its underlying type.
+  if (const EnumType *EnumTy = Ty->getAs<EnumType>())
+    Ty = EnumTy->getDecl()->getIntegerType();
+
+  return (Ty->isPromotableIntegerType() ?
+          ABIArgInfo::getExtend(Ty) : ABIArgInfo::getDirect());
+}
+
+void AIRABIInfo::computeInfo(CGFunctionInfo &FI) const {
+  // return type should never be indirect
+  // TODO: ... if the function is a kernel/vs/fs
+  FI.getReturnInfo() = classifyReturnType(FI.getReturnType());
+
+  for (auto &I : FI.arguments())
+    I.info = classifyArgumentType(I.type);
+
+  // Always honor user-specified calling convention.
+  if (FI.getCallingConvention() != llvm::CallingConv::C)
+    return;
+
+  FI.setEffectiveCallingConvention(getRuntimeCC());
 }
 
+Address AIRABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                              QualType Ty) const {
+  llvm_unreachable("AIR does not support varargs");
+}
+
+}
+
+//===----------------------------------------------------------------------===//
+// Vulkan/SPIR-V ABI Implementation
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+class VulkanABIInfo : public ABIInfo {
+public:
+  VulkanABIInfo(CodeGenTypes &CGT) : ABIInfo(CGT) {}
+
+  ABIArgInfo classifyReturnType(QualType RetTy, unsigned int CC) const;
+  ABIArgInfo classifyArgumentType(QualType Ty, unsigned int CC) const;
+
+  void computeInfo(CGFunctionInfo &FI) const override;
+  Address EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                    QualType Ty) const override;
+};
+
+class VulkanTargetCodeGenInfo : public TargetCodeGenInfo {
+public:
+  VulkanTargetCodeGenInfo(CodeGenTypes &CGT)
+    : TargetCodeGenInfo(std::make_unique<VulkanABIInfo>(CGT)) {}
+};
+
+ABIArgInfo VulkanABIInfo::classifyReturnType(QualType RetTy, unsigned int CC) const {
+  if (RetTy->isVoidType())
+    return ABIArgInfo::getIgnore();
+
+  // note: this is different from default ABI
+  if (!RetTy->isScalarType())
+    return ABIArgInfo::getDirect();
+
+  // Treat an enum type as its underlying type.
+  if (const EnumType *EnumTy = RetTy->getAs<EnumType>())
+    RetTy = EnumTy->getDecl()->getIntegerType();
+
+  return (RetTy->isPromotableIntegerType() ?
+          ABIArgInfo::getExtend(RetTy) : ABIArgInfo::getDirect());
+}
+
+ABIArgInfo VulkanABIInfo::classifyArgumentType(QualType Ty, unsigned int CC) const {
+  // direct array of images (not writable)
+  if (Ty->isArrayImageType(true))
+    return getNaturalAlignIndirect(Ty);
+
+  // all shader inputs must either be scalar or vector types, or arrays thereof
+  // -> expand all aggregates
+  if (CodeGenFunction::hasAggregateEvaluationKind(Ty) &&
+      Ty->isStructureOrClassType() &&
+      (CC == llvm::CallingConv::FLOOR_VERTEX ||
+       CC == llvm::CallingConv::FLOOR_FRAGMENT ||
+       CC == llvm::CallingConv::FLOOR_KERNEL)) {
+    return ABIArgInfo::getExpand();
+  }
+
+  if (isAggregateTypeForABI(Ty)) {
+    // Records with non-trivial destructors/copy-constructors should not be
+    // passed by value.
+    if (CGCXXABI::RecordArgABI RAA = getRecordArgABI(Ty, getCXXABI()))
+      return getNaturalAlignIndirect(Ty, RAA == CGCXXABI::RAA_DirectInMemory);
+
+    return getNaturalAlignIndirect(Ty);
+  }
+
+  // Treat an enum type as its underlying type.
+  if (const EnumType *EnumTy = Ty->getAs<EnumType>())
+    Ty = EnumTy->getDecl()->getIntegerType();
+
+  return (Ty->isPromotableIntegerType() ?
+          ABIArgInfo::getExtend(Ty) : ABIArgInfo::getDirect());
+}
+
+void VulkanABIInfo::computeInfo(CGFunctionInfo &FI) const {
+  FI.getReturnInfo() = classifyReturnType(FI.getReturnType(), FI.getCallingConvention());
+
+  for (auto &I : FI.arguments())
+    I.info = classifyArgumentType(I.type, FI.getCallingConvention());
+
+  // Always honor user-specified calling convention.
+  if (FI.getCallingConvention() != llvm::CallingConv::C)
+    return;
+
+  FI.setEffectiveCallingConvention(getRuntimeCC());
+}
+
+Address VulkanABIInfo::EmitVAArg(CodeGenFunction &CGF, Address VAListAddr,
+                                 QualType Ty) const {
+  llvm_unreachable("Vulkan/SPIR-V does not support varargs");
+}
+
+}
+
+//===----------------------------------------------------------------------===//
+// END OF ABI Implementation
+//===----------------------------------------------------------------------===//
+
 static bool appendType(SmallStringEnc &Enc, QualType QType,
                        const CodeGen::CodeGenModule &CGM,
                        TypeStringCache &TSC);
@@ -11280,9 +11524,14 @@ const TargetCodeGenInfo &CodeGenModule::getTargetCodeGenInfo() {
     return SetCGInfo(new ARCTargetCodeGenInfo(Types));
   case llvm::Triple::spir:
   case llvm::Triple::spir64:
+    if(Triple.getEnvironment() == llvm::Triple::EnvironmentType::Vulkan) {
+      return SetCGInfo(new VulkanTargetCodeGenInfo(Types));
+    }
     return SetCGInfo(new SPIRTargetCodeGenInfo(Types));
   case llvm::Triple::ve:
     return SetCGInfo(new VETargetCodeGenInfo(Types));
+  case llvm::Triple::air64:
+    return SetCGInfo(new AIRTargetCodeGenInfo(Types));
   }
 }
 
diff --git a/clang/lib/CodeGen/TargetInfo.h b/clang/lib/CodeGen/TargetInfo.h
index aa8bbb60a75f..5a5ed587e2a7 100644
--- a/clang/lib/CodeGen/TargetInfo.h
+++ b/clang/lib/CodeGen/TargetInfo.h
@@ -364,6 +364,8 @@ public:
     // DO NOTHING by default.
     return false;
   }
+
+  static bool TCGIisAggregateTypeForABI(QualType T);
 };
 
 } // namespace CodeGen
diff --git a/clang/lib/Driver/Driver.cpp b/clang/lib/Driver/Driver.cpp
index 2ddb753660e4..b5f5b165366c 100644
--- a/clang/lib/Driver/Driver.cpp
+++ b/clang/lib/Driver/Driver.cpp
@@ -2553,6 +2553,7 @@ class OffloadingActionBuilder final {
         // If the host input is not CUDA or HIP, we don't need to bother about
         // this input.
         if (!(IA->getType() == types::TY_CUDA ||
+              IA->getType() == types::TY_CUDAHeader ||
               IA->getType() == types::TY_HIP ||
               IA->getType() == types::TY_PP_HIP)) {
           // The builder will ignore this input.
@@ -2567,8 +2568,11 @@ class OffloadingActionBuilder final {
           return ABRT_Success;
 
         // Replicate inputs for each GPU architecture.
-        auto Ty = IA->getType() == types::TY_HIP ? types::TY_HIP_DEVICE
-                                                 : types::TY_CUDA_DEVICE;
+        auto Ty = types::TY_CUDA_DEVICE;
+        if (IA->getType() == types::TY_HIP)
+          Ty = types::TY_HIP_DEVICE;
+        else if (IA->getType() == types::TY_CUDAHeader)
+          Ty = types::TY_CUDAHeader;
         std::string CUID = FixedCUID.str();
         if (CUID.empty()) {
           if (UseCUID == CUID_Random)
@@ -3259,7 +3263,7 @@ class OffloadingActionBuilder final {
             C.MakeAction<LinkJobAction>(LI, types::TY_Image);
         OffloadAction::DeviceDependences DeviceLinkDeps;
         DeviceLinkDeps.add(*DeviceLinkAction, **TC, /*BoundArch=*/nullptr,
-		        Action::OFK_OpenMP);
+                Action::OFK_OpenMP);
         AL.push_back(C.MakeAction<OffloadAction>(DeviceLinkDeps,
             DeviceLinkAction->getType()));
         ++TC;
@@ -4000,6 +4004,16 @@ Action *Driver::ConstructPhaseAction(
       return C.MakeAction<CompileJobAction>(Input, types::TY_ModuleFile);
     if (Args.hasArg(options::OPT_verify_pch))
       return C.MakeAction<VerifyPCHJobAction>(Input, types::TY_Nothing);
+    if (Args.hasArg(options::OPT_emit_llvm_bc_32))
+      return C.MakeAction<CompileJobAction>(Input, types::TY_LLVM_BC_32);
+    if (Args.hasArg(options::OPT_emit_llvm_bc_50))
+      return C.MakeAction<CompileJobAction>(Input, types::TY_LLVM_BC_50);
+    if (Args.hasArg(options::OPT_emit_spirv))
+      return C.MakeAction<CompileJobAction>(Input, types::TY_SPIRV);
+    if (Args.hasArg(options::OPT_emit_spirv_container))
+      return C.MakeAction<CompileJobAction>(Input, types::TY_SPIRVC);
+    if (Args.hasArg(options::OPT_emit_metallib))
+      return C.MakeAction<CompileJobAction>(Input, types::TY_METALLIB);
     return C.MakeAction<CompileJobAction>(Input, types::TY_LLVM_BC);
   }
   case phases::Backend: {
@@ -4012,8 +4026,16 @@ Action *Driver::ConstructPhaseAction(
         (TargetDeviceOffloadKind == Action::OFK_HIP &&
          Args.hasFlag(options::OPT_fgpu_rdc, options::OPT_fno_gpu_rdc,
                       false))) {
-      types::ID Output =
-          Args.hasArg(options::OPT_S) ? types::TY_LLVM_IR : types::TY_LLVM_BC;
+      types::ID Output = types::TY_LLVM_BC;
+      if (Args.hasArg(options::OPT_S)) {
+        Output = types::TY_LLVM_IR;
+      } else {
+        if (Args.hasArg(options::OPT_emit_llvm_bc_32)) Output = types::TY_LLVM_BC_32;
+        if (Args.hasArg(options::OPT_emit_llvm_bc_50)) Output = types::TY_LLVM_BC_50;
+        if (Args.hasArg(options::OPT_emit_spirv)) Output = types::TY_SPIRV;
+        if (Args.hasArg(options::OPT_emit_spirv_container)) Output = types::TY_SPIRVC;
+        if (Args.hasArg(options::OPT_emit_metallib)) Output = types::TY_METALLIB;
+      }
       return C.MakeAction<BackendJobAction>(Input, Output);
     }
     return C.MakeAction<BackendJobAction>(Input, types::TY_PP_Asm);
@@ -5073,10 +5095,17 @@ const char *Driver::GetNamedOutputPath(Compilation &C, const JobAction &JA,
              Args.hasFlag(options::OPT_fgpu_rdc, options::OPT_fno_gpu_rdc,
                           false);
     };
-    if (!AtTopLevel && JA.getType() == types::TY_LLVM_BC &&
+    if (!AtTopLevel &&
+        (JA.getType() == types::TY_LLVM_BC ||
+         JA.getType() == types::TY_LLVM_BC_32 ||
+         JA.getType() == types::TY_LLVM_BC_50 ||
+         JA.getType() == types::TY_SPIRV ||
+         JA.getType() == types::TY_SPIRVC ||
+         JA.getType() == types::TY_METALLIB) &&
         (C.getArgs().hasArg(options::OPT_emit_llvm) ||
-         IsHIPRDCInCompilePhase(JA, C.getArgs())))
+         IsHIPRDCInCompilePhase(JA, C.getArgs()))) {
       Suffixed += ".tmp";
+    }
     Suffixed += '.';
     Suffixed += Suffix;
     NamedOutput = C.getArgs().MakeArgString(Suffixed.c_str());
diff --git a/clang/lib/Driver/ToolChains/Clang.cpp b/clang/lib/Driver/ToolChains/Clang.cpp
index 9856a42e1db5..4e9162683ef0 100644
--- a/clang/lib/Driver/ToolChains/Clang.cpp
+++ b/clang/lib/Driver/ToolChains/Clang.cpp
@@ -4495,6 +4495,18 @@ void Clang::ConstructJob(Compilation &C, const JobAction &JA,
       } else {
         CmdArgs.push_back("-emit-llvm-bc");
       }
+    } else if (JA.getType() == types::TY_LLVM_BC_32) {
+      CmdArgs.push_back("-emit-llvm-bc"); // order matters
+      CmdArgs.push_back("-llvm-bc-32");
+    } else if (JA.getType() == types::TY_LLVM_BC_50) {
+      CmdArgs.push_back("-emit-llvm-bc"); // order matters
+      CmdArgs.push_back("-llvm-bc-50");
+    } else if (JA.getType() == types::TY_SPIRV) {
+      CmdArgs.push_back("-emit-spirv");
+    } else if (JA.getType() == types::TY_SPIRVC) {
+      CmdArgs.push_back("-emit-spirv-container");
+    } else if (JA.getType() == types::TY_METALLIB) {
+      CmdArgs.push_back("-emit-metallib");
     } else if (JA.getType() == types::TY_IFS ||
                JA.getType() == types::TY_IFS_CPP) {
       StringRef ArgStr =
@@ -4524,7 +4536,7 @@ void Clang::ConstructJob(Compilation &C, const JobAction &JA,
     // loading the bitcode up in 'opt' or 'llc' and running passes gives the
     // same result as running passes here.  For LTO, we don't need to preserve
     // the use-list order, since serialization to bitcode is part of the flow.
-    if (JA.getType() == types::TY_LLVM_BC)
+    if (JA.getType() == types::TY_LLVM_BC) // NOTE: don't do this for 3.2
       CmdArgs.push_back("-emit-llvm-uselists");
 
     if (IsUsingLTO) {
diff --git a/clang/lib/Driver/ToolChains/Cuda.cpp b/clang/lib/Driver/ToolChains/Cuda.cpp
index 09418aa2390f..6668b9fd8c85 100644
--- a/clang/lib/Driver/ToolChains/Cuda.cpp
+++ b/clang/lib/Driver/ToolChains/Cuda.cpp
@@ -65,6 +65,8 @@ CudaVersion getCudaVersion(uint32_t raw_version) {
     return CudaVersion::CUDA_113;
   if (raw_version < 11050)
     return CudaVersion::CUDA_114;
+  if (raw_version < 11060)
+    return CudaVersion::CUDA_115;
   return CudaVersion::NEW;
 }
 
@@ -701,6 +703,7 @@ void CudaToolChain::addClangTargetOptions(
   // New CUDA versions often introduce new instructions that are only supported
   // by new PTX version, so we need to raise PTX level to enable them in NVPTX
   // back-end.
+#if 0 // we don't want this - the actual PTX version will be set elsewhere
   const char *PtxFeature = nullptr;
   switch (CudaInstallationVersion) {
 #define CASE_CUDA_VERSION(CUDA_VER, PTX_VER)                                   \
@@ -722,6 +725,10 @@ void CudaToolChain::addClangTargetOptions(
   default:
     PtxFeature = "+ptx42";
   }
+#else
+  // default to the min support libfloor PTX version (CUDA 9.0+)
+  const char *PtxFeature = "+ptx60";
+#endif
   CC1Args.append({"-target-feature", PtxFeature});
   if (DriverArgs.hasFlag(options::OPT_fcuda_short_ptr,
                          options::OPT_fno_cuda_short_ptr, false))
diff --git a/clang/lib/Driver/ToolChains/Darwin.cpp b/clang/lib/Driver/ToolChains/Darwin.cpp
index bf1425fe65ef..6c9e921aeed8 100644
--- a/clang/lib/Driver/ToolChains/Darwin.cpp
+++ b/clang/lib/Driver/ToolChains/Darwin.cpp
@@ -71,6 +71,8 @@ llvm::Triple::ArchType darwin::getArchTypeForMachOArchName(StringRef Str) {
       .Case("nvptx64", llvm::Triple::nvptx64)
       .Case("amdil", llvm::Triple::amdil)
       .Case("spir", llvm::Triple::spir)
+      .Case("spir64", llvm::Triple::spir64)
+      .Case("air64", llvm::Triple::air64)
       .Default(llvm::Triple::UnknownArch);
 }
 
@@ -882,6 +884,8 @@ ToolChain::CXXStdlibType Darwin::GetDefaultCXXStdlibType() const {
 
 /// Darwin provides an ARC runtime starting in MacOS X 10.7 and iOS 5.0.
 ObjCRuntime Darwin::getDefaultObjCRuntime(bool isNonFragile) const {
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64)
+    return ObjCRuntime(ObjCRuntime::FragileMacOSX, TargetVersion);
   if (isTargetWatchOSBased())
     return ObjCRuntime(ObjCRuntime::WatchOS, TargetVersion);
   if (isTargetIOSBased())
@@ -893,6 +897,8 @@ ObjCRuntime Darwin::getDefaultObjCRuntime(bool isNonFragile) const {
 
 /// Darwin provides a blocks runtime starting in MacOS X 10.6 and iOS 3.2.
 bool Darwin::hasBlocksRuntime() const {
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64)
+    return false;
   if (isTargetWatchOSBased())
     return true;
   else if (isTargetIOSBased())
@@ -1043,6 +1049,10 @@ DarwinClang::DarwinClang(const Driver &D, const llvm::Triple &Triple,
     : Darwin(D, Triple, Args) {}
 
 void DarwinClang::addClangWarningOptions(ArgStringList &CC1Args) const {
+  // nothing of interest in here for AIR/Metal
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64)
+    return;
+
   // Always error about undefined 'TARGET_OS_*' macros.
   CC1Args.push_back("-Wundef-prefix=TARGET_OS_");
   CC1Args.push_back("-Werror=undef-prefix");
@@ -1133,6 +1143,9 @@ void DarwinClang::AddLinkARCArgs(const ArgList &Args,
 }
 
 unsigned DarwinClang::GetDefaultDwarfVersion() const {
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64) {
+    return 4; // AIR is always Dwarf 4 (for Metal 2.x)
+  }
   // Default to use DWARF 2 on OS X 10.10 / iOS 8 and lower.
   if ((isTargetMacOSBased() && isMacosxVersionLT(10, 11)) ||
       (isTargetIOSBased() && isIPhoneOSVersionLT(9)))
@@ -2609,6 +2622,8 @@ void MachO::AddLinkRuntimeLibArgs(const ArgList &Args,
 }
 
 bool Darwin::isAlignedAllocationUnavailable() const {
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64) return false;
+
   llvm::Triple::OSType OS;
 
   if (isTargetMacCatalyst())
@@ -3003,6 +3018,9 @@ SanitizerMask Darwin::getSupportedSanitizers() const {
   const bool IsX86_64 = getTriple().getArch() == llvm::Triple::x86_64;
   const bool IsAArch64 = getTriple().getArch() == llvm::Triple::aarch64;
   SanitizerMask Res = ToolChain::getSupportedSanitizers();
+  // no additional ones
+  if (getTriple().getArch() == llvm::Triple::ArchType::air64)
+    return Res;
   Res |= SanitizerKind::Address;
   Res |= SanitizerKind::PointerCompare;
   Res |= SanitizerKind::PointerSubtract;
diff --git a/clang/lib/Driver/ToolChains/Darwin.h b/clang/lib/Driver/ToolChains/Darwin.h
index e46ff52a23a9..1d0bef3c63b3 100644
--- a/clang/lib/Driver/ToolChains/Darwin.h
+++ b/clang/lib/Driver/ToolChains/Darwin.h
@@ -525,6 +525,10 @@ public:
 
   LangOptions::StackProtectorMode
   GetDefaultStackProtectorLevel(bool KernelOrKext) const override {
+    // not supported on AIR/Metal
+    if (getTriple().getArch() == llvm::Triple::ArchType::air64)
+      return LangOptions::SSPOff;
+
     // Stack protectors default to on for user code on 10.5,
     // and for everything in 10.6 and beyond
     if (isTargetIOSBased() || isTargetWatchOSBased())
diff --git a/clang/lib/Driver/ToolChains/Gnu.cpp b/clang/lib/Driver/ToolChains/Gnu.cpp
index 5436f008ed4d..2cdd2fa4c02d 100644
--- a/clang/lib/Driver/ToolChains/Gnu.cpp
+++ b/clang/lib/Driver/ToolChains/Gnu.cpp
@@ -192,6 +192,9 @@ void tools::gcc::Compiler::RenderExtraToolArgs(const JobAction &JA,
   case types::TY_LTO_IR:
   case types::TY_LLVM_BC:
   case types::TY_LTO_BC:
+  case types::TY_LLVM_BC_32:
+  case types::TY_LLVM_BC_50:
+  case types::TY_METALLIB:
     CmdArgs.push_back("-c");
     break;
   // We assume we've got an "integrated" assembler in that gcc will produce an
diff --git a/clang/lib/Driver/Types.cpp b/clang/lib/Driver/Types.cpp
index 1bd187ad2fc0..61a316032d08 100644
--- a/clang/lib/Driver/Types.cpp
+++ b/clang/lib/Driver/Types.cpp
@@ -137,12 +137,17 @@ bool types::isAcceptedByClang(ID Id) {
   case TY_ObjCXX: case TY_PP_ObjCXX: case TY_PP_ObjCXX_Alias:
   case TY_CHeader: case TY_PP_CHeader:
   case TY_CLHeader:
+  case TY_CUDAHeader: case TY_PP_CUDAHeader:
+  case TY_MetalHeader: case TY_VulkanHeader:
   case TY_ObjCHeader: case TY_PP_ObjCHeader:
   case TY_CXXHeader: case TY_PP_CXXHeader:
   case TY_ObjCXXHeader: case TY_PP_ObjCXXHeader:
   case TY_CXXModule: case TY_PP_CXXModule:
   case TY_AST: case TY_ModuleFile: case TY_PCH:
   case TY_LLVM_IR: case TY_LLVM_BC:
+  case TY_LLVM_BC_32: case TY_LLVM_BC_50:
+  case TY_SPIRV: case TY_SPIRVC:
+  case TY_METALLIB:
     return true;
   }
 }
@@ -212,6 +217,7 @@ bool types::isCXX(ID Id) {
   case TY_ObjCXXHeader: case TY_PP_ObjCXXHeader:
   case TY_CXXModule: case TY_PP_CXXModule:
   case TY_CUDA: case TY_PP_CUDA: case TY_CUDA_DEVICE:
+  case TY_CUDAHeader: case TY_PP_CUDAHeader:
   case TY_HIP:
   case TY_PP_HIP:
   case TY_HIP_DEVICE:
@@ -226,6 +232,8 @@ bool types::isLLVMIR(ID Id) {
 
   case TY_LLVM_IR:
   case TY_LLVM_BC:
+  case TY_LLVM_BC_32:
+  case TY_LLVM_BC_50:
   case TY_LTO_IR:
   case TY_LTO_BC:
     return true;
@@ -240,6 +248,8 @@ bool types::isCuda(ID Id) {
   case TY_CUDA:
   case TY_PP_CUDA:
   case TY_CUDA_DEVICE:
+  case TY_CUDAHeader:
+  case TY_PP_CUDAHeader:
     return true;
   }
 }
@@ -285,6 +295,11 @@ types::ID types::lookupTypeForExtension(llvm::StringRef Ext) {
            .Case("S", TY_Asm)
            .Case("s", TY_PP_Asm)
            .Case("bc", TY_LLVM_BC)
+           .Case("bc32", TY_LLVM_BC_32) // not ideal
+           .Case("bc50", TY_LLVM_BC_50) // not ideal
+           .Case("spv", TY_SPIRV)
+           .Case("spvc", TY_SPIRVC)
+           .Case("metallib", TY_METALLIB)
            .Case("cc", TY_CXX)
            .Case("CC", TY_CXX)
            .Case("cl", TY_CL)
diff --git a/clang/lib/Edit/RewriteObjCFoundationAPI.cpp b/clang/lib/Edit/RewriteObjCFoundationAPI.cpp
index a3d388a5ae44..990677a888ea 100644
--- a/clang/lib/Edit/RewriteObjCFoundationAPI.cpp
+++ b/clang/lib/Edit/RewriteObjCFoundationAPI.cpp
@@ -1079,6 +1079,8 @@ static bool rewriteToNumericBoxedExpression(const ObjCMessageExpr *Msg,
     case CK_CopyAndAutoreleaseBlockObject:
     case CK_BuiltinFnToFnPtr:
     case CK_ZeroToOCLOpaqueType:
+    case CK_ZeroToOCLEvent:
+    case CK_ZeroToOCLQueue:
     case CK_IntToOCLSampler:
     case CK_MatrixCast:
       return false;
diff --git a/clang/lib/Frontend/CompilerInstance.cpp b/clang/lib/Frontend/CompilerInstance.cpp
index a9b9e6516d56..729298a6e924 100644
--- a/clang/lib/Frontend/CompilerInstance.cpp
+++ b/clang/lib/Frontend/CompilerInstance.cpp
@@ -1109,6 +1109,10 @@ void CompilerInstance::LoadRequestedPlugins() {
 /// Determine the appropriate source input kind based on language
 /// options.
 static Language getLanguageFromOptions(const LangOptions &LangOpts) {
+  if (LangOpts.Metal)
+    return Language::Metal;
+  if (LangOpts.Vulkan)
+    return Language::Vulkan;
   if (LangOpts.OpenCL)
     return Language::OpenCL;
   if (LangOpts.CUDA)
diff --git a/clang/lib/Frontend/CompilerInvocation.cpp b/clang/lib/Frontend/CompilerInvocation.cpp
index 8d33a59cb983..46ee7c6a756f 100644
--- a/clang/lib/Frontend/CompilerInvocation.cpp
+++ b/clang/lib/Frontend/CompilerInvocation.cpp
@@ -96,6 +96,7 @@
 #include <cstring>
 #include <memory>
 #include <string>
+#include <fstream>
 #include <tuple>
 #include <type_traits>
 #include <utility>
@@ -542,8 +543,10 @@ static unsigned getOptimizationLevel(ArgList &Args, InputKind IK,
                                      DiagnosticsEngine &Diags) {
   unsigned DefaultOpt = llvm::CodeGenOpt::None;
   if ((IK.getLanguage() == Language::OpenCL ||
-       IK.getLanguage() == Language::OpenCLCXX) &&
-      !Args.hasArg(OPT_cl_opt_disable))
+       IK.getLanguage() == Language::OpenCLCXX ||
+       IK.getLanguage() == Language::Metal ||
+       IK.getLanguage() == Language::Vulkan) &&
+      !Args.hasArg(OPT_cl_opt_disable) && !Args.hasArg(OPT_emit_spirv))
     DefaultOpt = llvm::CodeGenOpt::Default;
 
   if (Arg *A = Args.getLastArg(options::OPT_O_Group)) {
@@ -1832,6 +1835,24 @@ bool CompilerInvocation::ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args,
     Opts.LinkBitcodeFiles.push_back(F);
   }
 
+  Opts.EmitOpenCLArgMetadata = (Args.hasArg(OPT_cl_kernel_arg_info) ||
+                                Args.hasArg(OPT_emit_spirv) ||
+                                Args.hasArg(OPT_emit_spirv_container));
+  Opts.MetalIntelWorkarounds = Args.hasArg(OPT_metal_intel_workarounds);
+  Opts.MetalNvidiaWorkarounds = Args.hasArg(OPT_metal_nvidia_workarounds);
+  Opts.MetalNoArrayImage = Args.hasArg(OPT_metal_no_array_image);
+  Opts.MetalSoftPrintf = Args.hasArg(OPT_metal_soft_printf);
+  Opts.SPIRIntelWorkarounds = Args.hasArg(OPT_cl_spir_intel_workarounds);
+  Opts.VulkanIUBSize = uint32_t(std::min(uint64_t(~0u),
+      getLastArgUInt64Value(Args, OPT_vulkan_iub_size_EQ, 256)));
+  Opts.VulkanIUBCount = uint32_t(std::min(uint64_t(~0u),
+      getLastArgUInt64Value(Args, OPT_vulkan_iub_count_EQ, 4)));
+  Opts.VulkanLLVMPreStructurizationPass = Args.hasArg(OPT_vulkan_llvm_pre_structurization_pass);
+  Opts.VulkanSoftPrintf = Args.hasArg(OPT_vulkan_soft_printf);
+  Opts.SPIRCompileOptions = Args.getLastArgValue(OPT_cl_spir_compile_options).trim("\t\n\v\f\r\" ");
+  Opts.GraphicsPrimitiveID = Args.hasArg(OPT_graphics_primitive_id);
+  Opts.GraphicsBarycentricCoord = Args.hasArg(OPT_graphics_barycentric_coord);
+
   if (Args.getLastArg(OPT_femulated_tls) ||
       Args.getLastArg(OPT_fno_emulated_tls)) {
     Opts.ExplicitEmulatedTLS = true;
@@ -2403,6 +2424,11 @@ static const auto &getFrontendActionTable() {
       {frontend::DumpTokens, OPT_dump_tokens},
       {frontend::EmitAssembly, OPT_S},
       {frontend::EmitBC, OPT_emit_llvm_bc},
+      {frontend::EmitBC32, OPT_emit_llvm_bc_32},
+      {frontend::EmitBC50, OPT_emit_llvm_bc_50},
+      {frontend::EmitSPIRV, OPT_emit_spirv},
+      {frontend::EmitSPIRVContainer, OPT_emit_spirv_container},
+      {frontend::EmitMetalLib, OPT_emit_metallib},
       {frontend::EmitHTML, OPT_emit_html},
       {frontend::EmitLLVM, OPT_emit_llvm},
       {frontend::EmitLLVMOnly, OPT_emit_llvm_only},
@@ -2572,6 +2598,12 @@ static void GenerateFrontendArgs(const FrontendOptions &Opts,
     case Language::OpenCL:
       Lang = "cl";
       break;
+    case Language::Metal:
+      Lang = "metal";
+      break;
+    case Language::Vulkan:
+      Lang = "vulkan";
+      break;
     case Language::OpenCLCXX:
       Lang = "clcpp";
       break;
@@ -2763,8 +2795,14 @@ static bool ParseFrontendArgs(FrontendOptions &Opts, ArgList &Args,
     DashX = llvm::StringSwitch<InputKind>(XValue)
                 .Case("c", Language::C)
                 .Case("cl", Language::OpenCL)
+                .Case("cl-header", Language::OpenCL)
+                .Case("metal", Language::Metal)
+                .Case("metal-header", Language::Metal)
+                .Case("vulkan", Language::Vulkan)
+                .Case("vulkan-header", Language::Vulkan)
                 .Case("clcpp", Language::OpenCLCXX)
                 .Case("cuda", Language::CUDA)
+                .Case("cuda-header", Language::CUDA)
                 .Case("hip", Language::HIP)
                 .Case("c++", Language::CXX)
                 .Case("objective-c", Language::ObjC)
@@ -3132,6 +3170,12 @@ void CompilerInvocation::setLangDefaults(LangOptions &Opts, InputKind IK,
     case Language::OpenCLCXX:
       LangStd = LangStandard::lang_openclcpp10;
       break;
+    case Language::Metal:
+      LangStd = LangStandard::lang_metal20;
+      break;
+    case Language::Vulkan:
+      LangStd = LangStandard::lang_vulkan12;
+      break;
     case Language::CUDA:
       LangStd = LangStandard::lang_cuda;
       break;
@@ -3199,6 +3243,10 @@ void CompilerInvocation::setLangDefaults(LangOptions &Opts, InputKind IK,
     Opts.OpenCLVersion = 120;
   else if (LangStd == LangStandard::lang_opencl20)
     Opts.OpenCLVersion = 200;
+  else if (LangStd == LangStandard::lang_opencl21)
+    Opts.OpenCLVersion = 210;
+  else if (LangStd == LangStandard::lang_opencl22)
+    Opts.OpenCLVersion = 220;
   else if (LangStd == LangStandard::lang_opencl30)
     Opts.OpenCLVersion = 300;
   else if (LangStd == LangStandard::lang_openclcpp10)
@@ -3206,15 +3254,50 @@ void CompilerInvocation::setLangDefaults(LangOptions &Opts, InputKind IK,
   else if (LangStd == LangStandard::lang_openclcpp2021)
     Opts.OpenCLCPlusPlusVersion = 202100;
 
+  // as Metal is largely compiled as OpenCL, also enable + init opencl
+  if (LangStd == LangStandard::lang_metal20 ||
+      LangStd == LangStandard::lang_metal21 ||
+      LangStd == LangStandard::lang_metal22 ||
+      LangStd == LangStandard::lang_metal23 ||
+      LangStd == LangStandard::lang_metal24 ||
+      IK.getLanguage() == Language::Metal) {
+    Opts.Metal = 1;
+    Opts.OpenCL = 1;
+    Opts.OpenCLVersion = 120;
+
+    if (LangStd == LangStandard::lang_metal20)
+      Opts.MetalVersion = 200;
+    else if (LangStd == LangStandard::lang_metal21)
+      Opts.MetalVersion = 210;
+    else if (LangStd == LangStandard::lang_metal22)
+      Opts.MetalVersion = 220;
+    else if (LangStd == LangStandard::lang_metal23)
+      Opts.MetalVersion = 230;
+    else if (LangStd == LangStandard::lang_metal24)
+      Opts.MetalVersion = 240;
+  }
+
+  // as Vulkan is largely compiled as OpenCL, also enable + init opencl
+  if (LangStd == LangStandard::lang_vulkan12 ||
+      IK.getLanguage() == Language::Vulkan) {
+    Opts.Vulkan = 1;
+    Opts.OpenCL = 1;
+    Opts.OpenCLVersion = 200;
+
+    if (LangStd == LangStandard::lang_vulkan12)
+      Opts.VulkanVersion = 120;
+  }
+
   // OpenCL has some additional defaults.
   if (Opts.OpenCL) {
     Opts.AltiVec = 0;
     Opts.ZVector = 0;
     Opts.setDefaultFPContractMode(LangOptions::FPM_On);
-    Opts.OpenCLCPlusPlus = Opts.CPlusPlus;
-    Opts.OpenCLPipes = Opts.getOpenCLCompatibleVersion() == 200;
-    Opts.OpenCLGenericAddressSpace = Opts.getOpenCLCompatibleVersion() == 200;
+    Opts.OpenCLCPlusPlus = 0; // no, just no ...
+    Opts.OpenCLPipes = Opts.getOpenCLCompatibleVersion() >= 200;
+    Opts.OpenCLGenericAddressSpace = Opts.getOpenCLCompatibleVersion() >= 200;
 
+#if 0 // don't do that
     // Include default header file for OpenCL.
     if (Opts.IncludeDefaultHeader) {
       if (Opts.DeclareOpenCLBuiltins) {
@@ -3224,6 +3307,7 @@ void CompilerInvocation::setLangDefaults(LangOptions &Opts, InputKind IK,
         Includes.push_back("opencl-c.h");
       }
     }
+#endif
   }
 
   Opts.HIP = IK.getLanguage() == Language::HIP;
@@ -3243,13 +3327,15 @@ void CompilerInvocation::setLangDefaults(LangOptions &Opts, InputKind IK,
     Opts.setDefaultFPContractMode(LangOptions::FPM_Fast);
   }
 
+  Opts.FloorHostCompute = (T.getEnvironment() == llvm::Triple::EnvironmentType::FloorHostCompute);
+
   Opts.RenderScript = IK.getLanguage() == Language::RenderScript;
 
   // OpenCL and C++ both have bool, true, false keywords.
   Opts.Bool = Opts.OpenCL || Opts.CPlusPlus;
 
-  // OpenCL has half keyword
-  Opts.Half = Opts.OpenCL;
+  // OpenCL/Vulkan/Metal, CUDA and floor Host-Compute have the half keyword
+  Opts.Half = Opts.OpenCL || Opts.CUDA || Opts.FloorHostCompute;
 }
 
 /// Check if input file kind and language standard are compatible.
@@ -3269,6 +3355,12 @@ static bool IsInputCompatibleWithStandard(InputKind IK,
     return S.getLanguage() == Language::OpenCL ||
            S.getLanguage() == Language::OpenCLCXX;
 
+  case Language::Metal:
+    return S.getLanguage() == Language::Metal;
+
+  case Language::Vulkan:
+    return S.getLanguage() == Language::Vulkan;
+
   case Language::OpenCLCXX:
     return S.getLanguage() == Language::OpenCLCXX;
 
@@ -3307,6 +3399,10 @@ static const StringRef GetInputKindName(InputKind IK) {
     return "Objective-C++";
   case Language::OpenCL:
     return "OpenCL";
+  case Language::Metal:
+    return "Metal";
+  case Language::Vulkan:
+    return "Vulkan";
   case Language::OpenCLCXX:
     return "C++ for OpenCL";
   case Language::CUDA:
@@ -3351,6 +3447,8 @@ void CompilerInvocation::GenerateLangArgs(const LangOptions &Opts,
   case LangStandard::lang_opencl11:
   case LangStandard::lang_opencl12:
   case LangStandard::lang_opencl20:
+  case LangStandard::lang_opencl21:
+  case LangStandard::lang_opencl22:
   case LangStandard::lang_opencl30:
   case LangStandard::lang_openclcpp10:
   case LangStandard::lang_openclcpp2021:
@@ -3438,7 +3536,7 @@ void CompilerInvocation::GenerateLangArgs(const LangOptions &Opts,
       GenerateArg(Args, OPT_ftrigraphs, SA);
   }
 
-  if (Opts.Blocks && !(Opts.OpenCL && Opts.OpenCLVersion == 200))
+  if (Opts.Blocks && !(Opts.OpenCL && Opts.OpenCLVersion >= 200))
     GenerateArg(Args, OPT_fblocks, SA);
 
   if (Opts.ConvergentFunctions &&
@@ -3662,6 +3760,8 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
         .Cases("cl1.1", "CL1.1", LangStandard::lang_opencl11)
         .Cases("cl1.2", "CL1.2", LangStandard::lang_opencl12)
         .Cases("cl2.0", "CL2.0", LangStandard::lang_opencl20)
+        .Cases("cl2.1", "CL2.1", LangStandard::lang_opencl21)
+        .Cases("cl2.2", "CL2.2", LangStandard::lang_opencl22)
         .Cases("cl3.0", "CL3.0", LangStandard::lang_opencl30)
         .Cases("clc++", "CLC++", LangStandard::lang_openclcpp10)
         .Cases("clc++1.0", "CLC++1.0", LangStandard::lang_openclcpp10)
@@ -3676,6 +3776,36 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
       LangStd = OpenCLLangStd;
   }
 
+  // open libfloor function info file
+  if (const Arg *A = Args.getLastArg(OPT_floor_function_info)) {
+    if (A->getValue() != nullptr && strlen(A->getValue()) > 0) {
+      Opts.floor_function_info = new std::fstream(A->getValue(), std::ios::out | std::ios::binary);
+      if (Opts.floor_function_info == nullptr ||
+          !Opts.floor_function_info->is_open()) {
+        Diags.Report(diag::err_drv_floor_function_info);
+      }
+    }
+  }
+
+  // extract libfloor image capabilities
+  if (const Arg *A = Args.getLastArg(OPT_floor_image_capabilities)) {
+    StringRef image_caps = A->getValue();
+    Opts.floor_image_capabilities = (unsigned int)std::stoul(image_caps.str());
+  }
+
+  // metal lang options
+  if (Args.hasArg(OPT_metal_no_array_image)) {
+    Opts.metal_no_array_image = true;
+  }
+  if (Args.hasArg(OPT_metal_soft_printf)) {
+    Opts.metal_soft_printf = true;
+  }
+
+  // Vulkan lang options
+  if (Args.hasArg(OPT_vulkan_soft_printf)) {
+    Opts.vulkan_soft_printf = true;
+  }
+
   // These need to be parsed now. They are used to set OpenCL defaults.
   Opts.IncludeDefaultHeader = Args.hasArg(OPT_finclude_default_header);
   Opts.DeclareOpenCLBuiltins = Args.hasArg(OPT_fdeclare_opencl_builtins);
@@ -3830,11 +3960,15 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
       Args.hasFlag(OPT_ftrigraphs, OPT_fno_trigraphs, Opts.Trigraphs);
 
   Opts.Blocks = Args.hasArg(OPT_fblocks) || (Opts.OpenCL
-    && Opts.OpenCLVersion == 200);
+    && Opts.OpenCLVersion >= 200);
 
+#if 0 // we don't want this
   Opts.ConvergentFunctions = Opts.OpenCL || (Opts.CUDA && Opts.CUDAIsDevice) ||
                              Opts.SYCLIsDevice ||
                              Args.hasArg(OPT_fconvergent_functions);
+#else
+  Opts.ConvergentFunctions = false;
+#endif
 
   Opts.NoBuiltin = Args.hasArg(OPT_fno_builtin) || Opts.Freestanding;
   if (!Opts.NoBuiltin)
@@ -3879,7 +4013,9 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
       Opts.OpenMPIsDevice &&
       Args.hasArg(options::OPT_fopenmp_target_new_runtime);
 
+#if 0 // we don't want this
   Opts.ConvergentFunctions = Opts.ConvergentFunctions || Opts.OpenMPIsDevice;
+#endif
 
   if (Opts.OpenMP || Opts.OpenMPSimd) {
     if (int Version = getLastArgIntValue(
@@ -3904,8 +4040,7 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
   // Set the flag to prevent the implementation from emitting device exception
   // handling code for those requiring so.
   if ((Opts.OpenMPIsDevice && (T.isNVPTX() || T.isAMDGCN())) ||
-      Opts.OpenCLCPlusPlus) {
-
+      Opts.OpenCL || Opts.CUDA || Opts.OpenCLCPlusPlus) {
     Opts.Exceptions = 0;
     Opts.CXXExceptions = 0;
   }
@@ -4021,6 +4156,19 @@ bool CompilerInvocation::ParseLangArgs(LangOptions &Opts, ArgList &Args,
       Diags.Report(diag::err_drv_invalid_value) << A->getAsString(Args) << Val;
   }
 
+  Opts.CLVerifySPIR = Args.hasArg(OPT_cl_verify_spir);
+
+  if(const Arg* A = Args.getLastArg(OPT_cl_sampler_type)) {
+      Opts.CLSamplerOpaque  = llvm::StringSwitch<unsigned int>(A->getValue())
+        .Case("i32", 0u)
+        .Case("opaque", 1u)
+        .Default(~0u);
+      if(Opts.CLSamplerOpaque == ~0u)
+        Diags.Report(diag::err_drv_invalid_value) << A->getAsString(Args)
+                                                  << A->getValue();
+  } else
+    Opts.CLSamplerOpaque = 1;
+
   // Parse -fsanitize= arguments.
   parseSanitizerKinds("-fsanitize=", Args.getAllArgValues(OPT_fsanitize_EQ),
                       Diags, Opts.Sanitize);
@@ -4136,6 +4284,11 @@ static bool isStrictlyPreprocessorAction(frontend::ActionKind Action) {
   case frontend::ASTView:
   case frontend::EmitAssembly:
   case frontend::EmitBC:
+  case frontend::EmitBC32:
+  case frontend::EmitBC50:
+  case frontend::EmitSPIRV:
+  case frontend::EmitSPIRVContainer:
+  case frontend::EmitMetalLib:
   case frontend::EmitHTML:
   case frontend::EmitLLVM:
   case frontend::EmitLLVMOnly:
@@ -4477,6 +4630,11 @@ bool CompilerInvocation::CreateFromArgsImpl(
       Res.getTargetOpts().HostTriple = Res.getFrontendOpts().AuxTriple;
   }
 
+  if (LangOpts.Metal && Res.getCodeGenOpts().getDebugInfo() != codegenoptions::NoDebugInfo) {
+    // dwarf version must always be 4 for Metal 2.x
+    Res.getCodeGenOpts().DwarfVersion = 4;
+  }
+
   // Set the triple of the host for OpenMP device compile.
   if (LangOpts.OpenMPIsDevice)
     Res.getTargetOpts().HostTriple = Res.getFrontendOpts().AuxTriple;
diff --git a/clang/lib/Frontend/FrontendActions.cpp b/clang/lib/Frontend/FrontendActions.cpp
index b5544afa9f24..7ce52d2ca01d 100644
--- a/clang/lib/Frontend/FrontendActions.cpp
+++ b/clang/lib/Frontend/FrontendActions.cpp
@@ -890,6 +890,8 @@ void PrintPreambleAction::ExecuteAction() {
   case Language::ObjC:
   case Language::ObjCXX:
   case Language::OpenCL:
+  case Language::Metal:
+  case Language::Vulkan:
   case Language::OpenCLCXX:
   case Language::CUDA:
   case Language::HIP:
diff --git a/clang/lib/Frontend/InitPreprocessor.cpp b/clang/lib/Frontend/InitPreprocessor.cpp
index a3e1ca5d5226..3201ed13850f 100644
--- a/clang/lib/Frontend/InitPreprocessor.cpp
+++ b/clang/lib/Frontend/InitPreprocessor.cpp
@@ -435,7 +435,7 @@ static void InitializeStandardPredefinedMacros(const TargetInfo &TI,
 
   // OpenCL v1.0/1.1 s6.9, v1.2/2.0 s6.10: Preprocessor Directives and Macros.
   if (LangOpts.OpenCL) {
-    if (LangOpts.CPlusPlus) {
+    if (LangOpts.OpenCLCPlusPlus) {
       switch (LangOpts.OpenCLCPlusPlusVersion) {
       case 100:
         Builder.defineMacro("__OPENCL_CPP_VERSION__", "100");
@@ -467,6 +467,8 @@ static void InitializeStandardPredefinedMacros(const TargetInfo &TI,
         Builder.defineMacro("__OPENCL_C_VERSION__", "120");
         break;
       case 200:
+      case 210:
+      case 220:
         Builder.defineMacro("__OPENCL_C_VERSION__", "200");
         break;
       case 300:
@@ -944,8 +946,10 @@ static void InitializePredefinedMacros(const TargetInfo &TI,
   DefineFmt("__UINTPTR", TI.getUIntPtrType(), TI, Builder);
   DefineTypeWidth("__UINTPTR_WIDTH__", TI.getUIntPtrType(), TI, Builder);
 
-  if (TI.hasFloat16Type())
+  if (TI.hasFloat16Type()) {
     DefineFloatMacros(Builder, "FLT16", &TI.getHalfFormat(), "F16");
+    DefineFloatMacros(Builder, "HALF", &TI.getHalfFormat(), "H");
+  }
   DefineFloatMacros(Builder, "FLT", &TI.getFloatFormat(), "F");
   DefineFloatMacros(Builder, "DBL", &TI.getDoubleFormat(), "");
   DefineFloatMacros(Builder, "LDBL", &TI.getLongDoubleFormat(), "L");
diff --git a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
index 8e18f33af0cb..3be64a4f7d97 100644
--- a/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
+++ b/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp
@@ -52,6 +52,11 @@ CreateFrontendBaseAction(CompilerInstance &CI) {
   case DumpTokens:             return std::make_unique<DumpTokensAction>();
   case EmitAssembly:           return std::make_unique<EmitAssemblyAction>();
   case EmitBC:                 return std::make_unique<EmitBCAction>();
+  case EmitBC32:               return std::make_unique<EmitBC32Action>();
+  case EmitBC50:               return std::make_unique<EmitBC50Action>();
+  case EmitSPIRV:              return std::make_unique<EmitSPIRVAction>();
+  case EmitSPIRVContainer:     return std::make_unique<EmitSPIRVContainerAction>();
+  case EmitMetalLib:           return std::make_unique<EmitMetalLibAction>();
   case EmitHTML:               return std::make_unique<HTMLPrintAction>();
   case EmitLLVM:               return std::make_unique<EmitLLVMAction>();
   case EmitLLVMOnly:           return std::make_unique<EmitLLVMOnlyAction>();
diff --git a/clang/lib/Headers/CMakeLists.txt b/clang/lib/Headers/CMakeLists.txt
index 388697040dba..4202ad3d39a9 100644
--- a/clang/lib/Headers/CMakeLists.txt
+++ b/clang/lib/Headers/CMakeLists.txt
@@ -41,20 +41,6 @@ set(files
   bmi2intrin.h
   bmiintrin.h
   builtins.h
-  __clang_cuda_builtin_vars.h
-  __clang_cuda_math.h
-  __clang_cuda_cmath.h
-  __clang_cuda_complex_builtins.h
-  __clang_cuda_device_functions.h
-  __clang_cuda_intrinsics.h
-  __clang_cuda_texture_intrinsics.h
-  __clang_cuda_libdevice_declares.h
-  __clang_cuda_math_forward_declares.h
-  __clang_cuda_runtime_wrapper.h
-  __clang_hip_libdevice_declares.h
-  __clang_hip_cmath.h
-  __clang_hip_math.h
-  __clang_hip_runtime_wrapper.h
   cetintrin.h
   cet.h
   cldemoteintrin.h
@@ -96,8 +82,6 @@ set(files
   msa.h
   mwaitxintrin.h
   nmmintrin.h
-  opencl-c.h
-  opencl-c-base.h
   pkuintrin.h
   pmmintrin.h
   pconfigintrin.h
@@ -147,12 +131,6 @@ set(files
   xtestintrin.h
   )
 
-set(cuda_wrapper_files
-  cuda_wrappers/algorithm
-  cuda_wrappers/complex
-  cuda_wrappers/new
-)
-
 set(ppc_wrapper_files
   ppc_wrappers/mmintrin.h
   ppc_wrappers/xmmintrin.h
@@ -201,7 +179,7 @@ endfunction(clang_generate_header)
 
 
 # Copy header files from the source directory to the build directory
-foreach( f ${files} ${cuda_wrapper_files} ${ppc_wrapper_files} ${openmp_wrapper_files})
+foreach( f ${files} ${ppc_wrapper_files} ${openmp_wrapper_files})
   copy_header_to_output_dir(${CMAKE_CURRENT_SOURCE_DIR} ${f})
 endforeach( f )
 
@@ -233,11 +211,6 @@ install(
   DESTINATION ${header_install_dir}
   COMPONENT clang-resource-headers)
 
-install(
-  FILES ${cuda_wrapper_files}
-  DESTINATION ${header_install_dir}/cuda_wrappers
-  COMPONENT clang-resource-headers)
-
 install(
   FILES ${ppc_wrapper_files}
   DESTINATION ${header_install_dir}/ppc_wrappers
diff --git a/clang/lib/Lex/PPDirectives.cpp b/clang/lib/Lex/PPDirectives.cpp
index 1c2439ac9102..58fc9195883b 100644
--- a/clang/lib/Lex/PPDirectives.cpp
+++ b/clang/lib/Lex/PPDirectives.cpp
@@ -2501,7 +2501,7 @@ bool Preprocessor::ReadMacroParameterList(MacroInfo *MI, Token &Tok) {
              diag::ext_variadic_macro);
 
       // OpenCL v1.2 s6.9.e: variadic macros are not supported.
-      if (LangOpts.OpenCL && !LangOpts.OpenCLCPlusPlus) {
+      if (LangOpts.OpenCL && !LangOpts.OpenCLCPlusPlus && !LangOpts.CPlusPlus) {
         Diag(Tok, diag::ext_pp_opencl_variadic_macros);
       }
 
diff --git a/clang/lib/Parse/ParseDecl.cpp b/clang/lib/Parse/ParseDecl.cpp
index a0871062395e..31c4cdb28242 100644
--- a/clang/lib/Parse/ParseDecl.cpp
+++ b/clang/lib/Parse/ParseDecl.cpp
@@ -845,23 +845,6 @@ void Parser::ParseBorlandTypeAttributes(ParsedAttributes &attrs) {
   }
 }
 
-void Parser::ParseOpenCLKernelAttributes(ParsedAttributes &attrs) {
-  // Treat these like attributes
-  while (Tok.is(tok::kw___kernel)) {
-    IdentifierInfo *AttrName = Tok.getIdentifierInfo();
-    SourceLocation AttrNameLoc = ConsumeToken();
-    attrs.addNew(AttrName, AttrNameLoc, nullptr, AttrNameLoc, nullptr, 0,
-                 ParsedAttr::AS_Keyword);
-  }
-}
-
-void Parser::ParseOpenCLQualifiers(ParsedAttributes &Attrs) {
-  IdentifierInfo *AttrName = Tok.getIdentifierInfo();
-  SourceLocation AttrNameLoc = Tok.getLocation();
-  Attrs.addNew(AttrName, AttrNameLoc, nullptr, AttrNameLoc, nullptr, 0,
-               ParsedAttr::AS_Keyword);
-}
-
 void Parser::ParseNullabilityTypeSpecifiers(ParsedAttributes &attrs) {
   // Treat these like attributes, even though they're type specifiers.
   while (true) {
@@ -2611,7 +2594,7 @@ bool Parser::ParseImplicitInt(DeclSpec &DS, CXXScopeSpec *SS,
 
   // Early exit as Sema has a dedicated missing_actual_pipe_type diagnostic
   // for incomplete declarations such as `pipe p`.
-  if (getLangOpts().OpenCLCPlusPlus && DS.isTypeSpecPipe())
+  if ((getLangOpts().OpenCLCPlusPlus || getLangOpts().CPlusPlus) && DS.isTypeSpecPipe())
     return false;
 
   if (getLangOpts().CPlusPlus &&
@@ -3055,7 +3038,6 @@ static void SetupFixedPointError(const LangOptions &LangOpts,
 /// [C99]   'inline'
 /// [C++]   'virtual'
 /// [C++]   'explicit'
-/// [OpenCL] '__kernel'
 ///       'friend': [C++ dcl.friend]
 ///       'constexpr': [C++0x dcl.constexpr]
 void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
@@ -3101,6 +3083,7 @@ void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
 
     SourceLocation Loc = Tok.getLocation();
 
+#if 0 // unused
     // Helper for image types in OpenCL.
     auto handleOpenCLImageKW = [&] (StringRef Ext, TypeSpecifierType ImageTypeSpec) {
       // Check if the image type is supported and otherwise turn the keyword into an identifier
@@ -3113,6 +3096,7 @@ void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
       isInvalid = DS.SetTypeSpecType(ImageTypeSpec, Loc, PrevSpec, DiagID, Policy);
       return true;
     };
+#endif
 
     // Turn off usual access checking for template specializations and
     // instantiations.
@@ -3647,11 +3631,6 @@ void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
       ParseBorlandTypeAttributes(DS.getAttributes());
       continue;
 
-    // OpenCL single token adornments.
-    case tok::kw___kernel:
-      ParseOpenCLKernelAttributes(DS.getAttributes());
-      continue;
-
     // Nullability type specifiers.
     case tok::kw__Nonnull:
     case tok::kw__Nullable:
@@ -4015,15 +3994,26 @@ void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
       } else
         isInvalid = DS.SetTypePipe(true, Loc, PrevSpec, DiagID, Policy);
       break;
-// We only need to enumerate each image type once.
-#define IMAGE_READ_WRITE_TYPE(Type, Id, Ext)
-#define IMAGE_WRITE_TYPE(Type, Id, Ext)
-#define IMAGE_READ_TYPE(ImgType, Id, Ext) \
-    case tok::kw_##ImgType##_t: \
-      if (!handleOpenCLImageKW(Ext, DeclSpec::TST_##ImgType##_t)) \
-        goto DoneWithDeclSpec; \
+    case tok::kw_sampler_t:
+      isInvalid = DS.SetTypeSpecType(DeclSpec::TST_sampler_t, Loc,
+                                     PrevSpec, DiagID, Policy);
+      break;
+    case tok::kw_event_t:
+      isInvalid = DS.SetTypeSpecType(DeclSpec::TST_event_t, Loc,
+                                     PrevSpec, DiagID, Policy);
+      break;
+    case tok::kw_queue_t:
+      isInvalid = DS.SetTypeSpecType(DeclSpec::TST_queue_t, Loc,
+                                     PrevSpec, DiagID, Policy);
+      break;
+    case tok::kw_clk_event_t:
+      isInvalid = DS.SetTypeSpecType(DeclSpec::TST_clk_event_t, Loc,
+                                     PrevSpec, DiagID, Policy);
       break;
-#include "clang/Basic/OpenCLImageTypes.def"
+    case tok::kw_reserve_id_t:
+      isInvalid = DS.SetTypeSpecType(DeclSpec::TST_reserve_id_t, Loc,
+                                     PrevSpec, DiagID, Policy);
+    break;
     case tok::kw___unknown_anytype:
       isInvalid = DS.SetTypeSpecType(TST_unknown_anytype, Loc,
                                      PrevSpec, DiagID, Policy);
@@ -4128,36 +4118,6 @@ void Parser::ParseDeclarationSpecifiers(DeclSpec &DS,
                                  getLangOpts());
       break;
 
-    // OpenCL address space qualifiers:
-    case tok::kw___generic:
-      // generic address space is introduced only in OpenCL v2.0
-      // see OpenCL C Spec v2.0 s6.5.5
-      // OpenCL v3.0 introduces __opencl_c_generic_address_space
-      // feature macro to indicate if generic address space is supported
-      if (!Actions.getLangOpts().OpenCLGenericAddressSpace) {
-        DiagID = diag::err_opencl_unknown_type_specifier;
-        PrevSpec = Tok.getIdentifierInfo()->getNameStart();
-        isInvalid = true;
-        break;
-      }
-      LLVM_FALLTHROUGH;
-    case tok::kw_private:
-      // It's fine (but redundant) to check this for __generic on the
-      // fallthrough path; we only form the __generic token in OpenCL mode.
-      if (!getLangOpts().OpenCL)
-        goto DoneWithDeclSpec;
-      LLVM_FALLTHROUGH;
-    case tok::kw___private:
-    case tok::kw___global:
-    case tok::kw___local:
-    case tok::kw___constant:
-    // OpenCL access qualifiers:
-    case tok::kw___read_only:
-    case tok::kw___write_only:
-    case tok::kw___read_write:
-      ParseOpenCLQualifiers(DS.getAttributes());
-      break;
-
     case tok::less:
       // GCC ObjC supports types like "<SomeProtocol>" as a synonym for
       // "id<SomeProtocol>".  This is hopelessly old fashioned and dangerous,
@@ -5030,8 +4990,12 @@ bool Parser::isKnownToBeTypeSpecifier(const Token &Tok) const {
   case tok::kw__Decimal64:
   case tok::kw__Decimal128:
   case tok::kw___vector:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
+
+    // OpenCL specific types:
+  case tok::kw_sampler_t:
+  case tok::kw_event_t:
+  case tok::kw_queue_t:
+  case tok::kw_clk_event_t:
 
     // struct-or-union-specifier (C99) or class-specifier (C++)
   case tok::kw_class:
@@ -5112,8 +5076,12 @@ bool Parser::isTypeSpecifierQualifier() {
   case tok::kw__Decimal64:
   case tok::kw__Decimal128:
   case tok::kw___vector:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
+
+    // OpenCL specific types:
+  case tok::kw_sampler_t:
+  case tok::kw_event_t:
+  case tok::kw_queue_t:
+  case tok::kw_clk_event_t:
 
     // struct-or-union-specifier (C99) or class-specifier (C++)
   case tok::kw_class:
@@ -5158,19 +5126,10 @@ bool Parser::isTypeSpecifierQualifier() {
   case tok::kw__Null_unspecified:
 
   case tok::kw___kindof:
-
-  case tok::kw___private:
-  case tok::kw___local:
-  case tok::kw___global:
-  case tok::kw___constant:
-  case tok::kw___generic:
-  case tok::kw___read_only:
-  case tok::kw___read_write:
-  case tok::kw___write_only:
     return true;
 
-  case tok::kw_private:
-    return getLangOpts().OpenCL;
+  case tok::kw_reserve_id_t:
+    return getLangOpts().OpenCL && getLangOpts().OpenCLVersion >= 200;
 
   // C11 _Atomic
   case tok::kw__Atomic:
@@ -5284,6 +5243,13 @@ bool Parser::isDeclarationSpecifier(bool DisambiguatingWithExpression) {
   case tok::kw__Decimal128:
   case tok::kw___vector:
 
+    // OpenCL specific types:
+  case tok::kw_sampler_t:
+  case tok::kw_event_t:
+  case tok::kw_queue_t:
+  case tok::kw_clk_event_t:
+  case tok::kw_reserve_id_t:
+
     // struct-or-union-specifier (C99) or class-specifier (C++)
   case tok::kw_class:
   case tok::kw_struct:
@@ -5390,21 +5356,7 @@ bool Parser::isDeclarationSpecifier(bool DisambiguatingWithExpression) {
 
   case tok::kw___kindof:
 
-  case tok::kw___private:
-  case tok::kw___local:
-  case tok::kw___global:
-  case tok::kw___constant:
-  case tok::kw___generic:
-  case tok::kw___read_only:
-  case tok::kw___read_write:
-  case tok::kw___write_only:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
-
     return true;
-
-  case tok::kw_private:
-    return getLangOpts().OpenCL;
   }
 }
 
@@ -5608,22 +5560,6 @@ void Parser::ParseTypeQualifierListOpt(
                                  getLangOpts());
       break;
 
-    // OpenCL qualifiers:
-    case tok::kw_private:
-      if (!getLangOpts().OpenCL)
-        goto DoneWithTypeQuals;
-      LLVM_FALLTHROUGH;
-    case tok::kw___private:
-    case tok::kw___global:
-    case tok::kw___local:
-    case tok::kw___constant:
-    case tok::kw___generic:
-    case tok::kw___read_only:
-    case tok::kw___write_only:
-    case tok::kw___read_write:
-      ParseOpenCLQualifiers(DS.getAttributes());
-      break;
-
     case tok::kw___unaligned:
       isInvalid = DS.SetTypeQual(DeclSpec::TQ_unaligned, Loc, PrevSpec, DiagID,
                                  getLangOpts());
@@ -6524,7 +6460,8 @@ void Parser::InitCXXThisScopeForDeclaratorIfRelevant(
   // Carry on using the first addr space for the qualifiers of 'this'.
   // The diagnostic will be given later while creating the function
   // prototype for the method.
-  if (getLangOpts().OpenCLCPlusPlus) {
+#if 0 // TODO: do we want this?
+  if (getLangOpts().OpenCLCPlusPlus || getLangOpts().CPlusPlus) {
     for (ParsedAttr &attr : DS.getAttributes()) {
       LangAS ASIdx = attr.asOpenCLLangAS();
       if (ASIdx != LangAS::Default) {
@@ -6533,6 +6470,7 @@ void Parser::InitCXXThisScopeForDeclaratorIfRelevant(
       }
     }
   }
+#endif
   ThisScope.emplace(Actions, dyn_cast<CXXRecordDecl>(Actions.CurContext), Q,
                     IsCXX11MemberFunction);
 }
diff --git a/clang/lib/Parse/ParseDeclCXX.cpp b/clang/lib/Parse/ParseDeclCXX.cpp
index f5a6ffcff9e9..660864f8fe86 100644
--- a/clang/lib/Parse/ParseDeclCXX.cpp
+++ b/clang/lib/Parse/ParseDeclCXX.cpp
@@ -3255,7 +3255,7 @@ Parser::DeclGroupPtrTy Parser::ParseCXXClassMemberDeclarationWithPragmas(
   case tok::kw_private:
     // FIXME: We don't accept GNU attributes on access specifiers in OpenCL mode
     // yet.
-    if (getLangOpts().OpenCL && !NextToken().is(tok::colon))
+    if (getLangOpts().OpenCL && !getLangOpts().CPlusPlus && !NextToken().is(tok::colon))
       return ParseCXXClassMemberDeclaration(AS, AccessAttrs);
     LLVM_FALLTHROUGH;
   case tok::kw_public:
diff --git a/clang/lib/Parse/ParseExpr.cpp b/clang/lib/Parse/ParseExpr.cpp
index 2c8b4f9f441f..8c366e70886c 100644
--- a/clang/lib/Parse/ParseExpr.cpp
+++ b/clang/lib/Parse/ParseExpr.cpp
@@ -1526,8 +1526,10 @@ ExprResult Parser::ParseCastExpression(CastParseKind ParseKind,
   case tok::kw_typename:
   case tok::kw_typeof:
   case tok::kw___vector:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
+  case tok::kw_sampler_t:
+  case tok::kw_event_t:
+  case tok::kw_queue_t:
+  case tok::kw_clk_event_t:
   {
     if (!getLangOpts().CPlusPlus) {
       Diag(Tok, diag::err_expected_expression);
@@ -1811,7 +1813,7 @@ ExprResult Parser::ParseCastExpression(CastParseKind ParseKind,
   // These can be followed by postfix-expr pieces.
   PreferredType = SavedType;
   Res = ParsePostfixExpressionSuffix(Res);
-  if (getLangOpts().OpenCL &&
+  if (getLangOpts().OpenCL && !getLangOpts().CPlusPlus &&
       !getActions().getOpenCLOptions().isAvailableOption(
           "__cl_clang_function_pointers", getLangOpts()))
     if (Expr *PostfixExpr = Res.get()) {
diff --git a/clang/lib/Parse/ParseExprCXX.cpp b/clang/lib/Parse/ParseExprCXX.cpp
index 4e5c0ac6c1c1..2725465ffd6e 100644
--- a/clang/lib/Parse/ParseExprCXX.cpp
+++ b/clang/lib/Parse/ParseExprCXX.cpp
@@ -1266,7 +1266,7 @@ ExprResult Parser::ParseLambdaExpressionAfterIntroducer(
       for (const ParsedAttr &A : Attr)
         if (A.getKind() == ParsedAttr::AT_CUDADevice ||
             A.getKind() == ParsedAttr::AT_CUDAHost ||
-            A.getKind() == ParsedAttr::AT_CUDAGlobal)
+            A.getKind() == ParsedAttr::AT_ComputeKernel)
           Diag(A.getLoc(), diag::warn_cuda_attr_lambda_position)
               << A.getAttrName()->getName();
   };
@@ -1357,13 +1357,6 @@ ExprResult Parser::ParseLambdaExpressionAfterIntroducer(
         // Parse attribute-specifier[opt].
         MaybeParseCXX11Attributes(Attr, &DeclEndLoc);
 
-        // Parse OpenCL addr space attribute.
-        if (Tok.isOneOf(tok::kw___private, tok::kw___global, tok::kw___local,
-                        tok::kw___constant, tok::kw___generic)) {
-          ParseOpenCLQualifiers(DS.getAttributes());
-          ConsumeToken();
-        }
-
         SourceLocation FunLocalRangeEnd = DeclEndLoc;
 
         // Parse trailing-return-type[opt].
@@ -1433,8 +1426,6 @@ ExprResult Parser::ParseLambdaExpressionAfterIntroducer(
       ParseTrailingRequiresClause(D);
   } else if (Tok.isOneOf(tok::kw_mutable, tok::arrow, tok::kw___attribute,
                          tok::kw_constexpr, tok::kw_consteval,
-                         tok::kw___private, tok::kw___global, tok::kw___local,
-                         tok::kw___constant, tok::kw___generic,
                          tok::kw_requires, tok::kw_noexcept) ||
              (Tok.is(tok::l_square) && NextToken().is(tok::l_square))) {
     if (!getLangOpts().CPlusPlus2b)
@@ -2271,12 +2262,6 @@ void Parser::ParseCXXSimpleTypeSpecifier(DeclSpec &DS) {
   case tok::kw_bool:
     DS.SetTypeSpecType(DeclSpec::TST_bool, Loc, PrevSpec, DiagID, Policy);
     break;
-#define GENERIC_IMAGE_TYPE(ImgType, Id)                                        \
-  case tok::kw_##ImgType##_t:                                                  \
-    DS.SetTypeSpecType(DeclSpec::TST_##ImgType##_t, Loc, PrevSpec, DiagID,     \
-                       Policy);                                                \
-    break;
-#include "clang/Basic/OpenCLImageTypes.def"
 
   case tok::annot_decltype:
   case tok::kw_decltype:
diff --git a/clang/lib/Parse/ParsePragma.cpp b/clang/lib/Parse/ParsePragma.cpp
index 27e850127862..1d5a9f4de5ea 100644
--- a/clang/lib/Parse/ParsePragma.cpp
+++ b/clang/lib/Parse/ParsePragma.cpp
@@ -3441,12 +3441,6 @@ void PragmaUnrollHintHandler::HandlePragma(Preprocessor &PP,
     if (ParseLoopHintValue(PP, Tok, PragmaName, Option, ValueInParens, *Info))
       return;
 
-    // In CUDA, the argument to '#pragma unroll' should not be contained in
-    // parentheses.
-    if (PP.getLangOpts().CUDA && ValueInParens)
-      PP.Diag(Info->Toks[0].getLocation(),
-              diag::warn_pragma_unroll_cuda_value_in_parens);
-
     if (Tok.isNot(tok::eod)) {
       PP.Diag(Tok.getLocation(), diag::warn_pragma_extra_tokens_at_eol)
           << "unroll";
diff --git a/clang/lib/Parse/ParseTentative.cpp b/clang/lib/Parse/ParseTentative.cpp
index be3823ecda01..409b0393b9a9 100644
--- a/clang/lib/Parse/ParseTentative.cpp
+++ b/clang/lib/Parse/ParseTentative.cpp
@@ -1397,20 +1397,11 @@ Parser::isCXXDeclarationSpecifier(Parser::TPResult BracedCastResult,
   case tok::kw_volatile:
     return TPResult::True;
 
-    // OpenCL address space qualifiers
+  case tok::kw_sampler_t:
+  case tok::kw_event_t:
+  case tok::kw_queue_t:
+  case tok::kw_clk_event_t:
   case tok::kw_private:
-    if (!getLangOpts().OpenCL)
-      return TPResult::False;
-    LLVM_FALLTHROUGH;
-  case tok::kw___private:
-  case tok::kw___local:
-  case tok::kw___global:
-  case tok::kw___constant:
-  case tok::kw___generic:
-    // OpenCL access qualifiers
-  case tok::kw___read_only:
-  case tok::kw___write_only:
-  case tok::kw___read_write:
     // OpenCL pipe
   case tok::kw_pipe:
 
@@ -1640,8 +1631,6 @@ Parser::isCXXDeclarationSpecifier(Parser::TPResult BracedCastResult,
   case tok::kw___ibm128:
   case tok::kw_void:
   case tok::annot_decltype:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
     if (NextToken().is(tok::l_paren))
       return TPResult::Ambiguous;
 
@@ -1756,8 +1745,6 @@ bool Parser::isCXXDeclarationSpecifierAType() {
   case tok::kw_void:
   case tok::kw___unknown_anytype:
   case tok::kw___auto_type:
-#define GENERIC_IMAGE_TYPE(ImgType, Id) case tok::kw_##ImgType##_t:
-#include "clang/Basic/OpenCLImageTypes.def"
     return true;
 
   case tok::kw_auto:
diff --git a/clang/lib/Sema/DeclSpec.cpp b/clang/lib/Sema/DeclSpec.cpp
index 4405f29f3d99..795842406207 100644
--- a/clang/lib/Sema/DeclSpec.cpp
+++ b/clang/lib/Sema/DeclSpec.cpp
@@ -376,6 +376,11 @@ bool Declarator::isDeclarationOfFunction() const {
     case TST_BFloat16:
 #define GENERIC_IMAGE_TYPE(ImgType, Id) case TST_##ImgType##_t:
 #include "clang/Basic/OpenCLImageTypes.def"
+    case TST_sampler_t:
+    case TST_event_t:
+    case TST_queue_t:
+    case TST_clk_event_t:
+    case TST_reserve_id_t:
       return false;
 
     case TST_decltype_auto:
@@ -584,6 +589,11 @@ const char *DeclSpec::getSpecifierName(DeclSpec::TST T,
   case DeclSpec::TST_##ImgType##_t: \
     return #ImgType "_t";
 #include "clang/Basic/OpenCLImageTypes.def"
+  case DeclSpec::TST_sampler_t:   return "sampler_t";
+  case DeclSpec::TST_event_t:     return "event_t";
+  case DeclSpec::TST_queue_t:     return "queue_t";
+  case DeclSpec::TST_clk_event_t: return "clk_event_t";
+  case DeclSpec::TST_reserve_id_t: return "reserve_id_t";
   case DeclSpec::TST_error:       return "(error)";
   }
   llvm_unreachable("Unknown typespec!");
diff --git a/clang/lib/Sema/OpenCLBuiltins.td b/clang/lib/Sema/OpenCLBuiltins.td
index 8cf7ec58eff5..0d9bfe279911 100644
--- a/clang/lib/Sema/OpenCLBuiltins.td
+++ b/clang/lib/Sema/OpenCLBuiltins.td
@@ -214,7 +214,7 @@ class VolatileType<Type _Ty> : Type<_Ty.Name, _Ty.QTExpr> {
 
 // OpenCL image types (e.g. image2d).
 class ImageType<Type _Ty, string _AccessQualifier> :
-    Type<_Ty.Name, QualType<_Ty.QTExpr.TypeExpr # _AccessQualifier # "Ty", 0>> {
+    Type<_Ty.Name, QualType<_Ty.QTExpr.TypeExpr # "Ty", 0>> {
   let VecWidth = 0;
   let AccessQualifier = _AccessQualifier;
   // Inherited fields
@@ -354,6 +354,10 @@ def Image2dMsaa           : Type<"image2d_msaa_t", QualType<"Context.OCLImage2dM
 def Image2dArrayMsaa      : Type<"image2d_array_msaa_t", QualType<"Context.OCLImage2dArrayMSAA", 1>>;
 def Image2dMsaaDepth      : Type<"image2d_msaa_depth_t", QualType<"Context.OCLImage2dMSAADepth", 1>>;
 def Image2dArrayMsaaDepth : Type<"image2d_array_msaa_depth_t", QualType<"Context.OCLImage2dArrayMSAADepth", 1>>;
+def ImageCube             : Type<"imagecube_t", QualType<"Context.OCLImageCube", 1>>;
+def ImageCubeArray        : Type<"imagecube_array_t", QualType<"Context.OCLImageCubeArray", 1>>;
+def ImageCubeDepth        : Type<"imagecube_depth_t", QualType<"Context.OCLImageCubeDepth", 1>>;
+def ImageCubeArrayDepth   : Type<"imagecube_array_depth_t", QualType<"Context.OCLImageCubeArrayDepth", 1>>;
 
 def Sampler               : Type<"sampler_t", QualType<"Context.OCLSamplerTy">>;
 def ClkEvent              : Type<"clk_event_t", QualType<"Context.OCLClkEventTy">>;
@@ -1690,7 +1694,7 @@ let Extension = FuncExtKhrMipmapImage in {
   }
   // Added to section 6.13.14.5
   foreach aQual = ["RO", "WO", "RW"] in {
-    foreach imgTy = [Image1d, Image2d, Image3d, Image1dArray, Image2dArray, Image2dDepth, Image2dArrayDepth] in {
+    foreach imgTy = [Image1d, Image2d, Image3d, Image1dArray, Image2dArray, Image2dDepth, Image2dArrayDepth, ImageCube, ImageCubeArray, ImageCubeDepth, ImageCubeArrayDepth] in {
       def : Builtin<"get_image_num_mip_levels", [Int, ImageType<imgTy, aQual>]>;
     }
   }
diff --git a/clang/lib/Sema/Sema.cpp b/clang/lib/Sema/Sema.cpp
index 9fae2243cbbf..b01b9371be79 100644
--- a/clang/lib/Sema/Sema.cpp
+++ b/clang/lib/Sema/Sema.cpp
@@ -174,6 +174,7 @@ const uint64_t Sema::MaximumAlignment;
 Sema::Sema(Preprocessor &pp, ASTContext &ctxt, ASTConsumer &consumer,
            TranslationUnitKind TUKind, CodeCompleteConsumer *CodeCompleter)
     : ExternalSource(nullptr), isMultiplexExternalSource(false),
+      OpenCLFeatures(ctxt.getOpenCLFeatures()),
       CurFPFeatures(pp.getLangOpts()), LangOpts(pp.getLangOpts()), PP(pp),
       Context(ctxt), Consumer(consumer), Diags(PP.getDiagnostics()),
       SourceMgr(PP.getSourceManager()), CollectStats(false),
@@ -324,6 +325,11 @@ void Sema::Initialize() {
         Context.getTargetInfo().getSupportedOpenCLOpts(), getLangOpts());
     addImplicitTypedef("sampler_t", Context.OCLSamplerTy);
     addImplicitTypedef("event_t", Context.OCLEventTy);
+
+#define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
+    addImplicitTypedef(#ImgType #Suffix "_t", Context. SingletonId);
+#include "clang/Basic/OpenCLImageTypes.def"
+
     if (getLangOpts().getOpenCLCompatibleVersion() >= 200) {
       addImplicitTypedef("clk_event_t", Context.OCLClkEventTy);
       addImplicitTypedef("queue_t", Context.OCLQueueTy);
@@ -1431,8 +1437,12 @@ NamedDecl *Sema::getCurFunctionOrMethodDecl() {
 }
 
 LangAS Sema::getDefaultCXXMethodAddrSpace() const {
+#if 0 // we don't want this + we don't want any calls to this -> error if so
   if (getLangOpts().OpenCL)
     return getASTContext().getDefaultOpenCLPointeeAddrSpace();
+#else
+  assert(false && "should not be here - all calls to this should be disabled");
+#endif
   return LangAS::Default;
 }
 
diff --git a/clang/lib/Sema/SemaAttr.cpp b/clang/lib/Sema/SemaAttr.cpp
index 100f8e36a9b8..695b1e2b49e8 100644
--- a/clang/lib/Sema/SemaAttr.cpp
+++ b/clang/lib/Sema/SemaAttr.cpp
@@ -1106,6 +1106,7 @@ void Sema::ActOnPragmaFPContract(SourceLocation Loc,
     break;
   case LangOptions::FPM_Off:
     NewFPFeatures.setDisallowFPContract();
+    Context.disableFPContract();
     break;
   case LangOptions::FPM_FastHonorPragmas:
     llvm_unreachable("Should not happen");
diff --git a/clang/lib/Sema/SemaCUDA.cpp b/clang/lib/Sema/SemaCUDA.cpp
index 840b3daae63c..6c09ae8880c2 100644
--- a/clang/lib/Sema/SemaCUDA.cpp
+++ b/clang/lib/Sema/SemaCUDA.cpp
@@ -66,21 +66,13 @@ ExprResult Sema::ActOnCUDAExecConfigExpr(Scope *S, SourceLocation LLLLoc,
 
 Sema::CUDAFunctionTarget
 Sema::IdentifyCUDATarget(const ParsedAttributesView &Attrs) {
-  bool HasHostAttr = false;
-  bool HasDeviceAttr = false;
   bool HasGlobalAttr = false;
   bool HasInvalidTargetAttr = false;
   for (const ParsedAttr &AL : Attrs) {
     switch (AL.getKind()) {
-    case ParsedAttr::AT_CUDAGlobal:
+    case ParsedAttr::AT_ComputeKernel:
       HasGlobalAttr = true;
       break;
-    case ParsedAttr::AT_CUDAHost:
-      HasHostAttr = true;
-      break;
-    case ParsedAttr::AT_CUDADevice:
-      HasDeviceAttr = true;
-      break;
     case ParsedAttr::AT_CUDAInvalidTarget:
       HasInvalidTargetAttr = true;
       break;
@@ -89,19 +81,18 @@ Sema::IdentifyCUDATarget(const ParsedAttributesView &Attrs) {
     }
   }
 
+#if 0 // we don't want this
   if (HasInvalidTargetAttr)
     return CFT_InvalidTarget;
+#endif
 
   if (HasGlobalAttr)
     return CFT_Global;
 
-  if (HasHostAttr && HasDeviceAttr)
-    return CFT_HostDevice;
-
-  if (HasDeviceAttr)
-    return CFT_Device;
-
-  return CFT_Host;
+  // if not a kernel, always default to device
+  // this is IMO a much saner approach and doesn't require to add the __device__
+  // attribute to _all_ functions
+  return CFT_Device;
 }
 
 template <typename A>
@@ -115,6 +106,7 @@ static bool hasAttr(const FunctionDecl *D, bool IgnoreImplicitAttr) {
 /// IdentifyCUDATarget - Determine the CUDA compilation target for this function
 Sema::CUDAFunctionTarget Sema::IdentifyCUDATarget(const FunctionDecl *D,
                                                   bool IgnoreImplicitHDAttr) {
+#if 0 // we don't want this
   // Code that lives outside a function is run on the host.
   if (D == nullptr)
     return CFT_Host;
@@ -122,7 +114,7 @@ Sema::CUDAFunctionTarget Sema::IdentifyCUDATarget(const FunctionDecl *D,
   if (D->hasAttr<CUDAInvalidTargetAttr>())
     return CFT_InvalidTarget;
 
-  if (D->hasAttr<CUDAGlobalAttr>())
+  if (D->hasAttr<ComputeKernelAttr>())
     return CFT_Global;
 
   if (hasAttr<CUDADeviceAttr>(D, IgnoreImplicitHDAttr)) {
@@ -139,10 +131,16 @@ Sema::CUDAFunctionTarget Sema::IdentifyCUDATarget(const FunctionDecl *D,
   }
 
   return CFT_Host;
+#else // -> either device or global/kernel
+  if (D != nullptr && D->hasAttr<ComputeKernelAttr>())
+    return CFT_Global;
+  return CFT_Device;
+#endif
 }
 
 /// IdentifyTarget - Determine the CUDA compilation target for this variable.
 Sema::CUDAVariableTarget Sema::IdentifyCUDATarget(const VarDecl *Var) {
+#if 0 // we don't want this
   if (Var->hasAttr<HIPManagedAttr>())
     return CVT_Unified;
   if (Var->isConstexpr() && !hasExplicitAttr<CUDAConstantAttr>(Var))
@@ -171,6 +169,9 @@ Sema::CUDAVariableTarget Sema::IdentifyCUDATarget(const VarDecl *Var) {
     }
   }
   return CVT_Host;
+#else // -> always device
+  return CVT_Device;
+#endif
 }
 
 // * CUDA Call preference table
@@ -455,8 +456,8 @@ bool Sema::inferCUDATargetForImplicitSpecialMember(CXXRecordDecl *ClassDecl,
   // previously set ones.
   if (NeedsD && !HasD)
     MemberDecl->addAttr(CUDADeviceAttr::CreateImplicit(Context));
-  if (NeedsH && !HasH)
-    MemberDecl->addAttr(CUDAHostAttr::CreateImplicit(Context));
+  //if (NeedsH && !HasH)
+  //  MemberDecl->addAttr(CUDAHostAttr::CreateImplicit(Context));
 
   return false;
 }
@@ -662,8 +663,8 @@ void Sema::maybeAddCUDAHostDeviceAttrs(FunctionDecl *NewD,
   assert(getLangOpts().CUDA && "Should only be called during CUDA compilation");
 
   if (ForceCUDAHostDeviceDepth > 0) {
-    if (!NewD->hasAttr<CUDAHostAttr>())
-      NewD->addAttr(CUDAHostAttr::CreateImplicit(Context));
+    //if (!NewD->hasAttr<CUDAHostAttr>())
+    //  NewD->addAttr(CUDAHostAttr::CreateImplicit(Context));
     if (!NewD->hasAttr<CUDADeviceAttr>())
       NewD->addAttr(CUDADeviceAttr::CreateImplicit(Context));
     return;
@@ -671,7 +672,7 @@ void Sema::maybeAddCUDAHostDeviceAttrs(FunctionDecl *NewD,
 
   if (!getLangOpts().CUDAHostDeviceConstexpr || !NewD->isConstexpr() ||
       NewD->isVariadic() || NewD->hasAttr<CUDAHostAttr>() ||
-      NewD->hasAttr<CUDADeviceAttr>() || NewD->hasAttr<CUDAGlobalAttr>())
+      NewD->hasAttr<CUDADeviceAttr>() || NewD->hasAttr<ComputeKernelAttr>())
     return;
 
   // Is D a __device__ function with the same signature as NewD, ignoring CUDA
@@ -702,7 +703,7 @@ void Sema::maybeAddCUDAHostDeviceAttrs(FunctionDecl *NewD,
     return;
   }
 
-  NewD->addAttr(CUDAHostAttr::CreateImplicit(Context));
+  //NewD->addAttr(CUDAHostAttr::CreateImplicit(Context));
   NewD->addAttr(CUDADeviceAttr::CreateImplicit(Context));
 }
 
@@ -845,6 +846,7 @@ bool Sema::CheckCUDACall(SourceLocation Loc, FunctionDecl *Callee) {
 // should be diagnosed.
 void Sema::CUDACheckLambdaCapture(CXXMethodDecl *Callee,
                                   const sema::Capture &Capture) {
+#if 0 // we don't want this
   // In host compilation we only need to check lambda functions emitted on host
   // side. In such lambda functions, a reference capture is invalid only
   // if the lambda structure is populated by a device function or kernel then
@@ -868,7 +870,7 @@ void Sema::CUDACheckLambdaCapture(CXXMethodDecl *Callee,
   // to and called in a device function or kernel.
   bool CalleeIsDevice = Callee->hasAttr<CUDADeviceAttr>();
   bool CallerIsHost =
-      !Caller->hasAttr<CUDAGlobalAttr>() && !Caller->hasAttr<CUDADeviceAttr>();
+      !Caller->hasAttr<ComputeKernelAttr>() && !Caller->hasAttr<CUDADeviceAttr>();
   bool ShouldCheck = CalleeIsDevice && CallerIsHost;
   if (!ShouldCheck || !Capture.isReferenceCapture())
     return;
@@ -887,6 +889,7 @@ void Sema::CUDACheckLambdaCapture(CXXMethodDecl *Callee,
                           *this);
   }
   return;
+#endif
 }
 
 void Sema::CUDASetLambdaAttrs(CXXMethodDecl *Method) {
@@ -894,7 +897,7 @@ void Sema::CUDASetLambdaAttrs(CXXMethodDecl *Method) {
   if (Method->hasAttr<CUDAHostAttr>() || Method->hasAttr<CUDADeviceAttr>())
     return;
   Method->addAttr(CUDADeviceAttr::CreateImplicit(Context));
-  Method->addAttr(CUDAHostAttr::CreateImplicit(Context));
+  //Method->addAttr(CUDAHostAttr::CreateImplicit(Context));
 }
 
 void Sema::checkCUDATargetOverload(FunctionDecl *NewFD,
@@ -939,7 +942,7 @@ static void copyAttrIfPresent(Sema &S, FunctionDecl *FD,
 void Sema::inheritCUDATargetAttrs(FunctionDecl *FD,
                                   const FunctionTemplateDecl &TD) {
   const FunctionDecl &TemplateFD = *TD.getTemplatedDecl();
-  copyAttrIfPresent<CUDAGlobalAttr>(*this, FD, TemplateFD);
+  copyAttrIfPresent<ComputeKernelAttr>(*this, FD, TemplateFD);
   copyAttrIfPresent<CUDAHostAttr>(*this, FD, TemplateFD);
   copyAttrIfPresent<CUDADeviceAttr>(*this, FD, TemplateFD);
 }
diff --git a/clang/lib/Sema/SemaCast.cpp b/clang/lib/Sema/SemaCast.cpp
index 7ef1732496c2..42edf7ccce55 100644
--- a/clang/lib/Sema/SemaCast.cpp
+++ b/clang/lib/Sema/SemaCast.cpp
@@ -694,7 +694,7 @@ CastsAwayConstness(Sema &Self, QualType SrcType, QualType DestType,
           *CastAwayQualifiers = SrcCvrQuals - DestCvrQuals;
 
         // If we removed a cvr-qualifier, this is casting away 'constness'.
-        if (!DestCvrQuals.compatiblyIncludes(SrcCvrQuals)) {
+        if (!DestCvrQuals.compatiblyIncludes(SrcCvrQuals, false)) {
           if (TheOffendingSrcType)
             *TheOffendingSrcType = PrevUnwrappedSrcType;
           if (TheOffendingDestType)
@@ -1244,6 +1244,7 @@ void CastOperation::CheckStaticCast() {
 }
 
 static bool IsAddressSpaceConversion(QualType SrcType, QualType DestType) {
+#if 0 // we don't want this
   auto *SrcPtrType = SrcType->getAs<PointerType>();
   if (!SrcPtrType)
     return false;
@@ -1252,6 +1253,9 @@ static bool IsAddressSpaceConversion(QualType SrcType, QualType DestType) {
     return false;
   return SrcPtrType->getPointeeType().getAddressSpace() !=
          DestPtrType->getPointeeType().getAddressSpace();
+#else
+  return false;
+#endif
 }
 
 /// TryStaticCast - Check if a static cast can be performed, and do so if
@@ -2564,10 +2568,12 @@ static TryCastResult TryAddressSpaceCast(Sema &Self, ExprResult &SrcExpr,
     return TC_NotApplicable;
   auto SrcPointeeType = SrcPtrType->getPointeeType();
   auto DestPointeeType = DestPtrType->getPointeeType();
+#if 0 // we don't want this
   if (!DestPointeeType.isAddressSpaceOverlapping(SrcPointeeType)) {
     msg = diag::err_bad_cxx_cast_addr_space_mismatch;
     return TC_Failed;
   }
+#endif
   auto SrcPointeeTypeWithoutAS =
       Self.Context.removeAddrSpaceQualType(SrcPointeeType.getCanonicalType());
   auto DestPointeeTypeWithoutAS =
diff --git a/clang/lib/Sema/SemaChecking.cpp b/clang/lib/Sema/SemaChecking.cpp
index bf458f914c11..ce0ccb5d74a4 100644
--- a/clang/lib/Sema/SemaChecking.cpp
+++ b/clang/lib/Sema/SemaChecking.cpp
@@ -1083,9 +1083,11 @@ static bool SemaOpenCLBuiltinEnqueueKernel(Sema &S, CallExpr *TheCall) {
   return true;
 }
 
-/// Returns OpenCL access qual.
-static OpenCLAccessAttr *getOpenCLArgAccess(const Decl *D) {
-    return D->getAttr<OpenCLAccessAttr>();
+/// Returns libfloor (OpenCL/Metal/Vulkan) access qual.
+static ImageAccessAttr *getImageArgAccess(const Decl *D) {
+  if (D->hasAttr<ImageAccessAttr>())
+    return D->getAttr<ImageAccessAttr>();
+  return nullptr;
 }
 
 /// Returns true if pipe element type is different from the pointer.
@@ -1097,8 +1099,8 @@ static bool checkOpenCLPipeArg(Sema &S, CallExpr *Call) {
         << Call->getDirectCallee() << Arg0->getSourceRange();
     return true;
   }
-  OpenCLAccessAttr *AccessQual =
-      getOpenCLArgAccess(cast<DeclRefExpr>(Arg0)->getDecl());
+  ImageAccessAttr *AccessQual =
+      getImageArgAccess(cast<DeclRefExpr>(Arg0)->getDecl());
   // Validates the access qualifier is compatible with the call.
   // OpenCL v2.0 s6.13.16 - The access qualifiers for pipe should only be
   // read_only and write_only, and assumed to be read_only if no qualifier is
@@ -2004,7 +2006,7 @@ Sema::CheckBuiltinFunctionCall(FunctionDecl *FDecl, unsigned BuiltinID,
       auto *D = DRE->getDecl();
       if (!isa<FunctionDecl>(D) && !isa<VarDecl>(D))
         return false;
-      return D->hasAttr<CUDAGlobalAttr>() || D->hasAttr<CUDADeviceAttr>() ||
+      return D->hasAttr<ComputeKernelAttr>() || D->hasAttr<CUDADeviceAttr>() ||
              D->hasAttr<CUDAConstantAttr>() || D->hasAttr<HIPManagedAttr>();
     };
     if (!Check(TheCall)) {
diff --git a/clang/lib/Sema/SemaDecl.cpp b/clang/lib/Sema/SemaDecl.cpp
index 8228292a3153..4d72010308a5 100644
--- a/clang/lib/Sema/SemaDecl.cpp
+++ b/clang/lib/Sema/SemaDecl.cpp
@@ -2648,7 +2648,7 @@ static bool mergeDeclAttribute(Sema &S, NamedDecl *D,
                                       &S.Context.Idents.get(AA->getSpelling()));
   else if (S.getLangOpts().CUDA && isa<FunctionDecl>(D) &&
            (isa<CUDAHostAttr>(Attr) || isa<CUDADeviceAttr>(Attr) ||
-            isa<CUDAGlobalAttr>(Attr))) {
+            isa<ComputeKernelAttr>(Attr))) {
     // CUDA target attributes are part of function signature for
     // overloading purposes and must not be merged.
     return false;
@@ -2828,6 +2828,12 @@ static void checkNewAttributesAfterDef(Sema &S, Decl *New, const Decl *Old) {
       continue;
     }
 
+    if (isa<CUDADeviceAttr>(NewAttribute)) {
+      // since we're always compiling device code, but the device attr might not always be present (yet), allow this
+      ++I;
+      continue;
+    }
+
     S.Diag(NewAttribute->getLocation(),
            diag::warn_attribute_precede_definition);
     S.Diag(Def->getLocation(), diag::note_previous_definition);
@@ -6430,6 +6436,7 @@ bool Sema::inferObjCARCLifetime(ValueDecl *decl) {
 }
 
 void Sema::deduceOpenCLAddressSpace(ValueDecl *Decl) {
+#if 0 // we don't want this
   if (Decl->getType().hasAddressSpace())
     return;
   if (Decl->getType()->isDependentType())
@@ -6468,6 +6475,7 @@ void Sema::deduceOpenCLAddressSpace(ValueDecl *Decl) {
       Type = QualType(Context.getAsArrayType(Type), 0);
     Decl->setType(Type);
   }
+#endif
 }
 
 static void checkAttributesAfterMerging(Sema &S, NamedDecl &ND) {
@@ -6852,7 +6860,8 @@ static bool diagnoseOpenCLTypes(Sema &Se, VarDecl *NewVD) {
   }
 
   // OpenCL v1.0 s6.8.a.3: Pointers to functions are not allowed.
-  if (!Se.getOpenCLOptions().isAvailableOption("__cl_clang_function_pointers",
+  if (!Se.getLangOpts().CPlusPlus &&
+	  !Se.getOpenCLOptions().isAvailableOption("__cl_clang_function_pointers",
                                                Se.getLangOpts())) {
     QualType NR = R.getCanonicalType();
     while (NR->isPointerType() || NR->isMemberFunctionPointerType() ||
@@ -8001,12 +8010,27 @@ void Sema::CheckVariableDeclarationType(VarDecl *NewVD) {
 
   // OpenCL v1.2 s6.8 - The static qualifier is valid only in program
   // scope.
-  if (getLangOpts().OpenCLVersion == 120 &&
-      !getOpenCLOptions().isAvailableOption("cl_clang_storage_class_specifiers",
-                                            getLangOpts()) &&
-      NewVD->isStaticLocal()) {
-    Diag(NewVD->getLocation(), diag::err_static_function_scope);
-    NewVD->setInvalidDecl();
+  // (same for Metal/Vulkan, enabled for CUDA as well for compatibility)
+  if (NewVD->isStaticLocal() &&
+      ((getLangOpts().OpenCL &&
+        getLangOpts().OpenCLVersion >= 120 &&
+        !getOpenCLOptions().isAvailableOption("cl_clang_storage_class_specifiers", getLangOpts())) ||
+       getLangOpts().CUDA)) {
+    // however, if it is constexpr, this can be safely put into the constant address space
+    if (NewVD->isConstexpr()) {
+      // CUDA (backend) can handle this on its own
+      if (getLangOpts().OpenCL) {
+        QualType constant_T = Context.getAddrSpaceQualType(T, LangAS::opencl_constant);
+        TypeSourceInfo* constant_Tinfo = Context.getTrivialTypeSourceInfo(constant_T);
+        NewVD->setType(constant_Tinfo->getType());
+        NewVD->setTypeSourceInfo(constant_Tinfo);
+      }
+    }
+    // CUDA shared/local decls are allowed to be static, so ignore them
+    else if(!(getLangOpts().CUDA && NewVD->hasAttr<CUDASharedAttr>())) {
+      Diag(NewVD->getLocation(), diag::err_static_function_scope);
+      NewVD->setInvalidDecl();
+    }
     return;
   }
 
@@ -8036,16 +8060,46 @@ void Sema::CheckVariableDeclarationType(VarDecl *NewVD) {
       }
     }
 
-    // FIXME: Adding local AS in C++ for OpenCL might make sense.
-    if (NewVD->isFileVarDecl() || NewVD->isStaticLocal() ||
-        NewVD->hasExternalStorage()) {
-      if (!T->isSamplerT() && !T->isDependentType() &&
+    if (NewVD->isFileVarDecl() /*|| NewVD->isStaticLocal() ||
+        NewVD->hasExternalStorage()*/) {
+      if (!T->isSamplerT()) {
+        // if the variable doesn't have an address space, but is a global static const variable,
+        // automatically add the constant address space
+        if ((T.getAddressSpace() == LangAS::Default || T.getAddressSpace() == LangAS::opencl_private) &&
+            (NewVD->isStaticDataMember() || NewVD->hasGlobalStorage()) &&
+            T.isConstQualified()) {
+          QualType constant_T = Context.getAddrSpaceQualType(T, LangAS::opencl_constant);
+          TypeSourceInfo* constant_Tinfo = Context.getTrivialTypeSourceInfo(constant_T);
+          NewVD->setType(constant_Tinfo->getType());
+          NewVD->setTypeSourceInfo(constant_Tinfo);
+        } else {
+          int Scope = NewVD->isStaticLocal() | NewVD->hasExternalStorage() << 1;
+          if (!(T.getAddressSpace() == LangAS::opencl_constant ||
+                (T.getAddressSpace() == LangAS::opencl_global &&
+                 getLangOpts().OpenCLVersion >= 200))) {
+            if (getLangOpts().OpenCLVersion >= 200)
+              Diag(NewVD->getLocation(), diag::err_opencl_global_invalid_addr_space)
+                  << Scope << "global or constant";
+            else
+              Diag(NewVD->getLocation(), diag::err_opencl_global_invalid_addr_space)
+                  << Scope << "constant";
+            NewVD->setInvalidDecl();
+            return;
+          }
+        }
+      }
+    } else {
+      // TODO: does this need automatic "constant" handling?
+      // OpenCL C v2.0 s6.5.1 - Variables defined at program scope and static
+      // variables inside a function can also be declared in the global
+      // address space.
+      // FIXME: Adding local AS in C++ for OpenCL might make sense.
+      int Scope = NewVD->isStaticLocal() | NewVD->hasExternalStorage() << 1;
+      if (NewVD->isStaticLocal() &&
           !(T.getAddressSpace() == LangAS::opencl_constant ||
             (T.getAddressSpace() == LangAS::opencl_global &&
-             getOpenCLOptions().areProgramScopeVariablesSupported(
-                 getLangOpts())))) {
-        int Scope = NewVD->isStaticLocal() | NewVD->hasExternalStorage() << 1;
-        if (getOpenCLOptions().areProgramScopeVariablesSupported(getLangOpts()))
+             getLangOpts().OpenCLVersion >= 200))) {
+        if (getLangOpts().OpenCLVersion >= 200)
           Diag(NewVD->getLocation(), diag::err_opencl_global_invalid_addr_space)
               << Scope << "global or constant";
         else
@@ -8054,19 +8108,16 @@ void Sema::CheckVariableDeclarationType(VarDecl *NewVD) {
         NewVD->setInvalidDecl();
         return;
       }
-    } else {
-      if (T.getAddressSpace() == LangAS::opencl_global) {
-        Diag(NewVD->getLocation(), diag::err_opencl_function_variable)
-            << 1 /*is any function*/ << "global";
-        NewVD->setInvalidDecl();
-        return;
-      }
+
+      // OpenCL v1.1 s6.5.2 and s6.5.3 no local or constant variables
+      // in functions.
+#if 0 // everything gets inlined, so there is no need for this
       if (T.getAddressSpace() == LangAS::opencl_constant ||
           T.getAddressSpace() == LangAS::opencl_local) {
         FunctionDecl *FD = getCurFunctionDecl();
         // OpenCL v1.1 s6.5.2 and s6.5.3: no local or constant variables
         // in functions.
-        if (FD && !FD->hasAttr<OpenCLKernelAttr>()) {
+        if (FD && !FD->hasAttr<ComputeKernelAttr>()) {
           if (T.getAddressSpace() == LangAS::opencl_constant)
             Diag(NewVD->getLocation(), diag::err_opencl_function_variable)
                 << 0 /*non-kernel only*/ << "constant";
@@ -8078,7 +8129,7 @@ void Sema::CheckVariableDeclarationType(VarDecl *NewVD) {
         }
         // OpenCL v2.0 s6.5.2 and s6.5.3: local and constant variables must be
         // in the outermost scope of a kernel function.
-        if (FD && FD->hasAttr<OpenCLKernelAttr>()) {
+        if (FD && FD->hasAttr<ComputeKernelAttr>()) {
           if (!getCurScope()->isFunctionScope()) {
             if (T.getAddressSpace() == LangAS::opencl_constant)
               Diag(NewVD->getLocation(), diag::err_opencl_addrspace_scope)
@@ -8099,6 +8150,7 @@ void Sema::CheckVariableDeclarationType(VarDecl *NewVD) {
         NewVD->setInvalidDecl();
         return;
       }
+#endif
     }
   }
 
@@ -8737,7 +8789,7 @@ static bool isOpenCLSizeDependentType(ASTContext &C, QualType Ty) {
   return false;
 }
 
-static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT) {
+static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT, const bool is_metal) {
   if (PT->isDependentType())
     return InvalidKernelParam;
 
@@ -8745,13 +8797,15 @@ static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT) {
     QualType PointeeType = PT->getPointeeType();
     if (PointeeType.getAddressSpace() == LangAS::opencl_generic ||
         PointeeType.getAddressSpace() == LangAS::opencl_private ||
+        // NOTE: we do not support local address space kernel pointers!
+        PointeeType.getAddressSpace() == LangAS::opencl_local ||
         PointeeType.getAddressSpace() == LangAS::Default)
       return InvalidAddrSpacePtrKernelParam;
 
     if (PointeeType->isPointerType()) {
       // This is a pointer to pointer parameter.
       // Recursively check inner type.
-      OpenCLParamType ParamKind = getOpenCLKernelParameterType(S, PointeeType);
+      OpenCLParamType ParamKind = getOpenCLKernelParameterType(S, PointeeType, is_metal);
       if (ParamKind == InvalidAddrSpacePtrKernelParam ||
           ParamKind == InvalidKernelParam)
         return ParamKind;
@@ -8791,7 +8845,7 @@ static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT) {
   // This extension adds support for half scalar and vector types as built-in
   // types that can be used for arithmetic operations, conversions etc.
   if (!S.getOpenCLOptions().isAvailableOption("cl_khr_fp16", S.getLangOpts()) &&
-      PT->isHalfType())
+      PT->isHalfType() && !is_metal)
     return InvalidKernelParam;
 
   // Look into an array argument to check if it has a forbidden type.
@@ -8800,7 +8854,7 @@ static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT) {
     // Call ourself to check an underlying type of an array. Since the
     // getPointeeOrArrayElementType returns an innermost type which is not an
     // array, this recursive call only happens once.
-    return getOpenCLKernelParameterType(S, QualType(UnderlyingTy, 0));
+    return getOpenCLKernelParameterType(S, QualType(UnderlyingTy, 0), is_metal);
   }
 
   // C++ for OpenCL v1.0 s2.4:
@@ -8813,6 +8867,12 @@ static OpenCLParamType getOpenCLKernelParameterType(Sema &S, QualType PT) {
       !PT->isOpenCLSpecificType() && !PT.isPODType(S.Context))
     return InvalidKernelParam;
 
+  if (PT->isEventT())
+    return InvalidKernelParam;
+
+  if (PT->isReserveIDT())
+    return InvalidKernelParam;
+
   if (PT->isRecordType())
     return RecordKernelParam;
 
@@ -8823,7 +8883,8 @@ static void checkIsValidOpenCLKernelParameter(
   Sema &S,
   Declarator &D,
   ParmVarDecl *Param,
-  llvm::SmallPtrSetImpl<const Type *> &ValidTypes) {
+  llvm::SmallPtrSetImpl<const Type *> &ValidTypes,
+  const bool is_metal) {
   QualType PT = Param->getType();
 
   // Cache the valid types we encounter to avoid rechecking structs that are
@@ -8831,7 +8892,7 @@ static void checkIsValidOpenCLKernelParameter(
   if (ValidTypes.count(PT.getTypePtr()))
     return;
 
-  switch (getOpenCLKernelParameterType(S, PT)) {
+  switch (getOpenCLKernelParameterType(S, PT, is_metal)) {
   case PtrPtrKernelParam:
     // OpenCL v3.0 s6.11.a:
     // A kernel function argument cannot be declared as a pointer to a pointer
@@ -8921,6 +8982,8 @@ static void checkIsValidOpenCLKernelParameter(
       continue;
     }
 
+    // TODO: this should also check base classes
+
     // Adds everything except the original parameter declaration (which is not a
     // field itself) to the history stack.
     const RecordDecl *RD;
@@ -8942,13 +9005,17 @@ static void checkIsValidOpenCLKernelParameter(
     // Add a null marker so we know when we've gone back up a level
     VisitStack.push_back(nullptr);
 
+    // if this is an aggregate of images, all is well
+    if (RD->getTypeForDecl()->isAggregateImageType())
+      continue;
+
     for (const auto *FD : RD->fields()) {
       QualType QT = FD->getType();
 
       if (ValidTypes.count(QT.getTypePtr()))
         continue;
 
-      OpenCLParamType ParamType = getOpenCLKernelParameterType(S, QT);
+      OpenCLParamType ParamType = getOpenCLKernelParameterType(S, QT, is_metal);
       if (ParamType == ValidKernelParam)
         continue;
 
@@ -9559,6 +9626,7 @@ Sema::ActOnFunctionDeclarator(Scope *S, Declarator &D, DeclContext *DC,
   // Handle attributes.
   ProcessDeclAttributes(S, NewFD, D);
 
+#if 0 // we don't want this
   if (getLangOpts().OpenCL) {
     // OpenCL v1.1 s6.5: Using an address space qualifier in a function return
     // type declaration will generate a compilation error.
@@ -9569,6 +9637,7 @@ Sema::ActOnFunctionDeclarator(Scope *S, Declarator &D, DeclContext *DC,
       NewFD->setInvalidDecl();
     }
   }
+#endif
 
   checkTypeSupport(NewFD->getType(), D.getBeginLoc(), NewFD);
 
@@ -9957,7 +10026,7 @@ Sema::ActOnFunctionDeclarator(Scope *S, Declarator &D, DeclContext *DC,
     // -fcuda-allow-variadic-functions.
     if (!getLangOpts().CUDAAllowVariadicFunctions && NewFD->isVariadic() &&
         (NewFD->hasAttr<CUDADeviceAttr>() ||
-         NewFD->hasAttr<CUDAGlobalAttr>()) &&
+         NewFD->hasAttr<ComputeKernelAttr>()) &&
         !(II && II->isStr("printf") && NewFD->isExternC() &&
           !D.isFunctionDefinition())) {
       Diag(NewFD->getLocation(), diag::err_variadic_device_fn);
@@ -9967,14 +10036,7 @@ Sema::ActOnFunctionDeclarator(Scope *S, Declarator &D, DeclContext *DC,
   MarkUnusedFileScopedDecl(NewFD);
 
 
-
-  if (getLangOpts().OpenCL && NewFD->hasAttr<OpenCLKernelAttr>()) {
-    // OpenCL v1.2 s6.8 static is invalid for kernel functions.
-    if (SC == SC_Static) {
-      Diag(D.getIdentifierLoc(), diag::err_static_kernel);
-      D.setInvalidType();
-    }
-
+  if (getLangOpts().OpenCL && NewFD->hasAttr<ComputeKernelAttr>()) {
     // OpenCL v1.2, s6.9 -- Kernels can only have return type void.
     if (!NewFD->getReturnType()->isVoidType()) {
       SourceRange RTRange = NewFD->getReturnTypeSourceRange();
@@ -9983,10 +10045,23 @@ Sema::ActOnFunctionDeclarator(Scope *S, Declarator &D, DeclContext *DC,
                                 : FixItHint());
       D.setInvalidType();
     }
+  }
 
-    llvm::SmallPtrSet<const Type *, 16> ValidTypes;
-    for (auto Param : NewFD->parameters())
-      checkIsValidOpenCLKernelParameter(*this, D, Param, ValidTypes);
+  if (NewFD->hasAttr<ComputeKernelAttr>() ||
+      NewFD->hasAttr<GraphicsVertexShaderAttr>() ||
+      NewFD->hasAttr<GraphicsFragmentShaderAttr>()) {
+    // static is invalid for kernel/vertex/fragment functions.
+    if (SC == SC_Static) {
+      Diag(D.getIdentifierLoc(), diag::err_static_kernel);
+      D.setInvalidType();
+    }
+
+    // only check this for OpenCL/Metal/Vulkan
+    if (getLangOpts().OpenCL) {
+      llvm::SmallPtrSet<const Type *, 16> ValidTypes;
+      for (auto Param : NewFD->parameters())
+        checkIsValidOpenCLKernelParameter(*this, D, Param, ValidTypes, getLangOpts().Metal);
+	}
 
     if (getLangOpts().OpenCLCPlusPlus) {
       if (DC->isRecord()) {
@@ -11086,7 +11161,10 @@ bool Sema::CheckFunctionDeclaration(Scope *S, FunctionDecl *NewFD,
     // the function returns a UDT (class, struct, or union type) that is not C
     // compatible, and if it does, warn the user.
     // But, issue any diagnostic on the first declaration only.
-    if (Previous.empty() && NewFD->isExternC()) {
+    if (Previous.empty() && NewFD->isExternC() &&
+        // ignore this for vertex/fragment shaders
+        !NewFD->hasAttr<GraphicsVertexShaderAttr>() &&
+        !NewFD->hasAttr<GraphicsFragmentShaderAttr>()) {
       QualType R = NewFD->getReturnType();
       if (R->isIncompleteType() && !R->isVoidType())
         Diag(NewFD->getLocation(), diag::warn_return_value_udt_incomplete)
@@ -11167,9 +11245,9 @@ void Sema::CheckMain(FunctionDecl* FD, const DeclSpec& DS) {
     FD->setConstexprKind(ConstexprSpecKind::Unspecified);
   }
 
-  if (getLangOpts().OpenCL) {
+  if (getLangOpts().OpenCL || getLangOpts().CUDA) {
     Diag(FD->getLocation(), diag::err_opencl_no_main)
-        << FD->hasAttr<OpenCLKernelAttr>();
+        << FD->hasAttr<ComputeKernelAttr>();
     FD->setInvalidDecl();
     return;
   }
@@ -12422,10 +12500,12 @@ void Sema::AddInitializerToDecl(Decl *RealDecl, Expr *Init, bool DirectInit) {
     if (VDecl->isInvalidDecl()) {
       // do nothing
 
+#if 0 // we don't want this
     // OpenCL v1.2 s6.5.3: __constant locals must be constant-initialized.
     // This is true even in C++ for OpenCL.
     } else if (VDecl->getType().getAddressSpace() == LangAS::opencl_constant) {
       CheckForConstantInitializer(Init, DclT);
+#endif
 
     // Otherwise, C++ does not restrict the initializer.
     } else if (getLangOpts().CPlusPlus) {
@@ -12706,6 +12786,7 @@ void Sema::ActOnUninitializedDecl(Decl *RealDecl) {
       }
     }
 
+#if 0 // allow this for now, we need to be able to have "static const" class variables that are initialized later
     // OpenCL v1.1 s6.5.3: variables declared in the constant address space must
     // be initialized.
     if (!Var->isInvalidDecl() &&
@@ -12727,6 +12808,7 @@ void Sema::ActOnUninitializedDecl(Decl *RealDecl) {
         return;
       }
     }
+#endif
 
     if (!Var->isInvalidDecl() && RealDecl->hasAttr<LoaderUninitializedAttr>()) {
       if (Var->getStorageClass() == SC_Extern) {
@@ -13424,6 +13506,7 @@ void Sema::FinalizeDeclaration(Decl *ThisDecl) {
   if (VD->isStaticLocal())
     CheckStaticLocalForDllExport(VD);
 
+#if 0 // we don't want this
   // Perform check for initializers of device-side global variables.
   // CUDA allows empty constructors as initializers (see E.2.3.1, CUDA
   // 7.5). We must also apply the same checks to all __shared__
@@ -13431,6 +13514,7 @@ void Sema::FinalizeDeclaration(Decl *ThisDecl) {
   // constant initializers for __constant__ and __device__ variables.
   if (getLangOpts().CUDA)
     checkAllowedCUDAInitializer(VD);
+#endif
 
   // Grab the dllimport or dllexport attribute off of the VarDecl.
   const InheritableAttr *DLLAttr = getDLLAttr(VD);
@@ -13958,7 +14042,7 @@ ParmVarDecl *Sema::CheckParameter(DeclContext *DC, SourceLocation StartLoc,
   if (T.getAddressSpace() != LangAS::Default &&
       // OpenCL allows function arguments declared to be an array of a type
       // to be qualified with an address space.
-      !(getLangOpts().OpenCL &&
+      !((getLangOpts().OpenCL || getLangOpts().CUDA) &&
         (T->isArrayType() || T.getAddressSpace() == LangAS::opencl_private))) {
     Diag(NameLoc, diag::err_arg_with_address_space);
     New->setInvalidDecl();
@@ -13970,6 +14054,13 @@ ParmVarDecl *Sema::CheckParameter(DeclContext *DC, SourceLocation StartLoc,
     New->setInvalidDecl();
   }
 
+  // Passing pointer to image is invalid in OpenCL.
+  if (getLangOpts().OpenCL && T->isPointerType() &&
+      T->getPointeeType()->isImageType()) {
+    Diag(NameLoc, diag::err_pointer_to_image);
+    New->setInvalidDecl();
+  }
+
   return New;
 }
 
@@ -14009,6 +14100,28 @@ void Sema::ActOnFinishKNRParamDeclarations(Scope *S, Declarator &D,
   }
 }
 
+static void AggregateTypeCompleter(Sema& S, const CXXRecordDecl* decl) {
+	if(decl == nullptr) return;
+	
+	// make sure decl is complete
+	S.RequireCompleteType(decl->getBeginLoc(), QualType(decl->getTypeForDecl(), 0),
+						  diag::err_typecheck_decl_incomplete_type);
+	
+	// must have definition
+	if(!decl->hasDefinition()) return;
+	
+	// iterate over / recurse into all bases, and complete all their fields
+	for(const auto& base : decl->bases()) {
+		AggregateTypeCompleter(S, base.getType()->getAsCXXRecordDecl());
+	}
+	
+	// iterate over and complete all fields
+	for(const auto& field : decl->fields()) {
+		S.RequireCompleteType(field->getBeginLoc(), field->getType(),
+							  diag::err_typecheck_decl_incomplete_type);
+	}
+}
+
 Decl *
 Sema::ActOnStartOfFunctionDef(Scope *FnBodyScope, Declarator &D,
                               MultiTemplateParamsArg TemplateParameterLists,
@@ -14076,8 +14189,10 @@ ShouldWarnAboutMissingPrototype(const FunctionDecl *FD,
   if (FD->isFunctionTemplateSpecialization())
     return false;
 
-  // Don't warn for OpenCL kernels.
-  if (FD->hasAttr<OpenCLKernelAttr>())
+  // Don't warn for compute kernels, or vertex/fragment shaders.
+  if (FD->hasAttr<ComputeKernelAttr>() ||
+      FD->hasAttr<GraphicsVertexShaderAttr>() ||
+      FD->hasAttr<GraphicsFragmentShaderAttr>())
     return false;
 
   // Don't warn on explicitly deleted functions.
@@ -14330,6 +14445,31 @@ Decl *Sema::ActOnStartOfFunctionDef(Scope *FnBodyScope, Decl *D,
     }
   }
 
+  // for kernel and shader functions: make sure that function parameter types are complete,
+  // including pointer/pointee types that must always be complete as well, since their sizes
+  // and (possibly) structure need to be known later on
+  if(FD->hasAttr<ComputeKernelAttr>() ||
+     FD->hasAttr<GraphicsVertexShaderAttr>() ||
+     FD->hasAttr<GraphicsFragmentShaderAttr>()) {
+    for (const auto& Param : FD->parameters()) {
+      const auto param_type = Param->getType();
+      const CXXRecordDecl* cxx_rdecl = nullptr;
+      if(param_type->isPointerType()) {
+        const auto pointee_type = param_type->getPointeeType();
+        RequireCompleteType(Param->getLocation(), pointee_type,
+                            diag::err_typecheck_decl_incomplete_type);
+        cxx_rdecl = pointee_type->getAsCXXRecordDecl();
+      } else {
+        RequireCompleteType(Param->getLocation(), param_type,
+                            diag::err_typecheck_decl_incomplete_type);
+        cxx_rdecl = param_type->getAsCXXRecordDecl();
+      }
+
+      // if this is an aggregate, ensure that all contained types are also complete
+      AggregateTypeCompleter(*this, cxx_rdecl);
+    }
+  }
+
   // Introduce our parameters into the function scope
   for (auto Param : FD->parameters()) {
     Param->setOwningFunction(FD);
@@ -15154,8 +15294,8 @@ void Sema::AddKnownFunctionAttributes(FunctionDecl *FD) {
       if (getLangOpts().CUDAIsDevice !=
           Context.BuiltinInfo.isAuxBuiltinID(BuiltinID))
         FD->addAttr(CUDADeviceAttr::CreateImplicit(Context, FD->getLocation()));
-      else
-        FD->addAttr(CUDAHostAttr::CreateImplicit(Context, FD->getLocation()));
+      //else
+      //  FD->addAttr(CUDAHostAttr::CreateImplicit(Context, FD->getLocation()));
     }
 
     // Add known guaranteed alignment for allocation functions.
@@ -16934,7 +17074,8 @@ FieldDecl *Sema::CheckFieldDecl(DeclarationName Name, QualType T,
   if (LangOpts.OpenCL) {
     // OpenCL v1.2 s6.9b,r & OpenCL v2.0 s6.12.5 - The following types cannot be
     // used as structure or union field: image, sampler, event or block types.
-    if (T->isEventT() || T->isImageType() || T->isSamplerT() ||
+    // NOTE: image types are allowed and necessary part of aggregate images
+    if (T->isEventT() /* || T->isImageType() */ || T->isSamplerT() ||
         T->isBlockPointerType()) {
       Diag(Loc, diag::err_opencl_type_struct_or_union_field) << T;
       Record->setInvalidDecl();
@@ -16943,7 +17084,8 @@ FieldDecl *Sema::CheckFieldDecl(DeclarationName Name, QualType T,
     // OpenCL v1.2 s6.9.c: bitfields are not supported, unless Clang extension
     // is enabled.
     if (BitWidth && !getOpenCLOptions().isAvailableOption(
-                        "__cl_clang_bitfields", LangOpts)) {
+                        "__cl_clang_bitfields", LangOpts) &&
+        !getLangOpts().CPlusPlus) {
       Diag(Loc, diag::err_opencl_bitfields);
       InvalidDecl = true;
     }
diff --git a/clang/lib/Sema/SemaDeclAttr.cpp b/clang/lib/Sema/SemaDeclAttr.cpp
index 743b292d2975..c7a5262d334e 100644
--- a/clang/lib/Sema/SemaDeclAttr.cpp
+++ b/clang/lib/Sema/SemaDeclAttr.cpp
@@ -96,7 +96,7 @@ static bool hasFunctionProto(const Decl *D) {
 /// hasFunctionProto first).
 static unsigned getFunctionOrMethodNumParams(const Decl *D) {
   if (const FunctionType *FnTy = D->getFunctionType())
-    return cast<FunctionProtoType>(FnTy)->getNumParams();
+    return (hasFunctionProto(D) ? cast<FunctionProtoType>(FnTy)->getNumParams() : 0);
   if (const auto *BD = dyn_cast<BlockDecl>(D))
     return BD->getNumParams();
   return cast<ObjCMethodDecl>(D)->param_size();
@@ -378,6 +378,24 @@ static void handleSimpleAttribute(Sema &S, Decl *D,
   D->addAttr(::new (S.Context) AttrType(S.Context, CI));
 }
 
+template <typename AttrType>
+static void handleSimpleAttributeWithExclusions(Sema &S, Decl *D,
+                                                const ParsedAttr &AL) {
+  handleSimpleAttribute<AttrType>(S, D, AL);
+}
+
+/// Applies the given attribute to the Decl so long as the Decl doesn't
+/// already have one of the given incompatible attributes.
+template <typename AttrType, typename IncompatibleAttrType,
+          typename... IncompatibleAttrTypes>
+static void handleSimpleAttributeWithExclusions(Sema &S, Decl *D,
+                                                const ParsedAttr &AL) {
+  if (checkAttrMutualExclusion<IncompatibleAttrType>(S, D, AL))
+    return;
+  handleSimpleAttributeWithExclusions<AttrType, IncompatibleAttrTypes...>(S, D,
+                                                                          AL);
+}
+
 template <typename... DiagnosticArgs>
 static const Sema::SemaDiagnosticBuilder&
 appendDiagnostics(const Sema::SemaDiagnosticBuilder &Bldr) {
@@ -2748,6 +2766,166 @@ static void handleVisibilityAttr(Sema &S, Decl *D, const ParsedAttr &AL,
     D->addAttr(newAttr);
 }
 
+static void handleFloorImageDataTypeAttr(Sema &S, Decl *D, const ParsedAttr &Attr) {
+  if (!Attr.hasParsedType()) {
+    S.Diag(Attr.getLoc(), diag::err_attribute_wrong_number_arguments) << Attr << 1;
+    return;
+  }
+
+  TypeSourceInfo *ParmTSI = nullptr;
+  S.GetTypeFromParser(Attr.getTypeArg(), &ParmTSI);
+  D->addAttr(::new (S.Context) FloorImageDataTypeAttr(S.Context, Attr, ParmTSI));
+}
+
+static void handleFloorImageFlagsAttr(Sema &S, Decl *D, const ParsedAttr &Attr) {
+  if (!Attr.checkExactlyNumArgs(S, 1)) {
+    Attr.setInvalid();
+    return;
+  }
+  S.AddFloorImageFlagsAttr(Attr.getRange(), D, Attr.getArgAsExpr(0), Attr);
+}
+
+void Sema::AddFloorImageFlagsAttr(SourceRange AttrRange, Decl *D, Expr *E, const AttributeCommonInfo &CI) {
+  FloorImageFlagsAttr TmpAttr(Context, CI, E);
+  SourceLocation AttrLoc = AttrRange.getBegin();
+
+  QualType T;
+  if (ValueDecl *VD = dyn_cast<ValueDecl>(D))
+    T = VD->getType();
+  else {
+    Diag(AttrLoc, diag::err_attribute_argument_type) <<
+      &TmpAttr << AANT_ArgumentIntegerConstant;
+    return;
+  }
+
+  if (!E->isValueDependent()) {
+    Expr::EvalResult result;
+    if (!E->EvaluateAsRValue(result, Context) || !result.Val.isInt()) {
+      Diag(AttrLoc, diag::err_attribute_argument_n_type)
+        << &TmpAttr << 0u << AANT_ArgumentConstantExpr
+        << E->getSourceRange();
+      return;
+    }
+    auto& ImageFlags = result.Val.getInt();
+    if (ImageFlags.getBitWidth() != 64 || !ImageFlags.isUnsigned()) {
+      Diag(AttrLoc, diag::err_attribute_argument_invalid) <<
+        &TmpAttr << AANT_ArgumentIntegerConstant;
+      return;
+    }
+
+    auto attr = ::new (Context) FloorImageFlagsAttr(Context, CI, E);
+    attr->setEvalFlags(ImageFlags.getZExtValue());
+    D->addAttr(attr);
+    return;
+  }
+
+  // Save dependent expressions in the AST to be instantiated.
+  D->addAttr(::new (Context) FloorImageFlagsAttr(TmpAttr));
+}
+
+static void handleGraphicsFBOColorLocationAttr(Sema &S, Decl *D, const ParsedAttr &Attr) {
+  if (!Attr.checkExactlyNumArgs(S, 1)) {
+    Attr.setInvalid();
+    return;
+  }
+  S.AddGraphicsFBOColorLocationAttr(Attr.getRange(), D, Attr.getArgAsExpr(0),
+                                    Attr);
+}
+
+void Sema::AddGraphicsFBOColorLocationAttr(SourceRange AttrRange, Decl *D, Expr *E, const AttributeCommonInfo &CI) {
+  GraphicsFBOColorLocationAttr TmpAttr(Context, CI, E);
+  SourceLocation AttrLoc = AttrRange.getBegin();
+
+  QualType T;
+  if (ValueDecl *VD = dyn_cast<ValueDecl>(D))
+    T = VD->getType();
+  else {
+    Diag(AttrLoc, diag::err_attribute_argument_type) <<
+      &TmpAttr << AANT_ArgumentIntegerConstant;
+    return;
+  }
+
+  // TODO: check usage
+
+  if (!E->isValueDependent()) {
+    // TODO: might want to use/check isPotentialConstantExprUnevaluated
+
+    llvm::APSInt ColorLoc(32);
+    ExprResult ICE
+      = VerifyIntegerConstantExpression(E, &ColorLoc,
+          diag::err_expr_not_ice, AllowFoldKind::AllowFold);
+    if (ICE.isInvalid())
+      return;
+
+    // check for < 0 location
+    if (ColorLoc.isNegative()) {
+      unsigned diagID = Diags.getCustomDiagID(DiagnosticsEngine::Error, "%0");
+      Diags.Report(AttrRange.getBegin(), diagID) << "location must not be negative!";
+      return;
+    }
+
+    auto loc_attr = ::new (Context) GraphicsFBOColorLocationAttr(Context, CI, ICE.get());
+    loc_attr->setEvalLocation((unsigned int)ColorLoc.getZExtValue());
+    D->addAttr(loc_attr);
+    return;
+  }
+
+  // Save dependent expressions in the AST to be instantiated.
+  D->addAttr(::new (Context) GraphicsFBOColorLocationAttr(TmpAttr));
+}
+
+static void handleGraphicsFBODepthTypeAttr(Sema &S, Decl *D, const ParsedAttr &Attr) {
+  if (!Attr.checkExactlyNumArgs(S, 1)) {
+    Attr.setInvalid();
+    return;
+  }
+
+  GraphicsFBODepthTypeAttr::DepthQualifierType type;
+  if (Attr.isArgIdent(0)) {
+    IdentifierLoc *Ident = Attr.getArgAsIdent(0);
+    StringRef TypeString = Ident->Ident->getName();
+
+    if (!GraphicsFBODepthTypeAttr::ConvertStrToDepthQualifierType(TypeString, type)) {
+      S.Diag(Ident->Loc, diag::warn_attribute_type_not_supported)
+        << Attr << TypeString;
+      return;
+    }
+  }
+  else {
+    S.Diag(Attr.getLoc(), diag::err_attribute_argument_type) <<
+      Attr << AANT_ArgumentIdentifier;
+    return;
+  }
+
+  D->addAttr(::new (S.Context)
+             GraphicsFBODepthTypeAttr(S.Context, Attr, type));
+}
+
+static void handleRangeAttr(Sema &S, Decl *D, const ParsedAttr &Attr) {
+  if (!Attr.checkExactlyNumArgs(S, 2)) {
+    Attr.setInvalid();
+    return;
+  }
+
+  llvm::APSInt lower_bound(64), upper_bound(64);
+  ExprResult lower_res
+    = S.VerifyIntegerConstantExpression(Attr.getArgAsExpr(0), &lower_bound,
+        diag::err_expr_not_ice, clang::Sema::AllowFoldKind::AllowFold);
+  ExprResult upper_res
+    = S.VerifyIntegerConstantExpression(Attr.getArgAsExpr(1), &upper_bound,
+        diag::err_expr_not_ice, clang::Sema::AllowFoldKind::AllowFold);
+  if (lower_res.isInvalid() || upper_res.isInvalid()) return;
+
+  if ((lower_bound.isUnsigned() && lower_bound.getZExtValue() > upper_bound.getZExtValue()) ||
+	  (!lower_bound.isUnsigned() && lower_bound.getExtValue() > upper_bound.getExtValue())) {
+    unsigned diagID = S.getDiagnostics().getCustomDiagID(DiagnosticsEngine::Error, "%0");
+    S.Diag(Attr.getRange().getBegin(), diagID) << "lower bound must be lower than the upper bound";
+    return;
+  }
+
+  D->addAttr(::new (S.Context) RetRangeAttr(S.Context, Attr, lower_res.get(), upper_res.get()));
+}
+
 static void handleObjCDirectAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   // objc_direct cannot be set on methods declared in the context of a protocol
   if (isa<ObjCProtocolDecl>(D->getDeclContext())) {
@@ -4564,87 +4742,6 @@ static void handleOptimizeNoneAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
     D->addAttr(Optnone);
 }
 
-static void handleConstantAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  const auto *VD = cast<VarDecl>(D);
-  if (VD->hasLocalStorage()) {
-    S.Diag(AL.getLoc(), diag::err_cuda_nonstatic_constdev);
-    return;
-  }
-  // constexpr variable may already get an implicit constant attr, which should
-  // be replaced by the explicit constant attr.
-  if (auto *A = D->getAttr<CUDAConstantAttr>()) {
-    if (!A->isImplicit())
-      return;
-    D->dropAttr<CUDAConstantAttr>();
-  }
-  D->addAttr(::new (S.Context) CUDAConstantAttr(S.Context, AL));
-}
-
-static void handleSharedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  const auto *VD = cast<VarDecl>(D);
-  // extern __shared__ is only allowed on arrays with no length (e.g.
-  // "int x[]").
-  if (!S.getLangOpts().GPURelocatableDeviceCode && VD->hasExternalStorage() &&
-      !isa<IncompleteArrayType>(VD->getType())) {
-    S.Diag(AL.getLoc(), diag::err_cuda_extern_shared) << VD;
-    return;
-  }
-  if (S.getLangOpts().CUDA && VD->hasLocalStorage() &&
-      S.CUDADiagIfHostCode(AL.getLoc(), diag::err_cuda_host_shared)
-          << S.CurrentCUDATarget())
-    return;
-  D->addAttr(::new (S.Context) CUDASharedAttr(S.Context, AL));
-}
-
-static void handleGlobalAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  const auto *FD = cast<FunctionDecl>(D);
-  if (!FD->getReturnType()->isVoidType() &&
-      !FD->getReturnType()->getAs<AutoType>() &&
-      !FD->getReturnType()->isInstantiationDependentType()) {
-    SourceRange RTRange = FD->getReturnTypeSourceRange();
-    S.Diag(FD->getTypeSpecStartLoc(), diag::err_kern_type_not_void_return)
-        << FD->getType()
-        << (RTRange.isValid() ? FixItHint::CreateReplacement(RTRange, "void")
-                              : FixItHint());
-    return;
-  }
-  if (const auto *Method = dyn_cast<CXXMethodDecl>(FD)) {
-    if (Method->isInstance()) {
-      S.Diag(Method->getBeginLoc(), diag::err_kern_is_nonstatic_method)
-          << Method;
-      return;
-    }
-    S.Diag(Method->getBeginLoc(), diag::warn_kern_is_method) << Method;
-  }
-  // Only warn for "inline" when compiling for host, to cut down on noise.
-  if (FD->isInlineSpecified() && !S.getLangOpts().CUDAIsDevice)
-    S.Diag(FD->getBeginLoc(), diag::warn_kern_is_inline) << FD;
-
-  D->addAttr(::new (S.Context) CUDAGlobalAttr(S.Context, AL));
-  // In host compilation the kernel is emitted as a stub function, which is
-  // a helper function for launching the kernel. The instructions in the helper
-  // function has nothing to do with the source code of the kernel. Do not emit
-  // debug info for the stub function to avoid confusing the debugger.
-  if (S.LangOpts.HIP && !S.LangOpts.CUDAIsDevice)
-    D->addAttr(NoDebugAttr::CreateImplicit(S.Context));
-}
-
-static void handleDeviceAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  if (const auto *VD = dyn_cast<VarDecl>(D)) {
-    if (VD->hasLocalStorage()) {
-      S.Diag(AL.getLoc(), diag::err_cuda_nonstatic_constdev);
-      return;
-    }
-  }
-
-  if (auto *A = D->getAttr<CUDADeviceAttr>()) {
-    if (!A->isImplicit())
-      return;
-    D->dropAttr<CUDADeviceAttr>();
-  }
-  D->addAttr(::new (S.Context) CUDADeviceAttr(S.Context, AL));
-}
-
 static void handleManagedAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   if (const auto *VD = dyn_cast<VarDecl>(D)) {
     if (VD->hasLocalStorage()) {
@@ -4919,6 +5016,9 @@ bool Sema::CheckCallingConvAttr(const ParsedAttr &Attrs, CallingConv &CC,
   case ParsedAttr::AT_PreserveAll:
     CC = CC_PreserveAll;
     break;
+  case ParsedAttr::AT_GraphicsVertexShader: CC = CC_FloorVertex; break;
+  case ParsedAttr::AT_GraphicsFragmentShader: CC = CC_FloorFragment; break;
+  case ParsedAttr::AT_ComputeKernel: CC = CC_FloorKernel; break;
   default: llvm_unreachable("unexpected attribute kind");
   }
 
@@ -7544,53 +7644,6 @@ static void handleOpenCLNoSVMAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
         << AL << S.LangOpts.getOpenCLVersionString();
 }
 
-static void handleOpenCLAccessAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
-  if (D->isInvalidDecl())
-    return;
-
-  // Check if there is only one access qualifier.
-  if (D->hasAttr<OpenCLAccessAttr>()) {
-    if (D->getAttr<OpenCLAccessAttr>()->getSemanticSpelling() ==
-        AL.getSemanticSpelling()) {
-      S.Diag(AL.getLoc(), diag::warn_duplicate_declspec)
-          << AL.getAttrName()->getName() << AL.getRange();
-    } else {
-      S.Diag(AL.getLoc(), diag::err_opencl_multiple_access_qualifiers)
-          << D->getSourceRange();
-      D->setInvalidDecl(true);
-      return;
-    }
-  }
-
-  // OpenCL v2.0 s6.6 - read_write can be used for image types to specify that
-  // an image object can be read and written. OpenCL v2.0 s6.13.6 - A kernel
-  // cannot read from and write to the same pipe object. Using the read_write
-  // (or __read_write) qualifier with the pipe qualifier is a compilation error.
-  // OpenCL v3.0 s6.8 - For OpenCL C 2.0, or with the
-  // __opencl_c_read_write_images feature, image objects specified as arguments
-  // to a kernel can additionally be declared to be read-write.
-  // C++ for OpenCL 1.0 inherits rule from OpenCL C v2.0.
-  // C++ for OpenCL 2021 inherits rule from OpenCL C v3.0.
-  if (const auto *PDecl = dyn_cast<ParmVarDecl>(D)) {
-    const Type *DeclTy = PDecl->getType().getCanonicalType().getTypePtr();
-    if (AL.getAttrName()->getName().contains("read_write")) {
-      bool ReadWriteImagesUnsupported =
-          (S.getLangOpts().getOpenCLCompatibleVersion() < 200) ||
-          (S.getLangOpts().getOpenCLCompatibleVersion() == 300 &&
-           !S.getOpenCLOptions().isSupported("__opencl_c_read_write_images",
-                                             S.getLangOpts()));
-      if (ReadWriteImagesUnsupported || DeclTy->isPipeType()) {
-        S.Diag(AL.getLoc(), diag::err_opencl_invalid_read_write)
-            << AL << PDecl->getType() << DeclTy->isImageType();
-        D->setInvalidDecl(true);
-        return;
-      }
-    }
-  }
-
-  D->addAttr(::new (S.Context) OpenCLAccessAttr(S.Context, AL));
-}
-
 static void handleSYCLKernelAttr(Sema &S, Decl *D, const ParsedAttr &AL) {
   // The 'sycl_kernel' attribute applies only to function templates.
   const auto *FD = cast<FunctionDecl>(D);
@@ -8009,7 +8062,7 @@ static void ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D,
     handleCommonAttr(S, D, AL);
     break;
   case ParsedAttr::AT_CUDAConstant:
-    handleConstantAttr(S, D, AL);
+    handleSimpleAttributeWithExclusions<CUDAConstantAttr, CUDASharedAttr>(S, D, AL);
     break;
   case ParsedAttr::AT_PassObjectSize:
     handlePassObjectSizeAttr(S, D, AL);
@@ -8065,11 +8118,8 @@ static void ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D,
   case ParsedAttr::AT_CalledOnce:
     handleCalledOnceAttr(S, D, AL);
     break;
-  case ParsedAttr::AT_CUDAGlobal:
-    handleGlobalAttr(S, D, AL);
-    break;
   case ParsedAttr::AT_CUDADevice:
-    handleDeviceAttr(S, D, AL);
+    handleSimpleAttributeWithExclusions<CUDADeviceAttr, ComputeKernelAttr>(S, D, AL);
     break;
   case ParsedAttr::AT_HIPManaged:
     handleManagedAttr(S, D, AL);
@@ -8121,7 +8171,7 @@ static void ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D,
       handleSimpleAttribute<NoThrowAttr>(S, D, AL);
     break;
   case ParsedAttr::AT_CUDAShared:
-    handleSharedAttr(S, D, AL);
+    handleSimpleAttributeWithExclusions<CUDASharedAttr, CUDAConstantAttr>(S, D, AL);
     break;
   case ParsedAttr::AT_VecReturn:
     handleVecReturnAttr(S, D, AL);
@@ -8271,6 +8321,48 @@ static void ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D,
   case ParsedAttr::AT_Sentinel:
     handleSentinelAttr(S, D, AL);
     break;
+  case ParsedAttr::AT_ComputeKernel:
+    handleSimpleAttribute<ComputeKernelAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsVertexShader:
+    handleSimpleAttribute<GraphicsVertexShaderAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsFragmentShader:
+    handleSimpleAttribute<GraphicsFragmentShaderAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_RetRange:
+    handleRangeAttr(S, D, AL);
+    break;
+  case ParsedAttr::AT_ImageAccess:
+    handleSimpleAttribute<ImageAccessAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_FloorArgBuffer:
+    handleSimpleAttribute<FloorArgBufferAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_FloorImageDataType:
+    handleFloorImageDataTypeAttr(S, D, AL);
+    break;
+  case ParsedAttr::AT_FloorImageFlags:
+    handleFloorImageFlagsAttr(S, D, AL);
+    break;
+  case ParsedAttr::AT_VectorCompat:
+    handleSimpleAttribute<VectorCompatAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsFBOColorLocation:
+    handleGraphicsFBOColorLocationAttr(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsFBODepthType:
+    handleGraphicsFBODepthTypeAttr(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsVertexPosition:
+    handleSimpleAttribute<GraphicsVertexPositionAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsPointSize:
+    handleSimpleAttribute<GraphicsPointSizeAttr>(S, D, AL);
+    break;
+  case ParsedAttr::AT_GraphicsStageInput:
+    handleSimpleAttribute<GraphicsStageInputAttr>(S, D, AL);
+    break;
   case ParsedAttr::AT_Cleanup:
     handleCleanupAttr(S, D, AL);
     break;
@@ -8305,9 +8397,6 @@ static void ProcessDeclAttribute(Sema &S, Scope *scope, Decl *D,
   case ParsedAttr::AT_Pointer:
     handleLifetimeCategoryAttr(S, D, AL);
     break;
-  case ParsedAttr::AT_OpenCLAccess:
-    handleOpenCLAccessAttr(S, D, AL);
-    break;
   case ParsedAttr::AT_OpenCLNoSVM:
     handleOpenCLNoSVMAttr(S, D, AL);
     break;
@@ -8555,7 +8644,7 @@ void Sema::ProcessDeclAttributeList(Scope *S, Decl *D,
   // good to have a way to specify "these attributes must appear as a group",
   // for these. Additionally, it would be good to have a way to specify "these
   // attribute must never appear as a group" for attributes like cold and hot.
-  if (!D->hasAttr<OpenCLKernelAttr>()) {
+  if (!D->hasAttr<ComputeKernelAttr>()) {
     // These attributes cannot be applied to a non-kernel function.
     if (const auto *A = D->getAttr<ReqdWorkGroupSizeAttr>()) {
       // FIXME: This emits a different error message than
@@ -8571,7 +8660,7 @@ void Sema::ProcessDeclAttributeList(Scope *S, Decl *D,
     } else if (const auto *A = D->getAttr<OpenCLIntelReqdSubGroupSizeAttr>()) {
       Diag(D->getLocation(), diag::err_opencl_kernel_attr) << A;
       D->setInvalidDecl();
-    } else if (!D->hasAttr<CUDAGlobalAttr>()) {
+    } else if (!D->hasAttr<ComputeKernelAttr>()) {
       if (const auto *A = D->getAttr<AMDGPUFlatWorkGroupSizeAttr>()) {
         Diag(D->getLocation(), diag::err_attribute_wrong_decl_type)
             << A << ExpectedKernelFunction;
diff --git a/clang/lib/Sema/SemaDeclCXX.cpp b/clang/lib/Sema/SemaDeclCXX.cpp
index 05d146cb2494..76b08b2919f8 100644
--- a/clang/lib/Sema/SemaDeclCXX.cpp
+++ b/clang/lib/Sema/SemaDeclCXX.cpp
@@ -7416,7 +7416,9 @@ bool Sema::CheckExplicitlyDefaultedSpecialMember(CXXMethodDecl *MD,
     ReturnType = Type->getReturnType();
 
     QualType DeclType = Context.getTypeDeclType(RD);
+#if 0 // we don't want this
     DeclType = Context.getAddrSpaceQualType(DeclType, MD->getMethodQualifiers().getAddressSpace());
+#endif
     QualType ExpectedReturnType = Context.getLValueReferenceType(DeclType);
 
     if (!Context.hasSameType(ReturnType, ExpectedReturnType)) {
@@ -13250,10 +13252,12 @@ void Sema::setupImplicitSpecialMemberType(CXXMethodDecl *SpecialMem,
   // Build an exception specification pointing back at this constructor.
   FunctionProtoType::ExtProtoInfo EPI = getImplicitMethodEPI(*this, SpecialMem);
 
+#if 0 // we don't want this
   LangAS AS = getDefaultCXXMethodAddrSpace();
   if (AS != LangAS::Default) {
     EPI.TypeQuals.addAddressSpace(AS);
   }
+#endif
 
   auto QT = Context.getFunctionType(ResultTy, Args, EPI);
   SpecialMem->setType(QT);
@@ -14196,9 +14200,11 @@ CXXMethodDecl *Sema::DeclareImplicitCopyAssignment(CXXRecordDecl *ClassDecl) {
     return nullptr;
 
   QualType ArgType = Context.getTypeDeclType(ClassDecl);
+#if 0 // we don't want this
   LangAS AS = getDefaultCXXMethodAddrSpace();
   if (AS != LangAS::Default)
     ArgType = Context.getAddrSpaceQualType(ArgType, AS);
+#endif
   QualType RetType = Context.getLValueReferenceType(ArgType);
   bool Const = ClassDecl->implicitCopyAssignmentHasConstParam();
   if (Const)
@@ -14535,9 +14541,11 @@ CXXMethodDecl *Sema::DeclareImplicitMoveAssignment(CXXRecordDecl *ClassDecl) {
   // constructor rules.
 
   QualType ArgType = Context.getTypeDeclType(ClassDecl);
+#if 0 // we don't want this
   LangAS AS = getDefaultCXXMethodAddrSpace();
   if (AS != LangAS::Default)
     ArgType = Context.getAddrSpaceQualType(ArgType, AS);
+#endif
   QualType RetType = Context.getLValueReferenceType(ArgType);
   ArgType = Context.getRValueReferenceType(ArgType);
 
@@ -14913,9 +14921,11 @@ CXXConstructorDecl *Sema::DeclareImplicitCopyConstructor(
   if (Const)
     ArgType = ArgType.withConst();
 
+#if 0 // we don't want this
   LangAS AS = getDefaultCXXMethodAddrSpace();
   if (AS != LangAS::Default)
     ArgType = Context.getAddrSpaceQualType(ArgType, AS);
+#endif
 
   ArgType = Context.getLValueReferenceType(ArgType);
 
@@ -15053,9 +15063,11 @@ CXXConstructorDecl *Sema::DeclareImplicitMoveConstructor(
   QualType ClassType = Context.getTypeDeclType(ClassDecl);
 
   QualType ArgType = ClassType;
+#if 0 // we don't want this
   LangAS AS = getDefaultCXXMethodAddrSpace();
   if (AS != LangAS::Default)
     ArgType = Context.getAddrSpaceQualType(ClassType, AS);
+#endif
   ArgType = Context.getRValueReferenceType(ArgType);
 
   bool Constexpr = defaultedSpecialMemberIsConstexpr(*this, ClassDecl,
@@ -15614,7 +15626,7 @@ CheckOperatorNewDeleteTypes(Sema &SemaRef, const FunctionDecl *FnDecl,
   QualType ResultType =
       FnDecl->getType()->castAs<FunctionType>()->getReturnType();
 
-  if (SemaRef.getLangOpts().OpenCLCPlusPlus) {
+  if (SemaRef.getLangOpts().OpenCLCPlusPlus || SemaRef.getLangOpts().CPlusPlus) {
     // The operator is valid on any address space for OpenCL.
     // Drop address space from actual and expected result types.
     if (const auto *PtrTy = ResultType->getAs<PointerType>())
@@ -15649,7 +15661,7 @@ CheckOperatorNewDeleteTypes(Sema &SemaRef, const FunctionDecl *FnDecl,
       << FnDecl->getDeclName();
 
   QualType FirstParamType = FnDecl->getParamDecl(0)->getType();
-  if (SemaRef.getLangOpts().OpenCLCPlusPlus) {
+  if (SemaRef.getLangOpts().OpenCLCPlusPlus || SemaRef.getLangOpts().CPlusPlus) {
     // The operator is valid on any address space for OpenCL.
     // Drop address space from actual and expected first parameter types.
     if (const auto *PtrTy =
diff --git a/clang/lib/Sema/SemaExpr.cpp b/clang/lib/Sema/SemaExpr.cpp
index 2109ff25fe7a..fdcf8fcb5c06 100644
--- a/clang/lib/Sema/SemaExpr.cpp
+++ b/clang/lib/Sema/SemaExpr.cpp
@@ -3830,14 +3830,9 @@ ExprResult Sema::ActOnNumericConstant(const Token &Tok, Scope *UDLScope) {
                                               Tok.getLocation(), scale);
   } else if (Literal.isFloatingLiteral()) {
     QualType Ty;
-    if (Literal.isHalf){
-      if (getOpenCLOptions().isAvailableOption("cl_khr_fp16", getLangOpts()))
-        Ty = Context.HalfTy;
-      else {
-        Diag(Tok.getLocation(), diag::err_half_const_requires_fp16);
-        return ExprError();
-      }
-    } else if (Literal.isFloat)
+    if (Literal.isHalf)
+      Ty = Context.HalfTy;
+    else if (Literal.isFloat)
       Ty = Context.FloatTy;
     else if (Literal.isLong)
       Ty = Context.LongDoubleTy;
@@ -4081,8 +4076,8 @@ static bool CheckExtensionTraitOperandType(Sema &S, QualType T,
   // Allow sizeof(void)/alignof(void) as an extension, unless in OpenCL where
   // this is an error (OpenCL v1.1 s6.3.k)
   if (T->isVoidType()) {
-    unsigned DiagID = S.LangOpts.OpenCL ? diag::err_opencl_sizeof_alignof_type
-                                        : diag::ext_sizeof_alignof_void_type;
+    unsigned DiagID = S.LangOpts.OpenCL && !S.LangOpts.CPlusPlus ?
+      diag::err_opencl_sizeof_alignof_type : diag::ext_sizeof_alignof_void_type;
     S.Diag(Loc, DiagID) << getTraitSpelling(TraitKind) << ArgRange;
     return false;
   }
@@ -6816,7 +6811,7 @@ ExprResult Sema::BuildResolvedCallExpr(Expr *Fn, NamedDecl *NDecl,
   if (getLangOpts().CUDA) {
     if (Config) {
       // CUDA: Kernel calls must be to global functions
-      if (FDecl && !FDecl->hasAttr<CUDAGlobalAttr>())
+      if (FDecl && !FDecl->hasAttr<ComputeKernelAttr>())
         return ExprError(Diag(LParenLoc,diag::err_kern_call_not_global_function)
             << FDecl << Fn->getSourceRange());
 
@@ -6828,7 +6823,7 @@ ExprResult Sema::BuildResolvedCallExpr(Expr *Fn, NamedDecl *NDecl,
             << Fn->getType() << Fn->getSourceRange());
     } else {
       // CUDA: Calls to global functions must be configured
-      if (FDecl && FDecl->hasAttr<CUDAGlobalAttr>())
+      if (FDecl && FDecl->hasAttr<ComputeKernelAttr>())
         return ExprError(Diag(LParenLoc, diag::err_global_call_not_config)
             << FDecl << Fn->getSourceRange());
     }
@@ -14160,7 +14155,8 @@ ExprResult Sema::CreateBuiltinBinOp(SourceLocation OpLoc,
     // OpenCL special types - image, sampler, pipe, and blocks are to be used
     // only with a builtin functions and therefore should be disallowed here.
     if (LHSTy->isImageType() || RHSTy->isImageType() ||
-        LHSTy->isSamplerT() || RHSTy->isSamplerT() ||
+        // NOTE: sampler operations are allowed (these are integers)
+        //LHSTy->isSamplerT() || RHSTy->isSamplerT() ||
         LHSTy->isPipeType() || RHSTy->isPipeType() ||
         LHSTy->isBlockPointerType() || RHSTy->isBlockPointerType()) {
       ResultTy = InvalidOperands(OpLoc, LHS, RHS);
diff --git a/clang/lib/Sema/SemaExprCXX.cpp b/clang/lib/Sema/SemaExprCXX.cpp
index 571b790b31ea..aa39187dfb06 100644
--- a/clang/lib/Sema/SemaExprCXX.cpp
+++ b/clang/lib/Sema/SemaExprCXX.cpp
@@ -2403,7 +2403,8 @@ bool Sema::CheckAllocatedType(QualType AllocType, SourceLocation Loc,
     return Diag(Loc, diag::err_variably_modified_new_type)
              << AllocType;
   else if (AllocType.getAddressSpace() != LangAS::Default &&
-           !getLangOpts().OpenCLCPlusPlus)
+           !getLangOpts().OpenCLCPlusPlus &&
+           !(getLangOpts().OpenCL && getLangOpts().CPlusPlus))
     return Diag(Loc, diag::err_address_space_qualified_new)
       << AllocType.getUnqualifiedType()
       << AllocType.getQualifiers().getAddressSpaceAttributePrintValue();
@@ -3092,7 +3093,9 @@ void Sema::DeclareGlobalAllocationFunction(DeclarationName Name,
   else {
     // Host and device get their own declaration so each can be
     // defined or re-declared independently.
-    CreateAllocationFunctionDecl(CUDAHostAttr::CreateImplicit(Context));
+    if (!LangOpts.CUDAIsDevice) { // don't define if device-only
+      CreateAllocationFunctionDecl(CUDAHostAttr::CreateImplicit(Context));
+    }
     CreateAllocationFunctionDecl(CUDADeviceAttr::CreateImplicit(Context));
   }
 }
@@ -3554,7 +3557,8 @@ Sema::ActOnCXXDelete(SourceLocation StartLoc, bool UseGlobal,
     QualType PointeeElem = Context.getBaseElementType(Pointee);
 
     if (Pointee.getAddressSpace() != LangAS::Default &&
-        !getLangOpts().OpenCLCPlusPlus)
+        !getLangOpts().OpenCLCPlusPlus &&
+        !(getLangOpts().OpenCL && getLangOpts().CPlusPlus))
       return Diag(Ex.get()->getBeginLoc(),
                   diag::err_address_space_qualified_delete)
              << Pointee.getUnqualifiedType()
@@ -4556,12 +4560,23 @@ Sema::PerformImplicitConversion(Expr *From, QualType ToType,
   }
 
   case ICK_Zero_Event_Conversion:
-  case ICK_Zero_Queue_Conversion:
     From = ImpCastExprToType(From, ToType,
                              CK_ZeroToOCLOpaqueType,
                              From->getValueKind()).get();
     break;
 
+  case ICK_Zero_Queue_Conversion:
+    From = ImpCastExprToType(From, ToType,
+                             CK_ZeroToOCLQueue,
+                             From->getValueKind()).get();
+    break;
+
+  case ICK_Int_Sampler_Conversion:
+    From = ImpCastExprToType(From, ToType,
+                             CK_IntToOCLSampler,
+                             From->getValueKind()).get();
+    break;
+
   case ICK_Lvalue_To_Rvalue:
   case ICK_Array_To_Pointer:
   case ICK_Function_To_Pointer:
@@ -4590,6 +4605,7 @@ Sema::PerformImplicitConversion(Expr *From, QualType ToType,
     break;
 
   case ICK_Qualification: {
+#if 0 // we don't want this
     ExprValueKind VK = From->getValueKind();
     CastKind CK = CK_NoOp;
 
@@ -4606,6 +4622,11 @@ Sema::PerformImplicitConversion(Expr *From, QualType ToType,
     From = ImpCastExprToType(From, ToType.getNonLValueExprType(Context), CK, VK,
                              /*BasePath=*/nullptr, CCK)
                .get();
+#else // old behavior
+    ExprValueKind VK = From->getValueKind();
+    From = ImpCastExprToType(From, ToType.getNonLValueExprType(Context),
+                             CK_NoOp, VK, /*BasePath=*/nullptr, CCK).get();
+#endif
 
     if (SCS.DeprecatedStringLiteralToCharPtr &&
         !getLangOpts().WritableStrings) {
diff --git a/clang/lib/Sema/SemaExprMember.cpp b/clang/lib/Sema/SemaExprMember.cpp
index 83006f9d804a..3188c52e4d79 100644
--- a/clang/lib/Sema/SemaExprMember.cpp
+++ b/clang/lib/Sema/SemaExprMember.cpp
@@ -373,6 +373,7 @@ CheckExtVectorComponent(Sema &S, QualType baseType, ExprValueKind &VK,
     if (HexSwizzle)
       compStr++;
 
+    // TODO: add SPIR pass to scalarize all non-{1,2,3,4,8,16} vector uses
     while (*compStr) {
       if (!vecType->isAccessorWithinNumElements(*compStr++, HexSwizzle)) {
         S.Diag(OpLoc, diag::err_ext_vector_component_exceeds_length)
@@ -1803,7 +1804,8 @@ Sema::BuildFieldReferenceExpr(Expr *BaseExpr, bool IsArrow,
     Qualifiers MemberQuals =
         Context.getCanonicalType(MemberType).getQualifiers();
 
-    assert(!MemberQuals.hasAddressSpace());
+    // this should very well be possible
+    //assert(!MemberQuals.hasAddressSpace());
 
     Qualifiers Combined = BaseQuals + MemberQuals;
     if (Combined != MemberQuals)
diff --git a/clang/lib/Sema/SemaInit.cpp b/clang/lib/Sema/SemaInit.cpp
index 75686e294704..61c7e8964cea 100644
--- a/clang/lib/Sema/SemaInit.cpp
+++ b/clang/lib/Sema/SemaInit.cpp
@@ -3491,6 +3491,8 @@ void InitializationSequence::Step::Destroy() {
   case SK_StdInitializerListConstructorCall:
   case SK_OCLSamplerInit:
   case SK_OCLZeroOpaqueType:
+  case SK_OCLZeroEvent:
+  case SK_OCLZeroQueue:
     break;
 
   case SK_ConversionSequence:
@@ -3785,6 +3787,13 @@ void InitializationSequence::AddOCLZeroOpaqueTypeStep(QualType T) {
   Steps.push_back(S);
 }
 
+void InitializationSequence::AddOCLZeroQueueStep(QualType T) {
+  Step S;
+  S.Kind = SK_OCLZeroQueue;
+  S.Type = T;
+  Steps.push_back(S);
+}
+
 void InitializationSequence::RewrapReferenceInitList(QualType T,
                                                      InitListExpr *Syntactic) {
   assert(Syntactic->getNumInits() == 1 &&
@@ -4319,6 +4328,7 @@ static void TryReferenceListInitialization(Sema &S,
   }
   // Perform address space compatibility check.
   QualType cv1T1IgnoreAS = cv1T1;
+#if 0 // we don't want this
   if (T1Quals.hasAddressSpace()) {
     Qualifiers T2Quals;
     (void)S.Context.getUnqualifiedArrayType(InitList->getType(), T2Quals);
@@ -4332,6 +4342,7 @@ static void TryReferenceListInitialization(Sema &S,
     cv1T1IgnoreAS =
         S.Context.getQualifiedType(T1, T1Quals.withoutAddressSpace());
   }
+#endif
   // Not reference-related. Create a temporary and bind to that.
   InitializedEntity TempEntity =
       InitializedEntity::InitializeTemporary(cv1T1IgnoreAS);
@@ -4873,8 +4884,9 @@ static void TryReferenceInitializationCore(Sema &S,
   //       shall be an rvalue reference.
   //       For address spaces, we interpret this to mean that an addr space
   //       of a reference "cv1 T1" is a superset of addr space of "cv2 T2".
-  if (isLValueRef && !(T1Quals.hasConst() && !T1Quals.hasVolatile() &&
-                       T1Quals.isAddressSpaceSupersetOf(T2Quals))) {
+  // NOTE: we don't want the address space test here
+  if (isLValueRef && !(T1Quals.hasConst() && !T1Quals.hasVolatile() /*&&
+                       T1Quals.isAddressSpaceSupersetOf(T2Quals)*/)) {
     if (S.Context.getCanonicalType(T2) == S.Context.OverloadTy)
       Sequence.SetFailed(InitializationSequence::FK_AddressOfOverloadFailed);
     else if (ConvOvlResult && !Sequence.getFailedCandidateSet().empty())
@@ -4993,6 +5005,7 @@ static void TryReferenceInitializationCore(Sema &S,
   //         where "cv1 T1" is reference-compatible with "cv3 T3",
   //
   // DR1287 removes the "implicitly" here.
+  bool isOpenCLASRef = false;
   if (T2->isRecordType()) {
     if (RefRelationship == Sema::Ref_Incompatible) {
       ConvOvlResult = TryRefInitWithConversionFunction(
@@ -5013,8 +5026,15 @@ static void TryReferenceInitializationCore(Sema &S,
       return;
     }
 
-    Sequence.SetFailed(InitializationSequence::FK_ReferenceInitDropsQualifiers);
-    return;
+    if(S.getLangOpts().OpenCL &&
+       ((cv1T1.getAddressSpace() == LangAS::Default && cv2T2.getAddressSpace() != LangAS::Default) ||
+        (cv1T1.getAddressSpace() != LangAS::Default && cv2T2.getAddressSpace() == LangAS::Default))) {
+      isOpenCLASRef = true;
+    }
+    if(!isOpenCLASRef) {
+      Sequence.SetFailed(InitializationSequence::FK_ReferenceInitDropsQualifiers);
+      return;
+    }
   }
 
   //      - Otherwise, a temporary of type "cv1 T1" is created and initialized
@@ -5065,9 +5085,10 @@ static void TryReferenceInitializationCore(Sema &S,
   //        than, cv2; otherwise, the program is ill-formed.
   unsigned T1CVRQuals = T1Quals.getCVRQualifiers();
   unsigned T2CVRQuals = T2Quals.getCVRQualifiers();
+  // NOTE: we don't want the address space check here
   if (RefRelationship == Sema::Ref_Related &&
-      ((T1CVRQuals | T2CVRQuals) != T1CVRQuals ||
-       !T1Quals.isAddressSpaceSupersetOf(T2Quals))) {
+      ((T1CVRQuals | T2CVRQuals) != T1CVRQuals /*||
+       !T1Quals.isAddressSpaceSupersetOf(T2Quals)*/)) {
     Sequence.SetFailed(InitializationSequence::FK_ReferenceInitDropsQualifiers);
     return;
   }
@@ -5083,6 +5104,7 @@ static void TryReferenceInitializationCore(Sema &S,
 
   Sequence.AddReferenceBindingStep(cv1T1IgnoreAS, /*BindingTemporary=*/true);
 
+#if 0 // we don't want this
   if (T1Quals.hasAddressSpace()) {
     if (!Qualifiers::isAddressSpaceSupersetOf(T1Quals.getAddressSpace(),
                                               LangAS::Default)) {
@@ -5093,6 +5115,7 @@ static void TryReferenceInitializationCore(Sema &S,
     Sequence.AddQualificationConversionStep(cv1T1, isLValueRef ? VK_LValue
                                                                : VK_XValue);
   }
+#endif
 }
 
 /// Attempt character array initialization from a string literal
@@ -5635,6 +5658,20 @@ static bool TryOCLZeroOpaqueTypeInitialization(Sema &S,
   return false;
 }
 
+static bool TryOCLZeroQueueInitialization(Sema &S,
+                                          InitializationSequence &Sequence,
+                                          QualType DestType,
+                                          Expr *Initializer) {
+  if (!S.getLangOpts().OpenCL || S.getLangOpts().OpenCLVersion < 200 ||
+      !DestType->isQueueT() ||
+      !Initializer->isIntegerConstantExpr(S.getASTContext()) ||
+      (Initializer->EvaluateKnownConstInt(S.getASTContext()) != 0))
+    return false;
+
+  Sequence.AddOCLZeroQueueStep(DestType);
+  return true;
+}
+
 InitializationSequence::InitializationSequence(
     Sema &S, const InitializedEntity &Entity, const InitializationKind &Kind,
     MultiExprArg Args, bool TopLevelOfInitList, bool TreatUnavailableAsInvalid)
@@ -5903,10 +5940,21 @@ void InitializationSequence::InitializeFrom(Sema &S,
         tryObjCWritebackConversion(S, *this, Entity, Initializer)) {
       return;
     }
+  }
+
+  // need to try these when using C++ with OpenCL
+  if (!S.getLangOpts().CPlusPlus || S.getLangOpts().OpenCL) {
+    if (TryOCLSamplerInitialization(S, *this, DestType, Initializer))
+      return;
 
     if (TryOCLZeroOpaqueTypeInitialization(S, *this, DestType, Initializer))
       return;
 
+    if (TryOCLZeroQueueInitialization(S, *this, DestType, Initializer))
+      return;
+  }
+
+  if (!S.getLangOpts().CPlusPlus) {
     // Handle initialization in C
     AddCAssignmentStep(DestType);
     MaybeProduceObjCObject(S, *this, Entity);
@@ -7971,6 +8019,7 @@ ExprResult Sema::PerformQualificationConversion(Expr *E, QualType Ty,
 
   CastKind CK = CK_NoOp;
 
+#if 0 // we don't want this
   if (VK == VK_PRValue) {
     auto PointeeTy = Ty->getPointeeType();
     auto ExprPointeeTy = E->getType()->getPointeeType();
@@ -7980,6 +8029,7 @@ ExprResult Sema::PerformQualificationConversion(Expr *E, QualType Ty,
   } else if (Ty.getAddressSpace() != E->getType().getAddressSpace()) {
     CK = CK_AddressSpaceConversion;
   }
+#endif
 
   return ImpCastExprToType(E, Ty, CK, VK, /*BasePath=*/nullptr, CCK);
 }
@@ -8137,7 +8187,9 @@ ExprResult InitializationSequence::Perform(Sema &S,
   case SK_ProduceObjCObject:
   case SK_StdInitializerList:
   case SK_OCLSamplerInit:
-  case SK_OCLZeroOpaqueType: {
+  case SK_OCLZeroOpaqueType:
+  case SK_OCLZeroEvent:
+  case SK_OCLZeroQueue: {
     assert(Args.size() == 1);
     CurInit = Args[0];
     if (!CurInit.get()) return ExprError();
@@ -8242,7 +8294,10 @@ ExprResult InitializationSequence::Perform(Sema &S,
 
     case SK_BindReferenceToTemporary: {
       // Make sure the "temporary" is actually an rvalue.
-      assert(CurInit.get()->isPRValue() && "not a temporary");
+      // TODO: fix this!
+      //if(CurInit.get()->getType()) {
+      //assert(CurInit.get()->isPRValue() && "not a temporary");
+      //}
 
       // Check exception specifications
       if (S.CheckExceptionSpecCompatibility(CurInit.get(), DestType))
@@ -8823,6 +8878,7 @@ ExprResult InitializationSequence::Perform(Sema &S,
                                       CK_IntToOCLSampler);
       break;
     }
+    case SK_OCLZeroEvent:
     case SK_OCLZeroOpaqueType: {
       assert((Step->Type->isEventT() || Step->Type->isQueueT() ||
               Step->Type->isOCLIntelSubgroupAVCType()) &&
@@ -8833,6 +8889,15 @@ ExprResult InitializationSequence::Perform(Sema &S,
                                     CurInit.get()->getValueKind());
       break;
     }
+    case SK_OCLZeroQueue: {
+      assert(Step->Type->isQueueT() &&
+             "Event initialization on non queue type.");
+
+      CurInit = S.ImpCastExprToType(CurInit.get(), Step->Type,
+                                    CK_ZeroToOCLQueue,
+                                    CurInit.get()->getValueKind());
+      break;
+    }
     }
   }
 
@@ -9765,9 +9830,14 @@ void InitializationSequence::dump(raw_ostream &OS) const {
       OS << "OpenCL sampler_t from integer constant";
       break;
 
+    case SK_OCLZeroEvent:
     case SK_OCLZeroOpaqueType:
       OS << "OpenCL opaque type from zero";
       break;
+
+    case SK_OCLZeroQueue:
+      OS << "OpenCL queue_t from zero";
+      break;
     }
 
     OS << " [" << S->Type.getAsString() << ']';
diff --git a/clang/lib/Sema/SemaLambda.cpp b/clang/lib/Sema/SemaLambda.cpp
index c4a8b3487ab9..835778b1bf0d 100644
--- a/clang/lib/Sema/SemaLambda.cpp
+++ b/clang/lib/Sema/SemaLambda.cpp
@@ -926,9 +926,11 @@ void Sema::ActOnStartOfLambdaDefinition(LambdaIntroducer &Intro,
         /*IsVariadic=*/false, /*IsCXXMethod=*/true));
     EPI.HasTrailingReturn = true;
     EPI.TypeQuals.addConst();
+#if 0 // we don't want this
     LangAS AS = getDefaultCXXMethodAddrSpace();
     if (AS != LangAS::Default)
       EPI.TypeQuals.addAddressSpace(AS);
+#endif
 
     // C++1y [expr.prim.lambda]:
     //   The lambda return type is 'auto', which is replaced by the
diff --git a/clang/lib/Sema/SemaOverload.cpp b/clang/lib/Sema/SemaOverload.cpp
index a2af2ac6f7ee..95ce608ff4ab 100644
--- a/clang/lib/Sema/SemaOverload.cpp
+++ b/clang/lib/Sema/SemaOverload.cpp
@@ -23,6 +23,7 @@
 #include "clang/Basic/PartialDiagnostic.h"
 #include "clang/Basic/SourceManager.h"
 #include "clang/Basic/TargetInfo.h"
+#include "clang/Lex/Preprocessor.h"
 #include "clang/Sema/Initialization.h"
 #include "clang/Sema/Lookup.h"
 #include "clang/Sema/Overload.h"
@@ -143,9 +144,9 @@ ImplicitConversionRank clang::GetConversionRank(ImplicitConversionKind Kind) {
     ICR_Conversion,
     ICR_Conversion,
     ICR_Writeback_Conversion,
-    ICR_Exact_Match, // NOTE(gbiv): This may not be completely right --
-                     // it was omitted by the patch that added
-                     // ICK_Zero_Event_Conversion
+    ICR_Conversion,
+    ICR_Conversion,
+    ICR_Conversion,
     ICR_C_Conversion,
     ICR_C_Conversion_Extension
   };
@@ -181,7 +182,9 @@ static const char* GetImplicitConversionName(ImplicitConversionKind Kind) {
     "Block Pointer conversion",
     "Transparent Union Conversion",
     "Writeback conversion",
-    "OpenCL Zero Event Conversion",
+    "OpenCL Zero Event conversion",
+    "OpenCL Zero Queue Conversion",
+    "OpenCL Integer-to-Sampler conversion",
     "C specific type conversion",
     "Incompatible pointer conversion"
   };
@@ -1947,7 +1950,7 @@ static bool IsStandardConversion(Sema &S, Expr* From, QualType ToType,
     FromType = ToType;
   } else if (ToType->isSamplerT() &&
              From->isIntegerConstantExpr(S.getASTContext())) {
-    SCS.Second = ICK_Compatible_Conversion;
+    SCS.Second = ICK_Int_Sampler_Conversion;
     FromType = ToType;
   } else {
     // No second conversion required.
@@ -3226,9 +3229,11 @@ static bool isQualificationConversionStep(QualType FromType, QualType ToType,
 
   //   -- for every j > 0, if const is in cv 1,j then const is in cv
   //      2,j, and similarly for volatile.
-  if (!CStyle && !ToQuals.compatiblyIncludes(FromQuals))
+  // NOTE: we want to have hard address space checking here if both types do have an actual address space, but we do allow casting to/from default adress space
+  if (!CStyle && !ToQuals.compatiblyIncludes(FromQuals, true /* check AS */, true /* allow default <-> other AS cast */))
     return false;
 
+#if 0 // we don't want this
   // If address spaces mismatch:
   //  - in top level it is only valid to convert to addr space that is a
   //    superset in all cases apart from C-style casts where we allow
@@ -3239,6 +3244,7 @@ static bool isQualificationConversionStep(QualType FromType, QualType ToType,
        !(ToQuals.isAddressSpaceSupersetOf(FromQuals) ||
          (CStyle && FromQuals.isAddressSpaceSupersetOf(ToQuals)))))
     return false;
+#endif
 
   //   -- if the cv 1,j and cv 2,j are different, then const is in
   //      every cv for 0 < k < j.
@@ -4920,7 +4926,7 @@ TryReferenceInit(Sema &S, Expr *Init, QualType DeclType,
     // MS compiler ignores __unaligned qualifier for references; do the same.
     T1Quals.removeUnaligned();
     T2Quals.removeUnaligned();
-    if (!T1Quals.compatiblyIncludes(T2Quals))
+    if (!T1Quals.compatiblyIncludes(T2Quals, !S.Context.getLangOpts().OpenCL))
       return ICS;
   }
 
@@ -5402,6 +5408,7 @@ TryObjectArgumentInitialization(Sema &S, SourceLocation Loc, QualType FromType,
     return ICS;
   }
 
+#if 0 // we don't want this
   if (FromTypeCanon.hasAddressSpace()) {
     Qualifiers QualsImplicitParamType = ImplicitParamType.getQualifiers();
     Qualifiers QualsFromType = FromTypeCanon.getQualifiers();
@@ -5411,6 +5418,7 @@ TryObjectArgumentInitialization(Sema &S, SourceLocation Loc, QualType FromType,
       return ICS;
     }
   }
+#endif
 
   // Check that we have either the same type or a derived type. It
   // affects the conversion rank.
@@ -5554,14 +5562,14 @@ Sema::PerformObjectArgumentInitialization(Expr *From,
   }
 
   if (!Context.hasSameType(From->getType(), DestType)) {
-    CastKind CK;
+    CastKind CK = CK_NoOp;
+#if 0 // we don't want this
     QualType PteeTy = DestType->getPointeeType();
     LangAS DestAS =
         PteeTy.isNull() ? DestType.getAddressSpace() : PteeTy.getAddressSpace();
     if (FromRecordType.getAddressSpace() != DestAS)
       CK = CK_AddressSpaceConversion;
-    else
-      CK = CK_NoOp;
+#endif
     From = ImpCastExprToType(From, DestType, CK, From->getValueKind()).get();
   }
   return From;
@@ -5620,6 +5628,7 @@ static bool CheckConvertedConstantConversions(Sema &S,
   case ICK_Integral_Promotion:
   case ICK_Integral_Conversion: // Narrowing conversions are checked elsewhere.
   case ICK_Zero_Queue_Conversion:
+  case ICK_Int_Sampler_Conversion:
     return true;
 
   case ICK_Boolean_Conversion:
@@ -6440,6 +6449,7 @@ void Sema::AddOverloadCandidate(
       }
     }
 
+#if 0 // we don't want this
     // Check that the constructor is capable of constructing an object in the
     // destination address space.
     if (!Qualifiers::isAddressSpaceSupersetOf(
@@ -6448,6 +6458,7 @@ void Sema::AddOverloadCandidate(
       Candidate.Viable = false;
       Candidate.FailureKind = ovl_fail_object_addrspace_mismatch;
     }
+#endif
   }
 
   unsigned NumParams = Proto->getNumParams();
diff --git a/clang/lib/Sema/SemaStmt.cpp b/clang/lib/Sema/SemaStmt.cpp
index dc564e318d82..cc3bdb668b05 100644
--- a/clang/lib/Sema/SemaStmt.cpp
+++ b/clang/lib/Sema/SemaStmt.cpp
@@ -3827,7 +3827,7 @@ bool Sema::DeduceFunctionTypeFromReturnExpr(FunctionDecl *FD,
 
   // CUDA: Kernel function must have 'void' return type.
   if (getLangOpts().CUDA)
-    if (FD->hasAttr<CUDAGlobalAttr>() && !Deduced->isVoidType()) {
+    if (FD->hasAttr<ComputeKernelAttr>() && !Deduced->isVoidType()) {
       Diag(FD->getLocation(), diag::err_kern_type_not_void_return)
           << FD->getType() << FD->getSourceRange();
       return true;
diff --git a/clang/lib/Sema/SemaStmtAttr.cpp b/clang/lib/Sema/SemaStmtAttr.cpp
index 4f2977f89ce1..8579dd7f95ac 100644
--- a/clang/lib/Sema/SemaStmtAttr.cpp
+++ b/clang/lib/Sema/SemaStmtAttr.cpp
@@ -106,7 +106,7 @@ static Attr *handleLoopHintAttr(Sema &S, Stmt *St, const ParsedAttr &A,
     if (ValueExpr)
       SetHints(LoopHintAttr::UnrollCount, LoopHintAttr::Numeric);
     else
-      SetHints(LoopHintAttr::Unroll, LoopHintAttr::Enable);
+      SetHints(LoopHintAttr::Unroll, LoopHintAttr::Full);
   } else if (PragmaName == "nounroll_and_jam") {
     SetHints(LoopHintAttr::UnrollAndJam, LoopHintAttr::Disable);
   } else if (PragmaName == "unroll_and_jam") {
diff --git a/clang/lib/Sema/SemaTemplateDeduction.cpp b/clang/lib/Sema/SemaTemplateDeduction.cpp
index facd3e0998e8..279ac48125e9 100644
--- a/clang/lib/Sema/SemaTemplateDeduction.cpp
+++ b/clang/lib/Sema/SemaTemplateDeduction.cpp
@@ -3352,7 +3352,7 @@ Sema::SubstituteExplicitTemplateArguments(
       return TDK_SubstitutionFailure;
     // CUDA: Kernel function must have 'void' return type.
     if (getLangOpts().CUDA)
-      if (Function->hasAttr<CUDAGlobalAttr>() && !ResultType->isVoidType()) {
+      if (Function->hasAttr<ComputeKernelAttr>() && !ResultType->isVoidType()) {
         Diag(Function->getLocation(), diag::err_kern_type_not_void_return)
             << Function->getType() << Function->getSourceRange();
         return TDK_SubstitutionFailure;
@@ -3476,7 +3476,7 @@ CheckOriginalCallArgDeduction(Sema &S, TemplateDeductionInfo &Info,
 
     if (AQuals == DeducedAQuals) {
       // Qualifiers match; there's nothing to do.
-    } else if (!DeducedAQuals.compatiblyIncludes(AQuals)) {
+    } else if (!DeducedAQuals.compatiblyIncludes(AQuals, !S.getLangOpts().OpenCL)) {
       return Failed();
     } else {
       // Qualifiers are compatible, so have the argument type adopt the
@@ -3892,9 +3892,11 @@ static bool AdjustFunctionParmAndArgTypesForDeduction(
     //   "lvalue reference to A" is used in place of A for type deduction.
     if (isForwardingReference(QualType(ParamRefType, 0), FirstInnerIndex) &&
         Arg->isLValue()) {
+#if 0 // we don't want this
       if (S.getLangOpts().OpenCL && !ArgType.hasAddressSpace())
         ArgType = S.Context.getAddrSpaceQualType(
             ArgType, S.Context.getDefaultOpenCLPointeeAddrSpace());
+#endif
       ArgType = S.Context.getLValueReferenceType(ArgType);
     }
   } else {
diff --git a/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp b/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
index 7142e6ae56bb..c9893594f1ed 100644
--- a/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
+++ b/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp
@@ -643,6 +643,37 @@ attrToRetainOwnershipKind(const Attr *A) {
   }
 }
 
+static void instantiateDependentFloorImageDataTypeAttr(
+    Sema &S, const MultiLevelTemplateArgumentList &TemplateArgs,
+    const FloorImageDataTypeAttr *A, const Decl *Tmpl, Decl *New) {
+  TypeSourceInfo *Result = S.SubstType(A->getImageDataTypeLoc(), TemplateArgs,
+                                       A->getLocation(), DeclarationName());
+  if (Result) {
+    FloorImageDataTypeAttr *new_attr = new (S.getASTContext())
+        FloorImageDataTypeAttr(S.getASTContext(), *A, Result);
+    New->addAttr(new_attr);
+  }
+}
+
+static void instantiateDependentFloorImageFlagsAttr(
+    Sema &S, const MultiLevelTemplateArgumentList &TemplateArgs,
+    const FloorImageFlagsAttr *A, const Decl *Tmpl, Decl *New) {
+  EnterExpressionEvaluationContext Unevaluated(S, Sema::ExpressionEvaluationContext::ConstantEvaluated);
+  ExprResult Result = S.SubstExpr(A->getImageFlags(), TemplateArgs);
+  if (!Result.isInvalid())
+    S.AddFloorImageFlagsAttr(A->getLocation(), New, Result.getAs<Expr>(), *A);
+}
+
+static void instantiateDependentGraphicsFBOColorLocationAttr(
+    Sema &S, const MultiLevelTemplateArgumentList &TemplateArgs,
+    const GraphicsFBOColorLocationAttr *A, const Decl *Tmpl, Decl *New) {
+  // TODO: check Tmpl with isPotentialConstantExprUnevaluated?
+  EnterExpressionEvaluationContext Unevaluated(S, Sema::ExpressionEvaluationContext::ConstantEvaluated);
+  ExprResult Result = S.SubstExpr(A->getColorLocation(), TemplateArgs);
+  if (!Result.isInvalid())
+    S.AddGraphicsFBOColorLocationAttr(A->getLocation(), New, Result.getAs<Expr>(), *A);
+}
+
 void Sema::InstantiateAttrs(const MultiLevelTemplateArgumentList &TemplateArgs,
                             const Decl *Tmpl, Decl *New,
                             LateInstantiatedAttrVec *LateAttrs,
@@ -761,6 +792,22 @@ void Sema::InstantiateAttrs(const MultiLevelTemplateArgumentList &TemplateArgs,
       continue;
     }
 
+    auto *ImgType = dyn_cast<FloorImageDataTypeAttr>(TmplAttr);
+    if (ImgType && ImgType->getImageDataType()->isDependentType()) {
+      instantiateDependentFloorImageDataTypeAttr(*this, TemplateArgs, ImgType, Tmpl, New);
+      continue;
+    }
+
+    if (auto *ImgFlags = dyn_cast<FloorImageFlagsAttr>(TmplAttr)) {
+      instantiateDependentFloorImageFlagsAttr(*this, TemplateArgs, ImgFlags, Tmpl, New);
+      continue;
+    }
+
+    if (auto *ColorLoc = dyn_cast<GraphicsFBOColorLocationAttr>(TmplAttr)) {
+      instantiateDependentGraphicsFBOColorLocationAttr(*this, TemplateArgs, ColorLoc, Tmpl, New);
+      continue;
+    }
+
     assert(!TmplAttr->isPackExpansion());
     if (TmplAttr->isLateParsed() && LateAttrs) {
       // Late parsed attributes must be instantiated and attached after the
diff --git a/clang/lib/Sema/SemaTemplateVariadic.cpp b/clang/lib/Sema/SemaTemplateVariadic.cpp
index c0bb310e64fb..3c308e7578b2 100644
--- a/clang/lib/Sema/SemaTemplateVariadic.cpp
+++ b/clang/lib/Sema/SemaTemplateVariadic.cpp
@@ -908,6 +908,11 @@ bool Sema::containsUnexpandedParameterPacks(Declarator &D) {
   case TST_BFloat16:
 #define GENERIC_IMAGE_TYPE(ImgType, Id) case TST_##ImgType##_t:
 #include "clang/Basic/OpenCLImageTypes.def"
+  case TST_sampler_t:
+  case TST_event_t:
+  case TST_queue_t:
+  case TST_clk_event_t:
+  case TST_reserve_id_t:
   case TST_unknown_anytype:
   case TST_error:
     break;
diff --git a/clang/lib/Sema/SemaType.cpp b/clang/lib/Sema/SemaType.cpp
index 5918876c02bf..045d46c1c038 100644
--- a/clang/lib/Sema/SemaType.cpp
+++ b/clang/lib/Sema/SemaType.cpp
@@ -19,6 +19,7 @@
 #include "clang/AST/DeclObjC.h"
 #include "clang/AST/DeclTemplate.h"
 #include "clang/AST/Expr.h"
+#include "clang/AST/Type.h"
 #include "clang/AST/TypeLoc.h"
 #include "clang/AST/TypeLocVisitor.h"
 #include "clang/Basic/PartialDiagnostic.h"
@@ -125,13 +126,17 @@ static void diagnoseBadTypeAttribute(Sema &S, const ParsedAttr &attr,
   case ParsedAttr::AT_Pcs:                                                     \
   case ParsedAttr::AT_IntelOclBicc:                                            \
   case ParsedAttr::AT_PreserveMost:                                            \
-  case ParsedAttr::AT_PreserveAll
+  case ParsedAttr::AT_PreserveAll:                                             \
+  case ParsedAttr::AT_ComputeKernel:                                           \
+  case ParsedAttr::AT_GraphicsVertexShader:                                    \
+  case ParsedAttr::AT_GraphicsFragmentShader
 
 // Function type attributes.
 #define FUNCTION_TYPE_ATTRS_CASELIST                                           \
   case ParsedAttr::AT_NSReturnsRetained:                                       \
   case ParsedAttr::AT_NoReturn:                                                \
   case ParsedAttr::AT_Regparm:                                                 \
+  case ParsedAttr::AT_RetRange:                                                \
   case ParsedAttr::AT_CmseNSCall:                                              \
   case ParsedAttr::AT_AnyX86NoCallerSavedRegisters:                            \
   case ParsedAttr::AT_AnyX86NoCfCheck:                                         \
@@ -361,7 +366,8 @@ enum TypeAttrLocation {
 };
 
 static void processTypeAttrs(TypeProcessingState &state, QualType &type,
-                             TypeAttrLocation TAL, ParsedAttributesView &attrs);
+                             TypeAttrLocation TAL, ParsedAttributesView &attrs,
+                             Declarator &D);
 
 static bool handleFunctionTypeAttr(TypeProcessingState &state, ParsedAttr &attr,
                                    QualType &type);
@@ -1249,13 +1255,15 @@ TypeResult Sema::actOnObjCTypeArgsAndProtocolQualifiers(
   return CreateParsedType(Result, ResultTInfo);
 }
 
-static OpenCLAccessAttr::Spelling
+#if 0 // unused
+static ImageAccessAttr::Spelling
 getImageAccess(const ParsedAttributesView &Attrs) {
   for (const ParsedAttr &AL : Attrs)
-    if (AL.getKind() == ParsedAttr::AT_OpenCLAccess)
-      return static_cast<OpenCLAccessAttr::Spelling>(AL.getSemanticSpelling());
-  return OpenCLAccessAttr::Keyword_read_only;
+    if (AL.getKind() == ParsedAttr::AT_ImageAccess)
+      return static_cast<ImageAccessAttr::Spelling>(AL.getSemanticSpelling());
+  return ImageAccessAttr::GNU_image_read_only;
 }
+#endif
 
 /// Convert the specified declspec to the appropriate type
 /// object.
@@ -1700,22 +1708,28 @@ static QualType ConvertDeclSpecToType(TypeProcessingState &state) {
     }
     break;
 
-#define GENERIC_IMAGE_TYPE(ImgType, Id)                                        \
-  case DeclSpec::TST_##ImgType##_t:                                            \
-    switch (getImageAccess(DS.getAttributes())) {                              \
-    case OpenCLAccessAttr::Keyword_write_only:                                 \
-      Result = Context.Id##WOTy;                                               \
-      break;                                                                   \
-    case OpenCLAccessAttr::Keyword_read_write:                                 \
-      Result = Context.Id##RWTy;                                               \
-      break;                                                                   \
-    case OpenCLAccessAttr::Keyword_read_only:                                  \
-      Result = Context.Id##ROTy;                                               \
-      break;                                                                   \
-    case OpenCLAccessAttr::SpellingNotCalculated:                              \
-      llvm_unreachable("Spelling not yet calculated");                         \
-    }                                                                          \
+  case DeclSpec::TST_sampler_t:
+    Result = Context.OCLSamplerTy;
+    break;
+
+  case DeclSpec::TST_event_t:
+    Result = Context.OCLEventTy;
+    break;
+  case DeclSpec::TST_queue_t:
+    Result = Context.OCLQueueTy;
     break;
+  case DeclSpec::TST_clk_event_t:
+    Result = Context.OCLClkEventTy;
+    break;
+  case DeclSpec::TST_reserve_id_t:
+    Result = Context.OCLReserveIDTy;
+    break;
+
+#define GENERIC_IMAGE_TYPE(ImgType, Id)                                        \
+  case DeclSpec::TST_##ImgType##_t: {                                          \
+    Result = Context.Id##Ty;                                                   \
+    break;                                                                     \
+  }
 #include "clang/Basic/OpenCLImageTypes.def"
 
   case DeclSpec::TST_error:
@@ -1730,6 +1744,7 @@ static QualType ConvertDeclSpecToType(TypeProcessingState &state) {
   if (Result->containsErrors())
     declarator.setInvalidType();
 
+#if 0 // not tested here
   if (S.getLangOpts().OpenCL) {
     const auto &OpenCLOptions = S.getOpenCLOptions();
     bool IsOpenCLC30Compatible =
@@ -1758,6 +1773,7 @@ static QualType ConvertDeclSpecToType(TypeProcessingState &state) {
       declarator.setInvalidType();
     }
   }
+#endif
 
   bool IsFixedPointType = DS.getTypeSpecType() == DeclSpec::TST_accum ||
                           DS.getTypeSpecType() == DeclSpec::TST_fract;
@@ -1798,7 +1814,7 @@ static QualType ConvertDeclSpecToType(TypeProcessingState &state) {
   // attributes are pushed around.
   // pipe attributes will be handled later ( at GetFullTypeForDeclarator )
   if (!DS.isTypeSpecPipe())
-    processTypeAttrs(state, Result, TAL_DeclSpec, DS.getAttributes());
+    processTypeAttrs(state, Result, TAL_DeclSpec, DS.getAttributes(), declarator);
 
   // Apply const/volatile/restrict qualifiers to T.
   if (unsigned TypeQuals = DS.getTypeQualifiers()) {
@@ -2088,11 +2104,13 @@ bool Sema::CheckQualifiedFunctionForTypeId(QualType T, SourceLocation Loc) {
 
 // Helper to deduce addr space of a pointee type in OpenCL mode.
 static QualType deduceOpenCLPointeeAddrSpace(Sema &S, QualType PointeeType) {
+#if 0 // we don't want this
   if (!PointeeType->isUndeducedAutoType() && !PointeeType->isDependentType() &&
       !PointeeType->isSamplerT() &&
       !PointeeType.hasAddressSpace())
     PointeeType = S.getASTContext().getAddrSpaceQualType(
         PointeeType, S.getASTContext().getDefaultOpenCLPointeeAddrSpace());
+#endif
   return PointeeType;
 }
 
@@ -2118,7 +2136,7 @@ QualType Sema::BuildPointerType(QualType T,
     return QualType();
   }
 
-  if (T->isFunctionType() && getLangOpts().OpenCL &&
+  if (T->isFunctionType() && getLangOpts().OpenCL && !getLangOpts().CPlusPlus &&
       !getOpenCLOptions().isAvailableOption("__cl_clang_function_pointers",
                                             getLangOpts())) {
     Diag(Loc, diag::err_opencl_function_pointer) << /*pointer*/ 0;
@@ -2193,7 +2211,7 @@ QualType Sema::BuildReferenceType(QualType T, bool SpelledAsLValue,
   if (checkQualifiedFunction(*this, T, Loc, QFK_Reference))
     return QualType();
 
-  if (T->isFunctionType() && getLangOpts().OpenCL &&
+  if (T->isFunctionType() && getLangOpts().OpenCL && !getLangOpts().CPlusPlus &&
       !getOpenCLOptions().isAvailableOption("__cl_clang_function_pointers",
                                             getLangOpts())) {
     Diag(Loc, diag::err_opencl_function_pointer) << /*reference*/ 1;
@@ -2560,8 +2578,11 @@ QualType Sema::BuildArrayType(QualType T, ArrayType::ArraySizeModifier ASM,
     const QualType ArrType = Context.getBaseElementType(T);
     if (ArrType->isBlockPointerType() || ArrType->isPipeType() ||
         ArrType->isSamplerT() || ArrType->isImageType()) {
-      Diag(Loc, diag::err_opencl_invalid_type_array) << ArrType;
-      return QualType();
+      // allow C array of images for Vulkan and Metal
+      if (!((getLangOpts().Vulkan || getLangOpts().Metal) && ArrType->isImageType())) {
+        Diag(Loc, diag::err_opencl_invalid_type_array) << ArrType;
+        return QualType();
+      }
     }
   }
 
@@ -2760,7 +2781,7 @@ bool Sema::CheckFunctionReturnType(QualType T, SourceLocation Loc) {
   }
 
   // Functions cannot return half FP.
-  if (T->isHalfType() && !getLangOpts().HalfArgsAndReturns) {
+  if (T->isHalfType() && !getLangOpts().HalfArgsAndReturns && !LangOpts.OpenCL && !LangOpts.CUDA) {
     Diag(Loc, diag::err_parameters_retval_cannot_have_fp16_type) << 1 <<
       FixItHint::CreateInsertion(Loc, "*");
     return true;
@@ -2866,7 +2887,7 @@ QualType Sema::BuildFunctionType(QualType T,
     if (ParamType->isVoidType()) {
       Diag(Loc, diag::err_param_with_void_type);
       Invalid = true;
-    } else if (ParamType->isHalfType() && !getLangOpts().HalfArgsAndReturns) {
+    } else if (ParamType->isHalfType() && !getLangOpts().HalfArgsAndReturns && !LangOpts.OpenCL && !LangOpts.CUDA) {
       // Disallow half FP arguments.
       Diag(Loc, diag::err_parameters_retval_cannot_have_fp16_type) << 0 <<
         FixItHint::CreateInsertion(Loc, "*");
@@ -2935,7 +2956,7 @@ QualType Sema::BuildMemberPointerType(QualType T, QualType Class,
     return QualType();
   }
 
-  if (T->isFunctionType() && getLangOpts().OpenCL &&
+  if (T->isFunctionType() && getLangOpts().OpenCL && !getLangOpts().CPlusPlus &&
       !getOpenCLOptions().isAvailableOption("__cl_clang_function_pointers",
                                             getLangOpts())) {
     Diag(Loc, diag::err_opencl_function_pointer) << /*pointer*/ 0;
@@ -3355,7 +3376,7 @@ static QualType GetDeclSpecTypeForDeclarator(TypeProcessingState &state,
     // "void" instead.
     T = SemaRef.Context.VoidTy;
     processTypeAttrs(state, T, TAL_DeclSpec,
-                     D.getMutableDeclSpec().getAttributes());
+                     D.getMutableDeclSpec().getAttributes(), D);
     break;
 
   case UnqualifiedIdKind::IK_DeductionGuideName:
@@ -3930,14 +3951,22 @@ static CallingConv getCCForDeclaratorChunk(
   CallingConv CC = S.Context.getDefaultCallingConvention(FTI.isVariadic,
                                                          IsCXXInstanceMethod);
 
-  // Attribute AT_OpenCLKernel affects the calling convention for SPIR
-  // and AMDGPU targets, hence it cannot be treated as a calling
-  // convention attribute. This is the simplest place to infer
-  // calling convention for OpenCL kernels.
-  if (S.getLangOpts().OpenCL) {
+  // Attributes AT_ComputeKernel, AT_GraphicsVertexShader, AT_GraphicsFragmentShader
+  // affect the calling convention only on SPIR, AIR and CUDA targets, hence they cannot
+  // be treated as calling convention attributes. This is the simplest place to infer
+  // "floor_kernel"/"floor_vertex"/"floor_fragment".
+  if (CC == CC_FloorFunction) {
     for (const ParsedAttr &AL : D.getDeclSpec().getAttributes()) {
-      if (AL.getKind() == ParsedAttr::AT_OpenCLKernel) {
-        CC = CC_OpenCLKernel;
+      if (AL.getKind() == ParsedAttr::AT_ComputeKernel) {
+        CC = CC_FloorKernel;
+        break;
+      }
+      if (AL.getKind() == ParsedAttr::AT_GraphicsVertexShader) {
+        CC = CC_FloorVertex;
+        break;
+      }
+      if (AL.getKind() == ParsedAttr::AT_GraphicsFragmentShader) {
+        CC = CC_FloorFragment;
         break;
       }
     }
@@ -5074,7 +5103,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
                 << T << 0 /*pointer hint*/;
             D.setInvalidType(true);
           }
-        } else if (!S.getLangOpts().HalfArgsAndReturns) {
+        } else if (!S.getLangOpts().HalfArgsAndReturns && !S.getLangOpts().CUDA) {
           S.Diag(D.getIdentifierLoc(),
             diag::err_parameters_retval_cannot_have_fp16_type) << 1;
           D.setInvalidType(true);
@@ -5093,6 +5122,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
         // OpenCL doesn't support variadic functions and blocks
         // (s6.9.e and s6.12.5 OpenCL v2.0) except for printf.
         // We also allow here any toolchain reserved identifiers.
+#if 0 // we don't want this
         if (FTI.isVariadic &&
             !S.getOpenCLOptions().isAvailableOption(
                 "__cl_clang_variadic_functions", S.getLangOpts()) &&
@@ -5103,6 +5133,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
           S.Diag(D.getIdentifierLoc(), diag::err_opencl_variadic_function);
           D.setInvalidType(true);
         }
+#endif
       }
 
       // Methods cannot return interface types. All ObjC objects are
@@ -5297,7 +5328,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
                 D.setInvalidType();
                 Param->setInvalidDecl();
               }
-            } else if (!S.getLangOpts().HalfArgsAndReturns) {
+            } else if (!S.getLangOpts().HalfArgsAndReturns && !S.getLangOpts().CUDA) {
               S.Diag(Param->getLocation(),
                 diag::err_parameters_retval_cannot_have_fp16_type) << 0;
               D.setInvalidType();
@@ -5478,7 +5509,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
     case DeclaratorChunk::Pipe: {
       T = S.BuildReadPipeType(T, DeclType.Loc);
       processTypeAttrs(state, T, TAL_DeclSpec,
-                       D.getMutableDeclSpec().getAttributes());
+                       D.getMutableDeclSpec().getAttributes(), D);
       break;
     }
     }
@@ -5489,7 +5520,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
     }
 
     // See if there are any attributes on this declarator chunk.
-    processTypeAttrs(state, T, TAL_DeclChunk, DeclType.getAttrs());
+    processTypeAttrs(state, T, TAL_DeclChunk, DeclType.getAttrs(), D);
 
     if (DeclType.Kind != DeclaratorChunk::Paren) {
       if (ExpectNoDerefChunk && !IsNoDerefableChunk(DeclType))
@@ -5632,7 +5663,7 @@ static TypeSourceInfo *GetFullTypeForDeclarator(TypeProcessingState &state,
   }
 
   // Apply any undistributed attributes from the declarator.
-  processTypeAttrs(state, T, TAL_DeclName, D.getAttributes());
+  processTypeAttrs(state, T, TAL_DeclName, D.getAttributes(), D);
 
   // Diagnose any ignored type attributes.
   state.diagnoseIgnoredTypeAttrs(T);
@@ -6567,8 +6598,11 @@ static void HandleAddressSpaceTypeAttribute(QualType &Type,
     ASIdx = S.getLangOpts().SYCLIsDevice ? Attr.asSYCLLangAS()
                                          : Attr.asOpenCLLangAS();
 
-    if (ASIdx == LangAS::Default)
+    if (ASIdx == LangAS::Default) {
       llvm_unreachable("Invalid address space");
+      assert(Attr.getKind() == ParsedAttr::AT_PrivateAddressSpace);
+      ASIdx = LangAS::opencl_private;
+    }
 
     if (DiagnoseMultipleAddrSpaceAttributes(S, Type.getAddressSpace(), ASIdx,
                                             Attr.getLoc())) {
@@ -7411,6 +7445,12 @@ static Attr *getCCTypeAttr(ASTContext &Ctx, ParsedAttr &Attr) {
     return createSimpleAttr<PreserveMostAttr>(Ctx, Attr);
   case ParsedAttr::AT_PreserveAll:
     return createSimpleAttr<PreserveAllAttr>(Ctx, Attr);
+  case ParsedAttr::AT_GraphicsVertexShader:
+    return createSimpleAttr<GraphicsVertexShaderAttr>(Ctx, Attr);
+  case ParsedAttr::AT_GraphicsFragmentShader:
+    return createSimpleAttr<GraphicsFragmentShaderAttr>(Ctx, Attr);
+  case ParsedAttr::AT_ComputeKernel:
+    return createSimpleAttr<ComputeKernelAttr>(Ctx, Attr);
   }
   llvm_unreachable("unexpected attribute kind!");
 }
@@ -7961,62 +8001,6 @@ static void HandleArmMveStrictPolymorphismAttr(TypeProcessingState &State,
                               CurType, CurType);
 }
 
-/// Handle OpenCL Access Qualifier Attribute.
-static void HandleOpenCLAccessAttr(QualType &CurType, const ParsedAttr &Attr,
-                                   Sema &S) {
-  // OpenCL v2.0 s6.6 - Access qualifier can be used only for image and pipe type.
-  if (!(CurType->isImageType() || CurType->isPipeType())) {
-    S.Diag(Attr.getLoc(), diag::err_opencl_invalid_access_qualifier);
-    Attr.setInvalid();
-    return;
-  }
-
-  if (const TypedefType* TypedefTy = CurType->getAs<TypedefType>()) {
-    QualType BaseTy = TypedefTy->desugar();
-
-    std::string PrevAccessQual;
-    if (BaseTy->isPipeType()) {
-      if (TypedefTy->getDecl()->hasAttr<OpenCLAccessAttr>()) {
-        OpenCLAccessAttr *Attr =
-            TypedefTy->getDecl()->getAttr<OpenCLAccessAttr>();
-        PrevAccessQual = Attr->getSpelling();
-      } else {
-        PrevAccessQual = "read_only";
-      }
-    } else if (const BuiltinType* ImgType = BaseTy->getAs<BuiltinType>()) {
-
-      switch (ImgType->getKind()) {
-        #define IMAGE_TYPE(ImgType, Id, SingletonId, Access, Suffix) \
-      case BuiltinType::Id:                                          \
-        PrevAccessQual = #Access;                                    \
-        break;
-        #include "clang/Basic/OpenCLImageTypes.def"
-      default:
-        llvm_unreachable("Unable to find corresponding image type.");
-      }
-    } else {
-      llvm_unreachable("unexpected type");
-    }
-    StringRef AttrName = Attr.getAttrName()->getName();
-    if (PrevAccessQual == AttrName.ltrim("_")) {
-      // Duplicated qualifiers
-      S.Diag(Attr.getLoc(), diag::warn_duplicate_declspec)
-         << AttrName << Attr.getRange();
-    } else {
-      // Contradicting qualifiers
-      S.Diag(Attr.getLoc(), diag::err_opencl_multiple_access_qualifiers);
-    }
-
-    S.Diag(TypedefTy->getDecl()->getBeginLoc(),
-           diag::note_opencl_typedef_access_qualifier) << PrevAccessQual;
-  } else if (CurType->isPipeType()) {
-    if (Attr.getSemanticSpelling() == OpenCLAccessAttr::Keyword_write_only) {
-      QualType ElemType = CurType->castAs<PipeType>()->getElementType();
-      CurType = S.Context.getWritePipeType(ElemType);
-    }
-  }
-}
-
 /// HandleMatrixTypeAttr - "matrix_type" attribute, like ext_vector_type
 static void HandleMatrixTypeAttr(QualType &CurType, const ParsedAttr &Attr,
                                  Sema &S) {
@@ -8048,22 +8032,123 @@ static void HandleLifetimeBoundAttr(TypeProcessingState &State,
   }
 }
 
+static void deduceOpenCLImplicitAddrSpace(TypeProcessingState &State,
+                                          QualType &T, TypeAttrLocation TAL) {
+  Declarator &D = State.getDeclarator();
+
+  // Handle the cases where address space should not be deduced.
+  //
+  // The pointee type of a pointer type is always deduced since a pointer always
+  // points to some memory location which should has an address space.
+  //
+  // There are situations that at the point of certain declarations, the address
+  // space may be unknown and better to be left as default. For example, when
+  // defining a typedef or struct type, they are not associated with any
+  // specific address space. Later on, they may be used with any address space
+  // to declare a variable.
+  //
+  // The return value of a function is r-value, therefore should not have
+  // address space.
+  //
+  // The void type does not occupy memory, therefore should not have address
+  // space, except when it is used as a pointee type.
+  //
+  // Since LLVM assumes function type is in default address space, it should not
+  // have address space.
+  auto ChunkIndex = State.getCurrentChunkIndex();
+  bool IsPointee =
+      ChunkIndex > 0 &&
+      (D.getTypeObject(ChunkIndex - 1).Kind == DeclaratorChunk::Pointer ||
+       D.getTypeObject(ChunkIndex - 1).Kind == DeclaratorChunk::BlockPointer ||
+       D.getTypeObject(ChunkIndex - 1).Kind == DeclaratorChunk::Reference);
+  bool IsFuncReturnType =
+      ChunkIndex > 0 &&
+      D.getTypeObject(ChunkIndex - 1).Kind == DeclaratorChunk::Function;
+  bool IsFuncType =
+      ChunkIndex < D.getNumTypeObjects() &&
+      D.getTypeObject(ChunkIndex).Kind == DeclaratorChunk::Function;
+  if ( // Do not deduce addr space for function return type and function type,
+       // otherwise it will fail some sema check.
+      IsFuncReturnType || IsFuncType ||
+      // Do not deduce addr space for member types of struct, except the pointee
+      // type of a pointer member type.
+      (D.getContext() == DeclaratorContext::Member && !IsPointee) ||
+      // Do not deduce addr space for types used to define a typedef and the
+      // typedef itself, except the pointee type of a pointer type which is used
+      // to define the typedef.
+      (D.getDeclSpec().getStorageClassSpec() == DeclSpec::SCS_typedef &&
+       !IsPointee) ||
+      // Do not deduce addr space of the void type, e.g. in f(void), otherwise
+      // it will fail some sema check.
+      (T->isVoidType() && !IsPointee) ||
+      // Do not deduce address spaces for dependent types because they might end
+      // up instantiating to a type with an explicit address space qualifier.
+      T->isDependentType())
+    return;
+
+  LangAS ImpAddr = LangAS::Default;
+  // NOTE: disabled due to it being _very_ incompatible to other backends right now
+#if 0 // we don't want automatic address space deduction
+  // Put OpenCL automatic variable in private address space.
+  // OpenCL v1.2 s6.5:
+  // The default address space name for arguments to a function in a
+  // program, or local variables of a function is __private. All function
+  // arguments shall be in the __private address space.
+  if (State.getSema().getLangOpts().OpenCLVersion <= 120 &&
+      !State.getSema().getLangOpts().OpenCLCPlusPlus &&
+      !State.getSema().getLangOpts().CPlusPlus) {
+    ImpAddr = LangAS::opencl_private;
+  } else {
+    // If address space is not set, OpenCL 2.0 defines non private default
+    // address spaces for some cases:
+    // OpenCL 2.0, section 6.5:
+    // The address space for a variable at program scope or a static variable
+    // inside a function can either be __global or __constant, but defaults to
+    // __global if not specified.
+    // (...)
+    // Pointers that are declared without pointing to a named address space
+    // point to the generic address space.
+    if (IsPointee) {
+      ImpAddr = LangAS::opencl_generic;
+    } else {
+      if (D.getContext() == DeclaratorContext::TemplateArgContext) {
+        // Do not deduce address space for non-pointee type in template arg.
+      } else if (D.getContext() == DeclaratorContext::FileContext) {
+        ImpAddr = LangAS::opencl_global;
+      } else {
+        if (D.getDeclSpec().getStorageClassSpec() == DeclSpec::SCS_static ||
+            D.getDeclSpec().getStorageClassSpec() == DeclSpec::SCS_extern) {
+			if (D.getDeclSpec().isConstexprSpecified() ||
+				T.isConstQualified()) {
+				ImpAddr = LangAS::opencl_constant;
+			} else {
+				ImpAddr = LangAS::opencl_global;
+			}
+        } else {
+          ImpAddr = LangAS::opencl_private;
+        }
+      }
+    }
+  }
+#endif
+  T = State.getSema().Context.getAddrSpaceQualType(T, ImpAddr);
+}
+
 static bool isAddressSpaceKind(const ParsedAttr &attr) {
   auto attrKind = attr.getKind();
 
   return attrKind == ParsedAttr::AT_AddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLPrivateAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLGlobalAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLGlobalDeviceAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLGlobalHostAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLLocalAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLConstantAddressSpace ||
-         attrKind == ParsedAttr::AT_OpenCLGenericAddressSpace;
+         attrKind == ParsedAttr::AT_PrivateAddressSpace ||
+         attrKind == ParsedAttr::AT_GlobalAddressSpace ||
+         attrKind == ParsedAttr::AT_LocalAddressSpace ||
+         attrKind == ParsedAttr::AT_ConstantAddressSpace ||
+         attrKind == ParsedAttr::AT_GenericAddressSpace;
 }
 
 static void processTypeAttrs(TypeProcessingState &state, QualType &type,
                              TypeAttrLocation TAL,
-                             ParsedAttributesView &attrs) {
+                             ParsedAttributesView &attrs,
+                             Declarator &D) {
   // Scan through and apply attributes to this type where it makes sense.  Some
   // attributes (such as __address_space__, __vector_size__, etc) apply to the
   // type, but others can be present in the type specifiers even though they
@@ -8134,13 +8219,11 @@ static void processTypeAttrs(TypeProcessingState &state, QualType &type,
       // it it breaks large amounts of Linux software.
       attr.setUsedAsTypeAttr();
       break;
-    case ParsedAttr::AT_OpenCLPrivateAddressSpace:
-    case ParsedAttr::AT_OpenCLGlobalAddressSpace:
-    case ParsedAttr::AT_OpenCLGlobalDeviceAddressSpace:
-    case ParsedAttr::AT_OpenCLGlobalHostAddressSpace:
-    case ParsedAttr::AT_OpenCLLocalAddressSpace:
-    case ParsedAttr::AT_OpenCLConstantAddressSpace:
-    case ParsedAttr::AT_OpenCLGenericAddressSpace:
+    case ParsedAttr::AT_PrivateAddressSpace:
+    case ParsedAttr::AT_GlobalAddressSpace:
+    case ParsedAttr::AT_LocalAddressSpace:
+    case ParsedAttr::AT_ConstantAddressSpace:
+    case ParsedAttr::AT_GenericAddressSpace:
     case ParsedAttr::AT_AddressSpace:
       HandleAddressSpaceTypeAttribute(type, attr, state);
       attr.setUsedAsTypeAttr();
@@ -8177,8 +8260,16 @@ static void processTypeAttrs(TypeProcessingState &state, QualType &type,
       attr.setUsedAsTypeAttr();
       break;
     }
-    case ParsedAttr::AT_OpenCLAccess:
-      HandleOpenCLAccessAttr(type, attr, state.getSema());
+    case ParsedAttr::AT_ImageAccess:
+    case ParsedAttr::AT_FloorArgBuffer:
+    case ParsedAttr::AT_FloorImageDataType:
+    case ParsedAttr::AT_FloorImageFlags:
+    case ParsedAttr::AT_VectorCompat:
+    case ParsedAttr::AT_GraphicsFBOColorLocation:
+    case ParsedAttr::AT_GraphicsFBODepthType:
+    case ParsedAttr::AT_GraphicsVertexPosition:
+    case ParsedAttr::AT_GraphicsPointSize:
+    case ParsedAttr::AT_GraphicsStageInput:
       attr.setUsedAsTypeAttr();
       break;
     case ParsedAttr::AT_LifetimeBound:
@@ -8310,6 +8401,12 @@ static void processTypeAttrs(TypeProcessingState &state, QualType &type,
           attr.getMacroExpansionLoc());
     }
   }
+
+  if (!state.getSema().getLangOpts().OpenCL ||
+      type.getAddressSpace() != LangAS::Default)
+    return;
+
+  deduceOpenCLImplicitAddrSpace(state, type, TAL);
 }
 
 void Sema::completeExprArrayBound(Expr *E) {
diff --git a/clang/lib/Sema/TreeTransform.h b/clang/lib/Sema/TreeTransform.h
index cfc6e608bc59..f4e0a8b892f7 100644
--- a/clang/lib/Sema/TreeTransform.h
+++ b/clang/lib/Sema/TreeTransform.h
@@ -4757,8 +4757,10 @@ QualType TreeTransform<Derived>::RebuildQualifiedType(QualType T,
   //   [When] adding cv-qualifications on top of the function type [...] the
   //   cv-qualifiers are ignored.
   if (T->isFunctionType()) {
+#if 0 // we don't want this
     T = SemaRef.getASTContext().getAddrSpaceQualType(T,
                                                      Quals.getAddressSpace());
+#endif
     return T;
   }
 
diff --git a/clang/lib/StaticAnalyzer/Core/ExprEngineC.cpp b/clang/lib/StaticAnalyzer/Core/ExprEngineC.cpp
index 929ba8d94950..7825e7af2037 100644
--- a/clang/lib/StaticAnalyzer/Core/ExprEngineC.cpp
+++ b/clang/lib/StaticAnalyzer/Core/ExprEngineC.cpp
@@ -401,6 +401,8 @@ void ExprEngine::VisitCast(const CastExpr *CastE, const Expr *Ex,
       case CK_AnyPointerToBlockPointerCast:
       case CK_ObjCObjectLValueCast:
       case CK_ZeroToOCLOpaqueType:
+      case CK_ZeroToOCLEvent:
+      case CK_ZeroToOCLQueue:
       case CK_IntToOCLSampler:
       case CK_LValueBitCast:
       case CK_FloatingToFixedPoint:
diff --git a/clang/tools/driver/CMakeLists.txt b/clang/tools/driver/CMakeLists.txt
index 1bea470ed301..021b4f5b8fee 100644
--- a/clang/tools/driver/CMakeLists.txt
+++ b/clang/tools/driver/CMakeLists.txt
@@ -7,6 +7,7 @@ set( LLVM_LINK_COMPONENTS
   AggressiveInstCombine
   InstCombine
   Instrumentation
+  LibFloor
   MC
   MCParser
   ObjCARCOpts
diff --git a/clang/tools/driver/cc1_main.cpp b/clang/tools/driver/cc1_main.cpp
index fd3b25ccb3cb..d6542287c94b 100644
--- a/clang/tools/driver/cc1_main.cpp
+++ b/clang/tools/driver/cc1_main.cpp
@@ -18,6 +18,7 @@
 #include "clang/Config/config.h"
 #include "clang/Driver/DriverDiagnostic.h"
 #include "clang/Driver/Options.h"
+#include "clang/Lex/PreprocessorOptions.h"
 #include "clang/Frontend/CompilerInstance.h"
 #include "clang/Frontend/CompilerInvocation.h"
 #include "clang/Frontend/FrontendDiagnostic.h"
diff --git a/clang/tools/libclang/CIndex.cpp b/clang/tools/libclang/CIndex.cpp
index fb5c3fa9390a..ad72d4fcbfcb 100644
--- a/clang/tools/libclang/CIndex.cpp
+++ b/clang/tools/libclang/CIndex.cpp
@@ -5490,7 +5490,7 @@ CXString clang_getCursorKindSpelling(enum CXCursorKind Kind) {
     return cxstring::createRef("attribute(constant)");
   case CXCursor_CUDADeviceAttr:
     return cxstring::createRef("attribute(device)");
-  case CXCursor_CUDAGlobalAttr:
+  case CXCursor_ComputeKernelAttr:
     return cxstring::createRef("attribute(global)");
   case CXCursor_CUDAHostAttr:
     return cxstring::createRef("attribute(host)");
diff --git a/clang/tools/libclang/CXCursor.cpp b/clang/tools/libclang/CXCursor.cpp
index 590797474b00..5b5d4f325473 100644
--- a/clang/tools/libclang/CXCursor.cpp
+++ b/clang/tools/libclang/CXCursor.cpp
@@ -68,8 +68,8 @@ static CXCursorKind GetCursorKind(const Attr *A) {
     return CXCursor_CUDAConstantAttr;
   case attr::CUDADevice:
     return CXCursor_CUDADeviceAttr;
-  case attr::CUDAGlobal:
-    return CXCursor_CUDAGlobalAttr;
+  case attr::ComputeKernel:
+    return CXCursor_ComputeKernelAttr;
   case attr::CUDAHost:
     return CXCursor_CUDAHostAttr;
   case attr::CUDAShared:
diff --git a/clang/tools/libclang/CXType.cpp b/clang/tools/libclang/CXType.cpp
index 822ab3bb64b8..56a21a0ac3f9 100644
--- a/clang/tools/libclang/CXType.cpp
+++ b/clang/tools/libclang/CXType.cpp
@@ -669,8 +669,10 @@ CXCallingConv clang_getFunctionTypeCallingConv(CXType X) {
       TCALLINGCONV(SwiftAsync);
       TCALLINGCONV(PreserveMost);
       TCALLINGCONV(PreserveAll);
-    case CC_SpirFunction: return CXCallingConv_Unexposed;
-    case CC_OpenCLKernel: return CXCallingConv_Unexposed;
+    case CC_FloorKernel: return CXCallingConv_Unexposed;
+    case CC_FloorVertex: return CXCallingConv_Unexposed;
+    case CC_FloorFragment: return CXCallingConv_Unexposed;
+    case CC_FloorFunction: return CXCallingConv_Unexposed;
       break;
     }
 #undef TCALLINGCONV
diff --git a/clang/utils/TableGen/ClangOpenCLBuiltinEmitter.cpp b/clang/utils/TableGen/ClangOpenCLBuiltinEmitter.cpp
index 4795b008dda3..d157f6a85bcb 100644
--- a/clang/utils/TableGen/ClangOpenCLBuiltinEmitter.cpp
+++ b/clang/utils/TableGen/ClangOpenCLBuiltinEmitter.cpp
@@ -775,8 +775,10 @@ static void OCL2Qual(Sema &S, const OpenCLTypeStruct &Ty,
       Records.getAllDerivedDefinitions("ImageType");
 
   // Map an image type name to its 3 access-qualified types (RO, WO, RW).
-  StringMap<SmallVector<Record *, 3>> ImageTypesMap;
+  //StringMap<SmallVector<Record *, 3>> ImageTypesMap;
+  StringMap<Record *> ImageTypesMap; // we have no access qualifiers
   for (auto *IT : ImageTypes) {
+#if 0
     auto Entry = ImageTypesMap.find(IT->getValueAsString("Name"));
     if (Entry == ImageTypesMap.end()) {
       SmallVector<Record *, 3> ImageList;
@@ -786,6 +788,10 @@ static void OCL2Qual(Sema &S, const OpenCLTypeStruct &Ty,
     } else {
       Entry->second.push_back(IT);
     }
+#else
+    ImageTypesMap.insert(
+	 std::make_pair(IT->getValueAsString("Name"), IT));
+#endif
   }
 
   // Emit the cases for the image types.  For an image type name, there are 3
@@ -794,6 +800,7 @@ static void OCL2Qual(Sema &S, const OpenCLTypeStruct &Ty,
   // corresponding QualType into "QT".
   for (const auto &ITE : ImageTypesMap) {
     OS << "    case OCLT_" << ITE.getKey() << ":\n"
+#if 0
        << "      switch (Ty.AccessQualifier) {\n"
        << "        case OCLAQ_None:\n"
        << "          llvm_unreachable(\"Image without access qualifier\");\n";
@@ -810,6 +817,12 @@ static void OCL2Qual(Sema &S, const OpenCLTypeStruct &Ty,
     }
     OS << "      }\n"
        << "      break;\n";
+#else
+       << "      QT.push_back("
+       << ITE.getValue()->getValueAsDef("QTExpr")->getValueAsString("TypeExpr")
+       << ");\n"
+       << "      break;\n";
+#endif
   }
 
   // Switch cases for generic types.
diff --git a/libcxx/include/__algorithm/copy.h b/libcxx/include/__algorithm/copy.h
index e7e8b9e51a3e..98ce271ba052 100644
--- a/libcxx/include/__algorithm/copy.h
+++ b/libcxx/include/__algorithm/copy.h
@@ -12,7 +12,9 @@
 #include <__config>
 #include <__algorithm/unwrap_iter.h>
 #include <__iterator/iterator_traits.h>
+#if 0
 #include <cstring>
+#endif
 #include <type_traits>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
@@ -53,7 +55,7 @@ __copy(_Tp* __first, _Tp* __last, _Up* __result)
 {
     const size_t __n = static_cast<size_t>(__last - __first);
     if (__n > 0)
-        _VSTD::memmove(__result, __first, __n * sizeof(_Up));
+        __builtin_memmove(__result, __first, __n * sizeof(_Up));
     return __result + __n;
 }
 
diff --git a/libcxx/include/__algorithm/copy_backward.h b/libcxx/include/__algorithm/copy_backward.h
index 4a2f8c0c49cd..fee25da48340 100644
--- a/libcxx/include/__algorithm/copy_backward.h
+++ b/libcxx/include/__algorithm/copy_backward.h
@@ -12,7 +12,9 @@
 #include <__config>
 #include <__algorithm/unwrap_iter.h>
 #include <__iterator/iterator_traits.h>
+#if 0
 #include <cstring>
+#endif
 #include <type_traits>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
@@ -53,7 +55,7 @@ __copy_backward(_Tp* __first, _Tp* __last, _Up* __result)
     if (__n > 0)
     {
         __result -= __n;
-        _VSTD::memmove(__result, __first, __n * sizeof(_Up));
+        __builtin_memmove(__result, __first, __n * sizeof(_Up));
     }
     return __result;
 }
diff --git a/libcxx/include/__algorithm/copy_if.h b/libcxx/include/__algorithm/copy_if.h
index 230826f63af4..97500184c692 100644
--- a/libcxx/include/__algorithm/copy_if.h
+++ b/libcxx/include/__algorithm/copy_if.h
@@ -12,7 +12,9 @@
 #include <__config>
 #include <__algorithm/unwrap_iter.h>
 #include <__iterator/iterator_traits.h>
+#if 0
 #include <cstring>
+#endif
 #include <type_traits>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
diff --git a/libcxx/include/__algorithm/copy_n.h b/libcxx/include/__algorithm/copy_n.h
index 6417a0543e5a..e94e83f51283 100644
--- a/libcxx/include/__algorithm/copy_n.h
+++ b/libcxx/include/__algorithm/copy_n.h
@@ -13,7 +13,9 @@
 #include <__algorithm/copy.h>
 #include <__algorithm/unwrap_iter.h>
 #include <__iterator/iterator_traits.h>
+#if 0
 #include <cstring>
+#endif
 #include <type_traits>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
diff --git a/libcxx/include/__algorithm/move.h b/libcxx/include/__algorithm/move.h
index 7430bf087438..973798765fa1 100644
--- a/libcxx/include/__algorithm/move.h
+++ b/libcxx/include/__algorithm/move.h
@@ -12,7 +12,9 @@
 #include <__config>
 #include <__algorithm/unwrap_iter.h>
 #include <__utility/move.h>
+#if 0
 #include <cstring>
+#endif
 #include <utility>
 #include <type_traits>
 
@@ -54,7 +56,7 @@ __move(_Tp* __first, _Tp* __last, _Up* __result)
 {
     const size_t __n = static_cast<size_t>(__last - __first);
     if (__n > 0)
-        _VSTD::memmove(__result, __first, __n * sizeof(_Up));
+        __builtin_memmove(__result, __first, __n * sizeof(_Up));
     return __result + __n;
 }
 
diff --git a/libcxx/include/__algorithm/move_backward.h b/libcxx/include/__algorithm/move_backward.h
index ee72d39764ca..da0990f11023 100644
--- a/libcxx/include/__algorithm/move_backward.h
+++ b/libcxx/include/__algorithm/move_backward.h
@@ -11,7 +11,9 @@
 
 #include <__config>
 #include <__algorithm/unwrap_iter.h>
+#if 0
 #include <cstring>
+#endif
 #include <utility>
 #include <type_traits>
 
@@ -53,7 +55,7 @@ __move_backward(_Tp* __first, _Tp* __last, _Up* __result)
     if (__n > 0)
     {
         __result -= __n;
-        _VSTD::memmove(__result, __first, __n * sizeof(_Up));
+        __builtin_memmove(__result, __first, __n * sizeof(_Up));
     }
     return __result;
 }
diff --git a/libcxx/include/__algorithm/shuffle.h b/libcxx/include/__algorithm/shuffle.h
index 292e60d791ff..2e6a92510011 100644
--- a/libcxx/include/__algorithm/shuffle.h
+++ b/libcxx/include/__algorithm/shuffle.h
@@ -25,6 +25,8 @@ _LIBCPP_PUSH_MACROS
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
+
+#if 0 // this is not supported
 #if _LIBCPP_STD_VER <= 14 || defined(_LIBCPP_ENABLE_CXX17_REMOVED_RANDOM_SHUFFLE) \
   || defined(_LIBCPP_BUILDING_LIBRARY)
 class _LIBCPP_TYPE_VIS __rs_default;
@@ -98,6 +100,7 @@ random_shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last,
     }
 }
 #endif
+#endif
 
 template<class _RandomAccessIterator, class _UniformRandomNumberGenerator>
     void shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last,
diff --git a/libcxx/include/__algorithm/sort.h b/libcxx/include/__algorithm/sort.h
index 0ab6c44a0c5a..d5ce3f8ca3a7 100644
--- a/libcxx/include/__algorithm/sort.h
+++ b/libcxx/include/__algorithm/sort.h
@@ -269,6 +269,31 @@ __sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __c
     typedef typename iterator_traits<_RandomAccessIterator>::value_type value_type;
     const difference_type __limit = is_trivially_copy_constructible<value_type>::value &&
                                     is_trivially_copy_assignable<value_type>::value ? 30 : 6;
+#if 1
+    // compute note: keep the special cases if len <= 5, but always fallback to using
+    //               __insertion_sort_3 for len > 5 as it doesn't require recursion
+    difference_type __len = __last - __first;
+    switch (__len)
+    {
+    case 0:
+    case 1:
+        return;
+    case 2:
+        if (__comp(*--__last, *__first))
+            swap(*__first, *__last);
+        return;
+    case 3:
+        _VSTD::__sort3<_Compare>(__first, __first+1, --__last, __comp);
+        return;
+    case 4:
+        _VSTD::__sort4<_Compare>(__first, __first+1, __first+2, --__last, __comp);
+        return;
+    case 5:
+        _VSTD::__sort5<_Compare>(__first, __first+1, __first+2, __first+3, --__last, __comp);
+        return;
+    }
+    _VSTD::__insertion_sort_3<_Compare>(__first, __last, __comp);
+#else
     while (true)
     {
     __restart:
@@ -451,6 +476,7 @@ __sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __c
             __last = __i;
         }
     }
+#endif
 }
 
 template <class _Compare, class _Tp>
@@ -462,6 +488,7 @@ __sort(_Tp** __first, _Tp** __last, __less<_Tp*>&)
     _VSTD::__sort<__less<uintptr_t>&, uintptr_t*>((uintptr_t*)__first, (uintptr_t*)__last, __comp);
 }
 
+#if 0
 _LIBCPP_EXTERN_TEMPLATE(_LIBCPP_FUNC_VIS void __sort<__less<char>&, char*>(char*, char*, __less<char>&))
 #ifndef _LIBCPP_HAS_NO_WIDE_CHARACTERS
 _LIBCPP_EXTERN_TEMPLATE(_LIBCPP_FUNC_VIS void __sort<__less<wchar_t>&, wchar_t*>(wchar_t*, wchar_t*, __less<wchar_t>&))
@@ -499,6 +526,7 @@ _LIBCPP_EXTERN_TEMPLATE(_LIBCPP_FUNC_VIS bool __insertion_sort_incomplete<__less
 _LIBCPP_EXTERN_TEMPLATE(_LIBCPP_FUNC_VIS bool __insertion_sort_incomplete<__less<long double>&, long double*>(long double*, long double*, __less<long double>&))
 
 _LIBCPP_EXTERN_TEMPLATE(_LIBCPP_FUNC_VIS unsigned __sort5<__less<long double>&, long double*>(long double*, long double*, long double*, long double*, long double*, __less<long double>&))
+#endif
 
 template <class _RandomAccessIterator, class _Compare>
 inline _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17
diff --git a/libcxx/include/__algorithm/stable_partition.h b/libcxx/include/__algorithm/stable_partition.h
index 323b323c53dd..9220c0dd468a 100644
--- a/libcxx/include/__algorithm/stable_partition.h
+++ b/libcxx/include/__algorithm/stable_partition.h
@@ -21,6 +21,7 @@
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
+#if 0
 template <class _Predicate, class _ForwardIterator, class _Distance, class _Pair>
 _ForwardIterator
 __stable_partition(_ForwardIterator __first, _ForwardIterator __last, _Predicate __pred,
@@ -289,6 +290,7 @@ stable_partition(_ForwardIterator __first, _ForwardIterator __last, _Predicate _
 {
     return _VSTD::__stable_partition<_Predicate&>(__first, __last, __pred, typename iterator_traits<_ForwardIterator>::iterator_category());
 }
+#endif
 
 _LIBCPP_END_NAMESPACE_STD
 
diff --git a/libcxx/include/__bit_reference b/libcxx/include/__bit_reference
index a02492c077dd..301cbaed71b3 100644
--- a/libcxx/include/__bit_reference
+++ b/libcxx/include/__bit_reference
@@ -338,7 +338,7 @@ __fill_n_false(__bit_iterator<_Cp, false> __first, typename _Cp::size_type __n)
     }
     // do middle whole words
     __storage_type __nw = __n / __bits_per_word;
-    _VSTD::memset(_VSTD::__to_address(__first.__seg_), 0, __nw * sizeof(__storage_type));
+    __builtin_memset(_VSTD::__to_address(__first.__seg_), 0, __nw * sizeof(__storage_type));
     __n -= __nw * __bits_per_word;
     // do last partial word
     if (__n > 0)
@@ -368,7 +368,7 @@ __fill_n_true(__bit_iterator<_Cp, false> __first, typename _Cp::size_type __n)
     }
     // do middle whole words
     __storage_type __nw = __n / __bits_per_word;
-    _VSTD::memset(_VSTD::__to_address(__first.__seg_), -1, __nw * sizeof(__storage_type));
+    __builtin_memset(_VSTD::__to_address(__first.__seg_), -1, __nw * sizeof(__storage_type));
     __n -= __nw * __bits_per_word;
     // do last partial word
     if (__n > 0)
@@ -435,7 +435,7 @@ __copy_aligned(__bit_iterator<_Cp, _IsConst> __first, __bit_iterator<_Cp, _IsCon
         // __first.__ctz_ == 0;
         // do middle words
         __storage_type __nw = __n / __bits_per_word;
-        _VSTD::memmove(_VSTD::__to_address(__result.__seg_),
+        __builtin_memmove(_VSTD::__to_address(__result.__seg_),
                        _VSTD::__to_address(__first.__seg_),
                        __nw * sizeof(__storage_type));
         __n -= __nw * __bits_per_word;
@@ -576,7 +576,7 @@ __copy_backward_aligned(__bit_iterator<_Cp, _IsConst> __first, __bit_iterator<_C
         __storage_type __nw = __n / __bits_per_word;
         __result.__seg_ -= __nw;
         __last.__seg_ -= __nw;
-        _VSTD::memmove(_VSTD::__to_address(__result.__seg_),
+        __builtin_memmove(_VSTD::__to_address(__result.__seg_),
                        _VSTD::__to_address(__last.__seg_),
                        __nw * sizeof(__storage_type));
         __n -= __nw * __bits_per_word;
diff --git a/libcxx/include/__config b/libcxx/include/__config
index 0c6839025bca..6b1fa59e4b57 100644
--- a/libcxx/include/__config
+++ b/libcxx/include/__config
@@ -56,6 +56,8 @@
 #  define _LIBCPP_OBJECT_FORMAT_COFF  1
 #elif defined(__wasm__)
 #  define _LIBCPP_OBJECT_FORMAT_WASM  1
+#elif defined(FLOOR_COMPUTE)
+#  define _LIBCPP_OBJECT_FORMAT_FLOOR 1
 #else
    // ... add new file formats here ...
 #endif
@@ -476,11 +478,11 @@ typedef __char32_t char32_t;
 #define _LIBCPP_HAS_OBJC_ARC_WEAK
 #endif
 
-#if __has_extension(blocks)
+#if __has_extension(blocks) && 0 /* we don't want this */
 #  define _LIBCPP_HAS_EXTENSION_BLOCKS
 #endif
 
-#if defined(_LIBCPP_HAS_EXTENSION_BLOCKS) && defined(__APPLE__)
+#if defined(_LIBCPP_HAS_EXTENSION_BLOCKS) && defined(__APPLE__) && 0 /* we don't want this */
 #  define _LIBCPP_HAS_BLOCKS_RUNTIME
 #endif
 
diff --git a/libcxx/include/__config_site b/libcxx/include/__config_site
new file mode 100644
index 000000000000..424e8077a63c
--- /dev/null
+++ b/libcxx/include/__config_site
@@ -0,0 +1,16 @@
+//===----------------------------------------------------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef _LIBCPP_CONFIG_SITE
+#define _LIBCPP_CONFIG_SITE
+
+#define _LIBCPP_HAS_NO_STDIN
+#define _LIBCPP_HAS_NO_STDOUT
+#define _LIBCPP_HIDE_FROM_ABI_PER_TU_BY_DEFAULT
+
+#endif // _LIBCPP_CONFIG_SITE
diff --git a/libcxx/include/__debug b/libcxx/include/__debug
index 771e4316320b..9026b062c905 100644
--- a/libcxx/include/__debug
+++ b/libcxx/include/__debug
@@ -67,7 +67,7 @@ typedef void(*__libcpp_debug_function_type)(__libcpp_debug_info const&);
 
 /// __libcpp_debug_function - The handler function called when a _LIBCPP_ASSERT
 ///    fails.
-extern _LIBCPP_EXPORTED_FROM_ABI __libcpp_debug_function_type __libcpp_debug_function;
+//extern _LIBCPP_EXPORTED_FROM_ABI __libcpp_debug_function_type __libcpp_debug_function;
 
 /// __libcpp_abort_debug_function - A debug handler that aborts when called.
 _LIBCPP_NORETURN _LIBCPP_FUNC_VIS
diff --git a/libcxx/include/__functional/bind.h b/libcxx/include/__functional/bind.h
index 0b74d91b7746..0152e17b91a0 100644
--- a/libcxx/include/__functional/bind.h
+++ b/libcxx/include/__functional/bind.h
@@ -47,27 +47,27 @@ namespace placeholders
 template <int _Np> struct __ph {};
 
 #if defined(_LIBCPP_CXX03_LANG) || defined(_LIBCPP_BUILDING_LIBRARY)
-_LIBCPP_FUNC_VIS extern const __ph<1>   _1;
-_LIBCPP_FUNC_VIS extern const __ph<2>   _2;
-_LIBCPP_FUNC_VIS extern const __ph<3>   _3;
-_LIBCPP_FUNC_VIS extern const __ph<4>   _4;
-_LIBCPP_FUNC_VIS extern const __ph<5>   _5;
-_LIBCPP_FUNC_VIS extern const __ph<6>   _6;
-_LIBCPP_FUNC_VIS extern const __ph<7>   _7;
-_LIBCPP_FUNC_VIS extern const __ph<8>   _8;
-_LIBCPP_FUNC_VIS extern const __ph<9>   _9;
-_LIBCPP_FUNC_VIS extern const __ph<10> _10;
+_LIBCPP_FUNC_VIS extern constant const __ph<1>   _1;
+_LIBCPP_FUNC_VIS extern constant const __ph<2>   _2;
+_LIBCPP_FUNC_VIS extern constant const __ph<3>   _3;
+_LIBCPP_FUNC_VIS extern constant const __ph<4>   _4;
+_LIBCPP_FUNC_VIS extern constant const __ph<5>   _5;
+_LIBCPP_FUNC_VIS extern constant const __ph<6>   _6;
+_LIBCPP_FUNC_VIS extern constant const __ph<7>   _7;
+_LIBCPP_FUNC_VIS extern constant const __ph<8>   _8;
+_LIBCPP_FUNC_VIS extern constant const __ph<9>   _9;
+_LIBCPP_FUNC_VIS extern constant const __ph<10> _10;
 #else
-/* inline */ constexpr __ph<1>   _1{};
-/* inline */ constexpr __ph<2>   _2{};
-/* inline */ constexpr __ph<3>   _3{};
-/* inline */ constexpr __ph<4>   _4{};
-/* inline */ constexpr __ph<5>   _5{};
-/* inline */ constexpr __ph<6>   _6{};
-/* inline */ constexpr __ph<7>   _7{};
-/* inline */ constexpr __ph<8>   _8{};
-/* inline */ constexpr __ph<9>   _9{};
-/* inline */ constexpr __ph<10> _10{};
+/* inline */ constexpr constant __ph<1>   _1{};
+/* inline */ constexpr constant __ph<2>   _2{};
+/* inline */ constexpr constant __ph<3>   _3{};
+/* inline */ constexpr constant __ph<4>   _4{};
+/* inline */ constexpr constant __ph<5>   _5{};
+/* inline */ constexpr constant __ph<6>   _6{};
+/* inline */ constexpr constant __ph<7>   _7{};
+/* inline */ constexpr constant __ph<8>   _8{};
+/* inline */ constexpr constant __ph<9>   _9{};
+/* inline */ constexpr constant __ph<10> _10{};
 #endif // defined(_LIBCPP_CXX03_LANG) || defined(_LIBCPP_BUILDING_LIBRARY)
 
 }  // placeholders
diff --git a/libcxx/include/__functional/function.h b/libcxx/include/__functional/function.h
index dff44fc674d5..de3f521e284b 100644
--- a/libcxx/include/__functional/function.h
+++ b/libcxx/include/__functional/function.h
@@ -49,8 +49,10 @@ void __throw_bad_function_call()
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw bad_function_call();
 #else
+#if 0 // we don't have this
     _VSTD::abort();
 #endif
+#endif
 }
 
 #if defined(_LIBCPP_CXX03_LANG) && !defined(_LIBCPP_DISABLE_DEPRECATION_WARNINGS) && __has_attribute(deprecated)
@@ -258,10 +260,14 @@ public:
     virtual void destroy() _NOEXCEPT = 0;
     virtual void destroy_deallocate() _NOEXCEPT = 0;
     virtual _Rp operator()(_ArgTypes&& ...) = 0;
+#if 0
 #ifndef _LIBCPP_NO_RTTI
     virtual const void* target(const type_info&) const _NOEXCEPT = 0;
     virtual const std::type_info& target_type() const _NOEXCEPT = 0;
 #endif // _LIBCPP_NO_RTTI
+#else
+    const void* const __type_info;
+#endif
 };
 
 // __func implements __base for a given functor type.
@@ -587,8 +593,12 @@ struct __policy
     // True if this is the null policy (no value).
     const bool __is_null;
 
+#ifndef _LIBCPP_NO_RTTI
     // The target type. May be null if RTTI is disabled.
     const std::type_info* const __type_info;
+#else
+    const int* const __type_info;
+#endif
 
     // Returns a pointer to a static policy object suitable for the functor
     // type.
@@ -832,7 +842,7 @@ template <class _Rp, class... _ArgTypes> class __policy_func<_Rp(_ArgTypes...)>
     _LIBCPP_INLINE_VISIBILITY
     _Rp operator()(_ArgTypes&&... __args) const
     {
-        return __invoker_.__call_(_VSTD::addressof(__buf_),
+        return __invoker_.__call_(__builtin_addressof(__buf_),
                                   _VSTD::forward<_ArgTypes>(__args)...);
     }
 
@@ -1860,7 +1870,7 @@ template<class _Rp>
 void
 function<_Rp()>::swap(function& __f)
 {
-    if (_VSTD::addressof(__f) == this)
+    if (__builtin_addressof(__f) == this)
       return;
     if (__f_ == (__base*)&__buf_ && __f.__f_ == (__base*)&__f.__buf_)
     {
@@ -2140,7 +2150,7 @@ template<class _Rp, class _A0>
 void
 function<_Rp(_A0)>::swap(function& __f)
 {
-    if (_VSTD::addressof(__f) == this)
+    if (__builtin_addressof(__f) == this)
       return;
     if (__f_ == (__base*)&__buf_ && __f.__f_ == (__base*)&__f.__buf_)
     {
@@ -2420,7 +2430,7 @@ template<class _Rp, class _A0, class _A1>
 void
 function<_Rp(_A0, _A1)>::swap(function& __f)
 {
-    if (_VSTD::addressof(__f) == this)
+    if (__builtin_addressof(__f) == this)
       return;
     if (__f_ == (__base*)&__buf_ && __f.__f_ == (__base*)&__f.__buf_)
     {
@@ -2700,7 +2710,7 @@ template<class _Rp, class _A0, class _A1, class _A2>
 void
 function<_Rp(_A0, _A1, _A2)>::swap(function& __f)
 {
-    if (_VSTD::addressof(__f) == this)
+    if (__builtin_addressof(__f) == this)
       return;
     if (__f_ == (__base*)&__buf_ && __f.__f_ == (__base*)&__f.__buf_)
     {
diff --git a/libcxx/include/__functional/hash.h b/libcxx/include/__functional/hash.h
index b1a3ad94ae2d..b5a82c114cb9 100644
--- a/libcxx/include/__functional/hash.h
+++ b/libcxx/include/__functional/hash.h
@@ -17,7 +17,9 @@
 #include <__utility/pair.h>
 #include <__utility/swap.h>
 #include <cstdint>
+#if 0
 #include <cstring>
+#endif
 #include <cstddef>
 #include <limits>
 #include <type_traits>
@@ -34,7 +36,7 @@ _Size
 __loadword(const void* __p)
 {
     _Size __r;
-    _VSTD::memcpy(&__r, __p, sizeof(__r));
+    __builtin_memcpy(&__r, __p, sizeof(__r));
     return __r;
 }
 
diff --git a/libcxx/include/__functional/reference_wrapper.h b/libcxx/include/__functional/reference_wrapper.h
index e1e4abd80c23..70d286aac07b 100644
--- a/libcxx/include/__functional/reference_wrapper.h
+++ b/libcxx/include/__functional/reference_wrapper.h
@@ -44,13 +44,13 @@ public:
 #ifdef _LIBCPP_CXX03_LANG
     _LIBCPP_INLINE_VISIBILITY
     reference_wrapper(type& __f) _NOEXCEPT
-        : __f_(_VSTD::addressof(__f)) {}
+        : __f_(__builtin_addressof(__f)) {}
 #else
     template <class _Up, class = __enable_if_t<!__is_same_uncvref<_Up, reference_wrapper>::value, decltype(__fun(declval<_Up>())) >>
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17
     reference_wrapper(_Up&& __u) _NOEXCEPT_(noexcept(__fun(declval<_Up>()))) {
         type& __f = static_cast<_Up&&>(__u);
-        __f_ = _VSTD::addressof(__f);
+        __f_ = __builtin_addressof(__f);
     }
 #endif
 
diff --git a/libcxx/include/__functional_base b/libcxx/include/__functional_base
index ccc3f3a58ca5..a188223c38bb 100644
--- a/libcxx/include/__functional_base
+++ b/libcxx/include/__functional_base
@@ -19,10 +19,14 @@
 #include <__functional/weak_result_type.h>
 #include <__memory/allocator_arg_t.h>
 #include <__memory/uses_allocator.h>
+#if 0
 #include <exception>
 #include <new>
+#endif
 #include <type_traits>
+#if 0
 #include <typeinfo>
+#endif
 #include <utility>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
diff --git a/libcxx/include/__hash_table b/libcxx/include/__hash_table
index 126e1884a664..b46ac6e9d843 100644
--- a/libcxx/include/__hash_table
+++ b/libcxx/include/__hash_table
@@ -151,7 +151,7 @@ struct __hash_key_value_types {
   }
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type* __get_ptr(__node_value_type& __n) {
-    return _VSTD::addressof(__n);
+    return __builtin_addressof(__n);
   }
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type&& __move(__node_value_type& __v) {
@@ -191,7 +191,7 @@ struct __hash_key_value_types<__hash_value_type<_Key, _Tp> > {
 
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type* __get_ptr(__node_value_type& __n) {
-    return _VSTD::addressof(__n.__get_value());
+    return __builtin_addressof(__n.__get_value());
   }
   _LIBCPP_INLINE_VISIBILITY
   static pair<key_type&&, mapped_type&&> __move(__node_value_type& __v) {
@@ -1571,7 +1571,7 @@ __hash_table<_Tp, _Hash, _Equal, _Alloc>::__deallocate_node(__next_pointer __np)
             {
                 (*__p)->__c_ = nullptr;
                 if (--__c->end_ != __p)
-                    _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+                    __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
             }
         }
         __get_db()->unlock();
@@ -2599,7 +2599,7 @@ __hash_table<_Tp, _Hash, _Equal, _Alloc>::remove(const_iterator __p) _NOEXCEPT
         {
             (*__dp)->__c_ = nullptr;
             if (--__c->end_ != __dp)
-                _VSTD::memmove(__dp, __dp+1, (__c->end_ - __dp)*sizeof(__i_node*));
+                __builtin_memmove(__dp, __dp+1, (__c->end_ - __dp)*sizeof(__i_node*));
         }
     }
     __get_db()->unlock();
diff --git a/libcxx/include/__iterator/advance.h b/libcxx/include/__iterator/advance.h
index a60052a08f0d..3e96e02a075f 100644
--- a/libcxx/include/__iterator/advance.h
+++ b/libcxx/include/__iterator/advance.h
@@ -17,7 +17,9 @@
 #include <__iterator/incrementable_traits.h>
 #include <__iterator/iterator_traits.h>
 #include <__utility/move.h>
+#if 0
 #include <cstdlib>
+#endif
 #include <concepts>
 #include <limits>
 #include <type_traits>
@@ -181,7 +183,7 @@ public:
       return __n;
     }
 
-    _LIBCPP_UNREACHABLE();
+    __builtin_unreachable();
   }
 };
 
diff --git a/libcxx/include/__iterator/back_insert_iterator.h b/libcxx/include/__iterator/back_insert_iterator.h
index 844babe5c5ed..4d9fbd64ceca 100644
--- a/libcxx/include/__iterator/back_insert_iterator.h
+++ b/libcxx/include/__iterator/back_insert_iterator.h
@@ -45,7 +45,7 @@ public:
     typedef void reference;
     typedef _Container container_type;
 
-    _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_AFTER_CXX17 explicit back_insert_iterator(_Container& __x) : container(_VSTD::addressof(__x)) {}
+    _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_AFTER_CXX17 explicit back_insert_iterator(_Container& __x) : container(__builtin_addressof(__x)) {}
     _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_AFTER_CXX17 back_insert_iterator& operator=(const typename _Container::value_type& __value_)
         {container->push_back(__value_); return *this;}
 #ifndef _LIBCPP_CXX03_LANG
diff --git a/libcxx/include/__iterator/common_iterator.h b/libcxx/include/__iterator/common_iterator.h
index 9a142769e55a..333d02995591 100644
--- a/libcxx/include/__iterator/common_iterator.h
+++ b/libcxx/include/__iterator/common_iterator.h
@@ -42,7 +42,7 @@ class common_iterator {
 
   public:
     const iter_value_t<_Iter>* operator->() const {
-      return _VSTD::addressof(__value);
+      return __builtin_addressof(__value);
     }
   };
 
@@ -134,7 +134,7 @@ public:
       return _VSTD::__unchecked_get<_Iter>(__hold_);
     } else if constexpr (is_reference_v<iter_reference_t<_Iter>>) {
       auto&& __tmp = *_VSTD::__unchecked_get<_Iter>(__hold_);
-      return _VSTD::addressof(__tmp);
+      return __builtin_addressof(__tmp);
     } else {
       return __proxy(*_VSTD::__unchecked_get<_Iter>(__hold_));
     }
diff --git a/libcxx/include/__iterator/front_insert_iterator.h b/libcxx/include/__iterator/front_insert_iterator.h
index b229a99f1104..05a9ccb35259 100644
--- a/libcxx/include/__iterator/front_insert_iterator.h
+++ b/libcxx/include/__iterator/front_insert_iterator.h
@@ -45,7 +45,7 @@ public:
     typedef void reference;
     typedef _Container container_type;
 
-    _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17 explicit front_insert_iterator(_Container& __x) : container(_VSTD::addressof(__x)) {}
+    _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17 explicit front_insert_iterator(_Container& __x) : container(__builtin_addressof(__x)) {}
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17 front_insert_iterator& operator=(const typename _Container::value_type& __value_)
         {container->push_front(__value_); return *this;}
 #ifndef _LIBCPP_CXX03_LANG
diff --git a/libcxx/include/__iterator/insert_iterator.h b/libcxx/include/__iterator/insert_iterator.h
index 33117419881b..fda488f31a0e 100644
--- a/libcxx/include/__iterator/insert_iterator.h
+++ b/libcxx/include/__iterator/insert_iterator.h
@@ -56,7 +56,7 @@ public:
     typedef _Container container_type;
 
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17 insert_iterator(_Container& __x, __insert_iterator_iter_t<_Container> __i)
-        : container(_VSTD::addressof(__x)), iter(__i) {}
+        : container(__builtin_addressof(__x)), iter(__i) {}
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17 insert_iterator& operator=(const typename _Container::value_type& __value_)
         {iter = container->insert(iter, __value_); ++iter; return *this;}
 #ifndef _LIBCPP_CXX03_LANG
diff --git a/libcxx/include/__iterator/istream_iterator.h b/libcxx/include/__iterator/istream_iterator.h
index 979d714edf5d..609aebd271bf 100644
--- a/libcxx/include/__iterator/istream_iterator.h
+++ b/libcxx/include/__iterator/istream_iterator.h
@@ -45,14 +45,14 @@ private:
     _Tp __value_;
 public:
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR istream_iterator() : __in_stream_(nullptr), __value_() {}
-    _LIBCPP_INLINE_VISIBILITY istream_iterator(istream_type& __s) : __in_stream_(_VSTD::addressof(__s))
+    _LIBCPP_INLINE_VISIBILITY istream_iterator(istream_type& __s) : __in_stream_(__builtin_addressof(__s))
         {
             if (!(*__in_stream_ >> __value_))
                 __in_stream_ = nullptr;
         }
 
     _LIBCPP_INLINE_VISIBILITY const _Tp& operator*() const {return __value_;}
-    _LIBCPP_INLINE_VISIBILITY const _Tp* operator->() const {return _VSTD::addressof((operator*()));}
+    _LIBCPP_INLINE_VISIBILITY const _Tp* operator->() const {return __builtin_addressof((operator*()));}
     _LIBCPP_INLINE_VISIBILITY istream_iterator& operator++()
         {
             if (!(*__in_stream_ >> __value_))
diff --git a/libcxx/include/__iterator/ostream_iterator.h b/libcxx/include/__iterator/ostream_iterator.h
index 20a36742ccab..8d2abb52e39e 100644
--- a/libcxx/include/__iterator/ostream_iterator.h
+++ b/libcxx/include/__iterator/ostream_iterator.h
@@ -49,9 +49,9 @@ private:
     const char_type* __delim_;
 public:
     _LIBCPP_INLINE_VISIBILITY ostream_iterator(ostream_type& __s) _NOEXCEPT
-        : __out_stream_(_VSTD::addressof(__s)), __delim_(nullptr) {}
+        : __out_stream_(__builtin_addressof(__s)), __delim_(nullptr) {}
     _LIBCPP_INLINE_VISIBILITY ostream_iterator(ostream_type& __s, const _CharT* __delimiter) _NOEXCEPT
-        : __out_stream_(_VSTD::addressof(__s)), __delim_(__delimiter) {}
+        : __out_stream_(__builtin_addressof(__s)), __delim_(__delimiter) {}
     _LIBCPP_INLINE_VISIBILITY ostream_iterator& operator=(const _Tp& __value_)
         {
             *__out_stream_ << __value_;
diff --git a/libcxx/include/__iterator/reverse_iterator.h b/libcxx/include/__iterator/reverse_iterator.h
index fad9bc175b0c..c8dda1b79b4e 100644
--- a/libcxx/include/__iterator/reverse_iterator.h
+++ b/libcxx/include/__iterator/reverse_iterator.h
@@ -124,7 +124,7 @@ public:
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX14
     reference operator*() const {_Iter __tmp = current; return *--__tmp;}
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX14
-    pointer  operator->() const {return _VSTD::addressof(operator*());}
+    pointer  operator->() const {return __builtin_addressof(operator*());}
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX14
     reverse_iterator& operator++() {--current; return *this;}
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX14
diff --git a/libcxx/include/__iterator/wrap_iter.h b/libcxx/include/__iterator/wrap_iter.h
index 28872f9fa41a..7dcb18074c3d 100644
--- a/libcxx/include/__iterator/wrap_iter.h
+++ b/libcxx/include/__iterator/wrap_iter.h
@@ -55,7 +55,7 @@ public:
             : __i(__u.base())
     {
 #if _LIBCPP_DEBUG_LEVEL == 2
-        __get_db()->__iterator_copy(this, _VSTD::addressof(__u));
+        __get_db()->__iterator_copy(this, __builtin_addressof(__u));
 #endif
     }
 #if _LIBCPP_DEBUG_LEVEL == 2
@@ -63,14 +63,14 @@ public:
     __wrap_iter(const __wrap_iter& __x)
         : __i(__x.base())
     {
-        __get_db()->__iterator_copy(this, _VSTD::addressof(__x));
+        __get_db()->__iterator_copy(this, __builtin_addressof(__x));
     }
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_IF_NODEBUG
     __wrap_iter& operator=(const __wrap_iter& __x)
     {
-        if (this != _VSTD::addressof(__x))
+        if (this != __builtin_addressof(__x))
         {
-            __get_db()->__iterator_copy(this, _VSTD::addressof(__x));
+            __get_db()->__iterator_copy(this, __builtin_addressof(__x));
             __i = __x.__i;
         }
         return *this;
@@ -181,7 +181,7 @@ _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_IF_NODEBUG
 bool operator<(const __wrap_iter<_Iter1>& __x, const __wrap_iter<_Iter1>& __y) _NOEXCEPT
 {
 #if _LIBCPP_DEBUG_LEVEL == 2
-    _LIBCPP_ASSERT(__get_const_db()->__less_than_comparable(_VSTD::addressof(__x), _VSTD::addressof(__y)),
+    _LIBCPP_ASSERT(__get_const_db()->__less_than_comparable(__builtin_addressof(__x), __builtin_addressof(__y)),
                    "Attempted to compare incomparable iterators");
 #endif
     return __x.base() < __y.base();
@@ -265,7 +265,7 @@ operator-(const __wrap_iter<_Iter1>& __x, const __wrap_iter<_Iter2>& __y) _NOEXC
 #endif // C++03
 {
 #if _LIBCPP_DEBUG_LEVEL == 2
-    _LIBCPP_ASSERT(__get_const_db()->__less_than_comparable(_VSTD::addressof(__x), _VSTD::addressof(__y)),
+    _LIBCPP_ASSERT(__get_const_db()->__less_than_comparable(__builtin_addressof(__x), __builtin_addressof(__y)),
                    "Attempted to subtract incompatible iterators");
 #endif
     return __x.base() - __y.base();
diff --git a/libcxx/include/__mbstate_t.h b/libcxx/include/__mbstate_t.h
index 3489f9cc0e3a..b6f35ce97fe1 100644
--- a/libcxx/include/__mbstate_t.h
+++ b/libcxx/include/__mbstate_t.h
@@ -25,6 +25,9 @@
 // This is a gruesome hack, but I don't know how to make it cleaner for
 // the time being.
 
+#if 1
+typedef size_t mbstate_t;
+#else
 #ifndef _LIBCPP_HAS_NO_WIDE_CHARACTERS
 #   include <wchar.h> // for mbstate_t
 #elif __has_include(<bits/types/mbstate_t.h>)
@@ -34,6 +37,7 @@
 #else
 #   error "The library was configured without support for wide-characters, but we don't know how to get the definition of mbstate_t without <wchar.h> on your platform."
 #endif
+#endif
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
diff --git a/libcxx/include/__memory/allocator.h b/libcxx/include/__memory/allocator.h
index 283212fb703d..fee5b81b5d4e 100644
--- a/libcxx/include/__memory/allocator.h
+++ b/libcxx/include/__memory/allocator.h
@@ -129,11 +129,11 @@ public:
 
     _LIBCPP_DEPRECATED_IN_CXX17 _LIBCPP_INLINE_VISIBILITY
     pointer address(reference __x) const _NOEXCEPT {
-        return _VSTD::addressof(__x);
+        return __builtin_addressof(__x);
     }
     _LIBCPP_DEPRECATED_IN_CXX17 _LIBCPP_INLINE_VISIBILITY
     const_pointer address(const_reference __x) const _NOEXCEPT {
-        return _VSTD::addressof(__x);
+        return __builtin_addressof(__x);
     }
 
     _LIBCPP_NODISCARD_AFTER_CXX17 _LIBCPP_INLINE_VISIBILITY _LIBCPP_DEPRECATED_IN_CXX17
@@ -211,7 +211,7 @@ public:
 
     _LIBCPP_DEPRECATED_IN_CXX17 _LIBCPP_INLINE_VISIBILITY
     const_pointer address(const_reference __x) const _NOEXCEPT {
-        return _VSTD::addressof(__x);
+        return __builtin_addressof(__x);
     }
 
     _LIBCPP_NODISCARD_AFTER_CXX17 _LIBCPP_INLINE_VISIBILITY _LIBCPP_DEPRECATED_IN_CXX17
diff --git a/libcxx/include/__memory/construct_at.h b/libcxx/include/__memory/construct_at.h
index 789677d7a613..eeb568e3f18b 100644
--- a/libcxx/include/__memory/construct_at.h
+++ b/libcxx/include/__memory/construct_at.h
@@ -67,14 +67,14 @@ template <class _ForwardIterator>
 _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_AFTER_CXX17
 void destroy(_ForwardIterator __first, _ForwardIterator __last) {
     for (; __first != __last; ++__first)
-        _VSTD::destroy_at(_VSTD::addressof(*__first));
+        _VSTD::destroy_at(__builtin_addressof(*__first));
 }
 
 template <class _ForwardIterator, class _Size>
 _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_AFTER_CXX17
 _ForwardIterator destroy_n(_ForwardIterator __first, _Size __n) {
     for (; __n > 0; (void)++__first, --__n)
-        _VSTD::destroy_at(_VSTD::addressof(*__first));
+        _VSTD::destroy_at(__builtin_addressof(*__first));
     return __first;
 }
 
diff --git a/libcxx/include/__memory/pointer_traits.h b/libcxx/include/__memory/pointer_traits.h
index 07bb6d437d7e..4b45ce2ffb2e 100644
--- a/libcxx/include/__memory/pointer_traits.h
+++ b/libcxx/include/__memory/pointer_traits.h
@@ -148,7 +148,7 @@ public:
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17
     static pointer pointer_to(typename conditional<is_void<element_type>::value,
                                       __nat, element_type>::type& __r) _NOEXCEPT
-        {return _VSTD::addressof(__r);}
+        {return __builtin_addressof(__r);}
 };
 
 template <class _From, class _To>
diff --git a/libcxx/include/__memory/raw_storage_iterator.h b/libcxx/include/__memory/raw_storage_iterator.h
index abe9677cb668..d819d555f976 100644
--- a/libcxx/include/__memory/raw_storage_iterator.h
+++ b/libcxx/include/__memory/raw_storage_iterator.h
@@ -48,10 +48,10 @@ public:
     _LIBCPP_INLINE_VISIBILITY explicit raw_storage_iterator(_OutputIterator __x) : __x_(__x) {}
     _LIBCPP_INLINE_VISIBILITY raw_storage_iterator& operator*() {return *this;}
     _LIBCPP_INLINE_VISIBILITY raw_storage_iterator& operator=(const _Tp& __element)
-        {::new ((void*)_VSTD::addressof(*__x_)) _Tp(__element); return *this;}
+        {::new ((void*)__builtin_addressof(*__x_)) _Tp(__element); return *this;}
 #if _LIBCPP_STD_VER >= 14
     _LIBCPP_INLINE_VISIBILITY raw_storage_iterator& operator=(_Tp&& __element)
-        {::new ((void*)_VSTD::addressof(*__x_)) _Tp(_VSTD::move(__element)); return *this;}
+        {::new ((void*)__builtin_addressof(*__x_)) _Tp(_VSTD::move(__element)); return *this;}
 #endif
     _LIBCPP_INLINE_VISIBILITY raw_storage_iterator& operator++() {++__x_; return *this;}
     _LIBCPP_INLINE_VISIBILITY raw_storage_iterator  operator++(int)
diff --git a/libcxx/include/__memory/shared_ptr.h b/libcxx/include/__memory/shared_ptr.h
index 7b08641762d8..886112d3a9c1 100644
--- a/libcxx/include/__memory/shared_ptr.h
+++ b/libcxx/include/__memory/shared_ptr.h
@@ -25,7 +25,9 @@
 #include <__memory/unique_ptr.h>
 #include <__utility/forward.h>
 #include <cstddef>
+#if 0
 #include <cstdlib> // abort
+#endif
 #include <iosfwd>
 #include <stdexcept>
 #include <typeinfo>
@@ -128,7 +130,7 @@ public:
     bad_weak_ptr() _NOEXCEPT = default;
     bad_weak_ptr(const bad_weak_ptr&) _NOEXCEPT = default;
     virtual ~bad_weak_ptr() _NOEXCEPT;
-    virtual const char* what() const  _NOEXCEPT;
+    virtual constant const char* what() const  _NOEXCEPT;
 };
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
@@ -137,7 +139,7 @@ void __throw_bad_weak_ptr()
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw bad_weak_ptr();
 #else
-    _VSTD::abort();
+    __builtin_unreachable();
 #endif
 }
 
@@ -222,7 +224,9 @@ public:
     long use_count() const _NOEXCEPT {return __shared_count::use_count();}
     __shared_weak_count* lock() _NOEXCEPT;
 
+#ifndef _LIBCPP_NO_RTTI
     virtual const void* __get_deleter(const type_info&) const _NOEXCEPT;
+#endif
 private:
     virtual void __on_zero_shared_weak() _NOEXCEPT = 0;
 };
@@ -252,7 +256,7 @@ template <class _Tp, class _Dp, class _Alloc>
 const void*
 __shared_ptr_pointer<_Tp, _Dp, _Alloc>::__get_deleter(const type_info& __t) const _NOEXCEPT
 {
-    return __t == typeid(_Dp) ? _VSTD::addressof(__data_.first().second()) : nullptr;
+    return __t == typeid(_Dp) ? __builtin_addressof(__data_.first().second()) : nullptr;
 }
 
 #endif // _LIBCPP_NO_RTTI
@@ -504,13 +508,13 @@ public:
             typedef __allocator_destructor<_A2> _D2;
             _A2 __a2(__a);
             unique_ptr<_CntrlBlk, _D2> __hold2(__a2.allocate(1), _D2(__a2, 1));
-            ::new ((void*)_VSTD::addressof(*__hold2.get()))
+            ::new ((void*)__builtin_addressof(*__hold2.get()))
 #ifndef _LIBCPP_CXX03_LANG
                 _CntrlBlk(__p, _VSTD::move(__d), __a);
 #else
                 _CntrlBlk(__p, __d, __a);
 #endif // not _LIBCPP_CXX03_LANG
-            __cntrl_ = _VSTD::addressof(*__hold2.release());
+            __cntrl_ = __builtin_addressof(*__hold2.release());
             __enable_weak_this(__p, __p);
 #ifndef _LIBCPP_NO_EXCEPTIONS
         }
@@ -562,13 +566,13 @@ public:
             typedef __allocator_destructor<_A2> _D2;
             _A2 __a2(__a);
             unique_ptr<_CntrlBlk, _D2> __hold2(__a2.allocate(1), _D2(__a2, 1));
-            ::new ((void*)_VSTD::addressof(*__hold2.get()))
+            ::new ((void*)__builtin_addressof(*__hold2.get()))
 #ifndef _LIBCPP_CXX03_LANG
                 _CntrlBlk(__p, _VSTD::move(__d), __a);
 #else
                 _CntrlBlk(__p, __d, __a);
 #endif // not _LIBCPP_CXX03_LANG
-            __cntrl_ = _VSTD::addressof(*__hold2.release());
+            __cntrl_ = __builtin_addressof(*__hold2.release());
 #ifndef _LIBCPP_NO_EXCEPTIONS
         }
         catch (...)
@@ -951,9 +955,9 @@ shared_ptr<_Tp> allocate_shared(const _Alloc& __a, _Args&& ...__args)
     using _ControlBlock = __shared_ptr_emplace<_Tp, _Alloc>;
     using _ControlBlockAllocator = typename __allocator_traits_rebind<_Alloc, _ControlBlock>::type;
     __allocation_guard<_ControlBlockAllocator> __guard(__a, 1);
-    ::new ((void*)_VSTD::addressof(*__guard.__get())) _ControlBlock(__a, _VSTD::forward<_Args>(__args)...);
+    ::new ((void*)__builtin_addressof(*__guard.__get())) _ControlBlock(__a, _VSTD::forward<_Args>(__args)...);
     auto __control_block = __guard.__release_ptr();
-    return shared_ptr<_Tp>::__create_with_control_block((*__control_block).__get_elem(), _VSTD::addressof(*__control_block));
+    return shared_ptr<_Tp>::__create_with_control_block((*__control_block).__get_elem(), __builtin_addressof(*__control_block));
 }
 
 template<class _Tp, class ..._Args, class = __enable_if_t<!is_array<_Tp>::value> >
diff --git a/libcxx/include/__memory/temporary_buffer.h b/libcxx/include/__memory/temporary_buffer.h
index 06aa6c2936fe..6790481b7be5 100644
--- a/libcxx/include/__memory/temporary_buffer.h
+++ b/libcxx/include/__memory/temporary_buffer.h
@@ -53,7 +53,7 @@ get_temporary_buffer(ptrdiff_t __n) _NOEXCEPT
             return __r;
         }
 
-        __r.first = static_cast<_Tp*>(::operator new(__n * sizeof(_Tp), nothrow));
+        __r.first = static_cast<_Tp*>(__builtin_operator_new(__n * sizeof(_Tp)));
 #endif
 
         if (__r.first)
diff --git a/libcxx/include/__memory/uninitialized_algorithms.h b/libcxx/include/__memory/uninitialized_algorithms.h
index e83d62e0db08..be1ad1bdbdb7 100644
--- a/libcxx/include/__memory/uninitialized_algorithms.h
+++ b/libcxx/include/__memory/uninitialized_algorithms.h
@@ -33,7 +33,7 @@ uninitialized_copy(_InputIterator __f, _InputIterator __l, _ForwardIterator __r)
     {
 #endif
         for (; __f != __l; ++__f, (void) ++__r)
-            ::new ((void*)_VSTD::addressof(*__r)) value_type(*__f);
+            ::new ((void*)__builtin_addressof(*__r)) value_type(*__f);
 #ifndef _LIBCPP_NO_EXCEPTIONS
     }
     catch (...)
@@ -57,7 +57,7 @@ uninitialized_copy_n(_InputIterator __f, _Size __n, _ForwardIterator __r)
     {
 #endif
         for (; __n > 0; ++__f, (void) ++__r, (void) --__n)
-            ::new ((void*)_VSTD::addressof(*__r)) value_type(*__f);
+            ::new ((void*)__builtin_addressof(*__r)) value_type(*__f);
 #ifndef _LIBCPP_NO_EXCEPTIONS
     }
     catch (...)
@@ -81,7 +81,7 @@ uninitialized_fill(_ForwardIterator __f, _ForwardIterator __l, const _Tp& __x)
     {
 #endif
         for (; __f != __l; ++__f)
-            ::new ((void*)_VSTD::addressof(*__f)) value_type(__x);
+            ::new ((void*)__builtin_addressof(*__f)) value_type(__x);
 #ifndef _LIBCPP_NO_EXCEPTIONS
     }
     catch (...)
@@ -104,7 +104,7 @@ uninitialized_fill_n(_ForwardIterator __f, _Size __n, const _Tp& __x)
     {
 #endif
         for (; __n > 0; ++__f, (void) --__n)
-            ::new ((void*)_VSTD::addressof(*__f)) value_type(__x);
+            ::new ((void*)__builtin_addressof(*__f)) value_type(__x);
 #ifndef _LIBCPP_NO_EXCEPTIONS
     }
     catch (...)
@@ -128,7 +128,7 @@ void uninitialized_default_construct(_ForwardIterator __first, _ForwardIterator
     try {
 #endif
     for (; __idx != __last; ++__idx)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt;
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt;
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
         _VSTD::destroy(__first, __idx);
@@ -146,7 +146,7 @@ _ForwardIterator uninitialized_default_construct_n(_ForwardIterator __first, _Si
     try {
 #endif
     for (; __n > 0; ++__idx, (void) --__n)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt;
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt;
     return __idx;
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
@@ -166,7 +166,7 @@ void uninitialized_value_construct(_ForwardIterator __first, _ForwardIterator __
     try {
 #endif
     for (; __idx != __last; ++__idx)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt();
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt();
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
         _VSTD::destroy(__first, __idx);
@@ -184,7 +184,7 @@ _ForwardIterator uninitialized_value_construct_n(_ForwardIterator __first, _Size
     try {
 #endif
     for (; __n > 0; ++__idx, (void) --__n)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt();
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt();
     return __idx;
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
@@ -204,7 +204,7 @@ _ForwardIt uninitialized_move(_InputIt __first, _InputIt __last, _ForwardIt __fi
     try {
 #endif
     for (; __first != __last; ++__idx, (void) ++__first)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt(_VSTD::move(*__first));
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt(_VSTD::move(*__first));
     return __idx;
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
@@ -224,7 +224,7 @@ uninitialized_move_n(_InputIt __first, _Size __n, _ForwardIt __first_res) {
     try {
 #endif
     for (; __n > 0; ++__idx, (void) ++__first, --__n)
-        ::new ((void*)_VSTD::addressof(*__idx)) _Vt(_VSTD::move(*__first));
+        ::new ((void*)__builtin_addressof(*__idx)) _Vt(_VSTD::move(*__first));
     return {__first, __idx};
 #ifndef _LIBCPP_NO_EXCEPTIONS
     } catch (...) {
diff --git a/libcxx/include/__mutex_base b/libcxx/include/__mutex_base
index da2967164a68..0f1da7c6d80a 100644
--- a/libcxx/include/__mutex_base
+++ b/libcxx/include/__mutex_base
@@ -115,24 +115,24 @@ public:
     unique_lock() _NOEXCEPT : __m_(nullptr), __owns_(false) {}
     _LIBCPP_INLINE_VISIBILITY
     explicit unique_lock(mutex_type& __m)
-        : __m_(_VSTD::addressof(__m)), __owns_(true) {__m_->lock();}
+        : __m_(__builtin_addressof(__m)), __owns_(true) {__m_->lock();}
     _LIBCPP_INLINE_VISIBILITY
     unique_lock(mutex_type& __m, defer_lock_t) _NOEXCEPT
-        : __m_(_VSTD::addressof(__m)), __owns_(false) {}
+        : __m_(__builtin_addressof(__m)), __owns_(false) {}
     _LIBCPP_INLINE_VISIBILITY
     unique_lock(mutex_type& __m, try_to_lock_t)
-        : __m_(_VSTD::addressof(__m)), __owns_(__m.try_lock()) {}
+        : __m_(__builtin_addressof(__m)), __owns_(__m.try_lock()) {}
     _LIBCPP_INLINE_VISIBILITY
     unique_lock(mutex_type& __m, adopt_lock_t)
-        : __m_(_VSTD::addressof(__m)), __owns_(true) {}
+        : __m_(__builtin_addressof(__m)), __owns_(true) {}
     template <class _Clock, class _Duration>
     _LIBCPP_INLINE_VISIBILITY
         unique_lock(mutex_type& __m, const chrono::time_point<_Clock, _Duration>& __t)
-            : __m_(_VSTD::addressof(__m)), __owns_(__m.try_lock_until(__t)) {}
+            : __m_(__builtin_addressof(__m)), __owns_(__m.try_lock_until(__t)) {}
     template <class _Rep, class _Period>
     _LIBCPP_INLINE_VISIBILITY
         unique_lock(mutex_type& __m, const chrono::duration<_Rep, _Period>& __d)
-            : __m_(_VSTD::addressof(__m)), __owns_(__m.try_lock_for(__d)) {}
+            : __m_(__builtin_addressof(__m)), __owns_(__m.try_lock_for(__d)) {}
     _LIBCPP_INLINE_VISIBILITY
     ~unique_lock()
     {
diff --git a/libcxx/include/__ranges/ref_view.h b/libcxx/include/__ranges/ref_view.h
index 7567ac48f255..449e317c57b9 100644
--- a/libcxx/include/__ranges/ref_view.h
+++ b/libcxx/include/__ranges/ref_view.h
@@ -47,7 +47,7 @@ public:
         convertible_to<_Tp, _Range&> && requires { __fun(declval<_Tp>()); }
     _LIBCPP_HIDE_FROM_ABI
     constexpr ref_view(_Tp&& __t)
-      : __range_(_VSTD::addressof(static_cast<_Range&>(_VSTD::forward<_Tp>(__t))))
+      : __range_(__builtin_addressof(static_cast<_Range&>(_VSTD::forward<_Tp>(__t))))
     {}
 
     _LIBCPP_HIDE_FROM_ABI constexpr _Range& base() const { return *__range_; }
diff --git a/libcxx/include/__string b/libcxx/include/__string
index 1691b44ed537..0e62e7594c7e 100644
--- a/libcxx/include/__string
+++ b/libcxx/include/__string
@@ -349,7 +349,7 @@ struct _LIBCPP_TEMPLATE_VIS char_traits<char>
         {
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__move_constexpr(__s1, __s2, __n)
-                       : __n == 0 ? __s1 : (char_type*)_VSTD::memmove(__s1, __s2, __n);
+                       : __n == 0 ? __s1 : (char_type*)__builtin_memmove(__s1, __s2, __n);
         }
     static inline _LIBCPP_CONSTEXPR_AFTER_CXX17
     char_type* copy(char_type* __s1, const char_type* __s2, size_t __n) _NOEXCEPT
@@ -359,14 +359,14 @@ struct _LIBCPP_TEMPLATE_VIS char_traits<char>
             }
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__copy_constexpr(__s1, __s2, __n)
-                       : __n == 0 ? __s1 : (char_type*)_VSTD::memcpy(__s1, __s2, __n);
+                       : __n == 0 ? __s1 : (char_type*)__builtin_memcpy(__s1, __s2, __n);
         }
     static inline _LIBCPP_CONSTEXPR_AFTER_CXX17
     char_type* assign(char_type* __s, size_t __n, char_type __a) _NOEXCEPT
         {
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__assign_constexpr(__s, __n, __a)
-                       : __n == 0 ? __s : (char_type*)_VSTD::memset(__s, to_int_type(__a), __n);
+                       : __n == 0 ? __s : (char_type*)__builtin_memset(__s, to_int_type(__a), __n);
         }
 
     static inline _LIBCPP_CONSTEXPR int_type  not_eof(int_type __c) _NOEXCEPT
@@ -390,7 +390,7 @@ char_traits<char>::compare(const char_type* __s1, const char_type* __s2, size_t
 #if __has_feature(cxx_constexpr_string_builtins)
     return __builtin_memcmp(__s1, __s2, __n);
 #elif _LIBCPP_STD_VER <= 14
-    return _VSTD::memcmp(__s1, __s2, __n);
+    return __builtin_memcmp(__s1, __s2, __n);
 #else
     for (; __n; --__n, ++__s1, ++__s2)
     {
@@ -412,7 +412,7 @@ char_traits<char>::find(const char_type* __s, size_t __n, const char_type& __a)
 #if __has_feature(cxx_constexpr_string_builtins)
     return __builtin_char_memchr(__s, to_int_type(__a), __n);
 #elif _LIBCPP_STD_VER <= 14
-    return (const char_type*) _VSTD::memchr(__s, to_int_type(__a), __n);
+    return (const char_type*) __builtin_memchr(__s, to_int_type(__a), __n);
 #else
     for (; __n; --__n)
     {
@@ -589,7 +589,7 @@ struct _LIBCPP_TEMPLATE_VIS char_traits<char8_t>
         {
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__move_constexpr(__s1, __s2, __n)
-                       : __n == 0 ? __s1 : (char_type*)_VSTD::memmove(__s1, __s2, __n);
+                       : __n == 0 ? __s1 : (char_type*)__builtin_memmove(__s1, __s2, __n);
         }
 
     static _LIBCPP_CONSTEXPR_AFTER_CXX17
@@ -600,7 +600,7 @@ struct _LIBCPP_TEMPLATE_VIS char_traits<char8_t>
             }
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__copy_constexpr(__s1, __s2, __n)
-                       : __n == 0 ? __s1 : (char_type*)_VSTD::memcpy(__s1, __s2, __n);
+                       : __n == 0 ? __s1 : (char_type*)__builtin_memcpy(__s1, __s2, __n);
         }
 
     static _LIBCPP_CONSTEXPR_AFTER_CXX17
@@ -608,7 +608,7 @@ struct _LIBCPP_TEMPLATE_VIS char_traits<char8_t>
         {
             return __libcpp_is_constant_evaluated()
                        ? _VSTD::__assign_constexpr(__s, __n, __a)
-                       : __n == 0 ? __s : (char_type*)_VSTD::memset(__s, to_int_type(__a), __n);
+                       : __n == 0 ? __s : (char_type*)__builtin_memset(__s, to_int_type(__a), __n);
         }
 
     static inline constexpr int_type  not_eof(int_type __c) noexcept
diff --git a/libcxx/include/__tree b/libcxx/include/__tree
index f3f2e8d15f5d..87f7be097120 100644
--- a/libcxx/include/__tree
+++ b/libcxx/include/__tree
@@ -564,7 +564,7 @@ struct __tree_key_value_types {
   }
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type* __get_ptr(__node_value_type& __n) {
-    return _VSTD::addressof(__n);
+    return __builtin_addressof(__n);
   }
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type&& __move(__node_value_type& __v) {
@@ -611,7 +611,7 @@ struct __tree_key_value_types<__value_type<_Key, _Tp> > {
 
   _LIBCPP_INLINE_VISIBILITY
   static __container_value_type* __get_ptr(__node_value_type& __n) {
-    return _VSTD::addressof(__n.__get_value());
+    return __builtin_addressof(__n.__get_value());
   }
 
   _LIBCPP_INLINE_VISIBILITY
@@ -1081,7 +1081,7 @@ public:
         {return static_cast<__node_pointer>(__end_node()->__left_);}
 
     __node_base_pointer* __root_ptr() const _NOEXCEPT {
-        return _VSTD::addressof(__end_node()->__left_);
+        return __builtin_addressof(__end_node()->__left_);
     }
 
     typedef __tree_iterator<value_type, __node_pointer, difference_type>             iterator;
@@ -1977,7 +1977,7 @@ __tree<_Tp, _Compare, _Allocator>::__find_equal(__parent_pointer& __parent,
             if (value_comp()(__v, __nd->__value_))
             {
                 if (__nd->__left_ != nullptr) {
-                    __nd_ptr = _VSTD::addressof(__nd->__left_);
+                    __nd_ptr = __builtin_addressof(__nd->__left_);
                     __nd = static_cast<__node_pointer>(__nd->__left_);
                 } else {
                     __parent = static_cast<__parent_pointer>(__nd);
@@ -1987,7 +1987,7 @@ __tree<_Tp, _Compare, _Allocator>::__find_equal(__parent_pointer& __parent,
             else if (value_comp()(__nd->__value_, __v))
             {
                 if (__nd->__right_ != nullptr) {
-                    __nd_ptr = _VSTD::addressof(__nd->__right_);
+                    __nd_ptr = __builtin_addressof(__nd->__right_);
                     __nd = static_cast<__node_pointer>(__nd->__right_);
                 } else {
                     __parent = static_cast<__parent_pointer>(__nd);
diff --git a/libcxx/include/algorithm b/libcxx/include/algorithm
index 77a9d74a8390..d7242d2d32f0 100644
--- a/libcxx/include/algorithm
+++ b/libcxx/include/algorithm
@@ -649,7 +649,9 @@ template <class BidirectionalIterator, class Compare>
 #include <__debug>
 #include <__bits> // __libcpp_clz
 #include <cstddef>
+#if 0
 #include <cstring>
+#endif
 #include <functional>
 #include <initializer_list>
 #include <utility> // needed to provide swap_ranges.
diff --git a/libcxx/include/array b/libcxx/include/array
index 0ddfc955af9f..f483ab0190f9 100644
--- a/libcxx/include/array
+++ b/libcxx/include/array
@@ -112,7 +112,11 @@ template <size_t I, class T, size_t N> const T&& get(const array<T, N>&&) noexce
 #include <__debug>
 #include <__tuple>
 #include <algorithm>
+#if 0
 #include <cstdlib> // for _LIBCPP_UNREACHABLE
+#else
+#define _LIBCPP_UNREACHABLE __builtin_unreachable
+#endif
 #include <iterator>
 #include <stdexcept>
 #include <type_traits>
diff --git a/libcxx/include/atomic b/libcxx/include/atomic
index 2e2ee63b676d..3c7f7ee8e061 100644
--- a/libcxx/include/atomic
+++ b/libcxx/include/atomic
@@ -520,10 +520,14 @@ template <class T>
 
 #include <__availability>
 #include <__config>
+#if 0
 #include <__threading_support>
+#endif
 #include <cstddef>
+#if 0
 #include <cstdint>
 #include <cstring>
+#endif
 #include <type_traits>
 #include <version>
 
@@ -531,6 +535,7 @@ template <class T>
 #pragma GCC system_header
 #endif
 
+#if 0
 #ifdef _LIBCPP_HAS_NO_THREADS
 # error <atomic> is not supported on this single threaded system
 #endif
@@ -540,7 +545,9 @@ template <class T>
 #ifdef kill_dependency
 # error C++ standard library is incompatible with <stdatomic.h>
 #endif
+#endif
 
+#if 0
 #define _LIBCPP_CHECK_STORE_MEMORY_ORDER(__m) \
   _LIBCPP_DIAGNOSE_WARNING(__m == memory_order_consume || \
                            __m == memory_order_acquire || \
@@ -556,6 +563,19 @@ template <class T>
   _LIBCPP_DIAGNOSE_WARNING(__f == memory_order_release || \
                            __f == memory_order_acq_rel,   \
                         "memory order argument to atomic operation is invalid")
+#else // we only support "relaxed"
+#define _LIBCPP_CHECK_STORE_MEMORY_ORDER(__m) \
+  _LIBCPP_DIAGNOSE_WARNING(__m == memory_order_relaxed,   \
+                        "memory order argument to atomic operation is invalid")
+
+#define _LIBCPP_CHECK_LOAD_MEMORY_ORDER(__m) \
+  _LIBCPP_DIAGNOSE_WARNING(__m == memory_order_relaxed,   \
+                        "memory order argument to atomic operation is invalid")
+
+#define _LIBCPP_CHECK_EXCHANGE_MEMORY_ORDER(__m, __f) \
+  _LIBCPP_DIAGNOSE_WARNING(__f == memory_order_relaxed,   \
+                        "memory order argument to atomic operation is invalid")
+#endif
 
 _LIBCPP_BEGIN_NAMESPACE_STD
 
@@ -573,7 +593,8 @@ enum __legacy_memory_order {
 
 typedef underlying_type<__legacy_memory_order>::type __memory_order_underlying_t;
 
-#if _LIBCPP_STD_VER > 17
+#if 1 // we don't want any of these - memory order is provided by libfloor
+#elif _LIBCPP_STD_VER > 17
 
 enum class memory_order : __memory_order_underlying_t {
   relaxed = __mo_relaxed,
@@ -606,7 +627,7 @@ typedef enum memory_order {
 
 template <typename _Tp> _LIBCPP_INLINE_VISIBILITY
 bool __cxx_nonatomic_compare_equal(_Tp const& __lhs, _Tp const& __rhs) {
-    return _VSTD::memcmp(&__lhs, &__rhs, sizeof(_Tp)) == 0;
+    return __builtin_memcmp(&__lhs, &__rhs, sizeof(_Tp)) == 0;
 }
 
 static_assert((is_same<underlying_type<memory_order>::type, __memory_order_underlying_t>::value),
@@ -635,7 +656,19 @@ __cxx_atomic_assign_volatile(_Tp volatile& __a_value, _Tv volatile const& __val)
 
 #endif
 
-#if defined(_LIBCPP_HAS_GCC_ATOMIC_IMP)
+#if 1 // we don't want any of these - atomic functions are provided by libfloor
+
+template <typename _Tp>
+struct __cxx_atomic_base_impl {
+
+  _LIBCPP_INLINE_VISIBILITY
+    __cxx_atomic_base_impl() _NOEXCEPT = default;
+  _LIBCPP_CONSTEXPR explicit __cxx_atomic_base_impl(_Tp value) _NOEXCEPT
+    : __a_value(value) {}
+  _LIBCPP_DISABLE_EXTENSION_WARNING _Tp __a_value {};
+};
+
+#elif defined(_LIBCPP_HAS_GCC_ATOMIC_IMP)
 
 template <typename _Tp>
 struct __cxx_atomic_base_impl {
@@ -1075,6 +1108,21 @@ _Tp kill_dependency(_Tp __y) _NOEXCEPT
     return __y;
 }
 
+#if 1
+// fixed libfloor defines
+# define ATOMIC_BOOL_LOCK_FREE      0
+# define ATOMIC_CHAR_LOCK_FREE      0
+# define ATOMIC_CHAR8_T_LOCK_FREE   0
+# define ATOMIC_CHAR16_T_LOCK_FREE  0
+# define ATOMIC_CHAR32_T_LOCK_FREE  2
+# define ATOMIC_WCHAR_T_LOCK_FREE   0
+# define ATOMIC_SHORT_LOCK_FREE     0
+# define ATOMIC_INT_LOCK_FREE       2
+# define ATOMIC_LONG_LOCK_FREE      FLOOR_ATOMIC_LONG_LOCK_FREE
+# define ATOMIC_LLONG_LOCK_FREE     FLOOR_ATOMIC_LLONG_LOCK_FREE
+# define ATOMIC_POINTER_LOCK_FREE   FLOOR_ATOMIC_POINTER_LOCK_FREE
+
+#else
 #if defined(__CLANG_ATOMIC_BOOL_LOCK_FREE)
 # define ATOMIC_BOOL_LOCK_FREE      __CLANG_ATOMIC_BOOL_LOCK_FREE
 # define ATOMIC_CHAR_LOCK_FREE      __CLANG_ATOMIC_CHAR_LOCK_FREE
@@ -1104,6 +1152,7 @@ _Tp kill_dependency(_Tp __y) _NOEXCEPT
 # define ATOMIC_LLONG_LOCK_FREE     __GCC_ATOMIC_LLONG_LOCK_FREE
 # define ATOMIC_POINTER_LOCK_FREE   __GCC_ATOMIC_POINTER_LOCK_FREE
 #endif
+#endif
 
 #ifdef _LIBCPP_ATOMIC_ONLY_USE_BUILTINS
 
@@ -1213,7 +1262,7 @@ bool __cxx_atomic_compare_exchange_strong(volatile __cxx_atomic_lock_impl<_Tp>*
   _Tp __temp;
   __a->__lock();
   __cxx_atomic_assign_volatile(__temp, __a->__a_value);
-  bool __ret = (_VSTD::memcmp(&__temp, __expected, sizeof(_Tp)) == 0);
+  bool __ret = (__builtin_memcmp(&__temp, __expected, sizeof(_Tp)) == 0);
   if(__ret)
     __cxx_atomic_assign_volatile(__a->__a_value, __value);
   else
@@ -1226,11 +1275,11 @@ _LIBCPP_INLINE_VISIBILITY
 bool __cxx_atomic_compare_exchange_strong(__cxx_atomic_lock_impl<_Tp>* __a,
                                           _Tp* __expected, _Tp __value, memory_order, memory_order) {
   __a->__lock();
-  bool __ret = (_VSTD::memcmp(&__a->__a_value, __expected, sizeof(_Tp)) == 0);
+  bool __ret = (__builtin_memcmp(&__a->__a_value, __expected, sizeof(_Tp)) == 0);
   if(__ret)
-    _VSTD::memcpy(&__a->__a_value, &__value, sizeof(_Tp));
+    __builtin_memcpy(&__a->__a_value, &__value, sizeof(_Tp));
   else
-    _VSTD::memcpy(__expected, &__a->__a_value, sizeof(_Tp));
+    __builtin_memcpy(__expected, &__a->__a_value, sizeof(_Tp));
   __a->__unlock();
   return __ret;
 }
@@ -1242,7 +1291,7 @@ bool __cxx_atomic_compare_exchange_weak(volatile __cxx_atomic_lock_impl<_Tp>* __
   _Tp __temp;
   __a->__lock();
   __cxx_atomic_assign_volatile(__temp, __a->__a_value);
-  bool __ret = (_VSTD::memcmp(&__temp, __expected, sizeof(_Tp)) == 0);
+  bool __ret = (__builtin_memcmp(&__temp, __expected, sizeof(_Tp)) == 0);
   if(__ret)
     __cxx_atomic_assign_volatile(__a->__a_value, __value);
   else
@@ -1255,11 +1304,11 @@ _LIBCPP_INLINE_VISIBILITY
 bool __cxx_atomic_compare_exchange_weak(__cxx_atomic_lock_impl<_Tp>* __a,
                                         _Tp* __expected, _Tp __value, memory_order, memory_order) {
   __a->__lock();
-  bool __ret = (_VSTD::memcmp(&__a->__a_value, __expected, sizeof(_Tp)) == 0);
+  bool __ret = (__builtin_memcmp(&__a->__a_value, __expected, sizeof(_Tp)) == 0);
   if(__ret)
-    _VSTD::memcpy(&__a->__a_value, &__value, sizeof(_Tp));
+    __builtin_memcpy(&__a->__a_value, &__value, sizeof(_Tp));
   else
-    _VSTD::memcpy(__expected, &__a->__a_value, sizeof(_Tp));
+    __builtin_memcpy(__expected, &__a->__a_value, sizeof(_Tp));
   __a->__unlock();
   return __ret;
 }
@@ -1506,7 +1555,12 @@ _LIBCPP_INLINE_VISIBILITY void __cxx_atomic_notify_one(__cxx_atomic_impl<_Tp> co
 template <class _Atp, class _Fn>
 _LIBCPP_INLINE_VISIBILITY bool __cxx_atomic_wait(_Atp*, _Fn && __test_fn)
 {
+#if 0 // not supported
     return __libcpp_thread_poll_with_backoff(__test_fn, __libcpp_timed_backoff_policy());
+#else
+	(void)__test_fn;
+	return true;
+#endif
 }
 
 #endif // _LIBCPP_HAS_NO_PLATFORM_WAIT
@@ -1532,9 +1586,10 @@ _LIBCPP_INLINE_VISIBILITY bool __cxx_atomic_wait(_Atp* __a, _Tp const __val, mem
 
 // general atomic<T>
 
-template <class _Tp, bool = is_integral<_Tp>::value && !is_same<_Tp, bool>::value>
+template <class _Tp, class _TpAS = global _Tp, bool = is_integral<_Tp>::value && !is_same<_Tp, bool>::value>
 struct __atomic_base  // false
 {
+    typedef _TpAS* _ptr_type;
     mutable __cxx_atomic_impl<_Tp> __a_;
 
 #if defined(__cpp_lib_atomic_is_always_lock_free)
@@ -1543,85 +1598,87 @@ struct __atomic_base  // false
 
     _LIBCPP_INLINE_VISIBILITY
     bool is_lock_free() const volatile _NOEXCEPT
-        {return __cxx_atomic_is_lock_free(sizeof(_Tp));}
+        {return floor_atomic_is_lock_free(sizeof(_Tp));}
     _LIBCPP_INLINE_VISIBILITY
     bool is_lock_free() const _NOEXCEPT
         {return static_cast<__atomic_base const volatile*>(this)->is_lock_free();}
     _LIBCPP_INLINE_VISIBILITY
-    void store(_Tp __d, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
+    void store(_Tp __d, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
       _LIBCPP_CHECK_STORE_MEMORY_ORDER(__m)
-        {__cxx_atomic_store(&__a_, __d, __m);}
+        {floor_atomic_store(reinterpret_cast<_ptr_type>(&__a_), __d, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    void store(_Tp __d, memory_order __m = memory_order_seq_cst) _NOEXCEPT
+    void store(_Tp __d, memory_order __m = memory_order_relaxed) _NOEXCEPT
       _LIBCPP_CHECK_STORE_MEMORY_ORDER(__m)
-        {__cxx_atomic_store(&__a_, __d, __m);}
+        {floor_atomic_store(reinterpret_cast<_ptr_type>(&__a_), __d, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp load(memory_order __m = memory_order_seq_cst) const volatile _NOEXCEPT
+    _Tp load(memory_order __m = memory_order_relaxed) const volatile _NOEXCEPT
       _LIBCPP_CHECK_LOAD_MEMORY_ORDER(__m)
-        {return __cxx_atomic_load(&__a_, __m);}
+        {return floor_atomic_load(reinterpret_cast<_ptr_type>(&__a_), __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp load(memory_order __m = memory_order_seq_cst) const _NOEXCEPT
+    _Tp load(memory_order __m = memory_order_relaxed) const _NOEXCEPT
       _LIBCPP_CHECK_LOAD_MEMORY_ORDER(__m)
-        {return __cxx_atomic_load(&__a_, __m);}
+        {return floor_atomic_load(reinterpret_cast<_ptr_type>(&__a_), __m);}
     _LIBCPP_INLINE_VISIBILITY
     operator _Tp() const volatile _NOEXCEPT {return load();}
     _LIBCPP_INLINE_VISIBILITY
     operator _Tp() const _NOEXCEPT          {return load();}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_exchange(&__a_, __d, __m);}
+    _Tp exchange(_Tp __d, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_exchange(reinterpret_cast<_ptr_type>(&__a_), __d, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp exchange(_Tp __d, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_exchange(&__a_, __d, __m);}
+    _Tp exchange(_Tp __d, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_exchange(reinterpret_cast<_ptr_type>(&__a_), __d, __m);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_weak(_Tp& __e, _Tp __d,
                                memory_order __s, memory_order __f) volatile _NOEXCEPT
       _LIBCPP_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
-        {return __cxx_atomic_compare_exchange_weak(&__a_, &__e, __d, __s, __f);}
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __s, __f);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_weak(_Tp& __e, _Tp __d,
                                memory_order __s, memory_order __f) _NOEXCEPT
       _LIBCPP_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
-        {return __cxx_atomic_compare_exchange_weak(&__a_, &__e, __d, __s, __f);}
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __s, __f);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                  memory_order __s, memory_order __f) volatile _NOEXCEPT
       _LIBCPP_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
-        {return __cxx_atomic_compare_exchange_strong(&__a_, &__e, __d, __s, __f);}
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __s, __f);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_strong(_Tp& __e, _Tp __d,
                                  memory_order __s, memory_order __f) _NOEXCEPT
       _LIBCPP_CHECK_EXCHANGE_MEMORY_ORDER(__s, __f)
-        {return __cxx_atomic_compare_exchange_strong(&__a_, &__e, __d, __s, __f);}
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __s, __f);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_weak(_Tp& __e, _Tp __d,
-                              memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_compare_exchange_weak(&__a_, &__e, __d, __m, __m);}
+                              memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __m, __m);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_weak(_Tp& __e, _Tp __d,
-                               memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_compare_exchange_weak(&__a_, &__e, __d, __m, __m);}
+							   memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __m, __m);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_strong(_Tp& __e, _Tp __d,
-                              memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_compare_exchange_strong(&__a_, &__e, __d, __m, __m);}
+                              memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __m, __m);}
     _LIBCPP_INLINE_VISIBILITY
     bool compare_exchange_strong(_Tp& __e, _Tp __d,
-                                 memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_compare_exchange_strong(&__a_, &__e, __d, __m, __m);}
-
-    _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const volatile _NOEXCEPT
-        {__cxx_atomic_wait(&__a_, __v, __m);}
-    _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_seq_cst) const _NOEXCEPT
-        {__cxx_atomic_wait(&__a_, __v, __m);}
+                                 memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_compare_exchange_weak(reinterpret_cast<_ptr_type>(&__a_), &__e, __d, __m, __m);}
+
+#if 0 // not supported
+    _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_relaxed) const volatile _NOEXCEPT
+        {__cxx_atomic_wait(reinterpret_cast<_ptr_type>(&__a_), __v, __m);}
+    _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void wait(_Tp __v, memory_order __m = memory_order_relaxed) const _NOEXCEPT
+        {__cxx_atomic_wait(reinterpret_cast<_ptr_type>(&__a_), __v, __m);}
     _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void notify_one() volatile _NOEXCEPT
-        {__cxx_atomic_notify_one(&__a_);}
+        {__cxx_atomic_notify_one(reinterpret_cast<_ptr_type>(&__a_));}
     _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void notify_one() _NOEXCEPT
-        {__cxx_atomic_notify_one(&__a_);}
+        {__cxx_atomic_notify_one(reinterpret_cast<_ptr_type>(&__a_));}
     _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void notify_all() volatile _NOEXCEPT
-        {__cxx_atomic_notify_all(&__a_);}
+        {__cxx_atomic_notify_all(reinterpret_cast<_ptr_type>(&__a_));}
     _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY void notify_all() _NOEXCEPT
-        {__cxx_atomic_notify_all(&__a_);}
+        {__cxx_atomic_notify_all(reinterpret_cast<_ptr_type>(&__a_));}
+#endif
 
 #if _LIBCPP_STD_VER > 17
     _LIBCPP_INLINE_VISIBILITY constexpr
@@ -1634,13 +1691,7 @@ struct __atomic_base  // false
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR
     __atomic_base(_Tp __d) _NOEXCEPT : __a_(__d) {}
 
-#ifndef _LIBCPP_CXX03_LANG
     __atomic_base(const __atomic_base&) = delete;
-#else
-private:
-    _LIBCPP_INLINE_VISIBILITY
-    __atomic_base(const __atomic_base&);
-#endif
 };
 
 #if defined(__cpp_lib_atomic_is_always_lock_free)
@@ -1650,11 +1701,12 @@ _LIBCPP_CONSTEXPR bool __atomic_base<_Tp, __b>::is_always_lock_free;
 
 // atomic<Integral>
 
-template <class _Tp>
-struct __atomic_base<_Tp, true>
-    : public __atomic_base<_Tp, false>
+template <class _Tp, class _TpAS>
+struct __atomic_base<_Tp, _TpAS, true>
+    : public __atomic_base<_Tp, _TpAS, false>
 {
-    typedef __atomic_base<_Tp, false> __base;
+    typedef _TpAS* _ptr_type;
+    typedef __atomic_base<_Tp, _TpAS, false> __base;
 
     _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_AFTER_CXX17
     __atomic_base() _NOEXCEPT _LIBCPP_DEFAULT
@@ -1663,52 +1715,66 @@ struct __atomic_base<_Tp, true>
     _LIBCPP_CONSTEXPR __atomic_base(_Tp __d) _NOEXCEPT : __base(__d) {}
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}
+    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_add(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_fetch_add(&this->__a_, __op, __m);}
+    _Tp fetch_add(_Tp __op, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_add(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}
+    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_sub(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);}
+    _Tp fetch_sub(_Tp __op, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_sub(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}
+    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_and(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_fetch_and(&this->__a_, __op, __m);}
+    _Tp fetch_and(_Tp __op, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_and(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}
+    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_or(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_fetch_or(&this->__a_, __op, __m);}
+    _Tp fetch_or(_Tp __op, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_or(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT
-        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}
+    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_xor(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT
-        {return __cxx_atomic_fetch_xor(&this->__a_, __op, __m);}
+    _Tp fetch_xor(_Tp __op, memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_xor(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);}
 
+    // non-standard inc/dec
+    _LIBCPP_INLINE_VISIBILITY
+    _Tp _fetch_inc(memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_inc(reinterpret_cast<_ptr_type>(&this->__a_), __m);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator++(int) volatile _NOEXCEPT      {return fetch_add(_Tp(1));}
+    _Tp _fetch_inc(memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_inc(reinterpret_cast<_ptr_type>(&this->__a_), __m);}
+    _LIBCPP_INLINE_VISIBILITY
+    _Tp _fetch_dec(memory_order __m = memory_order_relaxed) volatile _NOEXCEPT
+        {return floor_atomic_fetch_dec(reinterpret_cast<_ptr_type>(&this->__a_), __m);}
+    _LIBCPP_INLINE_VISIBILITY
+    _Tp _fetch_dec(memory_order __m = memory_order_relaxed) _NOEXCEPT
+        {return floor_atomic_fetch_dec(reinterpret_cast<_ptr_type>(&this->__a_), __m);}
+
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator++(int) _NOEXCEPT               {return fetch_add(_Tp(1));}
+    _Tp operator++(int) volatile _NOEXCEPT      {return _fetch_inc();}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator--(int) volatile _NOEXCEPT      {return fetch_sub(_Tp(1));}
+    _Tp operator++(int) _NOEXCEPT               {return _fetch_inc();}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator--(int) _NOEXCEPT               {return fetch_sub(_Tp(1));}
+    _Tp operator--(int) volatile _NOEXCEPT      {return _fetch_dec();}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator++() volatile _NOEXCEPT         {return fetch_add(_Tp(1)) + _Tp(1);}
+    _Tp operator--(int) _NOEXCEPT               {return _fetch_dec();}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator++() _NOEXCEPT                  {return fetch_add(_Tp(1)) + _Tp(1);}
+    _Tp operator++() volatile _NOEXCEPT         {return _fetch_inc() + _Tp(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator--() volatile _NOEXCEPT         {return fetch_sub(_Tp(1)) - _Tp(1);}
+    _Tp operator++() _NOEXCEPT                  {return _fetch_inc() + _Tp(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp operator--() _NOEXCEPT                  {return fetch_sub(_Tp(1)) - _Tp(1);}
+    _Tp operator--() volatile _NOEXCEPT         {return _fetch_dec() - _Tp(1);}
+    _LIBCPP_INLINE_VISIBILITY
+    _Tp operator--() _NOEXCEPT                  {return _fetch_dec() - _Tp(1);}
     _LIBCPP_INLINE_VISIBILITY
     _Tp operator+=(_Tp __op) volatile _NOEXCEPT {return fetch_add(__op) + __op;}
     _LIBCPP_INLINE_VISIBILITY
@@ -1733,11 +1799,11 @@ struct __atomic_base<_Tp, true>
 
 // atomic<T>
 
-template <class _Tp>
+template <class _Tp, class _TpAS = global _Tp>
 struct atomic
-    : public __atomic_base<_Tp>
+    : public __atomic_base<_Tp, _TpAS>
 {
-    typedef __atomic_base<_Tp> __base;
+    typedef __atomic_base<_Tp, _TpAS> __base;
     typedef _Tp value_type;
     typedef value_type difference_type;
 
@@ -1765,79 +1831,80 @@ struct atomic
 
 // atomic<T*>
 
-template <class _Tp>
+template <class _Tp, class _TpAS>
 struct atomic<_Tp*>
-    : public __atomic_base<_Tp*>
+    : public __atomic_base<_Tp*, _TpAS*>
 {
+    typedef _TpAS* _ptr_type;
     typedef __atomic_base<_Tp*> __base;
-    typedef _Tp* value_type;
+    typedef _TpAS* value_type;
     typedef ptrdiff_t difference_type;
 
     _LIBCPP_INLINE_VISIBILITY
     atomic() _NOEXCEPT _LIBCPP_DEFAULT
 
     _LIBCPP_INLINE_VISIBILITY
-    _LIBCPP_CONSTEXPR atomic(_Tp* __d) _NOEXCEPT : __base(__d) {}
+    _LIBCPP_CONSTEXPR atomic(_ptr_type __d) _NOEXCEPT : __base(__d) {}
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator=(_Tp* __d) volatile _NOEXCEPT
+    _ptr_type operator=(_ptr_type __d) volatile _NOEXCEPT
         {__base::store(__d); return __d;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator=(_Tp* __d) _NOEXCEPT
+    _ptr_type operator=(_ptr_type __d) _NOEXCEPT
         {__base::store(__d); return __d;}
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT {
+    _ptr_type fetch_add(ptrdiff_t __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT {
         // __atomic_fetch_add accepts function pointers, guard against them.
         static_assert(!is_function<typename remove_pointer<_Tp>::type>::value, "Pointer to function isn't allowed");
-        return __cxx_atomic_fetch_add(&this->__a_, __op, __m);
+        return floor_atomic_fetch_add(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);
     }
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* fetch_add(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT {
+    _ptr_type fetch_add(ptrdiff_t __op, memory_order __m = memory_order_relaxed) _NOEXCEPT {
         // __atomic_fetch_add accepts function pointers, guard against them.
         static_assert(!is_function<typename remove_pointer<_Tp>::type>::value, "Pointer to function isn't allowed");
-        return __cxx_atomic_fetch_add(&this->__a_, __op, __m);
+        return floor_atomic_fetch_add(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);
     }
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) volatile _NOEXCEPT {
+    _ptr_type fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_relaxed) volatile _NOEXCEPT {
         // __atomic_fetch_add accepts function pointers, guard against them.
         static_assert(!is_function<typename remove_pointer<_Tp>::type>::value, "Pointer to function isn't allowed");
-        return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);
+        return floor_atomic_fetch_sub(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);
     }
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_seq_cst) _NOEXCEPT {
+    _ptr_type fetch_sub(ptrdiff_t __op, memory_order __m = memory_order_relaxed) _NOEXCEPT {
         // __atomic_fetch_add accepts function pointers, guard against them.
         static_assert(!is_function<typename remove_pointer<_Tp>::type>::value, "Pointer to function isn't allowed");
-        return __cxx_atomic_fetch_sub(&this->__a_, __op, __m);
+        return floor_atomic_fetch_sub(reinterpret_cast<_ptr_type>(&this->__a_), __op, __m);
     }
 
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator++(int) volatile _NOEXCEPT            {return fetch_add(1);}
+    _ptr_type operator++(int) volatile _NOEXCEPT            {return fetch_add(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator++(int) _NOEXCEPT                     {return fetch_add(1);}
+    _ptr_type operator++(int) _NOEXCEPT                     {return fetch_add(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator--(int) volatile _NOEXCEPT            {return fetch_sub(1);}
+    _ptr_type operator--(int) volatile _NOEXCEPT            {return fetch_sub(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator--(int) _NOEXCEPT                     {return fetch_sub(1);}
+    _ptr_type operator--(int) _NOEXCEPT                     {return fetch_sub(1);}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator++() volatile _NOEXCEPT               {return fetch_add(1) + 1;}
+    _ptr_type operator++() volatile _NOEXCEPT               {return fetch_add(1) + 1;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator++() _NOEXCEPT                        {return fetch_add(1) + 1;}
+    _ptr_type operator++() _NOEXCEPT                        {return fetch_add(1) + 1;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator--() volatile _NOEXCEPT               {return fetch_sub(1) - 1;}
+    _ptr_type operator--() volatile _NOEXCEPT               {return fetch_sub(1) - 1;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator--() _NOEXCEPT                        {return fetch_sub(1) - 1;}
+    _ptr_type operator--() _NOEXCEPT                        {return fetch_sub(1) - 1;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator+=(ptrdiff_t __op) volatile _NOEXCEPT {return fetch_add(__op) + __op;}
+    _ptr_type operator+=(ptrdiff_t __op) volatile _NOEXCEPT {return fetch_add(__op) + __op;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator+=(ptrdiff_t __op) _NOEXCEPT          {return fetch_add(__op) + __op;}
+    _ptr_type operator+=(ptrdiff_t __op) _NOEXCEPT          {return fetch_add(__op) + __op;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator-=(ptrdiff_t __op) volatile _NOEXCEPT {return fetch_sub(__op) - __op;}
+    _ptr_type operator-=(ptrdiff_t __op) volatile _NOEXCEPT {return fetch_sub(__op) - __op;}
     _LIBCPP_INLINE_VISIBILITY
-    _Tp* operator-=(ptrdiff_t __op) _NOEXCEPT          {return fetch_sub(__op) - __op;}
+    _ptr_type operator-=(ptrdiff_t __op) _NOEXCEPT          {return fetch_sub(__op) - __op;}
 
     atomic& operator=(const atomic&) = delete;
     atomic& operator=(const atomic&) volatile = delete;
@@ -1845,6 +1912,9 @@ struct atomic<_Tp*>
 
 // atomic_is_lock_free
 
+// NOTE: freestanding atomic<*> functions are not supported, either use the corresponding member functions,
+// or use the freestanding floor atomic functions, which operate on raw global/local type pointers
+#if 0
 template <class _Tp>
 _LIBCPP_INLINE_VISIBILITY
 bool
@@ -2076,6 +2146,7 @@ atomic_compare_exchange_strong_explicit(atomic<_Tp>* __o, typename atomic<_Tp>::
 
 // atomic_wait
 
+#if 0 // not supported
 template <class _Tp>
 _LIBCPP_AVAILABILITY_SYNC _LIBCPP_INLINE_VISIBILITY
 void atomic_wait(const volatile atomic<_Tp>* __o,
@@ -2143,6 +2214,7 @@ void atomic_notify_all(atomic<_Tp>* __o) _NOEXCEPT
 {
     __o->notify_all();
 }
+#endif
 
 // atomic_fetch_add
 
@@ -2365,9 +2437,12 @@ atomic_fetch_xor_explicit(atomic<_Tp>* __o, typename atomic<_Tp>::value_type __o
 {
     return __o->fetch_xor(__op, __m);
 }
+#endif
 
 // flag type and operations
 
+// NOTE: atomic_flag is not supported, use an appropriate atomic<uint32_t> instead
+#if 0
 typedef struct atomic_flag
 {
     __cxx_atomic_impl<_LIBCPP_ATOMIC_FLAG_TYPE> __a_;
@@ -2579,6 +2654,7 @@ atomic_flag_notify_all(atomic_flag* __o) _NOEXCEPT
 {
     __o->notify_all();
 }
+#endif
 
 // fences
 
@@ -2586,61 +2662,71 @@ inline _LIBCPP_INLINE_VISIBILITY
 void
 atomic_thread_fence(memory_order __m) _NOEXCEPT
 {
-    __cxx_atomic_thread_fence(__m);
+	floor_atomic_thread_fence(__m);
 }
 
 inline _LIBCPP_INLINE_VISIBILITY
 void
 atomic_signal_fence(memory_order __m) _NOEXCEPT
 {
-    __cxx_atomic_signal_fence(__m);
+	floor_atomic_signal_fence(__m);
 }
 
 // Atomics for standard typedef types
 
+#if 0 // not supported
 typedef atomic<bool>               atomic_bool;
 typedef atomic<char>               atomic_char;
 typedef atomic<signed char>        atomic_schar;
 typedef atomic<unsigned char>      atomic_uchar;
 typedef atomic<short>              atomic_short;
 typedef atomic<unsigned short>     atomic_ushort;
+#endif
 typedef atomic<int>                atomic_int;
 typedef atomic<unsigned int>       atomic_uint;
 typedef atomic<long>               atomic_long;
 typedef atomic<unsigned long>      atomic_ulong;
 typedef atomic<long long>          atomic_llong;
 typedef atomic<unsigned long long> atomic_ullong;
+#if 0 // not supported
 #ifndef _LIBCPP_HAS_NO_CHAR8_T
 typedef atomic<char8_t>            atomic_char8_t;
 #endif
 typedef atomic<char16_t>           atomic_char16_t;
+#endif
 typedef atomic<char32_t>           atomic_char32_t;
-#ifndef _LIBCPP_HAS_NO_WIDE_CHARACTERS
+#if !defined(_LIBCPP_HAS_NO_WIDE_CHARACTERS) && 0 // not supported
 typedef atomic<wchar_t>            atomic_wchar_t;
 #endif
 
+#if 0 // not supported
 typedef atomic<int_least8_t>   atomic_int_least8_t;
 typedef atomic<uint_least8_t>  atomic_uint_least8_t;
 typedef atomic<int_least16_t>  atomic_int_least16_t;
 typedef atomic<uint_least16_t> atomic_uint_least16_t;
+#endif
 typedef atomic<int_least32_t>  atomic_int_least32_t;
 typedef atomic<uint_least32_t> atomic_uint_least32_t;
 typedef atomic<int_least64_t>  atomic_int_least64_t;
 typedef atomic<uint_least64_t> atomic_uint_least64_t;
 
+#if 0 // not supported
 typedef atomic<int_fast8_t>   atomic_int_fast8_t;
 typedef atomic<uint_fast8_t>  atomic_uint_fast8_t;
 typedef atomic<int_fast16_t>  atomic_int_fast16_t;
 typedef atomic<uint_fast16_t> atomic_uint_fast16_t;
+#endif
 typedef atomic<int_fast32_t>  atomic_int_fast32_t;
 typedef atomic<uint_fast32_t> atomic_uint_fast32_t;
 typedef atomic<int_fast64_t>  atomic_int_fast64_t;
 typedef atomic<uint_fast64_t> atomic_uint_fast64_t;
 
+#if 0 // not supported
 typedef atomic< int8_t>  atomic_int8_t;
 typedef atomic<uint8_t>  atomic_uint8_t;
 typedef atomic< int16_t> atomic_int16_t;
 typedef atomic<uint16_t> atomic_uint16_t;
+#endif
 typedef atomic< int32_t> atomic_int32_t;
 typedef atomic<uint32_t> atomic_uint32_t;
 typedef atomic< int64_t> atomic_int64_t;
@@ -2680,7 +2766,11 @@ typedef conditional<_LIBCPP_CONTENTION_LOCK_FREE, __cxx_contention_t, unsigned c
 typedef atomic<__libcpp_signed_lock_free> atomic_signed_lock_free;
 typedef atomic<__libcpp_unsigned_lock_free> atomic_unsigned_lock_free;
 
+#if 0
 #define ATOMIC_FLAG_INIT {false}
+#else
+#define ATOMIC_FLAG_INIT {0u}
+#endif
 #define ATOMIC_VAR_INIT(__v) {__v}
 
 _LIBCPP_END_NAMESPACE_STD
diff --git a/libcxx/include/charconv b/libcxx/include/charconv
index 447cef97a210..b1e2066788ea 100644
--- a/libcxx/include/charconv
+++ b/libcxx/include/charconv
@@ -408,7 +408,7 @@ __sign_combinator(_It __first, _It __last, _Tp& __value, _Fn __f, _Ts... __args)
         if (__x <= __complement(__to_unsigned_like(__tl::min())))
         {
             __x = __complement(__x);
-            _VSTD::memcpy(&__value, &__x, sizeof(__x));
+            __builtin_memcpy(&__value, &__x, sizeof(__x));
             return __r;
         }
     }
diff --git a/libcxx/include/cstddef b/libcxx/include/cstddef
index 3e56d4452cc1..fbdd33336235 100644
--- a/libcxx/include/cstddef
+++ b/libcxx/include/cstddef
@@ -40,8 +40,12 @@ Types:
 #pragma GCC system_header
 #endif
 
+#if 0
 // Don't include our own <stddef.h>; we don't want to declare ::nullptr_t.
 #include_next <stddef.h>
+#else
+#include <stddef.h>
+#endif
 #include <__nullptr>
 
 _LIBCPP_BEGIN_NAMESPACE_STD
diff --git a/libcxx/include/cstdint b/libcxx/include/cstdint
index aa7c8b5cbf56..442489aef6aa 100644
--- a/libcxx/include/cstdint
+++ b/libcxx/include/cstdint
@@ -7,6 +7,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#if defined(FLOOR_COMPUTE_HOST_DEVICE) // we don't want this for other backends
+
 #ifndef _LIBCPP_CSTDINT
 #define _LIBCPP_CSTDINT
 
@@ -188,3 +190,5 @@ using ::uintmax_t _LIBCPP_USING_IF_EXISTS;
 _LIBCPP_END_NAMESPACE_STD
 
 #endif // _LIBCPP_CSTDINT
+
+#endif
diff --git a/libcxx/include/deque b/libcxx/include/deque
index 12e0399eaa01..cc405b059af7 100644
--- a/libcxx/include/deque
+++ b/libcxx/include/deque
@@ -1227,7 +1227,7 @@ __deque_base<_Tp, _Allocator>::clear() _NOEXCEPT
 {
     allocator_type& __a = __alloc();
     for (iterator __i = begin(), __e = end(); __i != __e; ++__i)
-        __alloc_traits::destroy(__a, _VSTD::addressof(*__i));
+        __alloc_traits::destroy(__a, __builtin_addressof(*__i));
     size() = 0;
     while (__map_.size() > 2)
     {
@@ -1910,7 +1910,7 @@ deque<_Tp, _Allocator>::push_back(const value_type& __v)
     if (__back_spare() == 0)
         __add_back_capacity();
     // __back_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()), __v);
+    __alloc_traits::construct(__a, __builtin_addressof(*__base::end()), __v);
     ++__base::size();
 }
 
@@ -1922,7 +1922,7 @@ deque<_Tp, _Allocator>::push_front(const value_type& __v)
     if (__front_spare() == 0)
         __add_front_capacity();
     // __front_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), __v);
+    __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), __v);
     --__base::__start_;
     ++__base::size();
 }
@@ -1936,7 +1936,7 @@ deque<_Tp, _Allocator>::push_back(value_type&& __v)
     if (__back_spare() == 0)
         __add_back_capacity();
     // __back_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()), _VSTD::move(__v));
+    __alloc_traits::construct(__a, __builtin_addressof(*__base::end()), _VSTD::move(__v));
     ++__base::size();
 }
 
@@ -1953,7 +1953,7 @@ deque<_Tp, _Allocator>::emplace_back(_Args&&... __args)
     if (__back_spare() == 0)
         __add_back_capacity();
     // __back_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()),
+    __alloc_traits::construct(__a, __builtin_addressof(*__base::end()),
                               _VSTD::forward<_Args>(__args)...);
     ++__base::size();
 #if _LIBCPP_STD_VER > 14
@@ -1969,7 +1969,7 @@ deque<_Tp, _Allocator>::push_front(value_type&& __v)
     if (__front_spare() == 0)
         __add_front_capacity();
     // __front_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), _VSTD::move(__v));
+    __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), _VSTD::move(__v));
     --__base::__start_;
     ++__base::size();
 }
@@ -1988,7 +1988,7 @@ deque<_Tp, _Allocator>::emplace_front(_Args&&... __args)
     if (__front_spare() == 0)
         __add_front_capacity();
     // __front_spare() >= 1
-    __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), _VSTD::forward<_Args>(__args)...);
+    __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), _VSTD::forward<_Args>(__args)...);
     --__base::__start_;
     ++__base::size();
 #if _LIBCPP_STD_VER > 14
@@ -2010,7 +2010,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, value_type&& __v)
         // __front_spare() >= 1
         if (__pos == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), _VSTD::move(__v));
+            __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), _VSTD::move(__v));
             --__base::__start_;
             ++__base::size();
         }
@@ -2018,7 +2018,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, value_type&& __v)
         {
             iterator __b = __base::begin();
             iterator __bm1 = _VSTD::prev(__b);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__bm1), _VSTD::move(*__b));
+            __alloc_traits::construct(__a, __builtin_addressof(*__bm1), _VSTD::move(*__b));
             --__base::__start_;
             ++__base::size();
             if (__pos > 1)
@@ -2034,14 +2034,14 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, value_type&& __v)
         size_type __de = __base::size() - __pos;
         if (__de == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()), _VSTD::move(__v));
+            __alloc_traits::construct(__a, __builtin_addressof(*__base::end()), _VSTD::move(__v));
             ++__base::size();
         }
         else
         {
             iterator __e = __base::end();
             iterator __em1 = _VSTD::prev(__e);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__e), _VSTD::move(*__em1));
+            __alloc_traits::construct(__a, __builtin_addressof(*__e), _VSTD::move(*__em1));
             ++__base::size();
             if (__de > 1)
                 __e = _VSTD::move_backward(__e - __de, __em1, __e);
@@ -2066,7 +2066,7 @@ deque<_Tp, _Allocator>::emplace(const_iterator __p, _Args&&... __args)
         // __front_spare() >= 1
         if (__pos == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), _VSTD::forward<_Args>(__args)...);
+            __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), _VSTD::forward<_Args>(__args)...);
             --__base::__start_;
             ++__base::size();
         }
@@ -2075,7 +2075,7 @@ deque<_Tp, _Allocator>::emplace(const_iterator __p, _Args&&... __args)
             __temp_value<value_type, _Allocator> __tmp(this->__alloc(), _VSTD::forward<_Args>(__args)...);
             iterator __b = __base::begin();
             iterator __bm1 = _VSTD::prev(__b);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__bm1), _VSTD::move(*__b));
+            __alloc_traits::construct(__a, __builtin_addressof(*__bm1), _VSTD::move(*__b));
             --__base::__start_;
             ++__base::size();
             if (__pos > 1)
@@ -2091,7 +2091,7 @@ deque<_Tp, _Allocator>::emplace(const_iterator __p, _Args&&... __args)
         size_type __de = __base::size() - __pos;
         if (__de == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()), _VSTD::forward<_Args>(__args)...);
+            __alloc_traits::construct(__a, __builtin_addressof(*__base::end()), _VSTD::forward<_Args>(__args)...);
             ++__base::size();
         }
         else
@@ -2099,7 +2099,7 @@ deque<_Tp, _Allocator>::emplace(const_iterator __p, _Args&&... __args)
             __temp_value<value_type, _Allocator> __tmp(this->__alloc(), _VSTD::forward<_Args>(__args)...);
             iterator __e = __base::end();
             iterator __em1 = _VSTD::prev(__e);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__e), _VSTD::move(*__em1));
+            __alloc_traits::construct(__a, __builtin_addressof(*__e), _VSTD::move(*__em1));
             ++__base::size();
             if (__de > 1)
                 __e = _VSTD::move_backward(__e - __de, __em1, __e);
@@ -2126,7 +2126,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, const value_type& __v)
         // __front_spare() >= 1
         if (__pos == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*--__base::begin()), __v);
+            __alloc_traits::construct(__a, __builtin_addressof(*--__base::begin()), __v);
             --__base::__start_;
             ++__base::size();
         }
@@ -2137,7 +2137,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, const value_type& __v)
             iterator __bm1 = _VSTD::prev(__b);
             if (__vt == pointer_traits<const_pointer>::pointer_to(*__b))
                 __vt = pointer_traits<const_pointer>::pointer_to(*__bm1);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__bm1), _VSTD::move(*__b));
+            __alloc_traits::construct(__a, __builtin_addressof(*__bm1), _VSTD::move(*__b));
             --__base::__start_;
             ++__base::size();
             if (__pos > 1)
@@ -2153,7 +2153,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, const value_type& __v)
         size_type __de = __base::size() - __pos;
         if (__de == 0)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*__base::end()), __v);
+            __alloc_traits::construct(__a, __builtin_addressof(*__base::end()), __v);
             ++__base::size();
         }
         else
@@ -2163,7 +2163,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, const value_type& __v)
             iterator __em1 = _VSTD::prev(__e);
             if (__vt == pointer_traits<const_pointer>::pointer_to(*__em1))
                 __vt = pointer_traits<const_pointer>::pointer_to(*__e);
-            __alloc_traits::construct(__a, _VSTD::addressof(*__e), _VSTD::move(*__em1));
+            __alloc_traits::construct(__a, __builtin_addressof(*__e), _VSTD::move(*__em1));
             ++__base::size();
             if (__de > 1)
                 __e = __move_backward_and_check(__e - __de, __em1, __e, __vt);
@@ -2190,7 +2190,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, size_type __n, const value_ty
         if (__n > __pos)
         {
             for (size_type __m = __n - __pos; __m; --__m, --__base::__start_, ++__base::size())
-                __alloc_traits::construct(__a, _VSTD::addressof(*--__i), __v);
+                __alloc_traits::construct(__a, __builtin_addressof(*--__i), __v);
             __n = __pos;
         }
         if (__n > 0)
@@ -2215,7 +2215,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, size_type __n, const value_ty
         if (__n > __de)
         {
             for (size_type __m = __n - __de; __m; --__m, (void) ++__i, ++__base::size())
-                __alloc_traits::construct(__a, _VSTD::addressof(*__i), __v);
+                __alloc_traits::construct(__a, __builtin_addressof(*__i), __v);
             __n = __de;
         }
         if (__n > 0)
@@ -2280,7 +2280,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, _BiIter __f, _BiIter __l,
         {
             __m = __pos < __n / 2 ? _VSTD::prev(__l, __pos) : _VSTD::next(__f, __n - __pos);
             for (_BiIter __j = __m; __j != __f; --__base::__start_, ++__base::size())
-                __alloc_traits::construct(__a, _VSTD::addressof(*--__i), *--__j);
+                __alloc_traits::construct(__a, __builtin_addressof(*--__i), *--__j);
             __n = __pos;
         }
         if (__n > 0)
@@ -2288,7 +2288,7 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, _BiIter __f, _BiIter __l,
             iterator __obn = __old_begin + __n;
             for (iterator __j = __obn; __j != __old_begin;)
             {
-                __alloc_traits::construct(__a, _VSTD::addressof(*--__i), _VSTD::move(*--__j));
+                __alloc_traits::construct(__a, __builtin_addressof(*--__i), _VSTD::move(*--__j));
                 --__base::__start_;
                 ++__base::size();
             }
@@ -2311,14 +2311,14 @@ deque<_Tp, _Allocator>::insert(const_iterator __p, _BiIter __f, _BiIter __l,
         {
             __m = __de < __n / 2 ? _VSTD::next(__f, __de) : _VSTD::prev(__l, __n - __de);
             for (_BiIter __j = __m; __j != __l; ++__i, (void) ++__j, ++__base::size())
-                __alloc_traits::construct(__a, _VSTD::addressof(*__i), *__j);
+                __alloc_traits::construct(__a, __builtin_addressof(*__i), *__j);
             __n = __de;
         }
         if (__n > 0)
         {
             iterator __oen = __old_end - __n;
             for (iterator __j = __oen; __j != __old_end; ++__i, (void) ++__j, ++__base::size())
-                __alloc_traits::construct(__a, _VSTD::addressof(*__i), _VSTD::move(*__j));
+                __alloc_traits::construct(__a, __builtin_addressof(*__i), _VSTD::move(*__j));
             if (__n < __de)
                 __old_end = _VSTD::move_backward(__old_end - __de, __oen, __old_end);
             _VSTD::copy_backward(__f, __m, __old_end);
@@ -2782,7 +2782,7 @@ deque<_Tp, _Allocator>::__move_construct_and_check(iterator __f, iterator __l,
     allocator_type& __a = __base::__alloc();
     // as if
     //   for (; __f != __l; ++__r, ++__f, ++__base::size())
-    //       __alloc_traits::construct(__a, _VSTD::addressof(*__r), _VSTD::move(*__f));
+    //       __alloc_traits::construct(__a, __builtin_addressof(*__r), _VSTD::move(*__f));
     difference_type __n = __l - __f;
     while (__n > 0)
     {
@@ -2797,7 +2797,7 @@ deque<_Tp, _Allocator>::__move_construct_and_check(iterator __f, iterator __l,
         if (__fb <= __vt && __vt < __fe)
             __vt = (const_iterator(static_cast<__map_const_pointer>(__f.__m_iter_), __vt) += __r - __f).__ptr_;
         for (; __fb != __fe; ++__fb, ++__r, ++__base::size())
-            __alloc_traits::construct(__a, _VSTD::addressof(*__r), _VSTD::move(*__fb));
+            __alloc_traits::construct(__a, __builtin_addressof(*__r), _VSTD::move(*__fb));
         __n -= __bs;
         __f += __bs;
     }
@@ -2814,7 +2814,7 @@ deque<_Tp, _Allocator>::__move_construct_backward_and_check(iterator __f, iterat
     // as if
     //   for (iterator __j = __l; __j != __f;)
     //   {
-    //       __alloc_traitsconstruct(__a, _VSTD::addressof(*--__r), _VSTD::move(*--__j));
+    //       __alloc_traitsconstruct(__a, __builtin_addressof(*--__r), _VSTD::move(*--__j));
     //       --__base::__start_;
     //       ++__base::size();
     //   }
@@ -2834,7 +2834,7 @@ deque<_Tp, _Allocator>::__move_construct_backward_and_check(iterator __f, iterat
             __vt = (const_iterator(static_cast<__map_const_pointer>(__l.__m_iter_), __vt) -= __l - __r + 1).__ptr_;
         while (__le != __lb)
         {
-            __alloc_traits::construct(__a, _VSTD::addressof(*--__r), _VSTD::move(*--__le));
+            __alloc_traits::construct(__a, __builtin_addressof(*--__r), _VSTD::move(*--__le));
             --__base::__start_;
             ++__base::size();
         }
@@ -2854,7 +2854,7 @@ deque<_Tp, _Allocator>::erase(const_iterator __f)
     if (static_cast<size_t>(__pos) <= (__base::size() - 1) / 2)
     {   // erase from front
         _VSTD::move_backward(__b, __p, _VSTD::next(__p));
-        __alloc_traits::destroy(__a, _VSTD::addressof(*__b));
+        __alloc_traits::destroy(__a, __builtin_addressof(*__b));
         --__base::size();
         ++__base::__start_;
         __maybe_remove_front_spare();
@@ -2862,7 +2862,7 @@ deque<_Tp, _Allocator>::erase(const_iterator __f)
     else
     {   // erase from back
         iterator __i = _VSTD::move(_VSTD::next(__p), __base::end(), __p);
-        __alloc_traits::destroy(__a, _VSTD::addressof(*__i));
+        __alloc_traits::destroy(__a, __builtin_addressof(*__i));
         --__base::size();
         __maybe_remove_back_spare();
     }
@@ -2884,7 +2884,7 @@ deque<_Tp, _Allocator>::erase(const_iterator __f, const_iterator __l)
         {   // erase from front
             iterator __i = _VSTD::move_backward(__b, __p, __p + __n);
             for (; __b != __i; ++__b)
-                __alloc_traits::destroy(__a, _VSTD::addressof(*__b));
+                __alloc_traits::destroy(__a, __builtin_addressof(*__b));
             __base::size() -= __n;
             __base::__start_ += __n;
             while (__maybe_remove_front_spare()) {
@@ -2894,7 +2894,7 @@ deque<_Tp, _Allocator>::erase(const_iterator __f, const_iterator __l)
         {   // erase from back
             iterator __i = _VSTD::move(__p + __n, __base::end(), __p);
             for (iterator __e = __base::end(); __i != __e; ++__i)
-                __alloc_traits::destroy(__a, _VSTD::addressof(*__i));
+                __alloc_traits::destroy(__a, __builtin_addressof(*__i));
             __base::size() -= __n;
             while (__maybe_remove_back_spare()) {
             }
@@ -2915,7 +2915,7 @@ deque<_Tp, _Allocator>::__erase_to_end(const_iterator __f)
         iterator __b = __base::begin();
         difference_type __pos = __f - __b;
         for (iterator __p = __b + __pos; __p != __e; ++__p)
-            __alloc_traits::destroy(__a, _VSTD::addressof(*__p));
+            __alloc_traits::destroy(__a, __builtin_addressof(*__p));
         __base::size() -= __n;
         while (__maybe_remove_back_spare()) {
         }
diff --git a/libcxx/include/exception b/libcxx/include/exception
index 816f259f878b..e86e02844fd1 100644
--- a/libcxx/include/exception
+++ b/libcxx/include/exception
@@ -80,7 +80,9 @@ template <class E> void rethrow_if_nested(const E& e);
 #include <__config>
 #include <__memory/addressof.h>
 #include <cstddef>
+#if 0
 #include <cstdlib>
+#endif
 #include <type_traits>
 #include <version>
 
@@ -103,7 +105,7 @@ public:
     _LIBCPP_INLINE_VISIBILITY exception(const exception&) _NOEXCEPT = default;
 
     virtual ~exception() _NOEXCEPT;
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 };
 
 class _LIBCPP_EXCEPTION_ABI bad_exception
@@ -112,7 +114,7 @@ class _LIBCPP_EXCEPTION_ABI bad_exception
 public:
     _LIBCPP_INLINE_VISIBILITY bad_exception() _NOEXCEPT {}
     virtual ~bad_exception() _NOEXCEPT;
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 };
 #endif // !_LIBCPP_ABI_VCRUNTIME
 
@@ -181,8 +183,10 @@ make_exception_ptr(_Ep __e) _NOEXCEPT
     }
 #else
     ((void)__e);
+#if 0 // NOTE/TODO: there is no compute abort - trap instead?
     _VSTD::abort();
 #endif
+#endif
 }
 
 #else // _LIBCPP_ABI_MICROSOFT
@@ -229,7 +233,7 @@ template<class _Ep>
 _LIBCPP_INLINE_VISIBILITY exception_ptr
 make_exception_ptr(_Ep __e) _NOEXCEPT
 {
-  return __copy_exception_ptr(_VSTD::addressof(__e), __GetExceptionInfo(__e));
+  return __copy_exception_ptr(__builtin_addressof(__e), __GetExceptionInfo(__e));
 }
 
 #endif // _LIBCPP_ABI_MICROSOFT
@@ -315,7 +319,7 @@ void
 rethrow_if_nested(const _Ep& __e,
                   typename enable_if< __can_dynamic_cast<_Ep, nested_exception>::value>::type* = 0)
 {
-    const nested_exception* __nep = dynamic_cast<const nested_exception*>(_VSTD::addressof(__e));
+    const nested_exception* __nep = dynamic_cast<const nested_exception*>(__builtin_addressof(__e));
     if (__nep)
         __nep->rethrow_nested();
 }
diff --git a/libcxx/include/experimental/coroutine b/libcxx/include/experimental/coroutine
index 54ec74b9f984..764457b46ad9 100644
--- a/libcxx/include/experimental/coroutine
+++ b/libcxx/include/experimental/coroutine
@@ -251,7 +251,7 @@ public:
         typedef typename remove_cv<_Promise>::type _RawPromise;
         coroutine_handle __tmp;
         __tmp.__handle_ = __builtin_coro_promise(
-            _VSTD::addressof(const_cast<_RawPromise&>(__promise)),
+            __builtin_addressof(const_cast<_RawPromise&>(__promise)),
              _LIBCPP_ALIGNOF(_Promise), true);
         return __tmp;
     }
diff --git a/libcxx/include/experimental/iterator b/libcxx/include/experimental/iterator
index 09ea2cbcc7dc..fa5cd1b4253c 100644
--- a/libcxx/include/experimental/iterator
+++ b/libcxx/include/experimental/iterator
@@ -77,10 +77,10 @@ public:
     typedef void                                 reference;
 
     ostream_joiner(ostream_type& __os, _Delim&& __d)
-        : __output_iter(_VSTD::addressof(__os)), __delim(_VSTD::move(__d)), __first(true) {}
+        : __output_iter(__builtin_addressof(__os)), __delim(_VSTD::move(__d)), __first(true) {}
 
     ostream_joiner(ostream_type& __os, const _Delim& __d)
-        : __output_iter(_VSTD::addressof(__os)), __delim(__d), __first(true) {}
+        : __output_iter(__builtin_addressof(__os)), __delim(__d), __first(true) {}
 
 
     template<typename _Tp>
diff --git a/libcxx/include/ext/hash_map b/libcxx/include/ext/hash_map
index 60e32b09e3e1..61d9689ff2d1 100644
--- a/libcxx/include/ext/hash_map
+++ b/libcxx/include/ext/hash_map
@@ -356,9 +356,9 @@ public:
     void operator()(pointer __p)
     {
         if (__second_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.second));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.second));
         if (__first_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.first));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.first));
         if (__p)
             __alloc_traits::deallocate(__na_, __p, 1);
     }
@@ -667,9 +667,9 @@ hash_map<_Key, _Tp, _Hash, _Pred, _Alloc>::__construct_node(const key_type& __k)
 {
     __node_allocator& __na = __table_.__node_alloc();
     __node_holder __h(__node_traits::allocate(__na, 1), _Dp(__na));
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.first), __k);
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.first), __k);
     __h.get_deleter().__first_constructed = true;
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.second));
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.second));
     __h.get_deleter().__second_constructed = true;
     return __h;
 }
diff --git a/libcxx/include/float.h b/libcxx/include/float.h
index 399ab4f11a5d..d279464cd807 100644
--- a/libcxx/include/float.h
+++ b/libcxx/include/float.h
@@ -70,6 +70,8 @@ Macros:
 
 */
 
+#if 0 // we don't want this
+
 #include <__config>
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
@@ -90,4 +92,6 @@ Macros:
 
 #endif // __cplusplus
 
+#endif
+
 #endif // _LIBCPP_FLOAT_H
diff --git a/libcxx/include/forward_list b/libcxx/include/forward_list
index 3454a784a1ca..e45f83f62ee5 100644
--- a/libcxx/include/forward_list
+++ b/libcxx/include/forward_list
@@ -618,7 +618,7 @@ __forward_list_base<_Tp, _Alloc>::clear() _NOEXCEPT
     for (__node_pointer __p = __before_begin()->__next_; __p != nullptr;)
     {
         __node_pointer __next = __p->__next_;
-        __node_traits::destroy(__a, _VSTD::addressof(__p->__value_));
+        __node_traits::destroy(__a, __builtin_addressof(__p->__value_));
         __node_traits::deallocate(__a, __p, 1);
         __p = __next;
     }
@@ -905,7 +905,7 @@ forward_list<_Tp, _Alloc>::forward_list(size_type __n)
                                                              __p = __p->__next_as_begin())
         {
             __h.reset(__node_traits::allocate(__a, 1));
-            __node_traits::construct(__a, _VSTD::addressof(__h->__value_));
+            __node_traits::construct(__a, __builtin_addressof(__h->__value_));
             __h->__next_ = nullptr;
             __p->__next_ = __h.release();
         }
@@ -927,7 +927,7 @@ forward_list<_Tp, _Alloc>::forward_list(size_type __n,
                                                              __p = __p->__next_as_begin())
         {
             __h.reset(__node_traits::allocate(__a, 1));
-            __node_traits::construct(__a, _VSTD::addressof(__h->__value_));
+            __node_traits::construct(__a, __builtin_addressof(__h->__value_));
             __h->__next_ = nullptr;
             __p->__next_ = __h.release();
         }
@@ -1130,7 +1130,7 @@ forward_list<_Tp, _Alloc>::emplace_front(_Args&&... __args)
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_),
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_),
                                   _VSTD::forward<_Args>(__args)...);
     __h->__next_ = base::__before_begin()->__next_;
     base::__before_begin()->__next_ = __h.release();
@@ -1146,7 +1146,7 @@ forward_list<_Tp, _Alloc>::push_front(value_type&& __v)
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_), _VSTD::move(__v));
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_), _VSTD::move(__v));
     __h->__next_ = base::__before_begin()->__next_;
     base::__before_begin()->__next_ = __h.release();
 }
@@ -1160,7 +1160,7 @@ forward_list<_Tp, _Alloc>::push_front(const value_type& __v)
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_), __v);
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_), __v);
     __h->__next_ = base::__before_begin()->__next_;
     base::__before_begin()->__next_ = __h.release();
 }
@@ -1172,7 +1172,7 @@ forward_list<_Tp, _Alloc>::pop_front()
     __node_allocator& __a = base::__alloc();
     __node_pointer __p = base::__before_begin()->__next_;
     base::__before_begin()->__next_ = __p->__next_;
-    __node_traits::destroy(__a, _VSTD::addressof(__p->__value_));
+    __node_traits::destroy(__a, __builtin_addressof(__p->__value_));
     __node_traits::deallocate(__a, __p, 1);
 }
 
@@ -1187,7 +1187,7 @@ forward_list<_Tp, _Alloc>::emplace_after(const_iterator __p, _Args&&... __args)
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_),
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_),
                                   _VSTD::forward<_Args>(__args)...);
     __h->__next_ = __r->__next_;
     __r->__next_ = __h.release();
@@ -1202,7 +1202,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p, value_type&& __v)
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_), _VSTD::move(__v));
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_), _VSTD::move(__v));
     __h->__next_ = __r->__next_;
     __r->__next_ = __h.release();
     return iterator(__r->__next_);
@@ -1218,7 +1218,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p, const value_type& __
     __node_allocator& __a = base::__alloc();
     typedef __allocator_destructor<__node_allocator> _Dp;
     unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-    __node_traits::construct(__a, _VSTD::addressof(__h->__value_), __v);
+    __node_traits::construct(__a, __builtin_addressof(__h->__value_), __v);
     __h->__next_ = __r->__next_;
     __r->__next_ = __h.release();
     return iterator(__r->__next_);
@@ -1235,7 +1235,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p, size_type __n,
         __node_allocator& __a = base::__alloc();
         typedef __allocator_destructor<__node_allocator> _Dp;
         unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-        __node_traits::construct(__a, _VSTD::addressof(__h->__value_), __v);
+        __node_traits::construct(__a, __builtin_addressof(__h->__value_), __v);
         __node_pointer __first = __h.release();
         __node_pointer __last = __first;
 #ifndef _LIBCPP_NO_EXCEPTIONS
@@ -1245,7 +1245,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p, size_type __n,
             for (--__n; __n != 0; --__n, __last = __last->__next_)
             {
                 __h.reset(__node_traits::allocate(__a, 1));
-                __node_traits::construct(__a, _VSTD::addressof(__h->__value_), __v);
+                __node_traits::construct(__a, __builtin_addressof(__h->__value_), __v);
                 __last->__next_ = __h.release();
             }
 #ifndef _LIBCPP_NO_EXCEPTIONS
@@ -1255,7 +1255,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p, size_type __n,
             while (__first != nullptr)
             {
                 __node_pointer __next = __first->__next_;
-                __node_traits::destroy(__a, _VSTD::addressof(__first->__value_));
+                __node_traits::destroy(__a, __builtin_addressof(__first->__value_));
                 __node_traits::deallocate(__a, __first, 1);
                 __first = __next;
             }
@@ -1285,7 +1285,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p,
         __node_allocator& __a = base::__alloc();
         typedef __allocator_destructor<__node_allocator> _Dp;
         unique_ptr<__node, _Dp> __h(__node_traits::allocate(__a, 1), _Dp(__a, 1));
-        __node_traits::construct(__a, _VSTD::addressof(__h->__value_), *__f);
+        __node_traits::construct(__a, __builtin_addressof(__h->__value_), *__f);
         __node_pointer __first = __h.release();
         __node_pointer __last = __first;
 #ifndef _LIBCPP_NO_EXCEPTIONS
@@ -1295,7 +1295,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p,
             for (++__f; __f != __l; ++__f, ((void)(__last = __last->__next_)))
             {
                 __h.reset(__node_traits::allocate(__a, 1));
-                __node_traits::construct(__a, _VSTD::addressof(__h->__value_), *__f);
+                __node_traits::construct(__a, __builtin_addressof(__h->__value_), *__f);
                 __last->__next_ = __h.release();
             }
 #ifndef _LIBCPP_NO_EXCEPTIONS
@@ -1305,7 +1305,7 @@ forward_list<_Tp, _Alloc>::insert_after(const_iterator __p,
             while (__first != nullptr)
             {
                 __node_pointer __next = __first->__next_;
-                __node_traits::destroy(__a, _VSTD::addressof(__first->__value_));
+                __node_traits::destroy(__a, __builtin_addressof(__first->__value_));
                 __node_traits::deallocate(__a, __first, 1);
                 __first = __next;
             }
@@ -1327,7 +1327,7 @@ forward_list<_Tp, _Alloc>::erase_after(const_iterator __f)
     __node_pointer __n = __p->__next_;
     __p->__next_ = __n->__next_;
     __node_allocator& __a = base::__alloc();
-    __node_traits::destroy(__a, _VSTD::addressof(__n->__value_));
+    __node_traits::destroy(__a, __builtin_addressof(__n->__value_));
     __node_traits::deallocate(__a, __n, 1);
     return iterator(__p->__next_);
 }
@@ -1349,7 +1349,7 @@ forward_list<_Tp, _Alloc>::erase_after(const_iterator __f, const_iterator __l)
             do
             {
                 __node_pointer __tmp = __n->__next_;
-                __node_traits::destroy(__a, _VSTD::addressof(__n->__value_));
+                __node_traits::destroy(__a, __builtin_addressof(__n->__value_));
                 __node_traits::deallocate(__a, __n, 1);
                 __n = __tmp;
             } while (__n != __e);
@@ -1382,7 +1382,7 @@ forward_list<_Tp, _Alloc>::resize(size_type __n)
                                                          __ptr = __ptr->__next_as_begin())
             {
                 __h.reset(__node_traits::allocate(__a, 1));
-                __node_traits::construct(__a, _VSTD::addressof(__h->__value_));
+                __node_traits::construct(__a, __builtin_addressof(__h->__value_));
                 __h->__next_ = nullptr;
                 __ptr->__next_ = __h.release();
             }
@@ -1414,7 +1414,7 @@ forward_list<_Tp, _Alloc>::resize(size_type __n, const value_type& __v)
                                                          __ptr = __ptr->__next_as_begin())
             {
                 __h.reset(__node_traits::allocate(__a, 1));
-                __node_traits::construct(__a, _VSTD::addressof(__h->__value_), __v);
+                __node_traits::construct(__a, __builtin_addressof(__h->__value_), __v);
                 __h->__next_ = nullptr;
                 __ptr->__next_ = __h.release();
             }
diff --git a/libcxx/include/fstream b/libcxx/include/fstream
index 6e84614ec105..751b5a1208e8 100644
--- a/libcxx/include/fstream
+++ b/libcxx/include/fstream
@@ -716,7 +716,7 @@ basic_filebuf<_CharT, _Traits>::underflow()
     int_type __c = traits_type::eof();
     if (this->gptr() == this->egptr())
     {
-        _VSTD::memmove(this->eback(), this->egptr() - __unget_sz, __unget_sz * sizeof(char_type));
+        __builtin_memmove(this->eback(), this->egptr() - __unget_sz, __unget_sz * sizeof(char_type));
         if (__always_noconv_)
         {
             size_t __nmemb = static_cast<size_t>(this->egptr() - this->eback() - __unget_sz);
@@ -733,7 +733,7 @@ basic_filebuf<_CharT, _Traits>::underflow()
         {
             _LIBCPP_ASSERT ( !(__extbufnext_ == NULL && (__extbufend_ != __extbufnext_)), "underflow moving from NULL" );
             if (__extbufend_ != __extbufnext_)
-                _VSTD::memmove(__extbuf_, __extbufnext_, __extbufend_ - __extbufnext_);
+                __builtin_memmove(__extbuf_, __extbufnext_, __extbufend_ - __extbufnext_);
             __extbufnext_ = __extbuf_ + (__extbufend_ - __extbufnext_);
             __extbufend_ = __extbuf_ + (__extbuf_ == __extbuf_min_ ? sizeof(__extbuf_min_) : __ebs_);
             size_t __nmemb = _VSTD::min(static_cast<size_t>(__ibs_ - __unget_sz),
diff --git a/libcxx/include/functional b/libcxx/include/functional
index 53a5f2bc3770..f5e3db9850b7 100644
--- a/libcxx/include/functional
+++ b/libcxx/include/functional
@@ -520,11 +520,15 @@ POLICY:  For non-variadic implementations, the number of arguments is limited
 #include <__functional/unwrap_ref.h>
 #include <__utility/forward.h>
 #include <concepts>
+#if 0
 #include <exception>
+#endif
 #include <memory>
 #include <tuple>
 #include <type_traits>
+#if 0
 #include <typeinfo>
+#endif
 #include <utility>
 #include <version>
 
diff --git a/libcxx/include/future b/libcxx/include/future
index 89acdba62bfa..1b857aa9e075 100644
--- a/libcxx/include/future
+++ b/libcxx/include/future
@@ -727,7 +727,7 @@ __assoc_state<_Rp&>::set_value(_Rp& __arg)
     unique_lock<mutex> __lk(this->__mut_);
     if (this->__has_value())
         __throw_future_error(future_errc::promise_already_satisfied);
-    __value_ = _VSTD::addressof(__arg);
+    __value_ = __builtin_addressof(__arg);
     this->__state_ |= base::__constructed | base::ready;
     __cv_.notify_all();
 }
@@ -739,7 +739,7 @@ __assoc_state<_Rp&>::set_value_at_thread_exit(_Rp& __arg)
     unique_lock<mutex> __lk(this->__mut_);
     if (this->__has_value())
         __throw_future_error(future_errc::promise_already_satisfied);
-    __value_ = _VSTD::addressof(__arg);
+    __value_ = __builtin_addressof(__arg);
     this->__state_ |= base::__constructed;
     __thread_local_data()->__make_ready_at_thread_exit(this);
 }
@@ -774,7 +774,7 @@ void
 __assoc_state_alloc<_Rp, _Alloc>::__on_zero_shared() _NOEXCEPT
 {
     if (this->__state_ & base::__constructed)
-        reinterpret_cast<_Rp*>(_VSTD::addressof(this->__value_))->~_Rp();
+        reinterpret_cast<_Rp*>(__builtin_addressof(this->__value_))->~_Rp();
     typedef typename __allocator_traits_rebind<_Alloc, __assoc_state_alloc>::type _Al;
     typedef allocator_traits<_Al> _ATraits;
     typedef pointer_traits<typename _ATraits::pointer> _PTraits;
@@ -1331,8 +1331,8 @@ promise<_Rp>::promise(allocator_arg_t, const _Alloc& __a0)
     typedef __allocator_destructor<_A2> _D2;
     _A2 __a(__a0);
     unique_ptr<_State, _D2> __hold(__a.allocate(1), _D2(__a, 1));
-    ::new ((void*)_VSTD::addressof(*__hold.get())) _State(__a0);
-    __state_ = _VSTD::addressof(*__hold.release());
+    ::new ((void*)__builtin_addressof(*__hold.get())) _State(__a0);
+    __state_ = __builtin_addressof(*__hold.release());
 }
 
 template <class _Rp>
@@ -1474,8 +1474,8 @@ promise<_Rp&>::promise(allocator_arg_t, const _Alloc& __a0)
     typedef __allocator_destructor<_A2> _D2;
     _A2 __a(__a0);
     unique_ptr<_State, _D2> __hold(__a.allocate(1), _D2(__a, 1));
-    ::new ((void*)_VSTD::addressof(*__hold.get())) _State(__a0);
-    __state_ = _VSTD::addressof(*__hold.release());
+    ::new ((void*)__builtin_addressof(*__hold.get())) _State(__a0);
+    __state_ = __builtin_addressof(*__hold.release());
 }
 
 template <class _Rp>
@@ -1593,8 +1593,8 @@ promise<void>::promise(allocator_arg_t, const _Alloc& __a0)
     typedef __allocator_destructor<_A2> _D2;
     _A2 __a(__a0);
     unique_ptr<_State, _D2> __hold(__a.allocate(1), _D2(__a, 1));
-    ::new ((void*)_VSTD::addressof(*__hold.get())) _State(__a0);
-    __state_ = _VSTD::addressof(*__hold.release());
+    ::new ((void*)__builtin_addressof(*__hold.get())) _State(__a0);
+    __state_ = __builtin_addressof(*__hold.release());
 }
 
 template <class _Rp>
@@ -1784,9 +1784,9 @@ __packaged_task_function<_Rp(_ArgTypes...)>::__packaged_task_function(
         _Ap __a(__a0);
         typedef __allocator_destructor<_Ap> _Dp;
         unique_ptr<__base, _Dp> __hold(__a.allocate(1), _Dp(__a, 1));
-        ::new ((void*)_VSTD::addressof(*__hold.get()))
+        ::new ((void*)__builtin_addressof(*__hold.get()))
             _FF(_VSTD::forward<_Fp>(__f), _Alloc(__a));
-        __f_ = _VSTD::addressof(*__hold.release());
+        __f_ = __builtin_addressof(*__hold.release());
     }
 }
 
diff --git a/libcxx/include/inttypes.h b/libcxx/include/inttypes.h
index 6b7cb2648a5e..ec5830964972 100644
--- a/libcxx/include/inttypes.h
+++ b/libcxx/include/inttypes.h
@@ -15,6 +15,7 @@
 #define _LIBCPP_INTTYPES_H
 #endif // _STD_TYPES_T
 
+#if 0 // we don't want this
 /*
     inttypes.h synopsis
 
@@ -259,4 +260,6 @@ uintmax_t wcstoumax(const wchar_t* restrict nptr, wchar_t** restrict endptr, int
 
 #endif // __cplusplus
 
+#endif
+
 #endif // _LIBCPP_INTTYPES_H
diff --git a/libcxx/include/limits b/libcxx/include/limits
index a6d517852493..eb42db78a159 100644
--- a/libcxx/include/limits
+++ b/libcxx/include/limits
@@ -343,6 +343,52 @@ protected:
     static _LIBCPP_CONSTEXPR const float_round_style round_style = round_to_nearest;
 };
 
+template <>
+class __libcpp_numeric_limits<__fp16, true>
+{
+protected:
+    typedef __fp16 type;
+
+    static _LIBCPP_CONSTEXPR const bool is_specialized = true;
+
+    static _LIBCPP_CONSTEXPR const bool is_signed = true;
+    static _LIBCPP_CONSTEXPR const int  digits = __HALF_MANT_DIG__;
+    static _LIBCPP_CONSTEXPR const int  digits10 = __HALF_DIG__;
+    static _LIBCPP_CONSTEXPR const int  max_digits10 = 2+(digits * 30103l)/100000l;
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type min() _NOEXCEPT {return __HALF_MIN__;}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type max() _NOEXCEPT {return __HALF_MAX__;}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type lowest() _NOEXCEPT {return -max();}
+
+    static _LIBCPP_CONSTEXPR const bool is_integer = false;
+    static _LIBCPP_CONSTEXPR const bool is_exact = false;
+    static _LIBCPP_CONSTEXPR const int  radix = __FLT_RADIX__;
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type epsilon() _NOEXCEPT {return __HALF_EPSILON__;}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type round_error() _NOEXCEPT {return 0.5h;}
+
+    static _LIBCPP_CONSTEXPR const int  min_exponent = __HALF_MIN_EXP__;
+    static _LIBCPP_CONSTEXPR const int  min_exponent10 = __HALF_MIN_10_EXP__;
+    static _LIBCPP_CONSTEXPR const int  max_exponent = __HALF_MAX_EXP__;
+    static _LIBCPP_CONSTEXPR const int  max_exponent10 = __HALF_MAX_10_EXP__;
+
+    static _LIBCPP_CONSTEXPR const bool has_infinity = true;
+    static _LIBCPP_CONSTEXPR const bool has_quiet_NaN = true;
+    static _LIBCPP_CONSTEXPR const bool has_signaling_NaN = true;
+    static _LIBCPP_CONSTEXPR const float_denorm_style has_denorm = denorm_present;
+    static _LIBCPP_CONSTEXPR const bool has_denorm_loss = false;
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type infinity() _NOEXCEPT {return __builtin_huge_valh();}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type quiet_NaN() _NOEXCEPT {return __builtin_nanh("");}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type signaling_NaN() _NOEXCEPT {return __builtin_nansh("");}
+    _LIBCPP_INLINE_VISIBILITY static _LIBCPP_CONSTEXPR type denorm_min() _NOEXCEPT {return __HALF_DENORM_MIN__;}
+
+    static _LIBCPP_CONSTEXPR const bool is_iec559 = true;
+    static _LIBCPP_CONSTEXPR const bool is_bounded = true;
+    static _LIBCPP_CONSTEXPR const bool is_modulo = false;
+
+    static _LIBCPP_CONSTEXPR const bool traps = false;
+    static _LIBCPP_CONSTEXPR const bool tinyness_before = false;
+    static _LIBCPP_CONSTEXPR const float_round_style round_style = round_to_nearest;
+};
+
 template <>
 class __libcpp_numeric_limits<double, true>
 {
@@ -486,51 +532,51 @@ public:
 };
 
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_specialized;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_specialized;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::digits;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::digits;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::max_digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::max_digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_signed;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_signed;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_integer;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_integer;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_exact;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_exact;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::radix;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::radix;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::min_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::min_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::min_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::min_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::max_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::max_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<_Tp>::max_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<_Tp>::max_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::has_infinity;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::has_infinity;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::has_quiet_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::has_quiet_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::has_signaling_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::has_signaling_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_denorm_style numeric_limits<_Tp>::has_denorm;
+    _LIBCPP_CONSTEXPR constant const float_denorm_style numeric_limits<_Tp>::has_denorm;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::has_denorm_loss;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::has_denorm_loss;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_iec559;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_iec559;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_bounded;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_bounded;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::is_modulo;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::is_modulo;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::traps;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::traps;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<_Tp>::tinyness_before;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<_Tp>::tinyness_before;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_round_style numeric_limits<_Tp>::round_style;
+    _LIBCPP_CONSTEXPR constant const float_round_style numeric_limits<_Tp>::round_style;
 
 template <class _Tp>
 class _LIBCPP_TEMPLATE_VIS numeric_limits<const _Tp>
@@ -579,51 +625,51 @@ public:
 };
 
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_specialized;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_specialized;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::digits;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::digits;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::max_digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::max_digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_signed;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_signed;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_integer;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_integer;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_exact;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_exact;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::radix;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::radix;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::min_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::min_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::min_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::min_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::max_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::max_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const _Tp>::max_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const _Tp>::max_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::has_infinity;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::has_infinity;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::has_quiet_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::has_quiet_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::has_signaling_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::has_signaling_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_denorm_style numeric_limits<const _Tp>::has_denorm;
+    _LIBCPP_CONSTEXPR constant const float_denorm_style numeric_limits<const _Tp>::has_denorm;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::has_denorm_loss;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::has_denorm_loss;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_iec559;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_iec559;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_bounded;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_bounded;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::is_modulo;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::is_modulo;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::traps;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::traps;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const _Tp>::tinyness_before;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const _Tp>::tinyness_before;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_round_style numeric_limits<const _Tp>::round_style;
+    _LIBCPP_CONSTEXPR constant const float_round_style numeric_limits<const _Tp>::round_style;
 
 template <class _Tp>
 class _LIBCPP_TEMPLATE_VIS numeric_limits<volatile _Tp>
@@ -672,51 +718,51 @@ public:
 };
 
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_specialized;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_specialized;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::digits;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::digits;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::max_digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::max_digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_signed;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_signed;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_integer;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_integer;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_exact;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_exact;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::radix;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::radix;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::min_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::min_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::min_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::min_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::max_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::max_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<volatile _Tp>::max_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<volatile _Tp>::max_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::has_infinity;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::has_infinity;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::has_quiet_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::has_quiet_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::has_signaling_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::has_signaling_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_denorm_style numeric_limits<volatile _Tp>::has_denorm;
+    _LIBCPP_CONSTEXPR constant const float_denorm_style numeric_limits<volatile _Tp>::has_denorm;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::has_denorm_loss;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::has_denorm_loss;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_iec559;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_iec559;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_bounded;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_bounded;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::is_modulo;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::is_modulo;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::traps;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::traps;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<volatile _Tp>::tinyness_before;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<volatile _Tp>::tinyness_before;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_round_style numeric_limits<volatile _Tp>::round_style;
+    _LIBCPP_CONSTEXPR constant const float_round_style numeric_limits<volatile _Tp>::round_style;
 
 template <class _Tp>
 class _LIBCPP_TEMPLATE_VIS numeric_limits<const volatile _Tp>
@@ -765,51 +811,51 @@ public:
 };
 
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_specialized;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_specialized;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::digits;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::digits;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::max_digits10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::max_digits10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_signed;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_signed;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_integer;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_integer;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_exact;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_exact;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::radix;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::radix;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::min_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::min_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::min_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::min_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::max_exponent;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::max_exponent;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const int numeric_limits<const volatile _Tp>::max_exponent10;
+    _LIBCPP_CONSTEXPR constant const int numeric_limits<const volatile _Tp>::max_exponent10;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::has_infinity;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::has_infinity;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::has_quiet_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::has_quiet_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::has_signaling_NaN;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::has_signaling_NaN;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_denorm_style numeric_limits<const volatile _Tp>::has_denorm;
+    _LIBCPP_CONSTEXPR constant const float_denorm_style numeric_limits<const volatile _Tp>::has_denorm;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::has_denorm_loss;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::has_denorm_loss;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_iec559;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_iec559;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_bounded;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_bounded;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::is_modulo;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::is_modulo;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::traps;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::traps;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const bool numeric_limits<const volatile _Tp>::tinyness_before;
+    _LIBCPP_CONSTEXPR constant const bool numeric_limits<const volatile _Tp>::tinyness_before;
 template <class _Tp>
-    _LIBCPP_CONSTEXPR const float_round_style numeric_limits<const volatile _Tp>::round_style;
+    _LIBCPP_CONSTEXPR constant const float_round_style numeric_limits<const volatile _Tp>::round_style;
 
 _LIBCPP_END_NAMESPACE_STD
 
diff --git a/libcxx/include/list b/libcxx/include/list
index 80abe1c890ae..d5dee235e808 100644
--- a/libcxx/include/list
+++ b/libcxx/include/list
@@ -760,7 +760,7 @@ __list_imp<_Tp, _Alloc>::clear() _NOEXCEPT
         {
             __node_pointer __np = __f->__as_node();
             __f = __f->__next_;
-            __node_alloc_traits::destroy(__na, _VSTD::addressof(__np->__value_));
+            __node_alloc_traits::destroy(__na, __builtin_addressof(__np->__value_));
             __node_alloc_traits::deallocate(__na, __np, 1);
         }
         __invalidate_all_iterators();
@@ -809,7 +809,7 @@ __list_imp<_Tp, _Alloc>::swap(__list_imp& __c)
         {
             __cn2->__add(*__p);
             if (--__cn1->end_ != __p)
-                _VSTD::memmove(__p, __p+1, (__cn1->end_ - __p)*sizeof(__i_node*));
+                __builtin_memmove(__p, __p+1, (__cn1->end_ - __p)*sizeof(__i_node*));
         }
         else
             (*__p)->__c_ = __cn1;
@@ -822,7 +822,7 @@ __list_imp<_Tp, _Alloc>::swap(__list_imp& __c)
         {
             __cn1->__add(*__p);
             if (--__cn2->end_ != __p)
-                _VSTD::memmove(__p, __p+1, (__cn2->end_ - __p)*sizeof(__i_node*));
+                __builtin_memmove(__p, __p+1, (__cn2->end_ - __p)*sizeof(__i_node*));
         }
         else
             (*__p)->__c_ = __cn2;
@@ -1455,7 +1455,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, const value_type& __x)
 #endif
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
     __link_nodes(__p.__ptr_, __hold->__as_link(), __hold->__as_link());
     ++base::__sz();
 #if _LIBCPP_DEBUG_LEVEL == 2
@@ -1482,7 +1482,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, size_type __n, const value_type& _
         size_type __ds = 0;
         __node_allocator& __na = base::__node_alloc();
         __hold_pointer __hold = __allocate_node(__na);
-        __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+        __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
         ++__ds;
 #if _LIBCPP_DEBUG_LEVEL == 2
         __r = iterator(__hold->__as_link(), this);
@@ -1498,7 +1498,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, size_type __n, const value_type& _
             for (--__n; __n != 0; --__n, (void) ++__e, ++__ds)
             {
                 __hold.reset(__node_alloc_traits::allocate(__na, 1));
-                __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+                __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
                 __e.__ptr_->__next_ = __hold->__as_link();
                 __hold->__prev_ = __e.__ptr_;
                 __hold.release();
@@ -1509,7 +1509,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, size_type __n, const value_type& _
         {
             while (true)
             {
-                __node_alloc_traits::destroy(__na, _VSTD::addressof(*__e));
+                __node_alloc_traits::destroy(__na, __builtin_addressof(*__e));
                 __link_pointer __prev = __e.__ptr_->__prev_;
                 __node_alloc_traits::deallocate(__na, __e.__ptr_->__as_node(), 1);
                 if (__prev == 0)
@@ -1548,7 +1548,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, _InpIter __f, _InpIter __l,
         size_type __ds = 0;
         __node_allocator& __na = base::__node_alloc();
         __hold_pointer __hold = __allocate_node(__na);
-        __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), *__f);
+        __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), *__f);
         ++__ds;
 #if _LIBCPP_DEBUG_LEVEL == 2
         __r = iterator(__hold.get()->__as_link(), this);
@@ -1564,7 +1564,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, _InpIter __f, _InpIter __l,
             for (++__f; __f != __l; ++__f, (void) ++__e, ++__ds)
             {
                 __hold.reset(__node_alloc_traits::allocate(__na, 1));
-                __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), *__f);
+                __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), *__f);
                 __e.__ptr_->__next_ = __hold.get()->__as_link();
                 __hold->__prev_ = __e.__ptr_;
                 __hold.release();
@@ -1575,7 +1575,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, _InpIter __f, _InpIter __l,
         {
             while (true)
             {
-                __node_alloc_traits::destroy(__na, _VSTD::addressof(*__e));
+                __node_alloc_traits::destroy(__na, __builtin_addressof(*__e));
                 __link_pointer __prev = __e.__ptr_->__prev_;
                 __node_alloc_traits::deallocate(__na, __e.__ptr_->__as_node(), 1);
                 if (__prev == 0)
@@ -1601,7 +1601,7 @@ list<_Tp, _Alloc>::push_front(const value_type& __x)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
     __link_pointer __nl = __hold->__as_link();
     __link_nodes_at_front(__nl, __nl);
     ++base::__sz();
@@ -1614,7 +1614,7 @@ list<_Tp, _Alloc>::push_back(const value_type& __x)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
     __link_nodes_at_back(__hold.get()->__as_link(), __hold.get()->__as_link());
     ++base::__sz();
     __hold.release();
@@ -1628,7 +1628,7 @@ list<_Tp, _Alloc>::push_front(value_type&& __x)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::move(__x));
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::move(__x));
     __link_nodes_at_front(__hold.get()->__as_link(), __hold.get()->__as_link());
     ++base::__sz();
     __hold.release();
@@ -1640,7 +1640,7 @@ list<_Tp, _Alloc>::push_back(value_type&& __x)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::move(__x));
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::move(__x));
     __link_nodes_at_back(__hold.get()->__as_link(), __hold.get()->__as_link());
     ++base::__sz();
     __hold.release();
@@ -1657,7 +1657,7 @@ list<_Tp, _Alloc>::emplace_front(_Args&&... __args)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
     __link_nodes_at_front(__hold.get()->__as_link(), __hold.get()->__as_link());
     ++base::__sz();
 #if _LIBCPP_STD_VER > 14
@@ -1678,7 +1678,7 @@ list<_Tp, _Alloc>::emplace_back(_Args&&... __args)
 {
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
     __link_pointer __nl = __hold->__as_link();
     __link_nodes_at_back(__nl, __nl);
     ++base::__sz();
@@ -1701,7 +1701,7 @@ list<_Tp, _Alloc>::emplace(const_iterator __p, _Args&&... __args)
 #endif
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::forward<_Args>(__args)...);
     __link_pointer __nl = __hold.get()->__as_link();
     __link_nodes(__p.__ptr_, __nl, __nl);
     ++base::__sz();
@@ -1724,7 +1724,7 @@ list<_Tp, _Alloc>::insert(const_iterator __p, value_type&& __x)
 #endif
     __node_allocator& __na = base::__node_alloc();
     __hold_pointer __hold = __allocate_node(__na);
-    __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), _VSTD::move(__x));
+    __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), _VSTD::move(__x));
     __link_pointer __nl = __hold->__as_link();
     __link_nodes(__p.__ptr_, __nl, __nl);
     ++base::__sz();
@@ -1757,13 +1757,13 @@ list<_Tp, _Alloc>::pop_front()
         {
             (*__p)->__c_ = nullptr;
             if (--__c->end_ != __p)
-                _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+                __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
         }
     }
     __get_db()->unlock();
 #endif
     __node_pointer __np = __n->__as_node();
-    __node_alloc_traits::destroy(__na, _VSTD::addressof(__np->__value_));
+    __node_alloc_traits::destroy(__na, __builtin_addressof(__np->__value_));
     __node_alloc_traits::deallocate(__na, __np, 1);
 }
 
@@ -1786,13 +1786,13 @@ list<_Tp, _Alloc>::pop_back()
         {
             (*__p)->__c_ = nullptr;
             if (--__c->end_ != __p)
-                _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+                __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
         }
     }
     __get_db()->unlock();
 #endif
     __node_pointer __np = __n->__as_node();
-    __node_alloc_traits::destroy(__na, _VSTD::addressof(__np->__value_));
+    __node_alloc_traits::destroy(__na, __builtin_addressof(__np->__value_));
     __node_alloc_traits::deallocate(__na, __np, 1);
 }
 
@@ -1822,13 +1822,13 @@ list<_Tp, _Alloc>::erase(const_iterator __p)
         {
             (*__ip)->__c_ = nullptr;
             if (--__c->end_ != __ip)
-                _VSTD::memmove(__ip, __ip+1, (__c->end_ - __ip)*sizeof(__i_node*));
+                __builtin_memmove(__ip, __ip+1, (__c->end_ - __ip)*sizeof(__i_node*));
         }
     }
     __get_db()->unlock();
 #endif
     __node_pointer __np = __n->__as_node();
-    __node_alloc_traits::destroy(__na, _VSTD::addressof(__np->__value_));
+    __node_alloc_traits::destroy(__na, __builtin_addressof(__np->__value_));
     __node_alloc_traits::deallocate(__na, __np, 1);
 #if _LIBCPP_DEBUG_LEVEL == 2
     return iterator(__r, this);
@@ -1868,13 +1868,13 @@ list<_Tp, _Alloc>::erase(const_iterator __f, const_iterator __l)
                 {
                     (*__p)->__c_ = nullptr;
                     if (--__c->end_ != __p)
-                        _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+                        __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
                 }
             }
             __get_db()->unlock();
 #endif
             __node_pointer __np = __n->__as_node();
-            __node_alloc_traits::destroy(__na, _VSTD::addressof(__np->__value_));
+            __node_alloc_traits::destroy(__na, __builtin_addressof(__np->__value_));
             __node_alloc_traits::deallocate(__na, __np, 1);
         }
     }
@@ -1897,7 +1897,7 @@ list<_Tp, _Alloc>::resize(size_type __n)
         size_type __ds = 0;
         __node_allocator& __na = base::__node_alloc();
         __hold_pointer __hold = __allocate_node(__na);
-        __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_));
+        __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_));
         ++__ds;
 #if _LIBCPP_DEBUG_LEVEL == 2
         iterator __r = iterator(__hold.release()->__as_link(), this);
@@ -1912,7 +1912,7 @@ list<_Tp, _Alloc>::resize(size_type __n)
             for (--__n; __n != 0; --__n, (void) ++__e, ++__ds)
             {
                 __hold.reset(__node_alloc_traits::allocate(__na, 1));
-                __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_));
+                __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_));
                 __e.__ptr_->__next_ = __hold.get()->__as_link();
                 __hold->__prev_ = __e.__ptr_;
                 __hold.release();
@@ -1923,7 +1923,7 @@ list<_Tp, _Alloc>::resize(size_type __n)
         {
             while (true)
             {
-                __node_alloc_traits::destroy(__na, _VSTD::addressof(*__e));
+                __node_alloc_traits::destroy(__na, __builtin_addressof(*__e));
                 __link_pointer __prev = __e.__ptr_->__prev_;
                 __node_alloc_traits::deallocate(__na, __e.__ptr_->__as_node(), 1);
                 if (__prev == 0)
@@ -1954,7 +1954,7 @@ list<_Tp, _Alloc>::resize(size_type __n, const value_type& __x)
         size_type __ds = 0;
         __node_allocator& __na = base::__node_alloc();
         __hold_pointer __hold = __allocate_node(__na);
-        __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+        __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
         ++__ds;
         __link_pointer __nl = __hold.release()->__as_link();
 #if _LIBCPP_DEBUG_LEVEL == 2
@@ -1970,7 +1970,7 @@ list<_Tp, _Alloc>::resize(size_type __n, const value_type& __x)
             for (--__n; __n != 0; --__n, (void) ++__e, ++__ds)
             {
                 __hold.reset(__node_alloc_traits::allocate(__na, 1));
-                __node_alloc_traits::construct(__na, _VSTD::addressof(__hold->__value_), __x);
+                __node_alloc_traits::construct(__na, __builtin_addressof(__hold->__value_), __x);
                 __e.__ptr_->__next_ = __hold.get()->__as_link();
                 __hold->__prev_ = __e.__ptr_;
                 __hold.release();
@@ -1981,7 +1981,7 @@ list<_Tp, _Alloc>::resize(size_type __n, const value_type& __x)
         {
             while (true)
             {
-                __node_alloc_traits::destroy(__na, _VSTD::addressof(*__e));
+                __node_alloc_traits::destroy(__na, __builtin_addressof(*__e));
                 __link_pointer __prev = __e.__ptr_->__prev_;
                 __node_alloc_traits::deallocate(__na, __e.__ptr_->__as_node(), 1);
                 if (__prev == 0)
@@ -2033,7 +2033,7 @@ list<_Tp, _Alloc>::splice(const_iterator __p, list& __c)
                     __cn1->__add(*__ip);
                     (*__ip)->__c_ = __cn1;
                     if (--__cn2->end_ != __ip)
-                        _VSTD::memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
+                        __builtin_memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
                 }
             }
             __db->unlock();
@@ -2078,7 +2078,7 @@ list<_Tp, _Alloc>::splice(const_iterator __p, list& __c, const_iterator __i)
                     __cn1->__add(*__ip);
                     (*__ip)->__c_ = __cn1;
                     if (--__cn2->end_ != __ip)
-                        _VSTD::memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
+                        __builtin_memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
                 }
             }
             __db->unlock();
@@ -2140,7 +2140,7 @@ list<_Tp, _Alloc>::splice(const_iterator __p, list& __c, const_iterator __f, con
                         __cn1->__add(*__ip);
                         (*__ip)->__c_ = __cn1;
                         if (--__cn2->end_ != __ip)
-                            _VSTD::memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
+                            __builtin_memmove(__ip, __ip+1, (__cn2->end_ - __ip)*sizeof(__i_node*));
                     }
                 }
             }
@@ -2232,7 +2232,7 @@ template <class _Comp>
 void
 list<_Tp, _Alloc>::merge(list& __c, _Comp __comp)
 {
-    if (this != _VSTD::addressof(__c))
+    if (this != __builtin_addressof(__c))
     {
         iterator __f1 = begin();
         iterator __e1 = end();
@@ -2273,7 +2273,7 @@ list<_Tp, _Alloc>::merge(list& __c, _Comp __comp)
                 __cn1->__add(*__p);
                 (*__p)->__c_ = __cn1;
                 if (--__cn2->end_ != __p)
-                    _VSTD::memmove(__p, __p+1, (__cn2->end_ - __p)*sizeof(__i_node*));
+                    __builtin_memmove(__p, __p+1, (__cn2->end_ - __p)*sizeof(__i_node*));
             }
         }
         __db->unlock();
diff --git a/libcxx/include/locale b/libcxx/include/locale
index c20e15f3e750..7bee7c899b95 100644
--- a/libcxx/include/locale
+++ b/libcxx/include/locale
@@ -4078,7 +4078,7 @@ wbuffer_convert<_Codecvt, _Elem, _Tr>::underflow()
     int_type __c = traits_type::eof();
     if (this->gptr() == this->egptr())
     {
-        _VSTD::memmove(this->eback(), this->egptr() - __unget_sz, __unget_sz * sizeof(char_type));
+        __builtin_memmove(this->eback(), this->egptr() - __unget_sz, __unget_sz * sizeof(char_type));
         if (__always_noconv_)
         {
             streamsize __nmemb = static_cast<streamsize>(this->egptr() - this->eback() - __unget_sz);
@@ -4095,7 +4095,7 @@ wbuffer_convert<_Codecvt, _Elem, _Tr>::underflow()
         {
              _LIBCPP_ASSERT(!(__extbufnext_ == NULL && (__extbufend_ != __extbufnext_)), "underflow moving from NULL" );
              if (__extbufend_ != __extbufnext_)
-                _VSTD::memmove(__extbuf_, __extbufnext_, __extbufend_ - __extbufnext_);
+                __builtin_memmove(__extbuf_, __extbufnext_, __extbufend_ - __extbufnext_);
             __extbufnext_ = __extbuf_ + (__extbufend_ - __extbufnext_);
             __extbufend_ = __extbuf_ + (__extbuf_ == __extbuf_min_ ? sizeof(__extbuf_min_) : __ebs_);
             streamsize __nmemb = _VSTD::min(static_cast<streamsize>(this->egptr() - this->eback() - __unget_sz),
diff --git a/libcxx/include/map b/libcxx/include/map
index 667dc0e3f883..49d5c208c9ad 100644
--- a/libcxx/include/map
+++ b/libcxx/include/map
@@ -651,9 +651,9 @@ public:
     void operator()(pointer __p) _NOEXCEPT
     {
         if (__second_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.__get_value().second));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.__get_value().second));
         if (__first_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.__get_value().first));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.__get_value().first));
         if (__p)
             __alloc_traits::deallocate(__na_, __p, 1);
     }
@@ -684,7 +684,7 @@ public:
     value_type& __get_value()
     {
 #if _LIBCPP_STD_VER > 14
-        return *_VSTD::launder(_VSTD::addressof(__cc));
+        return *_VSTD::launder(__builtin_addressof(__cc));
 #else
         return __cc;
 #endif
@@ -694,7 +694,7 @@ public:
     const value_type& __get_value() const
     {
 #if _LIBCPP_STD_VER > 14
-        return *_VSTD::launder(_VSTD::addressof(__cc));
+        return *_VSTD::launder(__builtin_addressof(__cc));
 #else
         return __cc;
 #endif
@@ -1567,9 +1567,9 @@ map<_Key, _Tp, _Compare, _Allocator>::__construct_node_with_key(const key_type&
 {
     __node_allocator& __na = __tree_.__node_alloc();
     __node_holder __h(__node_traits::allocate(__na, 1), _Dp(__na));
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.__get_value().first), __k);
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.__get_value().first), __k);
     __h.get_deleter().__first_constructed = true;
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.__get_value().second));
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.__get_value().second));
     __h.get_deleter().__second_constructed = true;
     return __h;
 }
diff --git a/libcxx/include/memory b/libcxx/include/memory
index b1d16ef082aa..e04286238f23 100644
--- a/libcxx/include/memory
+++ b/libcxx/include/memory
@@ -688,15 +688,19 @@ void* align(size_t alignment, size_t size, void*& ptr, size_t& space);
 #include <__memory/uses_allocator.h>
 #include <compare>
 #include <cstddef>
+#if 0
 #include <cstdint>
 #include <cstring>
+#endif
 #include <iosfwd>
 #include <iterator>
 #include <new>
 #include <stdexcept>
 #include <tuple>
 #include <type_traits>
+#if 0
 #include <typeinfo>
+#endif
 #include <utility>
 #include <version>
 
@@ -735,7 +739,7 @@ _LIBCPP_INLINE_VISIBILITY
 void __construct_forward_with_exception_guarantees(_Alloc&, _Tp* __begin1, _Tp* __end1, _Tp*& __begin2) {
     ptrdiff_t _Np = __end1 - __begin1;
     if (_Np > 0) {
-        _VSTD::memcpy(__begin2, __begin1, _Np * sizeof(_Tp));
+        __builtin_memcpy(__begin2, __begin1, _Np * sizeof(_Tp));
         __begin2 += _Np;
     }
 }
@@ -762,7 +766,7 @@ _LIBCPP_INLINE_VISIBILITY
 void __construct_range_forward(_Alloc&, _Source* __begin1, _Source* __end1, _Dest*& __begin2) {
     ptrdiff_t _Np = __end1 - __begin1;
     if (_Np > 0) {
-        _VSTD::memcpy(const_cast<_RawDest*>(__begin2), __begin1, _Np * sizeof(_Dest));
+        __builtin_memcpy(const_cast<_RawDest*>(__begin2), __begin1, _Np * sizeof(_Dest));
         __begin2 += _Np;
     }
 }
@@ -794,7 +798,7 @@ void __construct_backward_with_exception_guarantees(_Alloc&, _Tp* __begin1, _Tp*
     ptrdiff_t _Np = __end1 - __begin1;
     __end2 -= _Np;
     if (_Np > 0)
-        _VSTD::memcpy(static_cast<void*>(__end2), static_cast<void const*>(__begin1), _Np * sizeof(_Tp));
+        __builtin_memcpy(static_cast<void*>(__end2), static_cast<void const*>(__begin1), _Np * sizeof(_Tp));
 }
 
 struct __destruct_n
diff --git a/libcxx/include/new b/libcxx/include/new
index 711b3ccd225f..d52ac8d817d9 100644
--- a/libcxx/include/new
+++ b/libcxx/include/new
@@ -89,7 +89,9 @@ void  operator delete[](void* ptr, void*) noexcept;
 #include <__availability>
 #include <__config>
 #include <cstddef>
+#if 0
 #include <cstdlib>
+#endif
 #include <exception>
 #include <type_traits>
 #include <version>
@@ -121,7 +123,7 @@ namespace std  // purposefully not using versioning namespace
 
 #if !defined(_LIBCPP_ABI_VCRUNTIME)
 struct _LIBCPP_TYPE_VIS nothrow_t { explicit nothrow_t() = default; };
-extern _LIBCPP_FUNC_VIS const nothrow_t nothrow;
+_LIBCPP_FUNC_VIS static const nothrow_t nothrow;
 
 class _LIBCPP_EXCEPTION_ABI bad_alloc
     : public exception
@@ -129,7 +131,7 @@ class _LIBCPP_EXCEPTION_ABI bad_alloc
 public:
     bad_alloc() _NOEXCEPT;
     virtual ~bad_alloc() _NOEXCEPT;
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 };
 
 class _LIBCPP_EXCEPTION_ABI bad_array_new_length
@@ -138,7 +140,7 @@ class _LIBCPP_EXCEPTION_ABI bad_array_new_length
 public:
     bad_array_new_length() _NOEXCEPT;
     virtual ~bad_array_new_length() _NOEXCEPT;
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 };
 
 typedef void (*new_handler)();
@@ -155,7 +157,7 @@ void __throw_bad_array_new_length()
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw bad_array_new_length();
 #else
-    _VSTD::abort();
+    __builtin_unreachable();
 #endif
 }
 
@@ -185,7 +187,7 @@ inline constexpr destroying_delete_t destroying_delete{};
 #define _THROW_BAD_ALLOC
 #endif
 
-#if !defined(_LIBCPP_ABI_VCRUNTIME)
+#if !defined(_LIBCPP_ABI_VCRUNTIME) && 0
 
 _LIBCPP_NODISCARD_AFTER_CXX17 _LIBCPP_OVERRIDABLE_FUNC_VIS void* operator new(std::size_t __sz) _THROW_BAD_ALLOC;
 _LIBCPP_NODISCARD_AFTER_CXX17 _LIBCPP_OVERRIDABLE_FUNC_VIS void* operator new(std::size_t __sz, const std::nothrow_t&) _NOEXCEPT _LIBCPP_NOALIAS;
@@ -320,6 +322,7 @@ inline _LIBCPP_INLINE_VISIBILITY void __libcpp_deallocate_unsized(void* __ptr, s
 // Returns the allocated memory, or `nullptr` on failure.
 inline _LIBCPP_INLINE_VISIBILITY
 void* __libcpp_aligned_alloc(std::size_t __alignment, std::size_t __size) {
+#if 0 // not supported
 #if defined(_LIBCPP_MSVCRT_LIKE)
   return ::_aligned_malloc(__size, __alignment);
 #else
@@ -328,15 +331,22 @@ void* __libcpp_aligned_alloc(std::size_t __alignment, std::size_t __size) {
   // If posix_memalign fails, __result is unmodified so we still return `nullptr`.
   return __result;
 #endif
+#else
+	return nullptr;
+#endif
 }
 
 inline _LIBCPP_INLINE_VISIBILITY
 void __libcpp_aligned_free(void* __ptr) {
+#if 0 // not supported
 #if defined(_LIBCPP_MSVCRT_LIKE)
   ::_aligned_free(__ptr);
 #else
   ::free(__ptr);
 #endif
+#else
+	(void)__ptr;
+#endif
 }
 #endif // !_LIBCPP_HAS_NO_ALIGNED_ALLOCATION
 
diff --git a/libcxx/include/optional b/libcxx/include/optional
index b9ac71b04e2c..65370780a083 100644
--- a/libcxx/include/optional
+++ b/libcxx/include/optional
@@ -319,9 +319,9 @@ struct __optional_storage_base : __optional_destruct_base<_Tp>
     {
         _LIBCPP_ASSERT(!has_value(), "__construct called for engaged __optional_storage");
 #if _LIBCPP_STD_VER > 17
-        _VSTD::construct_at(_VSTD::addressof(this->__val_), _VSTD::forward<_Args>(__args)...);
+        _VSTD::construct_at(__builtin_addressof(this->__val_), _VSTD::forward<_Args>(__args)...);
 #else
-        ::new ((void*)_VSTD::addressof(this->__val_)) value_type(_VSTD::forward<_Args>(__args)...);
+        ::new ((void*)__builtin_addressof(this->__val_)) value_type(_VSTD::forward<_Args>(__args)...);
 #endif
         this->__engaged_ = true;
     }
@@ -386,7 +386,7 @@ struct __optional_storage_base<_Tp, true>
     template <class _UArg>
     _LIBCPP_INLINE_VISIBILITY
     constexpr explicit __optional_storage_base(in_place_t, _UArg&& __uarg)
-        :  __value_(_VSTD::addressof(__uarg))
+        :  __value_(__builtin_addressof(__uarg))
     {
       static_assert(__can_bind_reference<_UArg>(),
         "Attempted to construct a reference element in tuple from a "
@@ -416,7 +416,7 @@ struct __optional_storage_base<_Tp, true>
         static_assert(__can_bind_reference<_UArg>(),
             "Attempted to construct a reference element in tuple from a "
             "possible temporary");
-        __value_ = _VSTD::addressof(__val);
+        __value_ = __builtin_addressof(__val);
     }
 
     template <class _That>
@@ -879,7 +879,7 @@ public:
     operator->() const
     {
         _LIBCPP_ASSERT(this->has_value(), "optional operator-> called on a disengaged value");
-        return _VSTD::addressof(this->__get());
+        return __builtin_addressof(this->__get());
     }
 
     _LIBCPP_INLINE_VISIBILITY
@@ -888,7 +888,7 @@ public:
     operator->()
     {
         _LIBCPP_ASSERT(this->has_value(), "optional operator-> called on a disengaged value");
-        return _VSTD::addressof(this->__get());
+        return __builtin_addressof(this->__get());
     }
 
     _LIBCPP_INLINE_VISIBILITY
diff --git a/libcxx/include/regex b/libcxx/include/regex
index c0d1edac5ef1..ccd045f2cc12 100644
--- a/libcxx/include/regex
+++ b/libcxx/include/regex
@@ -6316,7 +6316,7 @@ public:
     _LIBCPP_INLINE_VISIBILITY
     reference operator*() const {return  __match_;}
     _LIBCPP_INLINE_VISIBILITY
-    pointer operator->() const  {return _VSTD::addressof(__match_);}
+    pointer operator->() const  {return __builtin_addressof(__match_);}
 
     regex_iterator& operator++();
     _LIBCPP_INLINE_VISIBILITY
@@ -6340,7 +6340,7 @@ regex_iterator<_BidirectionalIterator, _CharT, _Traits>::
                    const regex_type& __re, regex_constants::match_flag_type __m)
     : __begin_(__a),
       __end_(__b),
-      __pregex_(_VSTD::addressof(__re)),
+      __pregex_(__builtin_addressof(__re)),
       __flags_(__m)
 {
     _VSTD::regex_search(__begin_, __end_, __match_, *__pregex_, __flags_);
diff --git a/libcxx/include/shared_mutex b/libcxx/include/shared_mutex
index 8205c3e0af0c..f5337426b6e5 100644
--- a/libcxx/include/shared_mutex
+++ b/libcxx/include/shared_mutex
@@ -325,25 +325,25 @@ public:
 
     _LIBCPP_INLINE_VISIBILITY
     explicit shared_lock(mutex_type& __m)
-        : __m_(_VSTD::addressof(__m)),
+        : __m_(__builtin_addressof(__m)),
           __owns_(true)
         {__m_->lock_shared();}
 
     _LIBCPP_INLINE_VISIBILITY
     shared_lock(mutex_type& __m, defer_lock_t) _NOEXCEPT
-        : __m_(_VSTD::addressof(__m)),
+        : __m_(__builtin_addressof(__m)),
           __owns_(false)
         {}
 
     _LIBCPP_INLINE_VISIBILITY
     shared_lock(mutex_type& __m, try_to_lock_t)
-        : __m_(_VSTD::addressof(__m)),
+        : __m_(__builtin_addressof(__m)),
           __owns_(__m.try_lock_shared())
         {}
 
     _LIBCPP_INLINE_VISIBILITY
     shared_lock(mutex_type& __m, adopt_lock_t)
-        : __m_(_VSTD::addressof(__m)),
+        : __m_(__builtin_addressof(__m)),
           __owns_(true)
         {}
 
@@ -351,7 +351,7 @@ public:
         _LIBCPP_INLINE_VISIBILITY
         shared_lock(mutex_type& __m,
                     const chrono::time_point<_Clock, _Duration>& __abs_time)
-            : __m_(_VSTD::addressof(__m)),
+            : __m_(__builtin_addressof(__m)),
               __owns_(__m.try_lock_shared_until(__abs_time))
             {}
 
@@ -359,7 +359,7 @@ public:
         _LIBCPP_INLINE_VISIBILITY
         shared_lock(mutex_type& __m,
                     const chrono::duration<_Rep, _Period>& __rel_time)
-            : __m_(_VSTD::addressof(__m)),
+            : __m_(__builtin_addressof(__m)),
               __owns_(__m.try_lock_shared_for(__rel_time))
             {}
 
diff --git a/libcxx/include/stddef.h b/libcxx/include/stddef.h
index 2f8c0a5f5bf2..cb020612e8ce 100644
--- a/libcxx/include/stddef.h
+++ b/libcxx/include/stddef.h
@@ -7,6 +7,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#if defined(FLOOR_COMPUTE_HOST_DEVICE) // we don't want this for other backends
+
 #if defined(__need_ptrdiff_t) || defined(__need_size_t) || \
     defined(__need_wchar_t) || defined(__need_NULL) || defined(__need_wint_t)
 
@@ -54,3 +56,5 @@ using std::nullptr_t;
 #endif
 
 #endif // _LIBCPP_STDDEF_H
+
+#endif
diff --git a/libcxx/include/stdexcept b/libcxx/include/stdexcept
index c0470d1e1dd3..d4444f381f66 100644
--- a/libcxx/include/stdexcept
+++ b/libcxx/include/stdexcept
@@ -44,7 +44,9 @@ public:
 #include <__config>
 #include <exception>
 #include <iosfwd>  // for string forward decl
+#if 0
 #include <cstdlib>
+#endif
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
 #pragma GCC system_header
@@ -55,16 +57,16 @@ _LIBCPP_BEGIN_NAMESPACE_STD
 #ifndef _LIBCPP_ABI_VCRUNTIME
 class _LIBCPP_HIDDEN __libcpp_refstring
 {
-    const char* __imp_;
+    constant const char* __imp_;
 
     bool __uses_refcount() const;
 public:
-    explicit __libcpp_refstring(const char* __msg);
+    explicit __libcpp_refstring(constant const char* __msg);
     __libcpp_refstring(const __libcpp_refstring& __s) _NOEXCEPT;
     __libcpp_refstring& operator=(const __libcpp_refstring& __s) _NOEXCEPT;
     ~__libcpp_refstring();
 
-    const char* c_str() const _NOEXCEPT {return __imp_;}
+    constant const char* c_str() const _NOEXCEPT {return __imp_;}
 };
 #endif // !_LIBCPP_ABI_VCRUNTIME
 
@@ -81,18 +83,18 @@ private:
     _VSTD::__libcpp_refstring __imp_;
 public:
     explicit logic_error(const string&);
-    explicit logic_error(const char*);
+    explicit logic_error(const constant char*);
 
     logic_error(const logic_error&) _NOEXCEPT;
     logic_error& operator=(const logic_error&) _NOEXCEPT;
 
     virtual ~logic_error() _NOEXCEPT;
 
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 #else
 public:
     explicit logic_error(const _VSTD::string&); // Symbol uses versioned std::string
-    _LIBCPP_INLINE_VISIBILITY explicit logic_error(const char* __s) : exception(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit logic_error(constant const char* __s) : exception(__s) {}
 #endif
 };
 
@@ -104,18 +106,18 @@ private:
     _VSTD::__libcpp_refstring __imp_;
 public:
     explicit runtime_error(const string&);
-    explicit runtime_error(const char*);
+    explicit runtime_error(constant const char*);
 
     runtime_error(const runtime_error&) _NOEXCEPT;
     runtime_error& operator=(const runtime_error&) _NOEXCEPT;
 
     virtual ~runtime_error() _NOEXCEPT;
 
-    virtual const char* what() const _NOEXCEPT;
+    virtual constant const char* what() const _NOEXCEPT;
 #else
 public:
    explicit runtime_error(const _VSTD::string&); // Symbol uses versioned std::string
-   _LIBCPP_INLINE_VISIBILITY explicit runtime_error(const char* __s) : exception(__s) {}
+   _LIBCPP_INLINE_VISIBILITY explicit runtime_error(constant const char* __s) : exception(__s) {}
 #endif // _LIBCPP_ABI_VCRUNTIME
 };
 
@@ -124,7 +126,7 @@ class _LIBCPP_EXCEPTION_ABI domain_error
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit domain_error(const string& __s) : logic_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit domain_error(const char* __s)   : logic_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit domain_error(constant const char* __s)   : logic_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     domain_error(const domain_error&) _NOEXCEPT = default;
@@ -137,7 +139,7 @@ class _LIBCPP_EXCEPTION_ABI invalid_argument
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit invalid_argument(const string& __s) : logic_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit invalid_argument(const char* __s)   : logic_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit invalid_argument(constant const char* __s)   : logic_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     invalid_argument(const invalid_argument&) _NOEXCEPT = default;
@@ -150,7 +152,7 @@ class _LIBCPP_EXCEPTION_ABI length_error
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit length_error(const string& __s) : logic_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit length_error(const char* __s)   : logic_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit length_error(constant const char* __s)   : logic_error(__s) {}
 #ifndef _LIBCPP_ABI_VCRUNTIME
     length_error(const length_error&) _NOEXCEPT = default;
     virtual ~length_error() _NOEXCEPT;
@@ -162,7 +164,7 @@ class _LIBCPP_EXCEPTION_ABI out_of_range
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit out_of_range(const string& __s) : logic_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit out_of_range(const char* __s)   : logic_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit out_of_range(constant const char* __s)   : logic_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     out_of_range(const out_of_range&) _NOEXCEPT = default;
@@ -175,7 +177,7 @@ class _LIBCPP_EXCEPTION_ABI range_error
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit range_error(const string& __s) : runtime_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit range_error(const char* __s)   : runtime_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit range_error(constant const char* __s)   : runtime_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     range_error(const range_error&) _NOEXCEPT = default;
@@ -188,7 +190,7 @@ class _LIBCPP_EXCEPTION_ABI overflow_error
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit overflow_error(const string& __s) : runtime_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit overflow_error(const char* __s)   : runtime_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit overflow_error(constant const char* __s)   : runtime_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     overflow_error(const overflow_error&) _NOEXCEPT = default;
@@ -201,7 +203,7 @@ class _LIBCPP_EXCEPTION_ABI underflow_error
 {
 public:
     _LIBCPP_INLINE_VISIBILITY explicit underflow_error(const string& __s) : runtime_error(__s) {}
-    _LIBCPP_INLINE_VISIBILITY explicit underflow_error(const char* __s)   : runtime_error(__s) {}
+    _LIBCPP_INLINE_VISIBILITY explicit underflow_error(constant const char* __s)   : runtime_error(__s) {}
 
 #ifndef _LIBCPP_ABI_VCRUNTIME
     underflow_error(const underflow_error&) _NOEXCEPT = default;
@@ -214,94 +216,110 @@ public:
 _LIBCPP_BEGIN_NAMESPACE_STD
 
 // in the dylib
-_LIBCPP_NORETURN _LIBCPP_FUNC_VIS void __throw_runtime_error(const char*);
+_LIBCPP_NORETURN _LIBCPP_FUNC_VIS void __throw_runtime_error(constant const char*);
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_logic_error(const char*__msg)
+void __throw_logic_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw logic_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_domain_error(const char*__msg)
+void __throw_domain_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw domain_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_invalid_argument(const char*__msg)
+void __throw_invalid_argument(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw invalid_argument(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_length_error(const char*__msg)
+void __throw_length_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw length_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_out_of_range(const char*__msg)
+void __throw_out_of_range(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw out_of_range(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_range_error(const char*__msg)
+void __throw_range_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw range_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_overflow_error(const char*__msg)
+void __throw_overflow_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw overflow_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_NORETURN inline _LIBCPP_INLINE_VISIBILITY
-void __throw_underflow_error(const char*__msg)
+void __throw_underflow_error(constant const char*__msg)
 {
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw underflow_error(__msg);
 #else
     ((void)__msg);
+#if 0
     _VSTD::abort();
 #endif
+#endif
 }
 
 _LIBCPP_END_NAMESPACE_STD
diff --git a/libcxx/include/stdio.h b/libcxx/include/stdio.h
index f84122034891..2738ea7eebd5 100644
--- a/libcxx/include/stdio.h
+++ b/libcxx/include/stdio.h
@@ -13,7 +13,10 @@
 #pragma GCC system_header
 #endif
 
+// TODO: support this on CPU targets
+#if 0
 #include_next <stdio.h>
+#endif
 
 #elif !defined(_LIBCPP_STDIO_H)
 #define _LIBCPP_STDIO_H
diff --git a/libcxx/include/string b/libcxx/include/string
index 53b4e09b68f3..56a7d2617f8a 100644
--- a/libcxx/include/string
+++ b/libcxx/include/string
@@ -1700,7 +1700,7 @@ private:
     template<class _Tp>
     _LIBCPP_INLINE_VISIBILITY
     bool __addr_in_range(_Tp&& __t) const {
-        const volatile void *__p = _VSTD::addressof(__t);
+        const volatile void *__p = __builtin_addressof(__t);
         return data() <= __p && __p <= data() + size();
     }
 
@@ -1800,7 +1800,7 @@ basic_string<_CharT, _Traits, _Allocator>::__invalidate_iterators_past(size_type
             {
                 (*__p)->__c_ = nullptr;
                 if (--__c->end_ != __p)
-                    _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+                    __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
             }
         }
         __get_db()->unlock();
diff --git a/libcxx/include/type_traits b/libcxx/include/type_traits
index 5cd38f85cabe..75376e00f078 100644
--- a/libcxx/include/type_traits
+++ b/libcxx/include/type_traits
@@ -444,8 +444,10 @@ struct _LIBCPP_TEMPLATE_VIS integral_constant
 #endif
 };
 
+#if 0
 template <class _Tp, _Tp __v>
-_LIBCPP_CONSTEXPR const _Tp integral_constant<_Tp, __v>::value;
+_LIBCPP_CONSTEXPR constant const _Tp integral_constant<_Tp, __v>::value;
+#endif
 
 #if _LIBCPP_STD_VER > 14
 template <bool __b>
@@ -811,6 +813,7 @@ template <> struct __libcpp_is_unsigned_integer<__uint128_t>        : public tru
 
 template <class _Tp> struct __libcpp_is_floating_point              : public false_type {};
 template <>          struct __libcpp_is_floating_point<float>       : public true_type {};
+template <>          struct __libcpp_is_floating_point<__fp16>      : public true_type {};
 template <>          struct __libcpp_is_floating_point<double>      : public true_type {};
 template <>          struct __libcpp_is_floating_point<long double> : public true_type {};
 
diff --git a/libcxx/include/typeinfo b/libcxx/include/typeinfo
index 6026038ba5f9..e47816ce3b64 100644
--- a/libcxx/include/typeinfo
+++ b/libcxx/include/typeinfo
@@ -64,8 +64,10 @@ public:
 #include <type_traits>
 
 #ifdef _LIBCPP_NO_EXCEPTIONS
+#if 0
 #include <cstdlib>
 #endif
+#endif
 
 #if !defined(_LIBCPP_HAS_NO_PRAGMA_SYSTEM_HEADER)
 #pragma GCC system_header
@@ -349,7 +351,7 @@ class _LIBCPP_EXCEPTION_ABI bad_cast
   bad_cast() _NOEXCEPT;
   bad_cast(const bad_cast&) _NOEXCEPT = default;
   virtual ~bad_cast() _NOEXCEPT;
-  virtual const char* what() const _NOEXCEPT;
+  virtual constant const char* what() const _NOEXCEPT;
 };
 
 class _LIBCPP_EXCEPTION_ABI bad_typeid
@@ -358,7 +360,7 @@ class _LIBCPP_EXCEPTION_ABI bad_typeid
  public:
   bad_typeid() _NOEXCEPT;
   virtual ~bad_typeid() _NOEXCEPT;
-  virtual const char* what() const _NOEXCEPT;
+  virtual constant const char* what() const _NOEXCEPT;
 };
 
 }  // std
@@ -372,7 +374,7 @@ void __throw_bad_cast()
 #ifndef _LIBCPP_NO_EXCEPTIONS
     throw bad_cast();
 #else
-    _VSTD::abort();
+    __builtin_unreachable();
 #endif
 }
 _LIBCPP_END_NAMESPACE_STD
diff --git a/libcxx/include/unordered_map b/libcxx/include/unordered_map
index 417180a530e7..22cb7c54f619 100644
--- a/libcxx/include/unordered_map
+++ b/libcxx/include/unordered_map
@@ -697,9 +697,9 @@ public:
     void operator()(pointer __p) _NOEXCEPT
     {
         if (__second_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.__get_value().second));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.__get_value().second));
         if (__first_constructed)
-            __alloc_traits::destroy(__na_, _VSTD::addressof(__p->__value_.__get_value().first));
+            __alloc_traits::destroy(__na_, __builtin_addressof(__p->__value_.__get_value().first));
         if (__p)
             __alloc_traits::deallocate(__na_, __p, 1);
     }
@@ -723,7 +723,7 @@ public:
     value_type& __get_value()
     {
 #if _LIBCPP_STD_VER > 14
-        return *_VSTD::launder(_VSTD::addressof(__cc));
+        return *_VSTD::launder(__builtin_addressof(__cc));
 #else
         return __cc;
 #endif
@@ -733,7 +733,7 @@ public:
     const value_type& __get_value() const
     {
 #if _LIBCPP_STD_VER > 14
-        return *_VSTD::launder(_VSTD::addressof(__cc));
+        return *_VSTD::launder(__builtin_addressof(__cc));
 #else
         return __cc;
 #endif
@@ -1762,9 +1762,9 @@ unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::__construct_node_with_key(const
 {
     __node_allocator& __na = __table_.__node_alloc();
     __node_holder __h(__node_traits::allocate(__na, 1), _Dp(__na));
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.__get_value().first), __k);
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.__get_value().first), __k);
     __h.get_deleter().__first_constructed = true;
-    __node_traits::construct(__na, _VSTD::addressof(__h->__value_.__get_value().second));
+    __node_traits::construct(__na, __builtin_addressof(__h->__value_.__get_value().second));
     __h.get_deleter().__second_constructed = true;
     return __h;
 }
diff --git a/libcxx/include/variant b/libcxx/include/variant
index ea120d49f079..7e8985fdc4a6 100644
--- a/libcxx/include/variant
+++ b/libcxx/include/variant
@@ -226,7 +226,7 @@ namespace std { // explicitly not using versioning namespace
 
 class _LIBCPP_EXCEPTION_ABI _LIBCPP_AVAILABILITY_BAD_VARIANT_ACCESS bad_variant_access : public exception {
 public:
-  virtual const char* what() const _NOEXCEPT;
+  virtual constant const char* what() const _NOEXCEPT;
 };
 
 } // namespace std
@@ -255,7 +255,7 @@ void __throw_bad_variant_access() {
 #ifndef _LIBCPP_NO_EXCEPTIONS
         throw bad_variant_access();
 #else
-        _VSTD::abort();
+        __builtin_unreachable();
 #endif
 }
 
@@ -873,7 +873,7 @@ protected:
   template <size_t _Ip, class _Tp, class... _Args>
   inline _LIBCPP_INLINE_VISIBILITY
   static _Tp& __construct_alt(__alt<_Ip, _Tp>& __a, _Args&&... __args) {
-    ::new ((void*)_VSTD::addressof(__a))
+    ::new ((void*)__builtin_addressof(__a))
         __alt<_Ip, _Tp>(in_place, _VSTD::forward<_Args>(__args)...);
     return __a.__value;
   }
@@ -1146,7 +1146,7 @@ public:
           __that);
     } else {
       __impl* __lhs = this;
-      __impl* __rhs = _VSTD::addressof(__that);
+      __impl* __rhs = __builtin_addressof(__that);
       if (__lhs->__move_nothrow() && !__rhs->__move_nothrow()) {
         _VSTD::swap(__lhs, __rhs);
       }
@@ -1557,7 +1557,7 @@ inline _LIBCPP_INLINE_VISIBILITY
 constexpr auto* __generic_get_if(_Vp* __v) noexcept {
   using __variant_detail::__access::__variant;
   return __v && __holds_alternative<_Ip>(*__v)
-             ? _VSTD::addressof(__variant::__get_alt<_Ip>(*__v).__value)
+             ? __builtin_addressof(__variant::__get_alt<_Ip>(*__v).__value)
              : nullptr;
 }
 
diff --git a/libcxx/include/vector b/libcxx/include/vector
index 80819080a9fb..3f034e436600 100644
--- a/libcxx/include/vector
+++ b/libcxx/include/vector
@@ -2148,7 +2148,7 @@ vector<_Tp, _Allocator>::__invalidate_iterators_past(pointer __new_last) {
     if (__i->base() > __new_last) {
       (*__p)->__c_ = nullptr;
       if (--__c->end_ != __p)
-        _VSTD::memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
+        __builtin_memmove(__p, __p+1, (__c->end_ - __p)*sizeof(__i_node*));
     }
   }
   __get_db()->unlock();
diff --git a/lld/lib/ReaderWriter/MachO/MachONormalizedFileBinaryWriter.cpp b/lld/lib/ReaderWriter/MachO/MachONormalizedFileBinaryWriter.cpp
index 17b45b9ca827..6f61d307ee9f 100644
--- a/lld/lib/ReaderWriter/MachO/MachONormalizedFileBinaryWriter.cpp
+++ b/lld/lib/ReaderWriter/MachO/MachONormalizedFileBinaryWriter.cpp
@@ -1294,10 +1294,10 @@ void TrieNode::addSymbol(const Export& entry,
     }
   }
   if (entry.flags & EXPORT_SYMBOL_FLAGS_REEXPORT) {
-    assert(entry.otherOffset != 0);
+    assert(entry.otherOffset != uint32_t(0u));
   }
   if (entry.flags & EXPORT_SYMBOL_FLAGS_STUB_AND_RESOLVER) {
-    assert(entry.otherOffset != 0);
+    assert(entry.otherOffset != uint32_t(0u));
   }
   // No commonality with any existing child, make a new edge.
   auto *newNode = new (allocator) TrieNode(entry.name.copy(allocator));
diff --git a/llvm/CMakeLists.txt b/llvm/CMakeLists.txt
index f07f29a5378b..2e8c8f282fe7 100644
--- a/llvm/CMakeLists.txt
+++ b/llvm/CMakeLists.txt
@@ -45,7 +45,7 @@ project(LLVM
   VERSION ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}
   LANGUAGES C CXX ASM)
 
-set(CMAKE_CXX_STANDARD 14 CACHE STRING "C++ standard to conform to")
+set(CMAKE_CXX_STANDARD 20 CACHE STRING "C++ standard to conform to")
 set(CMAKE_CXX_STANDARD_REQUIRED YES)
 if (CYGWIN)
   # Cygwin is a bit stricter and lack things like 'strdup', 'stricmp', etc in
@@ -334,7 +334,8 @@ set(LLVM_ALL_TARGETS
   )
 
 # List of targets with JIT support:
-set(LLVM_TARGETS_WITH_JIT X86 PowerPC AArch64 ARM Mips SystemZ)
+# disable unnecessary jit stuff
+#set(LLVM_TARGETS_WITH_JIT X86 PowerPC AArch64 ARM Mips SystemZ)
 
 set(LLVM_TARGETS_TO_BUILD "all"
     CACHE STRING "Semicolon-separated list of targets to build, or \"all\".")
diff --git a/llvm/include/llvm/ADT/Triple.h b/llvm/include/llvm/ADT/Triple.h
index c848314cd9e9..43ac6044c47d 100644
--- a/llvm/include/llvm/ADT/Triple.h
+++ b/llvm/include/llvm/ADT/Triple.h
@@ -93,6 +93,7 @@ public:
     hsail64,        // AMD HSAIL with 64-bit pointers
     spir,           // SPIR: standard portable IR for OpenCL 32-bit version
     spir64,         // SPIR: standard portable IR for OpenCL 64-bit version
+    air64,          // AIR: Apple IR, used for Metal, always 64-bit
     kalimba,        // Kalimba: generic kalimba
     shave,          // SHAVE: Movidius vector VLIW processors
     lanai,          // Lanai: Lanai 32-bit
@@ -223,6 +224,9 @@ public:
     MuslEABIHF,
     MuslX32,
 
+    Vulkan,
+    FloorHostCompute,
+
     MSVC,
     Itanium,
     Cygnus,
diff --git a/llvm/include/llvm/Analysis/CodeMetrics.h b/llvm/include/llvm/Analysis/CodeMetrics.h
index 615591aa83ad..e6265c61a63b 100644
--- a/llvm/include/llvm/Analysis/CodeMetrics.h
+++ b/llvm/include/llvm/Analysis/CodeMetrics.h
@@ -76,7 +76,8 @@ struct CodeMetrics {
   /// Add information about a block to the current state.
   void analyzeBasicBlock(const BasicBlock *BB, const TargetTransformInfo &TTI,
                          const SmallPtrSetImpl<const Value *> &EphValues,
-                         bool PrepareForLTO = false);
+                         bool PrepareForLTO = false,
+						 const bool allow_duplicate = false);
 
   /// Collect a loop's ephemeral values (those used only by an assume
   /// or similar intrinsics in the loop).
diff --git a/llvm/include/llvm/AsmParser/LLToken.h b/llvm/include/llvm/AsmParser/LLToken.h
index f8ca054863ac..30f51506827f 100644
--- a/llvm/include/llvm/AsmParser/LLToken.h
+++ b/llvm/include/llvm/AsmParser/LLToken.h
@@ -149,8 +149,10 @@ enum Kind {
   kw_avr_signalcc,
   kw_ptx_kernel,
   kw_ptx_device,
-  kw_spir_kernel,
-  kw_spir_func,
+  kw_floor_kernel,
+  kw_floor_vertex,
+  kw_floor_fragment,
+  kw_floor_func,
   kw_x86_64_sysvcc,
   kw_win64cc,
   kw_webkit_jscc,
diff --git a/llvm/include/llvm/Bitcode/BitcodeWriter.h b/llvm/include/llvm/Bitcode/BitcodeWriter.h
index 7ad2d37a2a35..e90a4b956914 100644
--- a/llvm/include/llvm/Bitcode/BitcodeWriter.h
+++ b/llvm/include/llvm/Bitcode/BitcodeWriter.h
@@ -105,6 +105,72 @@ class raw_ostream;
         const std::map<std::string, GVSummaryMapTy> *ModuleToSummariesForIndex);
   };
 
+  class BitcodeWriter50 {
+    SmallVectorImpl<char> &Buffer;
+    std::unique_ptr<BitstreamWriter> Stream;
+
+    StringTableBuilder StrtabBuilder{StringTableBuilder::RAW};
+
+    // Owns any strings created by the irsymtab writer until we create the
+    // string table.
+    BumpPtrAllocator Alloc;
+
+    bool WroteStrtab = false, WroteSymtab = false;
+
+    void writeBlob(unsigned Block, unsigned Record, StringRef Blob);
+
+    std::vector<Module *> Mods;
+
+  public:
+    /// Create a BitcodeWriter50 that writes to Buffer.
+    BitcodeWriter50(SmallVectorImpl<char> &Buffer);
+
+    ~BitcodeWriter50();
+
+    /// Attempt to write a symbol table to the bitcode file. This must be called
+    /// at most once after all modules have been written.
+    ///
+    /// A reader does not require a symbol table to interpret a bitcode file;
+    /// the symbol table is needed only to improve link-time performance. So
+    /// this function may decide not to write a symbol table. It may so decide
+    /// if, for example, the target is unregistered or the IR is malformed.
+    void writeSymtab();
+
+    /// Write the bitcode file's string table. This must be called exactly once
+    /// after all modules and the optional symbol table have been written.
+    void writeStrtab();
+
+    /// Copy the string table for another module into this bitcode file. This
+    /// should be called after copying the module itself into the bitcode file.
+    void copyStrtab(StringRef Strtab);
+
+    /// Write the specified module to the buffer specified at construction time.
+    ///
+    /// If \c ShouldPreserveUseListOrder, encode the use-list order for each \a
+    /// Value in \c M.  These will be reconstructed exactly when \a M is
+    /// deserialized.
+    ///
+    /// If \c Index is supplied, the bitcode will contain the summary index
+    /// (currently for use in ThinLTO optimization).
+    ///
+    /// \p GenerateHash enables hashing the Module and including the hash in the
+    /// bitcode (currently for use in ThinLTO incremental build).
+    ///
+    /// If \p ModHash is non-null, when GenerateHash is true, the resulting
+    /// hash is written into ModHash. When GenerateHash is false, that value
+    /// is used as the hash instead of computing from the generated bitcode.
+    /// Can be used to produce the same module hash for a minimized bitcode
+    /// used just for the thin link as in the regular full bitcode that will
+    /// be used in the backend.
+    void writeModule(const Module *M, bool ShouldPreserveUseListOrder = false,
+                     const ModuleSummaryIndex *Index = nullptr,
+                     bool GenerateHash = false, ModuleHash *ModHash = nullptr);
+
+    void writeIndex(
+        const ModuleSummaryIndex *Index,
+        const std::map<std::string, GVSummaryMapTy> *ModuleToSummariesForIndex);
+  };
+
   /// Write the specified module to the specified raw output stream.
   ///
   /// For streams where it matters, the given stream should be in "binary"
@@ -131,6 +197,16 @@ class raw_ostream;
                           const ModuleSummaryIndex *Index = nullptr,
                           bool GenerateHash = false,
                           ModuleHash *ModHash = nullptr);
+  
+  void WriteBitcode32ToFile(const Module *M, raw_ostream &Out);
+  
+  void WriteBitcode50ToFile(const Module *M, raw_ostream &Out,
+                            bool ShouldPreserveUseListOrder = false,
+                            const ModuleSummaryIndex *Index = nullptr,
+                            bool GenerateHash = false,
+                            ModuleHash *ModHash = nullptr);
+  
+  void WriteMetalLibToFile(Module &M, raw_ostream &OS);
 
   /// Write the specified thin link bitcode file (i.e., the minimized bitcode
   /// file) to the given raw output stream, where it will be written in a new
@@ -152,6 +228,10 @@ class raw_ostream;
                         const std::map<std::string, GVSummaryMapTy>
                             *ModuleToSummariesForIndex = nullptr);
 
+  void WriteIndex50ToFile(const ModuleSummaryIndex &Index, raw_ostream &Out,
+                          const std::map<std::string, GVSummaryMapTy>
+                              *ModuleToSummariesForIndex = nullptr);
+
   /// If EmbedBitcode is set, save a copy of the llvm IR as data in the
   ///  __LLVM,__bitcode section (.llvmbc on non-MacOS).
   /// If available, pass the serialized module via the Buf parameter. If not,
diff --git a/llvm/include/llvm/Bitcode/BitcodeWriterPass.h b/llvm/include/llvm/Bitcode/BitcodeWriterPass.h
index dda5b20973c1..10a210534825 100644
--- a/llvm/include/llvm/Bitcode/BitcodeWriterPass.h
+++ b/llvm/include/llvm/Bitcode/BitcodeWriterPass.h
@@ -40,6 +40,15 @@ ModulePass *createBitcodeWriterPass(raw_ostream &Str,
                                     bool EmitSummaryIndex = false,
                                     bool EmitModuleHash = false);
 
+ModulePass *createBitcode32WriterPass(raw_ostream &Str);
+
+ModulePass *createBitcode50WriterPass(raw_ostream &Str,
+                                      bool ShouldPreserveUseListOrder = false,
+                                      bool EmitSummaryIndex = false,
+                                      bool EmitModuleHash = false);
+
+ModulePass *createMetalLibWriterPass(raw_ostream &Str);
+
 /// Check whether a pass is a BitcodeWriterPass.
 bool isBitcodeWriterPass(Pass *P);
 
@@ -75,6 +84,60 @@ public:
   static bool isRequired() { return true; }
 };
 
+class Bitcode32WriterPass : public PassInfoMixin<Bitcode32WriterPass> {
+  raw_ostream &OS;
+
+public:
+  /// \brief Construct a bitcode writer pass around a particular output stream.
+  explicit Bitcode32WriterPass(raw_ostream &OS) : OS(OS) {}
+
+  /// \brief Run the bitcode writer pass, and output the module to the selected
+  /// output stream.
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &);
+
+  static StringRef name() { return "Bitcode32WriterPass"; }
+};
+
+class BitcodeWriterPass50 : public PassInfoMixin<BitcodeWriterPass50> {
+  raw_ostream &OS;
+  bool ShouldPreserveUseListOrder;
+  bool EmitSummaryIndex;
+  bool EmitModuleHash;
+
+public:
+  /// Construct a bitcode writer pass around a particular output stream.
+  ///
+  /// If \c ShouldPreserveUseListOrder, encode use-list order so it can be
+  /// reproduced when deserialized.
+  ///
+  /// If \c EmitSummaryIndex, emit the summary index (currently
+  /// for use in ThinLTO optimization).
+  explicit BitcodeWriterPass50(raw_ostream &OS,
+                               bool ShouldPreserveUseListOrder = false,
+                               bool EmitSummaryIndex = false,
+                               bool EmitModuleHash = false)
+      : OS(OS), ShouldPreserveUseListOrder(ShouldPreserveUseListOrder),
+  EmitSummaryIndex(EmitSummaryIndex), EmitModuleHash(EmitModuleHash) {}
+
+  /// Run the bitcode writer pass, and output the module to the selected
+  /// output stream.
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &);
+};
+
+class MetalLibWriterPass : public PassInfoMixin<MetalLibWriterPass> {
+  raw_ostream &OS;
+
+public:
+  /// \brief Construct a bitcode writer pass around a particular output stream.
+  explicit MetalLibWriterPass(raw_ostream &OS) : OS(OS) {}
+
+  /// \brief Run the bitcode writer pass, and output the module to the selected
+  /// output stream.
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &);
+
+  static StringRef name() { return "MetalLibWriterPass"; }
+};
+
 }
 
 #endif
diff --git a/llvm/include/llvm/Bitcode/LLVMBitCodes.h b/llvm/include/llvm/Bitcode/LLVMBitCodes.h
index 04eb2739cbd5..4fa2a51c8f2e 100644
--- a/llvm/include/llvm/Bitcode/LLVMBitCodes.h
+++ b/llvm/include/llvm/Bitcode/LLVMBitCodes.h
@@ -19,6 +19,42 @@
 
 #include "llvm/Bitstream/BitCodes.h"
 
+// error when using > 3.2, > 3.5 or > 5.0 enums with 3.2, 3.5 or 5.0 resp.
+#if defined(__clang__) && (defined(LLVM_BITCODE_32) || defined(LLVM_BITCODE_35) || defined(LLVM_BITCODE_50))
+#if defined(LLVM_BITCODE_32)
+#define BC35 __attribute__((unavailable))
+#else
+#define BC35
+#endif
+
+#if defined(LLVM_BITCODE_32) || defined(LLVM_BITCODE_35)
+#define BC38 __attribute__((unavailable))
+#define BC40 __attribute__((unavailable))
+#define BC50 __attribute__((unavailable))
+#else
+#define BC38
+#define BC40
+#define BC50
+#endif
+
+#define BC80 __attribute__((unavailable))
+#define BC130 __attribute__((unavailable))
+#define BC140 __attribute__((unavailable))
+#define DONT_USE __attribute__((unavailable))
+
+#else
+
+#define BC35
+#define BC38
+#define BC40
+#define BC50
+#define BC80
+#define BC130
+#define BC140
+#define DONT_USE
+
+#endif
+
 namespace llvm {
 namespace bitc {
 // The only top-level block types are MODULE, IDENTIFICATION, STRTAB and SYMTAB.
@@ -28,7 +64,7 @@ enum BlockIDs {
 
   // Module sub-block id's.
   PARAMATTR_BLOCK_ID,
-  PARAMATTR_GROUP_BLOCK_ID,
+  PARAMATTR_GROUP_BLOCK_ID BC35,
 
   CONSTANTS_BLOCK_ID,
   FUNCTION_BLOCK_ID,
@@ -36,7 +72,7 @@ enum BlockIDs {
   // Block intended to contains information on the bitcode versioning.
   // Can be used to provide better error messages when we fail to parse a
   // bitcode file.
-  IDENTIFICATION_BLOCK_ID,
+  IDENTIFICATION_BLOCK_ID BC38,
 
   VALUE_SYMTAB_BLOCK_ID,
   METADATA_BLOCK_ID,
@@ -46,27 +82,27 @@ enum BlockIDs {
 
   USELIST_BLOCK_ID,
 
-  MODULE_STRTAB_BLOCK_ID,
-  GLOBALVAL_SUMMARY_BLOCK_ID,
+  MODULE_STRTAB_BLOCK_ID BC38,
+  GLOBALVAL_SUMMARY_BLOCK_ID BC38,
 
-  OPERAND_BUNDLE_TAGS_BLOCK_ID,
+  OPERAND_BUNDLE_TAGS_BLOCK_ID BC38,
 
-  METADATA_KIND_BLOCK_ID,
+  METADATA_KIND_BLOCK_ID BC38,
 
-  STRTAB_BLOCK_ID,
+  STRTAB_BLOCK_ID BC50,
 
-  FULL_LTO_GLOBALVAL_SUMMARY_BLOCK_ID,
+  FULL_LTO_GLOBALVAL_SUMMARY_BLOCK_ID BC50,
 
-  SYMTAB_BLOCK_ID,
+  SYMTAB_BLOCK_ID BC50,
 
-  SYNC_SCOPE_NAMES_BLOCK_ID,
+  SYNC_SCOPE_NAMES_BLOCK_ID BC50,
 };
 
 /// Identification block contains a string that describes the producer details,
 /// and an epoch that defines the auto-upgrade capability.
 enum IdentificationCodes {
-  IDENTIFICATION_CODE_STRING = 1, // IDENTIFICATION:      [strchr x N]
-  IDENTIFICATION_CODE_EPOCH = 2,  // EPOCH:               [epoch#]
+  IDENTIFICATION_CODE_STRING BC38 = 1, // IDENTIFICATION:      [strchr x N]
+  IDENTIFICATION_CODE_EPOCH BC38 = 2,  // EPOCH:               [epoch#]
 };
 
 /// The epoch that defines the auto-upgrade compatibility for the bitcode.
@@ -75,7 +111,7 @@ enum IdentificationCodes {
 /// generated by previous minor releases. We translate this by making the reader
 /// accepting only bitcode with the same epoch, except for the X.0 release which
 /// also accepts N-1.
-enum { BITCODE_CURRENT_EPOCH = 0 };
+enum { BITCODE_CURRENT_EPOCH BC38 = 0 };
 
 /// MODULE blocks have a number of optional fields and subblocks.
 enum ModuleCodes {
@@ -99,24 +135,27 @@ enum ModuleCodes {
   // ALIAS: [alias type, aliasee val#, linkage, visibility]
   MODULE_CODE_ALIAS_OLD = 9,
 
+  // MODULE_CODE_PURGEVALS: [numvals]
+  MODULE_CODE_PURGEVALS_OLD = 10,
+
   MODULE_CODE_GCNAME = 11, // GCNAME: [strchr x N]
-  MODULE_CODE_COMDAT = 12, // COMDAT: [selection_kind, name]
+  MODULE_CODE_COMDAT BC35 = 12, // COMDAT: [selection_kind, name]
 
-  MODULE_CODE_VSTOFFSET = 13, // VSTOFFSET: [offset]
+  MODULE_CODE_VSTOFFSET BC38 = 13, // VSTOFFSET: [offset]
 
   // ALIAS: [alias value type, addrspace, aliasee val#, linkage, visibility]
-  MODULE_CODE_ALIAS = 14,
+  MODULE_CODE_ALIAS BC38 = 14,
 
-  MODULE_CODE_METADATA_VALUES_UNUSED = 15,
+  MODULE_CODE_METADATA_VALUES_UNUSED BC38 = 15,
 
   // SOURCE_FILENAME: [namechar x N]
-  MODULE_CODE_SOURCE_FILENAME = 16,
+  MODULE_CODE_SOURCE_FILENAME BC40 = 16,
 
   // HASH: [5*i32]
-  MODULE_CODE_HASH = 17,
+  MODULE_CODE_HASH BC40 = 17,
 
   // IFUNC: [ifunc value type, addrspace, resolver val#, linkage, visibility]
-  MODULE_CODE_IFUNC = 18,
+  MODULE_CODE_IFUNC BC40 = 18,
 };
 
 /// PARAMATTR blocks have code for defining a parameter attribute set.
@@ -124,8 +163,8 @@ enum AttributeCodes {
   // Deprecated, but still needed to read old bitcode files.
   PARAMATTR_CODE_ENTRY_OLD = 1, // ENTRY: [paramidx0, attr0,
                                 //         paramidx1, attr1...]
-  PARAMATTR_CODE_ENTRY = 2,     // ENTRY: [attrgrp0, attrgrp1, ...]
-  PARAMATTR_GRP_CODE_ENTRY = 3  // ENTRY: [grpid, idx, attr0, attr1, ...]
+  PARAMATTR_CODE_ENTRY BC35 = 2,     // ENTRY: [attrgrp0, attrgrp1, ...]
+  PARAMATTR_GRP_CODE_ENTRY BC35 = 3  // ENTRY: [grpid, idx, attr0, attr1, ...]
 };
 
 /// TYPE blocks have codes for each type primitive they use.
@@ -166,35 +205,40 @@ enum TypeCodes {
 
   TYPE_CODE_FUNCTION = 21, // FUNCTION: [vararg, retty, paramty x N]
 
-  TYPE_CODE_TOKEN = 22, // TOKEN
+  TYPE_CODE_TOKEN BC38 = 22, // TOKEN
 
-  TYPE_CODE_BFLOAT = 23,  // BRAIN FLOATING POINT
-  TYPE_CODE_X86_AMX = 24, // X86 AMX
+  TYPE_CODE_BFLOAT BC130 = 23,  // BRAIN FLOATING POINT
+  TYPE_CODE_X86_AMX BC130 = 24, // X86 AMX
 
-  TYPE_CODE_OPAQUE_POINTER = 25, // OPAQUE_POINTER: [addrspace]
+  TYPE_CODE_OPAQUE_POINTER BC130 = 25, // OPAQUE_POINTER: [addrspace]
 };
 
 enum OperandBundleTagCode {
-  OPERAND_BUNDLE_TAG = 1, // TAG: [strchr x N]
+  OPERAND_BUNDLE_TAG BC38 = 1, // TAG: [strchr x N]
+};
+
+// The type symbol table only has one code (TST_ENTRY_CODE).
+enum TypeSymtabCodesOld {
+  TST_CODE_ENTRY_OLD = 1 // TST_ENTRY: [typeid, namechar x N]
 };
 
 enum SyncScopeNameCode {
-  SYNC_SCOPE_NAME = 1,
+  SYNC_SCOPE_NAME BC50 = 1,
 };
 
 // Value symbol table codes.
 enum ValueSymtabCodes {
   VST_CODE_ENTRY = 1,   // VST_ENTRY: [valueid, namechar x N]
   VST_CODE_BBENTRY = 2, // VST_BBENTRY: [bbid, namechar x N]
-  VST_CODE_FNENTRY = 3, // VST_FNENTRY: [valueid, offset, namechar x N]
+  VST_CODE_FNENTRY BC38 = 3, // VST_FNENTRY: [valueid, offset, namechar x N]
   // VST_COMBINED_ENTRY: [valueid, refguid]
-  VST_CODE_COMBINED_ENTRY = 5
+  VST_CODE_COMBINED_ENTRY BC38 = 5
 };
 
 // The module path symbol table only has one code (MST_CODE_ENTRY).
 enum ModulePathSymtabCodes {
-  MST_CODE_ENTRY = 1, // MST_ENTRY: [modid, namechar x N]
-  MST_CODE_HASH = 2,  // MST_HASH:  [5*i32]
+  MST_CODE_ENTRY BC38 = 1, // MST_ENTRY: [modid, namechar x N]
+  MST_CODE_HASH BC40 = 2,  // MST_HASH:  [5*i32]
 };
 
 // The summary section uses different codes in the per-module
@@ -202,77 +246,77 @@ enum ModulePathSymtabCodes {
 enum GlobalValueSummarySymtabCodes {
   // PERMODULE: [valueid, flags, instcount, numrefs, numrefs x valueid,
   //             n x (valueid)]
-  FS_PERMODULE = 1,
+  FS_PERMODULE BC40 = 1,
   // PERMODULE_PROFILE: [valueid, flags, instcount, numrefs,
   //                     numrefs x valueid,
   //                     n x (valueid, hotness)]
-  FS_PERMODULE_PROFILE = 2,
+  FS_PERMODULE_PROFILE BC40 = 2,
   // PERMODULE_GLOBALVAR_INIT_REFS: [valueid, flags, n x valueid]
-  FS_PERMODULE_GLOBALVAR_INIT_REFS = 3,
+  FS_PERMODULE_GLOBALVAR_INIT_REFS BC40 = 3,
   // COMBINED: [valueid, modid, flags, instcount, numrefs, numrefs x valueid,
   //            n x (valueid)]
-  FS_COMBINED = 4,
+  FS_COMBINED BC40 = 4,
   // COMBINED_PROFILE: [valueid, modid, flags, instcount, numrefs,
   //                    numrefs x valueid,
   //                    n x (valueid, hotness)]
-  FS_COMBINED_PROFILE = 5,
+  FS_COMBINED_PROFILE BC40 = 5,
   // COMBINED_GLOBALVAR_INIT_REFS: [valueid, modid, flags, n x valueid]
-  FS_COMBINED_GLOBALVAR_INIT_REFS = 6,
+  FS_COMBINED_GLOBALVAR_INIT_REFS BC40 = 6,
   // ALIAS: [valueid, flags, valueid]
-  FS_ALIAS = 7,
+  FS_ALIAS BC40 = 7,
   // COMBINED_ALIAS: [valueid, modid, flags, valueid]
-  FS_COMBINED_ALIAS = 8,
+  FS_COMBINED_ALIAS BC40 = 8,
   // COMBINED_ORIGINAL_NAME: [original_name_hash]
-  FS_COMBINED_ORIGINAL_NAME = 9,
+  FS_COMBINED_ORIGINAL_NAME BC40 = 9,
   // VERSION of the summary, bumped when adding flags for instance.
-  FS_VERSION = 10,
+  FS_VERSION BC40 = 10,
   // The list of llvm.type.test type identifiers used by the following function
   // that are used other than by an llvm.assume.
   // [n x typeid]
-  FS_TYPE_TESTS = 11,
+  FS_TYPE_TESTS BC50 = 11,
   // The list of virtual calls made by this function using
   // llvm.assume(llvm.type.test) intrinsics that do not have all constant
   // integer arguments.
   // [n x (typeid, offset)]
-  FS_TYPE_TEST_ASSUME_VCALLS = 12,
+  FS_TYPE_TEST_ASSUME_VCALLS BC50 = 12,
   // The list of virtual calls made by this function using
   // llvm.type.checked.load intrinsics that do not have all constant integer
   // arguments.
   // [n x (typeid, offset)]
-  FS_TYPE_CHECKED_LOAD_VCALLS = 13,
+  FS_TYPE_CHECKED_LOAD_VCALLS BC50 = 13,
   // Identifies a virtual call made by this function using an
   // llvm.assume(llvm.type.test) intrinsic with all constant integer arguments.
   // [typeid, offset, n x arg]
-  FS_TYPE_TEST_ASSUME_CONST_VCALL = 14,
+  FS_TYPE_TEST_ASSUME_CONST_VCALL BC50 = 14,
   // Identifies a virtual call made by this function using an
   // llvm.type.checked.load intrinsic with all constant integer arguments.
   // [typeid, offset, n x arg]
-  FS_TYPE_CHECKED_LOAD_CONST_VCALL = 15,
+  FS_TYPE_CHECKED_LOAD_CONST_VCALL BC50 = 15,
   // Assigns a GUID to a value ID. This normally appears only in combined
   // summaries, but it can also appear in per-module summaries for PGO data.
   // [valueid, guid]
-  FS_VALUE_GUID = 16,
+  FS_VALUE_GUID BC50 = 16,
   // The list of local functions with CFI jump tables. Function names are
   // strings in strtab.
   // [n * name]
-  FS_CFI_FUNCTION_DEFS = 17,
+  FS_CFI_FUNCTION_DEFS BC50 = 17,
   // The list of external functions with CFI jump tables. Function names are
   // strings in strtab.
   // [n * name]
-  FS_CFI_FUNCTION_DECLS = 18,
+  FS_CFI_FUNCTION_DECLS BC50 = 18,
   // Per-module summary that also adds relative block frequency to callee info.
   // PERMODULE_RELBF: [valueid, flags, instcount, numrefs,
   //                   numrefs x valueid,
   //                   n x (valueid, relblockfreq)]
-  FS_PERMODULE_RELBF = 19,
+  FS_PERMODULE_RELBF BC80 = 19,
   // Index-wide flags
-  FS_FLAGS = 20,
+  FS_FLAGS BC80 = 20,
   // Maps type identifier to summary information for that type identifier.
   // Produced by the thin link (only lives in combined index).
   // TYPE_ID: [typeid, kind, bitwidth, align, size, bitmask, inlinebits,
   //           n x (typeid, kind, name, numrba,
   //                numrba x (numarg, numarg x arg, kind, info, byte, bit))]
-  FS_TYPE_ID = 21,
+  FS_TYPE_ID BC80 = 21,
   // For background see overview at https://llvm.org/docs/TypeMetadata.html.
   // The type metadata includes both the type identifier and the offset of
   // the address point of the type (the address held by objects of that type
@@ -286,67 +330,67 @@ enum GlobalValueSummarySymtabCodes {
   // Exists in the per-module summary to provide information to thin link
   // for index-based whole program devirtualization.
   // TYPE_ID_METADATA: [typeid, n x (valueid, offset)]
-  FS_TYPE_ID_METADATA = 22,
+  FS_TYPE_ID_METADATA BC130 = 22,
   // Summarizes vtable definition for use in index-based whole program
   // devirtualization during the thin link.
   // PERMODULE_VTABLE_GLOBALVAR_INIT_REFS: [valueid, flags, varflags,
   //                                        numrefs, numrefs x valueid,
   //                                        n x (valueid, offset)]
-  FS_PERMODULE_VTABLE_GLOBALVAR_INIT_REFS = 23,
+  FS_PERMODULE_VTABLE_GLOBALVAR_INIT_REFS BC130 = 23,
   // The total number of basic blocks in the module.
-  FS_BLOCK_COUNT = 24,
+  FS_BLOCK_COUNT BC130 = 24,
   // Range information for accessed offsets for every argument.
   // [n x (paramno, range, numcalls, numcalls x (callee_guid, paramno, range))]
-  FS_PARAM_ACCESS = 25,
+  FS_PARAM_ACCESS BC130 = 25,
 };
 
 enum MetadataCodes {
   METADATA_STRING_OLD = 1,     // MDSTRING:      [values]
-  METADATA_VALUE = 2,          // VALUE:         [type num, value num]
-  METADATA_NODE = 3,           // NODE:          [n x md num]
+  METADATA_VALUE BC38 = 2,          // VALUE:         [type num, value num]
+  METADATA_NODE BC38 = 3,           // NODE:          [n x md num]
   METADATA_NAME = 4,           // STRING:        [values]
-  METADATA_DISTINCT_NODE = 5,  // DISTINCT_NODE: [n x md num]
+  METADATA_DISTINCT_NODE BC38 = 5,  // DISTINCT_NODE: [n x md num]
   METADATA_KIND = 6,           // [n x [id, name]]
-  METADATA_LOCATION = 7,       // [distinct, line, col, scope, inlined-at?]
+  METADATA_LOCATION BC38 = 7,       // [distinct, line, col, scope, inlined-at?]
   METADATA_OLD_NODE = 8,       // OLD_NODE:      [n x (type num, value num)]
   METADATA_OLD_FN_NODE = 9,    // OLD_FN_NODE:   [n x (type num, value num)]
   METADATA_NAMED_NODE = 10,    // NAMED_NODE:    [n x mdnodes]
   METADATA_ATTACHMENT = 11,    // [m x [value, [n x [id, mdnode]]]
-  METADATA_GENERIC_DEBUG = 12, // [distinct, tag, vers, header, n x md num]
-  METADATA_SUBRANGE = 13,      // [distinct, count, lo]
-  METADATA_ENUMERATOR = 14,    // [isUnsigned|distinct, value, name]
-  METADATA_BASIC_TYPE = 15,    // [distinct, tag, name, size, align, enc]
-  METADATA_FILE = 16, // [distinct, filename, directory, checksumkind, checksum]
-  METADATA_DERIVED_TYPE = 17,       // [distinct, ...]
-  METADATA_COMPOSITE_TYPE = 18,     // [distinct, ...]
-  METADATA_SUBROUTINE_TYPE = 19,    // [distinct, flags, types, cc]
-  METADATA_COMPILE_UNIT = 20,       // [distinct, ...]
-  METADATA_SUBPROGRAM = 21,         // [distinct, ...]
-  METADATA_LEXICAL_BLOCK = 22,      // [distinct, scope, file, line, column]
-  METADATA_LEXICAL_BLOCK_FILE = 23, //[distinct, scope, file, discriminator]
-  METADATA_NAMESPACE = 24, // [distinct, scope, file, name, line, exportSymbols]
-  METADATA_TEMPLATE_TYPE = 25,   // [distinct, scope, name, type, ...]
-  METADATA_TEMPLATE_VALUE = 26,  // [distinct, scope, name, type, value, ...]
-  METADATA_GLOBAL_VAR = 27,      // [distinct, ...]
-  METADATA_LOCAL_VAR = 28,       // [distinct, ...]
-  METADATA_EXPRESSION = 29,      // [distinct, n x element]
-  METADATA_OBJC_PROPERTY = 30,   // [distinct, name, file, line, ...]
-  METADATA_IMPORTED_ENTITY = 31, // [distinct, tag, scope, entity, line, name]
-  METADATA_MODULE = 32,          // [distinct, scope, name, ...]
-  METADATA_MACRO = 33,           // [distinct, macinfo, line, name, value]
-  METADATA_MACRO_FILE = 34,      // [distinct, macinfo, line, file, ...]
-  METADATA_STRINGS = 35,         // [count, offset] blob([lengths][chars])
-  METADATA_GLOBAL_DECL_ATTACHMENT = 36, // [valueid, n x [id, mdnode]]
-  METADATA_GLOBAL_VAR_EXPR = 37,        // [distinct, var, expr]
-  METADATA_INDEX_OFFSET = 38,           // [offset]
-  METADATA_INDEX = 39,                  // [bitpos]
-  METADATA_LABEL = 40,                  // [distinct, scope, name, file, line]
-  METADATA_STRING_TYPE = 41,            // [distinct, name, size, align,...]
+  METADATA_GENERIC_DEBUG BC38 = 12, // [distinct, tag, vers, header, n x md num]
+  METADATA_SUBRANGE BC38 = 13,      // [distinct, count, lo]
+  METADATA_ENUMERATOR BC38 = 14,    // [isUnsigned|distinct, value, name]
+  METADATA_BASIC_TYPE BC38 = 15,    // [distinct, tag, name, size, align, enc]
+  METADATA_FILE BC38 = 16, // [distinct, filename, directory, checksumkind, checksum]
+  METADATA_DERIVED_TYPE BC38 = 17,       // [distinct, ...]
+  METADATA_COMPOSITE_TYPE BC38 = 18,     // [distinct, ...]
+  METADATA_SUBROUTINE_TYPE BC38 = 19,    // [distinct, flags, types, cc]
+  METADATA_COMPILE_UNIT BC38 = 20,       // [distinct, ...]
+  METADATA_SUBPROGRAM BC38 = 21,         // [distinct, ...]
+  METADATA_LEXICAL_BLOCK BC38 = 22,      // [distinct, scope, file, line, column]
+  METADATA_LEXICAL_BLOCK_FILE BC38 = 23, //[distinct, scope, file, discriminator]
+  METADATA_NAMESPACE BC38 = 24, // [distinct, scope, file, name, line, exportSymbols]
+  METADATA_TEMPLATE_TYPE BC38 = 25,   // [distinct, scope, name, type, ...]
+  METADATA_TEMPLATE_VALUE BC38 = 26,  // [distinct, scope, name, type, value, ...]
+  METADATA_GLOBAL_VAR BC38 = 27,      // [distinct, ...]
+  METADATA_LOCAL_VAR BC38 = 28,       // [distinct, ...]
+  METADATA_EXPRESSION BC38 = 29,      // [distinct, n x element]
+  METADATA_OBJC_PROPERTY BC38 = 30,   // [distinct, name, file, line, ...]
+  METADATA_IMPORTED_ENTITY BC38 = 31, // [distinct, tag, scope, entity, line, name]
+  METADATA_MODULE BC38 = 32,          // [distinct, scope, name, ...]
+  METADATA_MACRO BC38 = 33,           // [distinct, macinfo, line, name, value]
+  METADATA_MACRO_FILE BC38 = 34,      // [distinct, macinfo, line, file, ...]
+  METADATA_STRINGS BC40 = 35,         // [count, offset] blob([lengths][chars])
+  METADATA_GLOBAL_DECL_ATTACHMENT BC40 = 36, // [valueid, n x [id, mdnode]]
+  METADATA_GLOBAL_VAR_EXPR BC50 = 37,        // [distinct, var, expr]
+  METADATA_INDEX_OFFSET BC50 = 38,           // [offset]
+  METADATA_INDEX BC50 = 39,                  // [bitpos]
+  METADATA_LABEL BC80 = 40,                  // [distinct, scope, name, file, line]
+  METADATA_STRING_TYPE BC130 = 41,            // [distinct, name, size, align,...]
   // Codes 42 and 43 are reserved for support for Fortran array specific debug
   // info.
-  METADATA_COMMON_BLOCK = 44,     // [distinct, scope, name, variable,...]
-  METADATA_GENERIC_SUBRANGE = 45, // [distinct, count, lo, up, stride]
-  METADATA_ARG_LIST = 46          // [n x [type num, value num]]
+  METADATA_COMMON_BLOCK BC130 = 44,     // [distinct, scope, name, variable,...]
+  METADATA_GENERIC_SUBRANGE BC130 = 45, // [distinct, count, lo, up, stride]
+  METADATA_ARG_LIST BC130 = 46          // [n x [type num, value num]]
 };
 
 // The constants block (CONSTANTS_BLOCK_ID) describes emission for each
@@ -377,11 +421,11 @@ enum ConstantsCodes {
   CST_CODE_DATA = 22,            // DATA:          [n x elements]
   CST_CODE_INLINEASM_OLD2 = 23,  // INLINEASM:     [sideeffect|alignstack|
                                  //                 asmdialect,asmstr,conststr]
-  CST_CODE_CE_GEP_WITH_INRANGE_INDEX = 24, //      [opty, flags, n x operands]
-  CST_CODE_CE_UNOP = 25,                   // CE_UNOP:      [opcode, opval]
-  CST_CODE_POISON = 26,                    // POISON
-  CST_CODE_DSO_LOCAL_EQUIVALENT = 27,      // DSO_LOCAL_EQUIVALENT [gvty, gv]
-  CST_CODE_INLINEASM = 28, // INLINEASM:     [sideeffect|alignstack|
+  CST_CODE_CE_GEP_WITH_INRANGE_INDEX BC50 = 24, //      [opty, flags, n x operands]
+  CST_CODE_CE_UNOP BC80 = 25,                   // CE_UNOP:      [opcode, opval]
+  CST_CODE_POISON BC130 = 26,                    // POISON
+  CST_CODE_DSO_LOCAL_EQUIVALENT BC130 = 27,      // DSO_LOCAL_EQUIVALENT [gvty, gv]
+  CST_CODE_INLINEASM BC130 = 28, // INLINEASM:     [sideeffect|alignstack|
                            //                 asmdialect|unwind,
                            //                 asmstr,conststr]
 };
@@ -403,7 +447,7 @@ enum CastOpcodes {
   CAST_PTRTOINT = 9,
   CAST_INTTOPTR = 10,
   CAST_BITCAST = 11,
-  CAST_ADDRSPACECAST = 12
+  CAST_ADDRSPACECAST BC35 = 12
 };
 
 /// UnaryOpcodes - These are values used in the bitcode files to encode which
@@ -449,8 +493,8 @@ enum RMWOperations {
   RMW_MIN = 8,
   RMW_UMAX = 9,
   RMW_UMIN = 10,
-  RMW_FADD = 11,
-  RMW_FSUB = 12
+  RMW_FADD BC130 = 11,
+  RMW_FSUB BC130 = 12
 };
 
 /// OverflowingBinaryOperatorOptionalFlags - Flags for serializing
@@ -464,14 +508,14 @@ enum OverflowingBinaryOperatorOptionalFlags {
 /// This is a fixed layout derived from the bitcode emitted by LLVM 5.0
 /// intended to decouple the in-memory representation from the serialization.
 enum FastMathMap {
-  UnsafeAlgebra   = (1 << 0), // Legacy
-  NoNaNs          = (1 << 1),
-  NoInfs          = (1 << 2),
-  NoSignedZeros   = (1 << 3),
-  AllowReciprocal = (1 << 4),
-  AllowContract   = (1 << 5),
-  ApproxFunc      = (1 << 6),
-  AllowReassoc    = (1 << 7)
+  UnsafeAlgebra   BC50 = (1 << 0), // Legacy
+  NoNaNs          BC50 = (1 << 1),
+  NoInfs          BC50 = (1 << 2),
+  NoSignedZeros   BC50 = (1 << 3),
+  AllowReciprocal BC50 = (1 << 4),
+  AllowContract   BC50 = (1 << 5),
+  ApproxFunc      BC50 = (1 << 6),
+  AllowReassoc    BC50 = (1 << 7)
 };
 
 /// PossiblyExactOperatorOptionalFlags - Flags for serializing
@@ -489,14 +533,20 @@ enum AtomicOrderingCodes {
   ORDERING_SEQCST = 6
 };
 
+/// Encoded SynchronizationScope values.
+enum AtomicSynchScopeCodesOld {
+  SYNCHSCOPE_SINGLETHREAD_OLD = 0,
+  SYNCHSCOPE_CROSSTHREAD_OLD = 1
+};
+
 /// Markers and flags for call instruction.
 enum CallMarkersFlags {
   CALL_TAIL = 0,
   CALL_CCONV = 1,
-  CALL_MUSTTAIL = 14,
-  CALL_EXPLICIT_TYPE = 15,
-  CALL_NOTAIL = 16,
-  CALL_FMF = 17 // Call has optional fast-math-flags.
+  CALL_MUSTTAIL BC35 = 14,
+  CALL_EXPLICIT_TYPE BC38 = 15,
+  CALL_NOTAIL BC38 = 16,
+  CALL_FMF BC38 = 17 // Call has optional fast-math-flags.
 };
 
 // The function body block (FUNCTION_BLOCK_ID) describes function bodies.  It
@@ -562,38 +612,38 @@ enum FunctionCodes {
                                   //        ordering, synchscope]
   FUNC_CODE_INST_STOREATOMIC_OLD = 42, // STORE: [ptrty,ptr,val, align, vol
                                        //         ordering, synchscope]
-  FUNC_CODE_INST_GEP = 43,             // GEP:  [inbounds, n x operands]
-  FUNC_CODE_INST_STORE = 44,       // STORE: [ptrty,ptr,valty,val, align, vol]
-  FUNC_CODE_INST_STOREATOMIC = 45, // STORE: [ptrty,ptr,val, align, vol
-  FUNC_CODE_INST_CMPXCHG = 46,     // CMPXCHG: [ptrty, ptr, cmp, val, vol,
+  FUNC_CODE_INST_GEP BC38 = 43,             // GEP:  [inbounds, n x operands]
+  FUNC_CODE_INST_STORE BC38 = 44,       // STORE: [ptrty,ptr,valty,val, align, vol]
+  FUNC_CODE_INST_STOREATOMIC BC38 = 45, // STORE: [ptrty,ptr,val, align, vol
+  FUNC_CODE_INST_CMPXCHG BC38 = 46,     // CMPXCHG: [ptrty, ptr, cmp, val, vol,
                                    //           success_ordering, synchscope,
                                    //           failure_ordering, weak]
-  FUNC_CODE_INST_LANDINGPAD = 47,  // LANDINGPAD: [ty,val,num,id0,val0...]
-  FUNC_CODE_INST_CLEANUPRET = 48,  // CLEANUPRET: [val] or [val,bb#]
-  FUNC_CODE_INST_CATCHRET = 49,    // CATCHRET: [val,bb#]
-  FUNC_CODE_INST_CATCHPAD = 50,    // CATCHPAD: [bb#,bb#,num,args...]
-  FUNC_CODE_INST_CLEANUPPAD = 51,  // CLEANUPPAD: [num,args...]
-  FUNC_CODE_INST_CATCHSWITCH =
+  FUNC_CODE_INST_LANDINGPAD BC38 = 47,  // LANDINGPAD: [ty,val,num,id0,val0...]
+  FUNC_CODE_INST_CLEANUPRET BC38 = 48,  // CLEANUPRET: [val] or [val,bb#]
+  FUNC_CODE_INST_CATCHRET BC38 = 49,    // CATCHRET: [val,bb#]
+  FUNC_CODE_INST_CATCHPAD BC38 = 50,    // CATCHPAD: [bb#,bb#,num,args...]
+  FUNC_CODE_INST_CLEANUPPAD BC38 = 51,  // CLEANUPPAD: [num,args...]
+  FUNC_CODE_INST_CATCHSWITCH BC38 =
       52, // CATCHSWITCH: [num,args...] or [num,args...,bb]
   // 53 is unused.
   // 54 is unused.
-  FUNC_CODE_OPERAND_BUNDLE = 55, // OPERAND_BUNDLE: [tag#, value...]
-  FUNC_CODE_INST_UNOP = 56,      // UNOP:       [opcode, ty, opval]
-  FUNC_CODE_INST_CALLBR = 57,    // CALLBR:     [attr, cc, norm, transfs,
+  FUNC_CODE_OPERAND_BUNDLE BC40 = 55, // OPERAND_BUNDLE: [tag#, value...]
+  FUNC_CODE_INST_UNOP BC80 = 56,      // UNOP:       [opcode, ty, opval]
+  FUNC_CODE_INST_CALLBR BC130 = 57,    // CALLBR:     [attr, cc, norm, transfs,
                                  //              fnty, fnid, args...]
-  FUNC_CODE_INST_FREEZE = 58,    // FREEZE: [opty, opval]
-  FUNC_CODE_INST_ATOMICRMW = 59, // ATOMICRMW: [ptrty, ptr, valty, val,
+  FUNC_CODE_INST_FREEZE BC130 = 58,    // FREEZE: [opty, opval]
+  FUNC_CODE_INST_ATOMICRMW BC130 = 59, // ATOMICRMW: [ptrty, ptr, valty, val,
                                  //             operation, align, vol,
                                  //             ordering, synchscope]
 };
 
 enum UseListCodes {
   USELIST_CODE_DEFAULT = 1, // DEFAULT: [index..., value-id]
-  USELIST_CODE_BB = 2       // BB: [index..., bb-id]
+  USELIST_CODE_BB BC38 = 2       // BB: [index..., bb-id]
 };
 
 enum AttributeKindCodes {
-  // = 0 is unused
+  ATTR_KIND_INVALID = 0,
   ATTR_KIND_ALIGNMENT = 1,
   ATTR_KIND_ALWAYS_INLINE = 2,
   ATTR_KIND_BY_VAL = 3,
@@ -631,63 +681,64 @@ enum AttributeKindCodes {
   ATTR_KIND_BUILTIN = 35,
   ATTR_KIND_COLD = 36,
   ATTR_KIND_OPTIMIZE_NONE = 37,
-  ATTR_KIND_IN_ALLOCA = 38,
-  ATTR_KIND_NON_NULL = 39,
-  ATTR_KIND_JUMP_TABLE = 40,
-  ATTR_KIND_DEREFERENCEABLE = 41,
-  ATTR_KIND_DEREFERENCEABLE_OR_NULL = 42,
-  ATTR_KIND_CONVERGENT = 43,
-  ATTR_KIND_SAFESTACK = 44,
-  ATTR_KIND_ARGMEMONLY = 45,
-  ATTR_KIND_SWIFT_SELF = 46,
-  ATTR_KIND_SWIFT_ERROR = 47,
-  ATTR_KIND_NO_RECURSE = 48,
-  ATTR_KIND_INACCESSIBLEMEM_ONLY = 49,
-  ATTR_KIND_INACCESSIBLEMEM_OR_ARGMEMONLY = 50,
-  ATTR_KIND_ALLOC_SIZE = 51,
-  ATTR_KIND_WRITEONLY = 52,
-  ATTR_KIND_SPECULATABLE = 53,
-  ATTR_KIND_STRICT_FP = 54,
-  ATTR_KIND_SANITIZE_HWADDRESS = 55,
-  ATTR_KIND_NOCF_CHECK = 56,
-  ATTR_KIND_OPT_FOR_FUZZING = 57,
-  ATTR_KIND_SHADOWCALLSTACK = 58,
-  ATTR_KIND_SPECULATIVE_LOAD_HARDENING = 59,
-  ATTR_KIND_IMMARG = 60,
-  ATTR_KIND_WILLRETURN = 61,
-  ATTR_KIND_NOFREE = 62,
-  ATTR_KIND_NOSYNC = 63,
-  ATTR_KIND_SANITIZE_MEMTAG = 64,
-  ATTR_KIND_PREALLOCATED = 65,
-  ATTR_KIND_NO_MERGE = 66,
-  ATTR_KIND_NULL_POINTER_IS_VALID = 67,
-  ATTR_KIND_NOUNDEF = 68,
-  ATTR_KIND_BYREF = 69,
-  ATTR_KIND_MUSTPROGRESS = 70,
-  ATTR_KIND_NO_CALLBACK = 71,
-  ATTR_KIND_HOT = 72,
-  ATTR_KIND_NO_PROFILE = 73,
-  ATTR_KIND_VSCALE_RANGE = 74,
-  ATTR_KIND_SWIFT_ASYNC = 75,
-  ATTR_KIND_NO_SANITIZE_COVERAGE = 76,
-  ATTR_KIND_ELEMENTTYPE = 77,
-  ATTR_KIND_DISABLE_SANITIZER_INSTRUMENTATION = 78,
+  //! NOTE: 38 - 41 are technically 3.5, but not supported by Metal (pre 2.0 / LLVM 5.0)
+  ATTR_KIND_IN_ALLOCA BC50 = 38,
+  ATTR_KIND_NON_NULL BC50 = 39,
+  ATTR_KIND_JUMP_TABLE BC50 = 40,
+  ATTR_KIND_DEREFERENCEABLE BC50 = 41,
+  ATTR_KIND_DEREFERENCEABLE_OR_NULL BC38 = 42,
+  ATTR_KIND_CONVERGENT BC38 = 43,
+  ATTR_KIND_SAFESTACK BC38 = 44,
+  ATTR_KIND_ARGMEMONLY BC38 = 45,
+  ATTR_KIND_SWIFT_SELF BC38 = 46,
+  ATTR_KIND_SWIFT_ERROR BC38 = 47,
+  ATTR_KIND_NO_RECURSE BC38 = 48,
+  ATTR_KIND_INACCESSIBLEMEM_ONLY BC38 = 49,
+  ATTR_KIND_INACCESSIBLEMEM_OR_ARGMEMONLY BC38 = 50,
+  ATTR_KIND_ALLOC_SIZE BC40 = 51,
+  ATTR_KIND_WRITEONLY BC40 = 52,
+  ATTR_KIND_SPECULATABLE BC50 = 53,
+  ATTR_KIND_STRICT_FP BC80 = 54,
+  ATTR_KIND_SANITIZE_HWADDRESS BC80 = 55,
+  ATTR_KIND_NOCF_CHECK BC80 = 56,
+  ATTR_KIND_OPT_FOR_FUZZING BC80 = 57,
+  ATTR_KIND_SHADOWCALLSTACK BC80 = 58,
+  ATTR_KIND_SPECULATIVE_LOAD_HARDENING BC80 = 59,
+  ATTR_KIND_IMMARG BC130 = 60,
+  ATTR_KIND_WILLRETURN BC130 = 61,
+  ATTR_KIND_NOFREE BC130 = 62,
+  ATTR_KIND_NOSYNC BC130 = 63,
+  ATTR_KIND_SANITIZE_MEMTAG BC130 = 64,
+  ATTR_KIND_PREALLOCATED BC130 = 65,
+  ATTR_KIND_NO_MERGE BC130 = 66,
+  ATTR_KIND_NULL_POINTER_IS_VALID BC130 = 67,
+  ATTR_KIND_NOUNDEF BC130 = 68,
+  ATTR_KIND_BYREF BC130 = 69,
+  ATTR_KIND_MUSTPROGRESS BC130 = 70,
+  ATTR_KIND_NO_CALLBACK BC130 = 71,
+  ATTR_KIND_HOT BC130 = 72,
+  ATTR_KIND_NO_PROFILE BC130 = 73,
+  ATTR_KIND_VSCALE_RANGE BC130 = 74,
+  ATTR_KIND_SWIFT_ASYNC BC130 = 75,
+  ATTR_KIND_NO_SANITIZE_COVERAGE BC130 = 76,
+  ATTR_KIND_ELEMENTTYPE BC130 = 77,
+  ATTR_KIND_DISABLE_SANITIZER_INSTRUMENTATION BC140 = 78,
 };
 
 enum ComdatSelectionKindCodes {
-  COMDAT_SELECTION_KIND_ANY = 1,
-  COMDAT_SELECTION_KIND_EXACT_MATCH = 2,
-  COMDAT_SELECTION_KIND_LARGEST = 3,
-  COMDAT_SELECTION_KIND_NO_DUPLICATES = 4,
-  COMDAT_SELECTION_KIND_SAME_SIZE = 5,
+  COMDAT_SELECTION_KIND_ANY BC35 = 1,
+  COMDAT_SELECTION_KIND_EXACT_MATCH BC35 = 2,
+  COMDAT_SELECTION_KIND_LARGEST BC35 = 3,
+  COMDAT_SELECTION_KIND_NO_DUPLICATES BC35 = 4,
+  COMDAT_SELECTION_KIND_SAME_SIZE BC35 = 5,
 };
 
 enum StrtabCodes {
-  STRTAB_BLOB = 1,
+  STRTAB_BLOB BC50 = 1,
 };
 
 enum SymtabCodes {
-  SYMTAB_BLOB = 1,
+  SYMTAB_BLOB BC50 = 1,
 };
 
 } // End bitc namespace
diff --git a/llvm/include/llvm/CodeGen/AsmPrinter.h b/llvm/include/llvm/CodeGen/AsmPrinter.h
index 87ecf51d4d49..c9747942ffe4 100644
--- a/llvm/include/llvm/CodeGen/AsmPrinter.h
+++ b/llvm/include/llvm/CodeGen/AsmPrinter.h
@@ -431,7 +431,7 @@ public:
   /// global value is specified, and if that global has an explicit alignment
   /// requested, it will override the alignment request if required for
   /// correctness.
-  void emitAlignment(Align Alignment, const GlobalObject *GV = nullptr) const;
+  virtual void emitAlignment(Align Alignment, const GlobalObject *GV = nullptr) const;
 
   /// Lower the specified LLVM Constant to an MCExpr.
   virtual const MCExpr *lowerConstant(const Constant *CV);
diff --git a/llvm/include/llvm/IR/CallingConv.h b/llvm/include/llvm/IR/CallingConv.h
index fd2854246522..9acb59c4a61b 100644
--- a/llvm/include/llvm/IR/CallingConv.h
+++ b/llvm/include/llvm/IR/CallingConv.h
@@ -133,23 +133,20 @@ namespace CallingConv {
     /// Passes all arguments in register or parameter space.
     PTX_Device = 72,
 
-    /// SPIR_FUNC - Calling convention for SPIR non-kernel device functions.
-    /// No lowering or expansion of arguments.
-    /// Structures are passed as a pointer to a struct with the byval attribute.
-    /// Functions can only call SPIR_FUNC and SPIR_KERNEL functions.
-    /// Functions can only have zero or one return values.
-    /// Variable arguments are not allowed, except for printf.
-    /// How arguments/return values are lowered are not specified.
-    /// Functions are only visible to the devices.
-    SPIR_FUNC = 75,
-
-    /// SPIR_KERNEL - Calling convention for SPIR kernel functions.
-    /// Inherits the restrictions of SPIR_FUNC, except
-    /// Cannot have non-void return values.
-    /// Cannot have variable arguments.
-    /// Can also be called by the host.
-    /// Is externally visible.
-    SPIR_KERNEL = 76,
+    /// AIR/Metal and SPIR-V/Vulkan vertex shader function calling convention
+    /// NOTE: for metal this is entirely virtual and will be stripped in the end
+    FLOOR_VERTEX = 73,
+    /// AIR/Metal and SPIR-V/Vulkan fragment shader function calling convention
+    /// NOTE: for metal this is entirely virtual and will be stripped in the end
+    FLOOR_FRAGMENT = 74,
+    /// OpenCL/SPIR/SPIR-V, AIR/Metal, CUDA and SPIR-V/Vulkan normal function calling convention (not an entry point)
+    /// NOTE: for metal this is entirely virtual and will be stripped in the end
+    /// NOTE: used to be SPIR_FUNC, must be 75 for binary compat
+    FLOOR_FUNC = 75,
+    /// OpenCL/SPIR/SPIR-V, AIR/Metal, CUDA and SPIR-V/Vulkan compute kernel function calling convention
+    /// NOTE: for metal this is entirely virtual and will be stripped in the end
+    /// NOTE: used to be SPIR_KERNEL, must be 76 for binary compat
+    FLOOR_KERNEL = 76,
 
     /// Intel_OCL_BI - Calling conventions for Intel OpenCL built-ins
     Intel_OCL_BI = 77,
diff --git a/llvm/include/llvm/IR/DebugInfoMetadata.h b/llvm/include/llvm/IR/DebugInfoMetadata.h
index 7a4152994b98..d6f05689d494 100644
--- a/llvm/include/llvm/IR/DebugInfoMetadata.h
+++ b/llvm/include/llvm/IR/DebugInfoMetadata.h
@@ -214,6 +214,9 @@ public:
       return true;
     }
   }
+
+  // necessary for compat with llvm 3.2
+  MDTuple* contained_node { nullptr };
 };
 
 /// Generic tagged DWARF-like metadata node.
@@ -528,6 +531,7 @@ public:
 /// TODO: Merge with directory/file node (including users).
 /// TODO: Canonicalize paths on creation.
 class DIFile : public DIScope {
+public: // NOTE: made this public for SPIRVReader
   friend class LLVMContextImpl;
   friend class MDNode;
 
@@ -1818,6 +1822,7 @@ public:
 
 /// Subprogram description.
 class DISubprogram : public DILocalScope {
+public: // NOTE: made this public for SPIRVReader
   friend class LLVMContextImpl;
   friend class MDNode;
 
@@ -1869,6 +1874,9 @@ public:
         (IsMainSubprogram ? SPFlagMainSubprogram : SPFlagZero));
   }
 
+  // necessary for llvm 3.2
+  Function* associated_function { nullptr };
+
 private:
   DIFlags Flags;
   DISPFlags SPFlags;
diff --git a/llvm/include/llvm/IR/DerivedTypes.h b/llvm/include/llvm/IR/DerivedTypes.h
index 8a1b26e699e3..a94b3bdb72cd 100644
--- a/llvm/include/llvm/IR/DerivedTypes.h
+++ b/llvm/include/llvm/IR/DerivedTypes.h
@@ -218,7 +218,8 @@ class StructType : public Type {
     SCDB_HasBody = 1,
     SCDB_Packed = 2,
     SCDB_IsLiteral = 4,
-    SCDB_IsSized = 8
+    SCDB_IsSized = 8,
+    SCDB_IsGraphicsIOType = 16
   };
 
   /// For a named struct that actually has a name, this is a pointer to the
@@ -280,6 +281,15 @@ public:
   /// yet. These prints as 'opaque' in .ll files.
   bool isOpaque() const { return (getSubclassData() & SCDB_HasBody) == 0; }
 
+  /// isGraphicsIOType - Return true if this type is used as a graphics vertex/fragment
+  /// shader input/output type.
+  bool isGraphicsIOType() const { return (getSubclassData() & SCDB_IsGraphicsIOType) != 0; }
+
+  /// setGraphicsIOType - Flags this type as a graphics input/output type.
+  void setGraphicsIOType() {
+    setSubclassData(getSubclassData() | SCDB_IsGraphicsIOType);
+  }
+
   /// isSized - Return true if this is a sized type.
   bool isSized(SmallPtrSetImpl<Type *> *Visited = nullptr) const;
 
diff --git a/llvm/include/llvm/IR/Function.h b/llvm/include/llvm/IR/Function.h
index e5c675a64af0..a4d593626aff 100644
--- a/llvm/include/llvm/IR/Function.h
+++ b/llvm/include/llvm/IR/Function.h
@@ -178,6 +178,13 @@ public:
     return cast<FunctionType>(getValueType());
   }
 
+  // function type is separate from value type, so mutateType will no longer change it
+  // -> need a new function to specifically mutate the function type
+  void mutateFunctionType(FunctionType* NewTy) {
+    // NOTE: ValueType is now part of GlobalValue (and there is still a separate Ty in Value)
+    ValueType = NewTy;
+  }
+
   /// Returns the type of the ret val.
   Type *getReturnType() const { return getFunctionType()->getReturnType(); }
 
diff --git a/llvm/include/llvm/IR/InstrTypes.h b/llvm/include/llvm/IR/InstrTypes.h
index a2180a8d4b2a..c1bc836f7a3c 100644
--- a/llvm/include/llvm/IR/InstrTypes.h
+++ b/llvm/include/llvm/IR/InstrTypes.h
@@ -1416,23 +1416,23 @@ public:
   void setCalledOperand(Value *V) { Op<CalledOperandOpEndIdx>() = V; }
 
   /// Sets the function called, including updating the function type.
-  void setCalledFunction(Function *Fn) {
-    setCalledFunction(Fn->getFunctionType(), Fn);
+  void setCalledFunction(Function *Fn, const bool allow_type_change = false) {
+    setCalledFunction(Fn->getFunctionType(), Fn, allow_type_change);
   }
 
   /// Sets the function called, including updating the function type.
-  void setCalledFunction(FunctionCallee Fn) {
-    setCalledFunction(Fn.getFunctionType(), Fn.getCallee());
+  void setCalledFunction(FunctionCallee Fn, const bool allow_type_change = false) {
+    setCalledFunction(Fn.getFunctionType(), Fn.getCallee(), allow_type_change);
   }
 
   /// Sets the function called, including updating to the specified function
   /// type.
-  void setCalledFunction(FunctionType *FTy, Value *Fn) {
+  void setCalledFunction(FunctionType *FTy, Value *Fn, const bool allow_type_change = false) {
     this->FTy = FTy;
     assert(cast<PointerType>(Fn->getType())->isOpaqueOrPointeeTypeMatches(FTy));
     // This function doesn't mutate the return type, only the function
     // type. Seems broken, but I'm just gonna stick an assert in for now.
-    assert(getType() == FTy->getReturnType());
+    assert(allow_type_change || getType() == FTy->getReturnType());
     setCalledOperand(Fn);
   }
 
diff --git a/llvm/include/llvm/IR/Metadata.h b/llvm/include/llvm/IR/Metadata.h
index 26d70b4db2d5..5700e01a431c 100644
--- a/llvm/include/llvm/IR/Metadata.h
+++ b/llvm/include/llvm/IR/Metadata.h
@@ -49,7 +49,9 @@ class raw_ostream;
 class Type;
 
 enum LLVMConstants : uint32_t {
-  DEBUG_METADATA_VERSION = 3 // Current debug info version number.
+  DEBUG_METADATA_VERSION = 3, // Current debug info version number.
+  DEBUG_METADATA_VERSION_32 = 1,
+  IOS_METAL_DEBUG_METADATA_VERSION = 360203
 };
 
 /// Magic number in the value profile metadata showing a target has been
@@ -390,7 +392,7 @@ public:
   }
 
   static void handleDeletion(Value *V);
-  static void handleRAUW(Value *From, Value *To);
+  static void handleRAUW(Value *From, Value *To, const bool AllowASChange = false);
 
 protected:
   /// Handle collisions after \a Value::replaceAllUsesWith().
diff --git a/llvm/include/llvm/IR/Metadata50.def b/llvm/include/llvm/IR/Metadata50.def
new file mode 100644
index 000000000000..03cdcab7dc47
--- /dev/null
+++ b/llvm/include/llvm/IR/Metadata50.def
@@ -0,0 +1,125 @@
+//===- llvm/IR/Metadata.def - Metadata definitions --------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Macros for running through all types of metadata.
+//
+//===----------------------------------------------------------------------===//
+
+#if !(defined HANDLE_METADATA || defined HANDLE_METADATA_LEAF ||               \
+      defined HANDLE_METADATA_BRANCH || defined HANDLE_MDNODE_LEAF ||          \
+      defined HANDLE_MDNODE_LEAF_UNIQUABLE || defined HANDLE_MDNODE_BRANCH ||  \
+      defined HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE ||                      \
+      defined HANDLE_SPECIALIZED_MDNODE_LEAF ||                                \
+      defined HANDLE_SPECIALIZED_MDNODE_BRANCH)
+#error "Missing macro definition of HANDLE_METADATA*"
+#endif
+
+// Handler for all types of metadata.
+#ifndef HANDLE_METADATA
+#define HANDLE_METADATA(CLASS)
+#endif
+
+// Handler for leaf nodes in the class hierarchy.
+#ifndef HANDLE_METADATA_LEAF
+#define HANDLE_METADATA_LEAF(CLASS) HANDLE_METADATA(CLASS)
+#endif
+
+// Handler for non-leaf nodes in the class hierarchy.
+#ifndef HANDLE_METADATA_BRANCH
+#define HANDLE_METADATA_BRANCH(CLASS) HANDLE_METADATA(CLASS)
+#endif
+
+// Handler for specialized and uniquable leaf nodes under MDNode.  Defers to
+// HANDLE_MDNODE_LEAF_UNIQUABLE if it's defined, otherwise to
+// HANDLE_SPECIALIZED_MDNODE_LEAF.
+#ifndef HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE
+#ifdef HANDLE_MDNODE_LEAF_UNIQUABLE
+#define HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(CLASS)                        \
+  HANDLE_MDNODE_LEAF_UNIQUABLE(CLASS)
+#else
+#define HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(CLASS)                        \
+  HANDLE_SPECIALIZED_MDNODE_LEAF(CLASS)
+#endif
+#endif
+
+// Handler for leaf nodes under MDNode.
+#ifndef HANDLE_MDNODE_LEAF_UNIQUABLE
+#define HANDLE_MDNODE_LEAF_UNIQUABLE(CLASS) HANDLE_MDNODE_LEAF(CLASS)
+#endif
+
+// Handler for leaf nodes under MDNode.
+#ifndef HANDLE_MDNODE_LEAF
+#define HANDLE_MDNODE_LEAF(CLASS) HANDLE_METADATA_LEAF(CLASS)
+#endif
+
+// Handler for non-leaf nodes under MDNode.
+#ifndef HANDLE_MDNODE_BRANCH
+#define HANDLE_MDNODE_BRANCH(CLASS) HANDLE_METADATA_BRANCH(CLASS)
+#endif
+
+// Handler for specialized leaf nodes under MDNode.
+#ifndef HANDLE_SPECIALIZED_MDNODE_LEAF
+#define HANDLE_SPECIALIZED_MDNODE_LEAF(CLASS) HANDLE_MDNODE_LEAF(CLASS)
+#endif
+
+// Handler for specialized non-leaf nodes under MDNode.
+#ifndef HANDLE_SPECIALIZED_MDNODE_BRANCH
+#define HANDLE_SPECIALIZED_MDNODE_BRANCH(CLASS) HANDLE_MDNODE_BRANCH(CLASS)
+#endif
+
+HANDLE_METADATA_LEAF(MDString)
+HANDLE_METADATA_BRANCH(ValueAsMetadata)
+HANDLE_METADATA_LEAF(ConstantAsMetadata)
+HANDLE_METADATA_LEAF(LocalAsMetadata)
+HANDLE_METADATA_LEAF(DistinctMDOperandPlaceholder)
+HANDLE_MDNODE_BRANCH(MDNode)
+HANDLE_MDNODE_LEAF_UNIQUABLE(MDTuple)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DILocation)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIExpression)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIGlobalVariableExpression)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DINode)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(GenericDINode)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DISubrange)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIEnumerator)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DIScope)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DIType)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIBasicType)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIDerivedType)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DICompositeType)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DISubroutineType)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIFile)
+HANDLE_SPECIALIZED_MDNODE_LEAF(DICompileUnit)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DILocalScope)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DISubprogram)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DILexicalBlockBase)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DILexicalBlock)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DILexicalBlockFile)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DINamespace)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIModule)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DITemplateParameter)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DITemplateTypeParameter)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DITemplateValueParameter)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DIVariable)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIGlobalVariable)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DILocalVariable)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIObjCProperty)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIImportedEntity)
+HANDLE_SPECIALIZED_MDNODE_BRANCH(DIMacroNode)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIMacro)
+HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE(DIMacroFile)
+
+#undef HANDLE_METADATA
+#undef HANDLE_METADATA_LEAF
+#undef HANDLE_METADATA_BRANCH
+#undef HANDLE_MDNODE_LEAF
+#undef HANDLE_MDNODE_LEAF_UNIQUABLE
+#undef HANDLE_MDNODE_BRANCH
+#undef HANDLE_SPECIALIZED_MDNODE_LEAF
+#undef HANDLE_SPECIALIZED_MDNODE_LEAF_UNIQUABLE
+#undef HANDLE_SPECIALIZED_MDNODE_BRANCH
diff --git a/llvm/include/llvm/IR/Value.h b/llvm/include/llvm/IR/Value.h
index fc2ed00d770f..fa1d822b8a0f 100644
--- a/llvm/include/llvm/IR/Value.h
+++ b/llvm/include/llvm/IR/Value.h
@@ -265,7 +265,8 @@ public:
 private:
   void destroyValueName();
   enum class ReplaceMetadataUses { No, Yes };
-  void doRAUW(Value *New, ReplaceMetadataUses);
+  /// If 'AllowASChange' is true, address space changes are valid.
+  void doRAUW(Value *New, ReplaceMetadataUses, const bool AllowASChange = false);
   void setNameImpl(const Twine &Name);
 
 public:
@@ -299,7 +300,8 @@ public:
   /// Go through the uses list for this definition and make each use point to
   /// "V" instead of "this".  After this completes, 'this's use list is
   /// guaranteed to be empty.
-  void replaceAllUsesWith(Value *V);
+  /// If 'AllowASChange' is true, address space changes are valid.
+  void replaceAllUsesWith(Value *V, const bool AllowASChange = false);
 
   /// Change non-metadata uses of this to point to a new Value.
   ///
diff --git a/llvm/include/llvm/IR/ValueHandle.h b/llvm/include/llvm/IR/ValueHandle.h
index 29560815ea55..dcce29d9ba27 100644
--- a/llvm/include/llvm/IR/ValueHandle.h
+++ b/llvm/include/llvm/IR/ValueHandle.h
@@ -116,7 +116,7 @@ protected:
 public:
   // Callbacks made from Value.
   static void ValueIsDeleted(Value *V);
-  static void ValueIsRAUWd(Value *Old, Value *New);
+  static void ValueIsRAUWd(Value *Old, Value *New, const bool AllowASChange = false);
 
 private:
   // Internal implementation details.
diff --git a/llvm/include/llvm/InitializePasses.h b/llvm/include/llvm/InitializePasses.h
index 02e2e95dac94..fd98567f8576 100644
--- a/llvm/include/llvm/InitializePasses.h
+++ b/llvm/include/llvm/InitializePasses.h
@@ -27,6 +27,9 @@ void initializeTransformUtils(PassRegistry&);
 /// Initialize all passes linked into the ScalarOpts library.
 void initializeScalarOpts(PassRegistry&);
 
+/// Initialize all passes linked into the LibFloor library.
+void initializeLibFloor(PassRegistry&);
+
 /// Initialize all passes linked into the ObjCARCOpts library.
 void initializeObjCARCOpts(PassRegistry&);
 
@@ -455,9 +458,30 @@ void initializeWasmEHPreparePass(PassRegistry&);
 void initializeWholeProgramDevirtPass(PassRegistry&);
 void initializeWinEHPreparePass(PassRegistry&);
 void initializeWriteBitcodePassPass(PassRegistry&);
+void initializeWriteBitcodePass50Pass(PassRegistry&);
 void initializeWriteThinLTOBitcodePass(PassRegistry&);
 void initializeXRayInstrumentationPass(PassRegistry&);
 
+// LibFloor passes
+void initializeAddressSpaceFixPass(PassRegistry&);
+void initializeEverythingInlinerPass(PassRegistry&);
+void initializeCUDAImagePass(PassRegistry&);
+void initializeCUDAFinalPass(PassRegistry&);
+void initializeMetalFirstPass(PassRegistry&);
+void initializeMetalFinalPass(PassRegistry&);
+void initializeMetalFinalModuleCleanupPass(PassRegistry&);
+void initializeMetalImagePass(PassRegistry&);
+void initializeSPIRFinalPass(PassRegistry&);
+void initializeSPIRImagePass(PassRegistry&);
+void initializeCFGStructurizationPass(PassRegistry&);
+void initializeVulkanImagePass(PassRegistry&);
+void initializeVulkanFinalPass(PassRegistry&);
+void initializeVulkanBuiltinParamHandlingPass(PassRegistry&);
+void initializeVulkanPreFinalPass(PassRegistry&);
+void initializeVulkanFinalModuleCleanupPass(PassRegistry&);
+void initializePropagateRangeInfoPass(PassRegistry&);
+void initializeFMACombinerPass(PassRegistry&);
+
 } // end namespace llvm
 
 #endif // LLVM_INITIALIZEPASSES_H
diff --git a/llvm/include/llvm/LinkAllPasses.h b/llvm/include/llvm/LinkAllPasses.h
index 45978828a8ce..0e2160216d3c 100644
--- a/llvm/include/llvm/LinkAllPasses.h
+++ b/llvm/include/llvm/LinkAllPasses.h
@@ -51,6 +51,7 @@
 #include "llvm/Transforms/Scalar/GVN.h"
 #include "llvm/Transforms/Scalar/InstSimplifyPass.h"
 #include "llvm/Transforms/Scalar/Scalarizer.h"
+#include "llvm/Transforms/LibFloor.h"
 #include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include "llvm/Transforms/Utils/UnifyFunctionExitNodes.h"
@@ -118,7 +119,7 @@ namespace {
       (void) llvm::createInductiveRangeCheckEliminationPass();
       (void) llvm::createIndVarSimplifyPass();
       (void) llvm::createInstSimplifyLegacyPass();
-      (void) llvm::createInstructionCombiningPass();
+      (void) llvm::createInstructionCombiningPass(false);
       (void) llvm::createInternalizePass();
       (void) llvm::createLCSSAPass();
       (void) llvm::createLegacyDivergenceAnalysisPass();
@@ -213,7 +214,7 @@ namespace {
       (void) llvm::createLoopVectorizePass();
       (void) llvm::createSLPVectorizerPass();
       (void) llvm::createLoadStoreVectorizerPass();
-      (void) llvm::createVectorCombinePass();
+      (void) llvm::createVectorCombinePass(false);
       (void) llvm::createPartiallyInlineLibCallsPass();
       (void) llvm::createScalarizerPass();
       (void) llvm::createSeparateConstOffsetFromGEPPass();
@@ -246,6 +247,25 @@ namespace {
             llvm::AAMDNodes()); // for -print-alias-sets
       (void) llvm::AreStatisticsEnabled();
       (void) llvm::sys::RunningOnValgrind();
+
+      // LibFloor passes
+      (void) llvm::createEverythingInlinerPass();
+      (void) llvm::createAddressSpaceFixPass();
+      (void) llvm::createCUDAImagePass(0);
+      (void) llvm::createCUDAFinalPass();
+      (void) llvm::createMetalFirstPass(false, false);
+      (void) llvm::createMetalFinalPass(false, false);
+      (void) llvm::createMetalFinalModuleCleanupPass();
+      (void) llvm::createMetalImagePass(0);
+      (void) llvm::createSPIRFinalPass();
+      (void) llvm::createSPIRImagePass(0, false);
+      (void) llvm::createCFGStructurizationPass();
+      (void) llvm::createVulkanImagePass();
+      (void) llvm::createVulkanFinalPass();
+      (void) llvm::createVulkanPreFinalPass();
+      (void) llvm::createVulkanFinalModuleCleanupPass();
+      (void) llvm::createPropagateRangeInfoPass();
+      (void) llvm::createFMACombinerPass();
     }
   } ForcePassLinking; // Force link by creating a global definition.
 }
diff --git a/llvm/include/llvm/SPIRVerifier/SpirErrors.h b/llvm/include/llvm/SPIRVerifier/SpirErrors.h
new file mode 100644
index 000000000000..c9832b81f8cc
--- /dev/null
+++ b/llvm/include/llvm/SPIRVerifier/SpirErrors.h
@@ -0,0 +1,144 @@
+//===--------------------------- SpirErrors.h ----------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef __SPIR_ERRORS_H__
+#define __SPIR_ERRORS_H__
+
+#include <list>
+
+namespace llvm {
+  class Type;
+  class Value;
+  class Metadata;
+  class MDNode;
+  class NamedMDNode;
+  class StringRef;
+  class raw_ostream;
+}
+
+using namespace llvm;
+namespace SPIR {
+//
+// Validation Errors
+//
+
+typedef enum {
+  // Module (general) errors
+  // Type errors
+  ERR_INVALID_OCL_TYPE,
+  ERR_INVALID_LLVM_TYPE,
+  ERR_INVALID_KERNEL_RETURN_TYPE,
+  ERR_KERNEL_ARG_PTRPTR,
+  ERR_KERNEL_ARG_AS0,
+  ERR_MISMATCH_OCL_AND_LLVM_TYPES,
+  ERR_INVALID_GLOBAL_AS3_VAR,
+  ERR_INVALID_GLOBAL_VAR_ADDRESS_SPACE,
+  // Instruction errors
+  ERR_INVALID_INTRINSIC,
+  ERR_INVALID_ADDR_SPACE,
+  ERR_INVALID_ADDR_SPACE_CAST,
+  ERR_INVALID_INDIRECT_CALL,
+  ERR_INVALID_MEM_FENCE,
+  // Function errors
+  ERR_INVALID_CALLING_CONVENTION,
+  ERR_INVALID_LINKAGE_TYPE,
+  // Metadata errors
+  ERR_INVALID_CORE_FEATURE,
+  ERR_INVALID_KHR_EXT,
+  ERR_INVALID_COMPILER_OPTION,
+  ERR_MISSING_NAMED_METADATA,
+  ERR_INVALID_METADATA_KERNEL,
+  ERR_INVALID_METADATA_KERNEL_INFO,
+  ERR_MISSING_METADATA_KERNEL_INFO,
+  ERR_INVALID_METADATA_VERSION,
+  ERR_MISMATCH_METADATA_ADDR_SPACE,
+
+  SPIR_ERROR_NUM
+} SPIR_ERROR_TYPE;
+
+struct ErrorPrinter {
+  virtual ~ErrorPrinter() {};
+
+  /// @brief prints all errors to given output stream.
+  /// @param S output stream.
+  /// @param LITMode prints error names only in when set to true
+  virtual void print(llvm::raw_ostream &S, bool LITMode) const = 0;
+
+  /// @brief Checks if the module has errors.
+  /// @returns true if errors list is not emtpy.
+  virtual bool hasErrors() const = 0;
+};
+
+struct ErrorCreator {
+  virtual ~ErrorCreator() {};
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param S llvm string that leaded to the error.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::StringRef S) = 0;
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param V llvm value that leaded to the error.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::Value *V) = 0;
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param MD llvm metadata that leaded to the error.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::Metadata *MD) = 0;
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param NMD llvm named metadata node that leaded to the error.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::NamedMDNode *NMD) = 0;
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param T llvm type that leaded to the error.
+  /// @param S name of the function that T in its prototype.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::Type *T,
+                                             const llvm::StringRef S) = 0;
+
+  /// @brief Creates and adds new error to the error list.
+  /// @param Err error type to be added.
+  /// @param T llvm type that leaded to the error.
+  /// @param V value that T is its type.
+  virtual void addError(SPIR_ERROR_TYPE Err, const llvm::Type *T,
+                                             const llvm::Value *V) = 0;
+
+};
+
+struct ValidationError;
+typedef std::list<const ValidationError*> ErrorList;
+
+
+struct ErrorHolder : ErrorCreator, ErrorPrinter {
+  ErrorHolder();
+  virtual ~ErrorHolder();
+
+  /// Implementation of the pure virtual methods of ErrorCreator interface
+  void addError(SPIR_ERROR_TYPE Err, const llvm::StringRef S) override;
+  void addError(SPIR_ERROR_TYPE Err, const llvm::Value *V) override;
+  void addError(SPIR_ERROR_TYPE Err, const llvm::Metadata *MD) override;
+  void addError(SPIR_ERROR_TYPE Err, const llvm::NamedMDNode *NMD) override;
+  void addError(SPIR_ERROR_TYPE Err, const llvm::Type *T, const llvm::StringRef S) override;
+  void addError(SPIR_ERROR_TYPE Err, const llvm::Type *T, const llvm::Value *V) override;
+
+  /// Implementation of the pure virtual methods of ErrorPrinter interface
+  void print(llvm::raw_ostream &S, bool LITMode) const override;
+  bool hasErrors() const override;
+
+private:
+  /// @brief List of errors found in the module
+  ErrorList EL;
+};
+
+
+} // End SPIR namespace
+#endif // __SPIR_ERRORS_H__
diff --git a/llvm/include/llvm/SPIRVerifier/SpirIterators.h b/llvm/include/llvm/SPIRVerifier/SpirIterators.h
new file mode 100644
index 000000000000..f8b38d90c35f
--- /dev/null
+++ b/llvm/include/llvm/SPIRVerifier/SpirIterators.h
@@ -0,0 +1,467 @@
+//===------------------------- SpirIterators.h ---------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef __SPIR_ITERATORS_H__
+#define __SPIR_ITERATORS_H__
+
+#include <list>
+#include <map>
+
+namespace llvm {
+class Value;
+class Instruction;
+class BasicBlock;
+class Function;
+class Module;
+class MDNode;
+class GlobalVariable;
+}
+
+using namespace llvm;
+
+namespace SPIR {
+
+struct ErrorCreator;
+
+//
+// Executor interfaces.
+//
+
+/// @brief Interface for executor on llvm value.
+struct ValueExecutor {
+  virtual ~ValueExecutor() = default;
+  virtual void execute(const Value*) = 0;
+};
+
+/// @brief Interface for executor on llvm instruction.
+struct InstructionExecutor {
+  virtual ~InstructionExecutor() = default;
+  virtual void execute(const Instruction*) = 0;
+};
+
+/// @brief Interface for executor on llvm function.
+struct FunctionExecutor {
+  virtual ~FunctionExecutor() = default;
+  virtual void execute(const Function*) = 0;
+};
+
+/// @brief Interface for executor on llvm global variables.
+struct GlobalVariableExecutor {
+  virtual ~GlobalVariableExecutor() = default;
+  virtual void execute(const GlobalVariable*) = 0;
+};
+
+/// @brief Interface for executor on llvm module.
+struct ModuleExecutor {
+  virtual ~ModuleExecutor() = default;
+  virtual void execute(const Module*) = 0;
+};
+
+/// @brief Interface for executor on llvm metadata node.
+struct MDNodeExecutor {
+  virtual ~MDNodeExecutor() = default;
+  virtual void execute(const MDNode*) = 0;
+};
+
+typedef std::list<ValueExecutor*> ValueExecutorList;
+typedef std::list<InstructionExecutor*> InstructionExecutorList;
+typedef std::list<FunctionExecutor*> FunctionExecutorList;
+typedef std::list<GlobalVariableExecutor*> GlobalVariableExecutorList;
+typedef std::list<ModuleExecutor*> ModuleExecutorList;
+typedef std::list<MDNodeExecutor*> MDNodeExecutorList;
+
+//
+// Iterator classes.
+//
+
+struct BasicBlockIterator {
+  /// @brief Constructor.
+  /// @param IEL list of instruction executors.
+  BasicBlockIterator(InstructionExecutorList& IEL) : m_iel(IEL) {
+  }
+
+  /// @brief Iterates over the instructions in a basic block
+  ///        and execute all executors from the list on each instruction.
+  /// @param Basic block to iterate over.
+  void execute(const BasicBlock& BB);
+
+private:
+  /// @brief List of instruction executors.
+  InstructionExecutorList& m_iel;
+};
+
+struct FunctionIterator {
+  /// @brief Constructor.
+  /// @param FEL list of function executors.
+  /// @param BBI basic block iterator (optional).
+  FunctionIterator(FunctionExecutorList& FEL, BasicBlockIterator *BBI = 0) :
+    m_fel(FEL), m_bbi(BBI) {
+  }
+
+  /// @brief Iterates over the basic blocks in a function.
+  /// @param F function to iterate over.
+  void execute(const Function& F);
+
+private:
+  /// @brief List of function executors.
+  FunctionExecutorList& m_fel;
+  /// @brief Basic block iterator.
+  BasicBlockIterator *m_bbi;
+};
+
+struct GlobalVariableIterator {
+  /// @hbrief Constructor.
+  /// @param GVEL list of global variable executors
+  GlobalVariableIterator(GlobalVariableExecutorList& GVEL) : m_gvel(GVEL) {
+  }
+
+  /// @brief Execute all the executors from the list on GlobalVariable.
+  /// @param GV Global variable to process.
+  void execute(const GlobalVariable& GV);
+
+private:
+  /// @brief List of GlobalVAriable executors.
+  GlobalVariableExecutorList& m_gvel;
+};
+
+struct ModuleIterator {
+  /// @brief Constructor.
+  /// @param MEL list of module executors.
+  /// @param FI function iterator (optional).
+  ModuleIterator(ModuleExecutorList& MEL, FunctionIterator *FI = 0,
+         GlobalVariableIterator *GI = 0) :
+    m_mel(MEL), m_fi(FI), m_gi(GI) {
+  }
+
+  /// @brief Iterates over the functions in a module.
+  /// @param M module to iterate over.
+  void execute(const Module& M);
+
+private:
+  /// @brief List of module executors.
+  ModuleExecutorList& m_mel;
+  /// @brief Function iterator.
+  FunctionIterator *m_fi;
+  /// @brief Global value iterator.
+  GlobalVariableIterator *m_gi;
+};
+
+/// @brief Iterates over the metadata nodes.
+struct MetaDataIterator {
+  /// @brief Constructor.
+  /// @param NEL list of metadata node executors.
+  MetaDataIterator(MDNodeExecutorList& NEL) : m_nel(NEL) {
+  }
+  /// @brief Iterates over the operands of a metadata node.
+  /// @param M module to iterate over.
+  void execute(const MDNode& Node);
+
+private:
+  /// @brief List of Metadata node executors.
+  MDNodeExecutorList& m_nel;
+};
+
+
+//
+// Module data holder class.
+//
+
+struct DataHolder {
+  DataHolder() :
+    Is32Bit(true),
+    HasDoubleFeature(false), HasImageFeature(false),
+    HASFp16Extension(false) {
+  }
+
+  /// @brief Sizeof pointer indectaor
+  bool Is32Bit;
+
+  // Core Features
+
+  /// @brief indicator for presence of cl_double core feature
+  bool HasDoubleFeature;
+
+  /// @brief indicator for presence of cl_images core feature
+  bool HasImageFeature;
+
+  // KHR Extensions
+
+  /// @brief indicator for presence of cl_khr_fp16 KHR extension
+  bool HASFp16Extension;
+};
+
+//
+// Verify Executor classes.
+//
+
+struct VerifyCall : public InstructionExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  VerifyCall(ErrorCreator *EH) : ErrCreator(EH) {
+  }
+
+  /// @brief Verify that given instruction is not invalid call instruction.
+  /// @param I instruction to verify.
+  void execute(const Instruction *I) override;
+
+private:
+  ErrorCreator *ErrCreator;
+};
+
+struct VerifyBitcast : public InstructionExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  VerifyBitcast(ErrorCreator *EH) : ErrCreator(EH) {
+  }
+
+  /// @brief Verify that given instruction is not invalid bitcast instruction
+  ///        and that it has no invalid bitcast constant expression operands.
+  /// @param I instruction to verify.
+  void execute(const Instruction *I) override;
+
+private:
+  ErrorCreator *ErrCreator;
+};
+
+struct VerifyInstructionType : public InstructionExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyInstructionType(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  /// @brief Verify that given instruction has a valid type.
+  /// @param I instruction to verify.
+  void execute(const Instruction *I) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+};
+
+struct VerifyFunctionPrototype : public FunctionExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyFunctionPrototype(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  /// @brief Verify that given function has valid prototype.
+  /// @param F function to verify.
+  void execute(const Function *F) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+};
+
+struct VerifyKernelPrototype : public FunctionExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyKernelPrototype(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  /// @brief Verify that given OpenCL kernel has valid prototype.
+  /// @param F function to verify.
+  void execute(const Function *F) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  __attribute__((unused)) DataHolder *Data;
+};
+
+struct VerifyGlobalVariable : public GlobalVariableExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyGlobalVariable(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  /// @brief Verify varoius properties of given global variable
+  /// @param F function to verify.
+  void execute(const GlobalVariable *GV) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  __attribute__((unused)) DataHolder *Data;
+};
+
+struct VerifyMetadataArgAddrSpace : public MDNodeExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param F the function metadata arg base type is describing.
+  VerifyMetadataArgAddrSpace(ErrorCreator *EH, Function *F) :
+    ErrCreator(EH), Func(F), WasFound(false) {
+  }
+
+  /// @brief Verify that given metadata node is valid arg type metadata.
+  /// @param Node metadata node to verify.
+  void execute(const MDNode *Node) override;
+
+  bool found() {
+    return WasFound;
+  }
+
+private:
+  ErrorCreator *ErrCreator;
+  Function *Func;
+  bool WasFound;
+};
+
+struct VerifyMetadataArgType : public MDNodeExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  VerifyMetadataArgType(ErrorCreator *EH) : ErrCreator(EH), WasFound(false) {
+  }
+
+  /// @brief Verify that given metadata node is valid arg type metadata.
+  /// @param Node metadata node to verify.
+  void execute(const MDNode *Node) override;
+
+  bool found() {
+    return WasFound;
+  }
+
+private:
+  __attribute__((unused)) ErrorCreator *ErrCreator;
+  bool WasFound;
+};
+
+struct VerifyMetadataArgBaseType : public MDNodeExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param F the function metadata arg base type is describing.
+  VerifyMetadataArgBaseType(ErrorCreator *EH, Function *F, DataHolder *D) :
+    ErrCreator(EH), Func(F), Data(D), WasFound(false) {
+  }
+
+  /// @brief Verify that given metadata node is valid arg base type metadata.
+  /// @param Node metadata node to verify.
+  void execute(const MDNode *Node) override;
+
+  bool found() {
+    return WasFound;
+  }
+
+private:
+  ErrorCreator *ErrCreator;
+  Function *Func;
+  DataHolder *Data;
+  bool WasFound;
+};
+
+typedef std::map<const Function*, const MDNode*> FunctionToMDNodeMap;
+struct VerifyMetadataKernel : public MDNodeExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  VerifyMetadataKernel(ErrorCreator *EH,
+    DataHolder *D, FunctionToMDNodeMap& Map) :
+    ErrCreator(EH), Data(D), FoundMap(Map) {
+  }
+
+  /// @brief Verify that given metadata node is valid arg type metadata.
+  /// @param Node metadata node to verify.
+  void execute(const MDNode *Node) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+  FunctionToMDNodeMap& FoundMap;
+};
+
+struct VerifyMetadataKernels : public ModuleExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyMetadataKernels(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  void execute(const Module *M) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+};
+
+struct VerifyMetadataVersions : public ModuleExecutor {
+
+  typedef enum {
+    VERSION_OCL,
+    VERSION_SPIR,
+
+    OPENCL_VERISON_NUM
+  } OPENCL_VERSION_TYPE;
+
+  /// @brief Constructor.
+  /// @param EH error holder.
+  VerifyMetadataVersions(ErrorCreator *EH, OPENCL_VERSION_TYPE VTy) :
+    ErrCreator(EH), VType(VTy) {
+  }
+
+  void execute(const Module *M) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  OPENCL_VERSION_TYPE VType;
+};
+
+struct VerifyMetadataCoreFeatures : public ModuleExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.s
+  VerifyMetadataCoreFeatures(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  void execute(const Module *M) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+};
+
+struct VerifyMetadataKHRExtensions : public ModuleExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyMetadataKHRExtensions(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  void execute(const Module *M) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  DataHolder *Data;
+};
+
+struct VerifyMetadataCompilerOptions : public ModuleExecutor {
+  /// @brief Constructor.
+  /// @param EH error holder.
+  /// @param D data holder.
+  VerifyMetadataCompilerOptions(ErrorCreator *EH, DataHolder *D) :
+    ErrCreator(EH), Data(D) {
+  }
+
+  void execute(const Module *M) override;
+
+private:
+  ErrorCreator *ErrCreator;
+  __attribute__((unused)) DataHolder *Data;
+};
+
+} // End SPIR namespace
+
+#endif // __SPIR_ITERATORS_H__
diff --git a/llvm/include/llvm/SPIRVerifier/SpirTables.h b/llvm/include/llvm/SPIRVerifier/SpirTables.h
new file mode 100644
index 000000000000..e14e3062395d
--- /dev/null
+++ b/llvm/include/llvm/SPIRVerifier/SpirTables.h
@@ -0,0 +1,171 @@
+//===--------------------------- SpirTable.h -----------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef __SPIR_TABLES_H__
+#define __SPIR_TABLES_H__
+
+#include <string>
+
+namespace SPIR {
+
+//
+// Constant definitions.
+//
+
+enum AddrSpace {
+  PRIVATE_ADDR_SPACE = 0,
+  GLOBAL_ADDR_SPACE = 1,
+  CONSTANT_ADDR_SPACE = 2,
+  LOCAL_ADDR_SPACE = 3
+};
+
+#define EXTREN_DCL_ARRAY_LENGTH(arr) extern const unsigned arr##_len
+
+extern const char *CORE_FEATURE_CL_DOUBLES;
+extern const char *CORE_FEATURE_CL_IMAGES;
+extern const char *g_valid_core_feature[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_core_feature);
+
+extern const char *EXTENSION_CL_KHR_FP16;
+extern const char *g_valid_khr_ext[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_khr_ext);
+
+extern const char *g_valid_compiler_options[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_compiler_options);
+
+///
+/// OpenCL C Type tables
+///
+extern const char *g_valid_ocl_primitives[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_ocl_primitives);
+
+extern const char *g_valid_ocl_vector_element_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_ocl_vector_element_types);
+
+extern const char *g_valid_ocl_opaque_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_ocl_opaque_types);
+
+extern const char *g_opencl_opaque_sufix;
+
+extern const char *g_ignored_ocl_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_ignored_ocl_types);
+
+///
+/// OpenCL C Type tables
+///
+extern const char *g_valid_llvm_primitives[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_llvm_primitives);
+
+extern const char *g_valid_llvm_vector_element_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_llvm_vector_element_types);
+
+extern const char *g_valid_llvm_image_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_llvm_image_types);
+
+extern const char *g_valid_llvm_opaque_types[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_llvm_opaque_types);
+
+extern const char *g_llvm_opaque_prefix;
+
+///
+/// Other OpenCL Tables
+///
+extern const char *g_valid_vector_type_lengths[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_vector_type_lengths);
+
+extern const char *g_valid_instrinsic[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_instrinsic);
+
+extern const char *g_ignored_instrinsic[];
+EXTREN_DCL_ARRAY_LENGTH(g_ignored_instrinsic);
+
+extern const char *g_valid_sync_bi[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_sync_bi);
+
+extern const char *g_valid_address_space[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_address_space);
+
+extern const char *g_valid_calling_convention[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_calling_convention);
+
+extern const char *g_valid_linkage_type[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_linkage_type);
+
+extern const char *OPENCL_KERNELS;
+extern const char *OPENCL_SPIR_VERSION;
+extern const char *OPENCL_OCL_VERSION;
+extern const char *OPENCL_KHR_EXTENSIONS;
+extern const char *OPENCL_CORE_FEATURES;
+extern const char *OPENCL_COMPILER_OPTIONS;
+extern const char *g_valid_named_metadata[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_named_metadata);
+
+extern const char *KERNEL_ARG_ADDR_SPACE;
+extern const char *KERNEL_ARG_TY;
+extern const char *KERNEL_ARG_BASE_TY;
+extern const char *g_valid_kernel_arg_info[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_kernel_arg_info);
+
+extern const char *g_valid_version_names[];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_version_names);
+
+extern const char *g_valid_spir_versions[][2];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_spir_versions);
+
+extern const char *g_valid_ocl_versions[][2];
+EXTREN_DCL_ARRAY_LENGTH(g_valid_ocl_versions);
+
+
+
+
+///
+/// get error info message functions
+///
+
+extern std::string getValidOpenCLTypeMsg();
+
+extern std::string getValidLLVMTypeMsg();
+
+extern std::string getValidKernelReturnTypeMsg();
+
+extern std::string getValidIntrinsicMsg();
+
+extern std::string getValidAddressSpaceMsg();
+
+extern std::string getValidCallingConventionMsg();
+
+extern std::string getValidLinkageTypeMsg();
+
+extern std::string getValidGlobalAS3VariableMsg();
+
+extern std::string getValidGlobalVarAddressSpacesMsg();
+
+extern std::string getValidIndirectCallMsg();
+
+extern std::string getValidKernelArgInfoMsg();
+
+extern std::string getValidKernelArgAddressSpaceMsg();
+
+extern std::string getValidVersionMsg();
+
+extern std::string getValidMemFenceMsg();
+
+extern std::string getMapOpenCLToLLVMMsg();
+
+extern std::string getValidNamedMetadataMsg();
+
+extern std::string getValidCoreFeaturesMsg();
+
+extern std::string getValidKHRExtensionsMsg();
+
+extern std::string getValidCompilerOptionsMsg();
+
+} // End SPIR namespace
+
+#endif // __SPIR_TABLES_H__
diff --git a/llvm/include/llvm/SPIRVerifier/SpirValidation.h b/llvm/include/llvm/SPIRVerifier/SpirValidation.h
new file mode 100644
index 000000000000..79788a927cb8
--- /dev/null
+++ b/llvm/include/llvm/SPIRVerifier/SpirValidation.h
@@ -0,0 +1,59 @@
+//===------------------------ SpirValidation.h ---------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#ifndef __SPIR_VALIDATION_H__
+#define __SPIR_VALIDATION_H__
+
+#include "llvm/SPIRVerifier/SpirErrors.h"
+#include "llvm/Pass.h"
+
+namespace SPIR {
+
+/// @brief Indicates whether a given module is a valid SPIR module
+///        according to SPIR 1.2 spec.
+class SpirValidation : public llvm::ModulePass {
+public:
+
+  /// @brief Pass identification, replacement for typeid.
+  static char ID;
+
+  /// @brief Constructor.
+  SpirValidation();
+
+  /// @brief Distructor.
+  virtual ~SpirValidation();
+
+  /// @brief Provides name of pass.
+  StringRef getPassName() const override;
+
+  /// @brief LLVM Module pass entry.
+  /// @param M Module to transform.
+  /// @returns true if changed.
+  bool runOnModule(llvm::Module&) override;
+
+  /// @brief returns instance of ErrorPrinter implementation.
+  /// @returns error printer instance.
+  const ErrorPrinter *getErrorPrinter() const {
+    return &ErrHolder;
+  }
+
+private:
+
+  /// @brief Holder for errors found in the module
+  ErrorHolder ErrHolder;
+};
+
+} // End SPIR namespace
+
+
+namespace llvm {
+  ModulePass *createSpirValidationPass();
+}
+
+#endif // __SPIR_VALIDATION_H__
diff --git a/llvm/include/llvm/Transforms/IPO.h b/llvm/include/llvm/Transforms/IPO.h
index 67b9a93c47b2..26f4c0b2b336 100644
--- a/llvm/include/llvm/Transforms/IPO.h
+++ b/llvm/include/llvm/Transforms/IPO.h
@@ -116,6 +116,12 @@ Pass *createFunctionInliningPass(unsigned OptLevel, unsigned SizeOptLevel,
                                  bool DisableInlineHotCallSite);
 Pass *createFunctionInliningPass(InlineParams &Params);
 
+//===----------------------------------------------------------------------===//
+/// createEverythingInlinerPass - Return a new pass object that inlines
+/// everything, unless it was marked "noinline".
+Pass *createEverythingInlinerPass();
+Pass *createEverythingInlinerPass(bool InsertLifetime);
+
 //===----------------------------------------------------------------------===//
 /// createPruneEHPass - Return a new pass object which transforms invoke
 /// instructions into calls, if the callee can _not_ unwind the stack.
diff --git a/llvm/include/llvm/Transforms/IPO/PassManagerBuilder.h b/llvm/include/llvm/Transforms/IPO/PassManagerBuilder.h
index 7f321a688aff..fdeff2998b53 100644
--- a/llvm/include/llvm/Transforms/IPO/PassManagerBuilder.h
+++ b/llvm/include/llvm/Transforms/IPO/PassManagerBuilder.h
@@ -186,6 +186,20 @@ public:
   /// Path of the sample Profile data file.
   std::string PGOSampleUse;
 
+  bool EnableAddressSpaceFix;
+  bool EnableCUDAPasses;
+  bool EnableMetalPasses;
+  bool EnableMetalIntelWorkarounds;
+  bool EnableMetalNvidiaWorkarounds;
+  bool EnableSPIRPasses;
+  bool EnableSPIRIntelWorkarounds;
+  bool EnableVerifySPIR;
+  bool EnableVulkanPasses;
+  bool EnableVulkanLLVMPreStructurizationPass;
+
+  // can't rely on clang header here, so just use a uint32_t
+  unsigned int floor_image_capabilities { 0 };
+
 private:
   /// ExtensionList - This is list of all of the extensions that are registered.
   std::vector<std::pair<ExtensionPointTy, ExtensionFn>> Extensions;
diff --git a/llvm/include/llvm/Transforms/InstCombine/InstCombine.h b/llvm/include/llvm/Transforms/InstCombine/InstCombine.h
index 6dee38c83b36..77fc78f7fc66 100644
--- a/llvm/include/llvm/Transforms/InstCombine/InstCombine.h
+++ b/llvm/include/llvm/Transforms/InstCombine/InstCombine.h
@@ -27,10 +27,11 @@ namespace llvm {
 class InstCombinePass : public PassInfoMixin<InstCombinePass> {
   InstructionWorklist Worklist;
   const unsigned MaxIterations;
+  bool isVulkan;
 
 public:
-  explicit InstCombinePass();
-  explicit InstCombinePass(unsigned MaxIterations);
+  explicit InstCombinePass(bool isVulkan_ = false);
+  explicit InstCombinePass(unsigned MaxIterations, bool isVulkan_ = false);
 
   PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);
 };
@@ -42,12 +43,13 @@ public:
 class InstructionCombiningPass : public FunctionPass {
   InstructionWorklist Worklist;
   const unsigned MaxIterations;
+  const bool isVulkan;
 
 public:
   static char ID; // Pass identification, replacement for typeid
 
-  explicit InstructionCombiningPass();
-  explicit InstructionCombiningPass(unsigned MaxIterations);
+  explicit InstructionCombiningPass(bool isVulkan_ = false);
+  explicit InstructionCombiningPass(unsigned MaxIterations, bool isVulkan_ = false);
 
   void getAnalysisUsage(AnalysisUsage &AU) const override;
   bool runOnFunction(Function &F) override;
@@ -65,8 +67,11 @@ public:
 // into:
 //    %Z = add int 2, %X
 //
-FunctionPass *createInstructionCombiningPass();
-FunctionPass *createInstructionCombiningPass(unsigned MaxIterations);
+// "isVulkan" is necessary, because we want to prevent certain illegal
+// instruction combines when generating IR for Vulkan.
+//
+FunctionPass *createInstructionCombiningPass(bool isVulkan = false);
+FunctionPass *createInstructionCombiningPass(unsigned MaxIterations, bool isVulkan = false);
 }
 
 #undef DEBUG_TYPE
diff --git a/llvm/include/llvm/Transforms/InstCombine/InstCombiner.h b/llvm/include/llvm/Transforms/InstCombine/InstCombiner.h
index 42735b730886..3b3a447f0250 100644
--- a/llvm/include/llvm/Transforms/InstCombine/InstCombiner.h
+++ b/llvm/include/llvm/Transforms/InstCombine/InstCombiner.h
@@ -64,6 +64,9 @@ protected:
   // Mode in which we are running the combiner.
   const bool MinimizeSize;
 
+  /// prevent certain illegal combines when generating IR for Vulkan.
+  const bool isVulkan;
+
   AAResults *AA;
 
   // Required analyses.
@@ -84,13 +87,13 @@ protected:
 
 public:
   InstCombiner(InstructionWorklist &Worklist, BuilderTy &Builder,
-               bool MinimizeSize, AAResults *AA, AssumptionCache &AC,
+               bool MinimizeSize, bool isVulkan_, AAResults *AA, AssumptionCache &AC,
                TargetLibraryInfo &TLI, TargetTransformInfo &TTI,
                DominatorTree &DT, OptimizationRemarkEmitter &ORE,
                BlockFrequencyInfo *BFI, ProfileSummaryInfo *PSI,
                const DataLayout &DL, LoopInfo *LI)
       : TTI(TTI), Builder(Builder), Worklist(Worklist),
-        MinimizeSize(MinimizeSize), AA(AA), AC(AC), TLI(TLI), DT(DT), DL(DL),
+        MinimizeSize(MinimizeSize), isVulkan(isVulkan_), AA(AA), AC(AC), TLI(TLI), DT(DT), DL(DL),
         SQ(DL, &TLI, &DT, &AC), ORE(ORE), BFI(BFI), PSI(PSI), LI(LI) {}
 
   virtual ~InstCombiner() {}
diff --git a/llvm/include/llvm/Transforms/LibFloor.h b/llvm/include/llvm/Transforms/LibFloor.h
new file mode 100644
index 000000000000..28a470519393
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor.h
@@ -0,0 +1,152 @@
+//===-- LibFloor.h - LibFloor Transformations -------------------*- C++ -*-===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This header file defines prototypes for accessor functions that expose passes
+// in the LibFloor transformations library.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_H
+#define LLVM_TRANSFORMS_LIBFLOOR_H
+
+#include "llvm/Transforms/Utils/SimplifyCFGOptions.h"
+#include <functional>
+#include <cstdint>
+
+namespace llvm {
+
+class Function;
+class FunctionPass;
+class ModulePass;
+class Pass;
+
+//===----------------------------------------------------------------------===//
+//
+// AddressSpaceFix - This pass fixes (intentionally) broken uses of addrspace
+// pointers that should be non-addrspace pointers.
+//
+ModulePass *createAddressSpaceFixPass();
+
+//===----------------------------------------------------------------------===//
+//
+// CUDAImage - This pass applies CUDA-specific floor image transformations.
+//
+FunctionPass *createCUDAImagePass(const uint32_t image_capabilities = 0);
+
+//===----------------------------------------------------------------------===//
+//
+// CUDAFinal - final pass, making CUDA related IR changes.
+//
+FunctionPass *createCUDAFinalPass();
+
+//===----------------------------------------------------------------------===//
+//
+// MetalFirst - This pass fixes Metal/AIR issues.
+//
+FunctionPass *createMetalFirstPass(const bool enable_intel_workarounds = false,
+                                   const bool enable_nvidia_workarounds = false);
+
+//===----------------------------------------------------------------------===//
+//
+// MetalFinal - This pass fixes Metal/AIR issues.
+//
+FunctionPass *createMetalFinalPass(const bool enable_intel_workarounds = false,
+                                   const bool enable_nvidia_workarounds = false);
+
+//===----------------------------------------------------------------------===//
+//
+// MetalFinalModuleCleanup - This pass removes any calling convention attributes
+// and removes unused functions/prototypes/externs.
+//
+ModulePass *createMetalFinalModuleCleanupPass();
+
+//===----------------------------------------------------------------------===//
+//
+// MetalImage - This pass applies Metal-specific floor image transformations.
+//
+FunctionPass *createMetalImagePass(const uint32_t image_capabilities = 0);
+
+//===----------------------------------------------------------------------===//
+//
+// SPIRFinal - This pass fixes LLVM IR to be SPIR-compliant.
+//
+FunctionPass *createSPIRFinalPass();
+
+//===----------------------------------------------------------------------===//
+//
+// SPIRImage - This pass applies SPIR-specific floor image transformations.
+//
+FunctionPass *createSPIRImagePass(const uint32_t image_capabilities = 0,
+                                  const bool enable_intel_workarounds = false);
+
+//===----------------------------------------------------------------------===//
+//
+// CFGStructurization - This pass transforms the CFG into a structurized CFG.
+//
+FunctionPass *createCFGStructurizationPass();
+
+//===----------------------------------------------------------------------===//
+//
+// VulkanImage - This pass applies SPIR-V-specific floor image transformations.
+//
+FunctionPass *createVulkanImagePass(const uint32_t image_capabilities = 0);
+
+//===----------------------------------------------------------------------===//
+//
+// VulkanFinal - This pass fixes Vulkan/SPIR-V issues.
+//
+FunctionPass *createVulkanFinalPass();
+
+//===----------------------------------------------------------------------===//
+//
+// VulkanBuiltinParamHandling - This pass handles builtin -> parameter
+// replacement for Vulkan.
+//
+FunctionPass *createVulkanBuiltinParamHandlingPass();
+
+//===----------------------------------------------------------------------===//
+//
+// VulkanPreFinal - This pass fixes Vulkan/SPIR-V issues, prior to CFG
+// structurization and VulkanFinal.
+//
+FunctionPass *createVulkanPreFinalPass();
+
+//===----------------------------------------------------------------------===//
+//
+// VulkanFinalModuleCleanup - This pass removes unused functions/etc.
+//
+ModulePass *createVulkanFinalModuleCleanupPass();
+
+//===----------------------------------------------------------------------===//
+//
+// PropagateRangeInfo - This pass propagates range metadata info.
+//
+FunctionPass *createPropagateRangeInfoPass();
+
+//===----------------------------------------------------------------------===//
+//
+// FMACombiner - This pass combines and recombines fmul/fadd/fsub/fneg/fma
+// instructions to fma instructions.
+//
+FunctionPass *createFMACombinerPass();
+
+} // End llvm namespace
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/AddressSpaceFix.h b/llvm/include/llvm/Transforms/LibFloor/AddressSpaceFix.h
new file mode 100644
index 000000000000..72391a5e911d
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/AddressSpaceFix.h
@@ -0,0 +1,46 @@
+//==- AddressSpaceFix.cpp - OpenCL/Metal/Vulkan and related addrspace fixes -=//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements an address space fixer for OpenCL/Metal/Vulkan.
+//
+// This is implemented as a module pass that iterates over all functions, then
+// over all call instructions in there, fixing all calls that require a
+// different address space then what is provided by the called function.
+// Since this requires the address space information "from the top",
+// this can't be implemented as a bottom-up SCC pass.
+// Note that this will duplicate any functions that don't have matching address
+// space parameters and thus heavily depends on proper inlining later on.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_ADDRESSSPACEFIX_H
+#define LLVM_TRANSFORMS_LIBFLOOR_ADDRESSSPACEFIX_H
+
+namespace llvm {
+	//! fixes the address space of all users of the specified instruction
+	//! NOTE: call instructions will *not* be fixed by this (there should be no functions/calls when calling this)
+	void fix_instruction_users(LLVMContext &ctx,
+	                           Instruction &instr,
+	                           Value &parent,
+	                           const uint32_t address_space,
+	                           std::vector<ReturnInst *> &returns);
+} // namespace llvm
+
+#endif //LLVM_TRANSFORMS_ADDRESSSPACEFIX_HPP
diff --git a/llvm/include/llvm/Transforms/LibFloor/FloorGPUTTI.h b/llvm/include/llvm/Transforms/LibFloor/FloorGPUTTI.h
new file mode 100644
index 000000000000..d34eea96dd93
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/FloorGPUTTI.h
@@ -0,0 +1,267 @@
+//===-- FloorGPUTTI.h - LibFloor GPU TTI implementation ---------*- C++ -*-===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// TODO !
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_FLOORGPUTTI_H
+#define LLVM_TRANSFORMS_LIBFLOOR_FLOORGPUTTI_H
+
+#include "clang/Basic/CodeGenOptions.h"
+#include "clang/Basic/LangOptions.h"
+#include "clang/Basic/TargetOptions.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/TargetTransformInfo.h"
+#include "llvm/CodeGen/BasicTTIImpl.h"
+#include "llvm/CodeGen/TargetSubtargetInfo.h"
+#include "llvm/IR/Module.h"
+
+namespace llvm {
+
+//! (dummy) libfloor GPU target machine
+class LibFloorGPUTargetMachine : public LLVMTargetMachine {
+public:
+	// TODO: DataLayout, TargetOptions
+	explicit LibFloorGPUTargetMachine() :
+	LLVMTargetMachine({}, "", {}, "", "", {},
+					  Reloc::Static, CodeModel::Tiny, CodeGenOpt::Aggressive) {}
+	
+	static LibFloorGPUTargetMachine* get_instance() {
+		static LibFloorGPUTargetMachine instance;
+		return &instance;
+	}
+	
+	bool isNoopAddrSpaceCast(unsigned FromAS, unsigned ToAS) const override {
+		return (FromAS == ToAS);
+	}
+	
+};
+
+//! (dummy) libfloor GPU sub-target
+class LibFloorGPUSubTarget : public TargetSubtargetInfo {
+public:
+	// TODO: Triple, "CPU"?
+	LibFloorGPUSubTarget() :
+	TargetSubtargetInfo({}, "", "", "", {}, {}, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr) {}
+	
+	static LibFloorGPUSubTarget* get_instance() {
+		static LibFloorGPUSubTarget instance;
+		return &instance;
+	}
+	
+	// TODO: getSchedModel, getCacheSize, getCacheLineSize, getPrefetchDistance, getMinPrefetchStride, getMaxPrefetchIterationsAhead, enableWritePrefetching, getCacheAssociativity, useAA,
+	
+};
+
+//! libfloor GPU target lowering
+class LibFloorGPUTargetLowering : public TargetLowering {
+protected:
+	const bool is_metal { false };
+	const bool is_vulkan { false };
+	
+public:
+	explicit LibFloorGPUTargetLowering(const bool is_metal_, const bool is_vulkan_) :
+	TargetLowering(*LibFloorGPUTargetMachine::get_instance()), is_metal(is_metal_), is_vulkan(is_vulkan_) {}
+	
+	static LibFloorGPUTargetLowering* get_instance(const bool is_metal_, const bool is_vulkan_) {
+		static LibFloorGPUTargetLowering instance(is_metal_, is_vulkan_);
+		return &instance;
+	}
+	
+	//! Metal: allow misaligned access under certain pre-conditions
+	//! Vulkan: TBD
+	bool impl_allowsMisalignedMemoryAccesses(unsigned BitWidth, unsigned AddressSpace, Align Alignment, bool *Fast) const {
+		if (is_metal) {
+			// both alignment and #bytes must be 1 or divisble by 2
+			const auto align = Alignment.value();
+			if (align > 1u && align % 2u != 0) {
+				return false;
+			}
+			const auto bytes = BitWidth / 8u;
+			if (bytes % 2u != 0) {
+				return false;
+			}
+			// alignment must be smaller or equal to #bytes
+			if (bytes < align) {
+				return false;
+			}
+			// bytes must be a multiple of alignment
+			if (((bytes / align) * align) != bytes) {
+				return false;
+			}
+			// always flag as fast
+			if (Fast) {
+				*Fast = true;
+			}
+			return true;
+		}
+		return false;
+	}
+	
+	bool allowsMisalignedMemoryAccesses(EVT evt, unsigned AddrSpace = 0, Align Alignment = Align(1),
+										MachineMemOperand::Flags Flags = MachineMemOperand::MONone,
+										bool *Fast = nullptr) const override {
+		return impl_allowsMisalignedMemoryAccesses(evt.getSizeInBits(), AddrSpace, Alignment, Fast);
+	}
+	
+	bool allowsMisalignedMemoryAccesses(LLT llt, unsigned AddrSpace = 0, Align Alignment = Align(1),
+										MachineMemOperand::Flags Flags = MachineMemOperand::MONone,
+										bool *Fast = nullptr) const override {
+		return impl_allowsMisalignedMemoryAccesses(llt.getSizeInBits(), AddrSpace, Alignment, Fast);
+	}
+	
+};
+
+//! libfloor target transform implementation
+class LibFloorGPUTTIImpl : public TargetTransformInfoImplCRTPBase<LibFloorGPUTTIImpl> {
+public:
+	using crtp_base_class = TargetTransformInfoImplCRTPBase<LibFloorGPUTTIImpl>;
+	friend crtp_base_class;
+	using TTI = TargetTransformInfo;
+
+	explicit LibFloorGPUTTIImpl(const Function& F, const clang::CodeGenOptions& CodeGenOpts_,
+								const clang::TargetOptions& TargetOpts_,
+								const clang::LangOptions& LangOpts_,
+								Module& M_) :
+	crtp_base_class(F.getParent()->getDataLayout()),
+	CodeGenOpts(CodeGenOpts_), TargetOpts(TargetOpts_), LangOpts(LangOpts_), M(M_),
+	is_metal(Triple(M.getTargetTriple()).getArch() == Triple::ArchType::air64),
+	is_vulkan(Triple(M.getTargetTriple()).getArch() == Triple::ArchType::spir64 &&
+			  Triple(M.getTargetTriple()).getEnvironment() == Triple::EnvironmentType::Vulkan) {}
+	
+	//! restrict to width of 4
+	unsigned getMaximumVF(unsigned, unsigned) const { return 4; }
+	
+	unsigned getLoadStoreVecRegBitWidth(unsigned /* AddrSpace */ ) const {
+		return 128;
+	}
+	unsigned getLoadVectorFactor(unsigned VF, unsigned LoadSize,
+								 unsigned ChainSizeInBytes,
+								 VectorType *VecTy) const {
+		const auto max_bit_width = getLoadStoreVecRegBitWidth(0);
+		const auto VecRegBitWidth = VF * LoadSize;
+		if (VecRegBitWidth > max_bit_width && VecTy->getScalarSizeInBits() < 32) {
+			return max_bit_width / LoadSize;
+		}
+		return VF;
+	}
+	unsigned getStoreVectorFactor(unsigned VF, unsigned StoreSize,
+								  unsigned ChainSizeInBytes,
+								  VectorType *VecTy) const {
+		const auto max_bit_width = getLoadStoreVecRegBitWidth(0);
+		const auto VecRegBitWidth = VF * StoreSize;
+		if (VecRegBitWidth > max_bit_width) {
+			return max_bit_width / StoreSize;
+		}
+		return VF;
+	}
+	
+	//! yes - this is a GPU target
+	bool hasBranchDivergence() const { return true; }
+	
+	//! yes - this is a GPU target
+	bool useGPUDivergenceAnalysis() const { return true; }
+	
+	//! yes - we'll inline everything anyways
+	bool areInlineCompatible(const Function*, const Function*) const { return true; }
+	
+	/*InstructionCost getIntrinsicInstrCost(const IntrinsicCostAttributes &ICA,
+										  TTI::TargetCostKind CostKind) {
+		;;
+	}*/
+	InstructionCost getMemcpyCost(const Instruction *I) const {
+		return 255;
+	}
+	
+	bool allowsMisalignedMemoryAccesses(LLVMContext &Context, unsigned BitWidth,
+										unsigned AddressSpace, Align Alignment,
+										bool *Fast) {
+		return TLI->impl_allowsMisalignedMemoryAccesses(BitWidth, AddressSpace, Alignment, Fast);
+	}
+	
+	InstructionCost getShuffleCost(TTI::ShuffleKind Kind,
+								   VectorType *VT, ArrayRef<int> Mask,
+								   int Index, VectorType *SubTp) {
+		return 0;
+	}
+	
+	InstructionCost getVectorInstrCost(unsigned Opcode, Type *ValTy, unsigned Index) {
+		if (Index == 0 && (Opcode == Instruction::ExtractElement ||
+						   Opcode == Instruction::InsertElement)) {
+			return 0;
+		}
+		return crtp_base_class::getVectorInstrCost(Opcode, ValTy, Index);
+	}
+	
+	InstructionCost getArithmeticInstrCost(unsigned Opcode, Type *Ty, TTI::TargetCostKind CostKind,
+										   TTI::OperandValueKind Opd1Info = TTI::OK_AnyValue,
+										   TTI::OperandValueKind Opd2Info = TTI::OK_AnyValue,
+										   TTI::OperandValueProperties Opd1PropInfo = TTI::OP_None,
+										   TTI::OperandValueProperties Opd2PropInfo = TTI::OP_None,
+										   ArrayRef<const Value *> Args = ArrayRef<const Value *>(),
+										   const Instruction *CxtI = nullptr) {
+		auto cost = crtp_base_class::getArithmeticInstrCost(Opcode, Ty, CostKind, Opd1Info, Opd2Info,
+															Opd1PropInfo, Opd2PropInfo, Args, CxtI);
+		if (is_metal && Ty->isVectorTy()) {
+			// Metal: for vectors, bias cost so that it equals scalar cost
+			if (auto fixed_vec_type = dyn_cast<FixedVectorType>(Ty); fixed_vec_type) {
+				cost /= fixed_vec_type->getNumElements();
+			}
+		}
+		return cost;
+	}
+	
+	InstructionCost getArithmeticReductionCost(unsigned Opcode, VectorType *Ty,
+											   Optional<FastMathFlags> FMF,
+											   TTI::TargetCostKind CostKind) {
+		// for now: penalize reduction ops (TODO: all fadd/fmul to combine to dot product)
+		return crtp_base_class::getArithmeticReductionCost(Opcode, Ty, FMF, CostKind) * 1024;
+	}
+	
+protected:
+	const clang::CodeGenOptions& CodeGenOpts;
+	const clang::TargetOptions& TargetOpts;
+	const clang::LangOptions& LangOpts;
+	const Module& M;
+	const bool is_metal { false };
+	const bool is_vulkan { false };
+	
+	const LibFloorGPUSubTarget* ST {
+		LibFloorGPUSubTarget::get_instance()
+	};
+	const LibFloorGPUTargetLowering* TLI {
+		LibFloorGPUTargetLowering::get_instance(is_metal, is_vulkan)
+	};
+	
+public:
+	const LibFloorGPUSubTarget* getST() const {
+		return ST;
+	}
+	
+	const LibFloorGPUTargetLowering* getTLI() const {
+		return TLI;
+	}
+	
+};
+
+} // namespace llvm
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/FloorImage.h b/llvm/include/llvm/Transforms/LibFloor/FloorImage.h
new file mode 100644
index 000000000000..613e9c09e640
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/FloorImage.h
@@ -0,0 +1,139 @@
+//===-- FloorImage.h - base class for image transformations------*- C++ -*-===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This header file and class define and implement the base class for all
+// image transformations (CUDA and opaque, as used for Metal, OpenCL and Vulkan).
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_FLOORIMAGE_H
+#define LLVM_TRANSFORMS_LIBFLOOR_FLOORIMAGE_H
+
+#include <algorithm>
+#include <cstdarg>
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <array>
+
+#include "llvm/Pass.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "FloorImageType.h"
+
+namespace llvm {
+	struct FloorImageBasePass : public FunctionPass, InstVisitor<FloorImageBasePass> {
+		friend class InstVisitor<FloorImageBasePass>;
+		
+		enum class IMAGE_TYPE_ID {
+			CUDA,
+			OPAQUE
+		};
+		
+		explicit FloorImageBasePass(char &ID,
+									const IMAGE_TYPE_ID& image_type_id,
+									const uint32_t& image_capabilities);
+		
+		bool runOnFunction(Function &F) override;
+		
+		using InstVisitor<FloorImageBasePass>::visit;
+		void visit(Instruction& I);
+		void visitCallBase(CallBase& CB);
+		
+		void handle_image(CallBase& CB, const StringRef& func_name);
+		void handle_image_query(CallBase& CB, const StringRef& func_name);
+		
+		virtual void handle_read_image(Instruction& I,
+									   const StringRef& func_name,
+									   llvm::Value* img_handle_arg,
+									   const COMPUTE_IMAGE_TYPE& image_type,
+									   llvm::ConstantInt* const_sampler_arg,
+									   llvm::Value* dyn_sampler_arg,
+									   llvm::Value* coord_arg,
+									   llvm::Value* layer_arg,
+									   llvm::Value* sample_arg,
+									   llvm::Value* offset_arg,
+									   const SmallVector<llvm::Value*, 3>& offset_elems,
+									   const bool is_offset,
+									   llvm::Value* lod_or_bias_arg,
+									   const bool is_lod_or_bias, // true: lod, false: bias
+									   llvm::Value* dpdx_arg,
+									   llvm::Value* dpdy_arg,
+									   const bool is_gradient,
+									   const COMPARE_FUNCTION& compare_function,
+									   llvm::Value* compare_value_arg,
+									   const bool is_compare) = 0;
+		
+		virtual void handle_write_image(Instruction& I,
+										const StringRef& func_name,
+										llvm::Value* img_handle_arg,
+										const COMPUTE_IMAGE_TYPE& full_image_type,
+										const COMPUTE_IMAGE_TYPE& image_type,
+										const COMPUTE_IMAGE_TYPE& format_type,
+										const COMPUTE_IMAGE_TYPE& data_type,
+										llvm::Value* rt_image_type,
+										const bool& is_normalized,
+										const uint32_t& image_channel_count,
+										llvm::Value* coord_arg,
+										llvm::Value* layer_arg,
+										llvm::Value* lod_arg,
+										const bool is_lod,
+										llvm::Value* data_arg) = 0;
+		
+		virtual void handle_get_image_dim(Instruction& I,
+										  const StringRef& func_name,
+										  llvm::Value* img_handle_arg,
+										  const COMPUTE_IMAGE_TYPE& full_image_type,
+										  const COMPUTE_IMAGE_TYPE& image_type,
+										  llvm::Value* lod_arg) = 0;
+		
+	protected:
+		const IMAGE_TYPE_ID image_type_id;
+		const char* image_read_prefix;
+		const char* image_write_prefix;
+		const char* image_get_dim_prefix;
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		Instruction* alloca_insert { nullptr };
+		std::shared_ptr<IRBuilder<>> builder;
+		bool was_modified { false };
+		bool is_fragment_shader { false };
+		bool is_metal { false };
+		bool is_metal_2_3 { false };
+		bool is_metal_2_4 { false };
+		IMAGE_CAPABILITY image_capabilities { IMAGE_CAPABILITY::NONE };
+		
+		llvm::AttributeList nounwind_readnone_attr;
+		llvm::AttributeList nounwind_attr;
+		
+		// depth compare s/w emulation if not supported by backend h/w or s/w
+		void emulate_depth_compare(llvm::Value*& dst_vec,
+								   llvm::Value* tex_value,
+								   const COMPARE_FUNCTION& compare_function,
+								   llvm::Value* compare_value_arg);
+		
+	};
+}
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/FloorImageType.h b/llvm/include/llvm/Transforms/LibFloor/FloorImageType.h
new file mode 100644
index 000000000000..3f424ae4ce62
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/FloorImageType.h
@@ -0,0 +1,348 @@
+//===-- FloorImageType.h - base class for image transformations--*- C++ -*-===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This header file and class define and implement the base class for all
+// image transformations (CUDA and opaque, as used for Metal, OpenCL and Vulkan).
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_FLOORIMAGETYPE_H
+#define LLVM_TRANSFORMS_LIBFLOOR_FLOORIMAGETYPE_H
+
+#include <cstdarg>
+#include <cstdint>
+#include <type_traits>
+
+// condensed version of the COMPUTE_IMAGE_TYPE defined by floor
+enum class COMPUTE_IMAGE_TYPE : uint64_t {
+	//! invalid/uninitialized
+	NONE					= (0ull),
+	
+	//////////////////////////////////////////
+	// -> image flags and types
+	
+	//! bits 60-63: extended type flags
+	__EXT_FLAG_MASK			= (0xF000'0000'0000'0000ull),
+	__EXT_FLAG_SHIFT		= (60ull),
+	//! extended type: in combination with FLAG_MSAA, an MSAA image can be made transient, i.e. does not need to be stored in memory
+	//! NOTE: only applicable for Metal and Vulkan
+	FLAG_TRANSIENT			= (1ull << (__EXT_FLAG_SHIFT + 0ull)),
+	__UNUSED_EXT_FLAG_1		= (1ull << (__EXT_FLAG_SHIFT + 1ull)),
+	__UNUSED_EXT_FLAG_2		= (1ull << (__EXT_FLAG_SHIFT + 2ull)),
+	__UNUSED_EXT_FLAG_3		= (1ull << (__EXT_FLAG_SHIFT + 3ull)),
+	
+	//! bits 38-59: unused
+	
+	//! bits 35-37: anisotropy (stored as power-of-two)
+	__ANISOTROPY_MASK		= (0x0000'0038'0000'0000ull),
+	__ANISOTROPY_SHIFT		= (35ull),
+	ANISOTROPY_1			= (0ull << __ANISOTROPY_SHIFT),
+	ANISOTROPY_2			= (1ull << __ANISOTROPY_SHIFT),
+	ANISOTROPY_4			= (2ull << __ANISOTROPY_SHIFT),
+	ANISOTROPY_8			= (3ull << __ANISOTROPY_SHIFT),
+	ANISOTROPY_16			= (4ull << __ANISOTROPY_SHIFT),
+	
+	//! bits 32-34: multi-sampling sample count (stored as power-of-two)
+	__SAMPLE_COUNT_MASK		= (0x0000'0007'0000'0000ull),
+	__SAMPLE_COUNT_SHIFT	= (32ull),
+	SAMPLE_COUNT_1			= (0ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_2			= (1ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_4			= (2ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_8			= (3ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_16			= (4ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_32			= (5ull << __SAMPLE_COUNT_SHIFT),
+	SAMPLE_COUNT_64			= (6ull << __SAMPLE_COUNT_SHIFT),
+	
+	//! bits 20-31: type flags
+	__FLAG_MASK				= (0x0000'0000'FFFC'0000ull),
+	__FLAG_SHIFT			= (20ull),
+	//! base type: image is an array (aka has layers)
+	FLAG_ARRAY				= (1ull << (__FLAG_SHIFT + 0ull)),
+	//! base type: image is a buffer object
+	FLAG_BUFFER				= (1ull << (__FLAG_SHIFT + 1ull)),
+	//! base type: image uses mutli-sampling (consists of multiple samples)
+	FLAG_MSAA				= (1ull << (__FLAG_SHIFT + 2ull)),
+	//! base type: image is a cube map
+	FLAG_CUBE				= (1ull << (__FLAG_SHIFT + 3ull)),
+	//! base type: image is a depth image
+	FLAG_DEPTH				= (1ull << (__FLAG_SHIFT + 4ull)),
+	//! base type: image is a stencil image
+	FLAG_STENCIL			= (1ull << (__FLAG_SHIFT + 5ull)),
+	//! base type: image is a render target (Metal) / renderbuffer (OpenGL) / framebuffer attachment (Vulkan)
+	//! NOTE: only applicable when using OpenGL sharing, Metal or Vulkan
+	FLAG_RENDER_TARGET		= (1ull << (__FLAG_SHIFT + 6ull)),
+	//! optional type: image uses mip-mapping, i.e. has multiple LODs
+	FLAG_MIPMAPPED			= (1ull << (__FLAG_SHIFT + 7ull)),
+	//! optional type: image uses a fixed channel count
+	//! NOTE: only used internally, serves no purpose on the user-side
+	FLAG_FIXED_CHANNELS		= (1ull << (__FLAG_SHIFT + 8ull)),
+	//! optional type: image uses gather sampling (aka tld4/fetch4)
+	FLAG_GATHER				= (1ull << (__FLAG_SHIFT + 9ull)),
+	//! optional type: when using integer storage formats, the data is normalized in [0, 1]
+	FLAG_NORMALIZED			= (1ull << (__FLAG_SHIFT + 10ull)),
+	//! optional type: image data contains sRGB data
+	FLAG_SRGB				= (1ull << (__FLAG_SHIFT + 11ull)),
+	
+	//! bits 18-19: channel layout
+	__LAYOUT_MASK			= (0x0000'0000'000C'0000ull),
+	__LAYOUT_SHIFT			= (18ull),
+	LAYOUT_RGBA				= (0ull << __LAYOUT_SHIFT),
+	LAYOUT_BGRA				= (1ull << __LAYOUT_SHIFT),
+	LAYOUT_ABGR				= (2ull << __LAYOUT_SHIFT),
+	LAYOUT_ARGB				= (3ull << __LAYOUT_SHIFT),
+	//! layout convenience aliases
+	LAYOUT_R				= LAYOUT_RGBA,
+	LAYOUT_RG				= LAYOUT_RGBA,
+	LAYOUT_RGB				= LAYOUT_RGBA,
+	LAYOUT_BGR				= LAYOUT_BGRA,
+	
+	//! bits 16-17: dimensionality
+	//! NOTE: cube maps and arrays use the dimensionality of their underlying image data
+	//!       -> 2D for cube maps, 2D for 2D arrays, 1D for 1D arrays
+	__DIM_MASK				= (0x0000'0000'0003'0000ull),
+	__DIM_SHIFT				= (16ull),
+	DIM_1D					= (1ull << __DIM_SHIFT),
+	DIM_2D					= (2ull << __DIM_SHIFT),
+	DIM_3D					= (3ull << __DIM_SHIFT),
+	
+	//! bits 14-15: channel count
+	__CHANNELS_MASK			= (0x0000'0000'0000'C000ull),
+	__CHANNELS_SHIFT		= (14ull),
+	CHANNELS_1				= (0ull << __CHANNELS_SHIFT),
+	CHANNELS_2				= (1ull << __CHANNELS_SHIFT),
+	CHANNELS_3				= (2ull << __CHANNELS_SHIFT),
+	CHANNELS_4				= (3ull << __CHANNELS_SHIFT),
+	//! channel convenience aliases
+	R 						= CHANNELS_1,
+	RG 						= CHANNELS_2,
+	RGB 					= CHANNELS_3,
+	RGBA					= CHANNELS_4,
+	
+	//! bits 12-13: storage data type
+	__DATA_TYPE_MASK		= (0x0000'0000'0000'3000ull),
+	__DATA_TYPE_SHIFT		= (12ull),
+	INT						= (1ull << __DATA_TYPE_SHIFT),
+	UINT					= (2ull << __DATA_TYPE_SHIFT),
+	FLOAT					= (3ull << __DATA_TYPE_SHIFT),
+	
+	//! bits 10-11: access qualifier
+	__ACCESS_MASK			= (0x0000'0000'0000'0C00ull),
+	__ACCESS_SHIFT			= (10ull),
+	//! image is read-only (exluding host operations)
+	READ					= (1ull << __ACCESS_SHIFT),
+	//! image is write-only (exluding host operations)
+	WRITE					= (2ull << __ACCESS_SHIFT),
+	//! image is read-write
+	//! NOTE: also applies if neither is set
+	READ_WRITE				= (READ | WRITE),
+	
+	//! bits 6-9: compressed formats
+	__COMPRESSION_MASK		= (0x0000'0000'0000'03C0),
+	__COMPRESSION_SHIFT		= (6ull),
+	//! image data is not compressed
+	UNCOMPRESSED			= (0ull << __COMPRESSION_SHIFT),
+	//! S3TC/DXTn
+	BC1						= (1ull << __COMPRESSION_SHIFT),
+	BC2						= (2ull << __COMPRESSION_SHIFT),
+	BC3						= (3ull << __COMPRESSION_SHIFT),
+	//! RGTC1/RGTC2
+	RGTC					= (4ull << __COMPRESSION_SHIFT),
+	BC4						= RGTC,
+	BC5						= RGTC,
+	//! BPTC/BPTC_FLOAT
+	BPTC					= (5ull << __COMPRESSION_SHIFT),
+	BC6H					= BPTC,
+	BC7						= BPTC,
+	//! PVRTC
+	PVRTC					= (6ull << __COMPRESSION_SHIFT),
+	//! PVRTC2
+	PVRTC2					= (7ull << __COMPRESSION_SHIFT),
+	//! EAC/ETC1
+	EAC						= (8ull << __COMPRESSION_SHIFT),
+	ETC1					= EAC,
+	//! ETC2
+	ETC2					= (9ull << __COMPRESSION_SHIFT),
+	//! ASTC
+	ASTC					= (10ull << __COMPRESSION_SHIFT),
+	
+	//! bits 0-5: formats
+	//! NOTE: unless specified otherwise, a format is usable with any channel count
+	//! NOTE: not all backends support all formats (for portability, stick to 8-bit/16-bit/32-bit)
+	//! NOTE: channel layout / order is determined by LAYOUT_* -> bit/channel order in here can be different to the actual layout
+	__FORMAT_MASK			= (0x0000'0000'0000'003Full),
+	//! 1 bit per channel
+	FORMAT_1				= (1ull),
+	//! 2 bits per channel
+	FORMAT_2				= (2ull),
+	//! 3 channel format: 3-bit/3-bit/2-bit
+	FORMAT_3_3_2			= (3ull),
+	//! 4 bits per channel or YUV444
+	FORMAT_4				= (4ull),
+	//! YUV420
+	FORMAT_4_2_0			= (5ull),
+	//! YUV411
+	FORMAT_4_1_1			= (6ull),
+	//! YUV422
+	FORMAT_4_2_2			= (7ull),
+	//! 3 channel format: 5-bit/5-bit/5-bit
+	FORMAT_5_5_5			= (8ull),
+	//! 4 channel format: 5-bit/5-bit/5-bit/1-bit
+	FORMAT_5_5_5_ALPHA_1	= (9ull),
+	//! 3 channel format: 5-bit/6-bit/5-bit
+	FORMAT_5_6_5			= (10ull),
+	//! 8 bits per channel
+	FORMAT_8				= (11ull),
+	//! 3 channel format: 9-bit/9-bit/9-bit (5-bit exp)
+	FORMAT_9_9_9_EXP_5		= (12ull),
+	//! 3 or 4 channel format: 10-bit/10-bit/10-bit(/10-bit)
+	FORMAT_10				= (13ull),
+	//! 4 channel format: 10-bit/10-bit/10-bit/2-bit
+	FORMAT_10_10_10_ALPHA_2	= (14ull),
+	//! 3 channel format: 11-bit/11-bit/10-bit
+	FORMAT_11_11_10			= (15ull),
+	//! 3 channel format: 12-bit/12-bit/12-bit
+	FORMAT_12_12_12			= (16ull),
+	//! 4 channel format: 12-bit/12-bit/12-bit/12-bit
+	FORMAT_12_12_12_12		= (17ull),
+	//! 16 bits per channel
+	FORMAT_16				= (18ull),
+	//! 2 channel format: 16-bit/8-bit
+	FORMAT_16_8				= (19ull),
+	//! 1 channel format: 24-bit
+	FORMAT_24				= (20ull),
+	//! 2 channel format: 24-bit/8-bit
+	FORMAT_24_8				= (21ull),
+	//! 32 bits per channel
+	FORMAT_32				= (22ull),
+	//! 2 channel format: 32-bit/8-bit
+	FORMAT_32_8				= (23ull),
+	//! 64 bits per channel
+	FORMAT_64				= (24ull),
+	__FORMAT_MAX			= FORMAT_64,
+	
+	//////////////////////////////////////////
+	// -> base image types
+	//! 1D image
+	IMAGE_1D				= DIM_1D,
+	//! array of 1D images
+	IMAGE_1D_ARRAY			= DIM_1D | FLAG_ARRAY,
+	//! 1D image buffer (special format on some platforms)
+	IMAGE_1D_BUFFER			= DIM_1D | FLAG_BUFFER,
+	
+	//! 2D image
+	IMAGE_2D				= DIM_2D,
+	//! array of 2D images
+	IMAGE_2D_ARRAY			= DIM_2D | FLAG_ARRAY,
+	//! multi-sampled 2D image
+	IMAGE_2D_MSAA			= DIM_2D | FLAG_MSAA,
+	//! array of multi-sampled 2D images
+	IMAGE_2D_MSAA_ARRAY		= DIM_2D | FLAG_MSAA | FLAG_ARRAY,
+	
+	//! cube map image
+	IMAGE_CUBE				= DIM_2D | FLAG_CUBE,
+	//! array of cube map images
+	IMAGE_CUBE_ARRAY		= DIM_2D | FLAG_CUBE | FLAG_ARRAY,
+	
+	//! 2D depth image
+	IMAGE_DEPTH				= FLAG_DEPTH | CHANNELS_1 | IMAGE_2D,
+	//! combined 2D depth + stencil image
+	IMAGE_DEPTH_STENCIL		= FLAG_DEPTH | CHANNELS_2 | IMAGE_2D | FLAG_STENCIL,
+	//! array of 2D depth images
+	IMAGE_DEPTH_ARRAY		= FLAG_DEPTH | CHANNELS_1 | IMAGE_2D_ARRAY,
+	//! depth cube map image
+	IMAGE_DEPTH_CUBE		= FLAG_DEPTH | CHANNELS_1 | IMAGE_CUBE,
+	//! array of depth cube map images
+	IMAGE_DEPTH_CUBE_ARRAY	= FLAG_DEPTH | CHANNELS_1 | IMAGE_CUBE | FLAG_ARRAY,
+	//! multi-sampled 2D depth image
+	IMAGE_DEPTH_MSAA		= FLAG_DEPTH | CHANNELS_1 | IMAGE_2D_MSAA,
+	//! array of multi-sampled 2D depth images
+	IMAGE_DEPTH_MSAA_ARRAY	= FLAG_DEPTH | CHANNELS_1 | IMAGE_2D_MSAA_ARRAY,
+	
+	//! 3D image
+	IMAGE_3D				= DIM_3D,
+	
+	//
+	BASE_TYPE_MASK			= (__DIM_MASK |
+							   FLAG_ARRAY | FLAG_BUFFER | FLAG_CUBE | FLAG_DEPTH | FLAG_MSAA | FLAG_STENCIL),
+	
+};
+__attribute__((always_inline, used)) static constexpr COMPUTE_IMAGE_TYPE operator|(const COMPUTE_IMAGE_TYPE& e0,
+																				   const COMPUTE_IMAGE_TYPE& e1) {
+	return (COMPUTE_IMAGE_TYPE)((typename std::underlying_type<COMPUTE_IMAGE_TYPE>::type)e0 |
+								(typename std::underlying_type<COMPUTE_IMAGE_TYPE>::type)e1);
+}
+__attribute__((always_inline, used)) static constexpr COMPUTE_IMAGE_TYPE operator&(const COMPUTE_IMAGE_TYPE& e0,
+																			 const COMPUTE_IMAGE_TYPE& e1) {
+	return (COMPUTE_IMAGE_TYPE)((typename std::underlying_type<COMPUTE_IMAGE_TYPE>::type)e0 &
+								(typename std::underlying_type<COMPUTE_IMAGE_TYPE>::type)e1);
+}
+template <COMPUTE_IMAGE_TYPE flag, typename int_type = typename std::underlying_type<COMPUTE_IMAGE_TYPE>::type>
+__attribute__((always_inline, used)) static constexpr bool has_flag(const COMPUTE_IMAGE_TYPE& enum_object) {
+	return ((int_type(flag) & int_type(enum_object)) == int_type(flag));
+}
+
+//! returns the dimensionality of the specified image type
+__attribute__((always_inline, used)) static constexpr uint32_t image_dim_count(const COMPUTE_IMAGE_TYPE& image_type) {
+	return uint32_t(image_type & COMPUTE_IMAGE_TYPE::__DIM_MASK) >> uint32_t(COMPUTE_IMAGE_TYPE::__DIM_SHIFT);
+}
+
+// compare function used by depth compare reads
+enum class COMPARE_FUNCTION : uint32_t {
+	NEVER				= 0u,
+	LESS				= 1u,
+	EQUAL				= 2u,
+	LESS_OR_EQUAL		= 3u,
+	GREATER				= 4u,
+	NOT_EQUAL			= 5u,
+	GREATER_OR_EQUAL	= 6u,
+	ALWAYS				= 7u,
+	__MAX_COMPARE_FUNCTION
+};
+
+// device image capabilities
+enum class IMAGE_CAPABILITY : uint32_t {
+	NONE					= (0u),
+	BASIC					= (1u << 0u),
+	
+	DEPTH_READ				= (1u << 1u),
+	DEPTH_WRITE				= (1u << 2u),
+	MSAA_READ				= (1u << 3u),
+	MSAA_WRITE				= (1u << 4u),
+	MSAA_ARRAY_READ			= (1u << 5u),
+	MSAA_ARRAY_WRITE		= (1u << 6u),
+	CUBE_READ				= (1u << 7u),
+	CUBE_WRITE				= (1u << 8u),
+	CUBE_ARRAY_READ			= (1u << 9u),
+	CUBE_ARRAY_WRITE		= (1u << 10u),
+	MIPMAP_READ				= (1u << 11u),
+	MIPMAP_WRITE			= (1u << 12u),
+	OFFSET_READ				= (1u << 13u),
+	OFFSET_WRITE			= (1u << 14u),
+	
+	DEPTH_COMPARE			= (1u << 16u),
+	GATHER					= (1u << 17u),
+	READ_WRITE				= (1u << 18u),
+};
+template <IMAGE_CAPABILITY flag, typename int_type = typename std::underlying_type<IMAGE_CAPABILITY>::type>
+__attribute__((always_inline, used)) static constexpr bool has_flag(const IMAGE_CAPABILITY& enum_object) {
+	return ((int_type(flag) & int_type(enum_object)) == int_type(flag));
+}
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/StructuralAnalysis.h b/llvm/include/llvm/Transforms/LibFloor/StructuralAnalysis.h
new file mode 100644
index 000000000000..6cce99340608
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/StructuralAnalysis.h
@@ -0,0 +1,324 @@
+//===- StructuralAnalysis.h - ---------------------------------------------===//
+//
+// Copyright (c) 2015, Computer Architecture and Systems Laboratory at Georgia Tech
+// Copyright (c) 2016 - 2017, Florian Ziesche (LLVM port + general fixes/cleanup)
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice, this
+//   list of conditions and the following disclaimer.
+//
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+//
+// * Neither the name of gpuocelot nor the names of its
+//   contributors may be used to endorse or promote products derived from
+//   this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE
+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+//===----------------------------------------------------------------------===//
+// \author  Haicheng Wu <hwu36@gatech.edu>
+// \date    Monday April 4, 2011
+// \brief   The header file for the StructuralAnalysis pass.
+//===----------------------------------------------------------------------===//
+//
+// This file defines the class of Structural Analysis which will return the
+// control tree and unstructured branches of a function
+//
+// ref: "Using Hammock Graphs to Structure Programs",
+// Fubo Zhang and Erik H. DHollander
+// -> https://biblio.ugent.be/publication/291746/file/451220
+//
+// ref: "Characterization and Transformation of Unstructured Control Flow in GPU
+// Applications", Haicheng Wu, Gregory Diamos, Si Li, and Sudhakar Yalamanchili
+// -> http://www.gdiamos.net/papers/caches-paper.pdf
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_STRUCTURALANALYSIS_H
+#define LLVM_TRANSFORMS_LIBFLOOR_STRUCTURALANALYSIS_H
+
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/PostDominators.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/raw_ostream.h"
+
+#include <map>
+#include <set>
+#include <unordered_map>
+#include <unordered_set>
+#include <vector>
+
+// TODO: modernize/c++ify this
+
+namespace llvm {
+
+// StructuralAnalysis - This class holds all the methods and data structures
+class StructuralAnalysis {
+public:
+  enum EdgeClass { TREE, FORWARD, BACK, CROSS };
+  typedef std::pair<BasicBlock *, BasicBlock *> EdgeLLVMTy;
+  typedef std::vector<EdgeLLVMTy> EdgeVecTy;
+  typedef std::vector<BasicBlock *> BBVecTy;
+
+  // Types defined in Fig 7.38 of Muchnick book
+  enum RegionTy {
+    Nil,
+    Block,
+    IfThen,
+    IfThenElse,
+    Case,
+    SelfLoop,
+    NaturalLoop,
+    Improper,
+    Unreachable
+  };
+
+  // NodeTy - This type is used for the CFG node
+  struct NodeTy {
+    // Whether it is an original or combined from original
+    bool isCombined{false};
+    // Map to the corresponding BasicBlock* if it is original
+    BasicBlock *BB{nullptr};
+    std::vector<NodeTy *> predNode; // Predecessor of the node
+    std::vector<NodeTy *> succNode; // Successor of the node
+    // If isCombined is true, it points to the entry of the nodeset
+    NodeTy *entryNode{nullptr};
+    NodeTy *parentNode{nullptr};     // Parent Node in Control Tree
+    std::vector<NodeTy *> childNode; // Child Nodes in Control Tree
+    EdgeVecTy outgoingBR;            // Outgoing Branches of a loop
+    EdgeVecTy incomingBR;            // Incoming Branches of a loop
+    BasicBlock *entryBB{nullptr};    // The entry Basic Block
+    BasicBlock *exitBB{nullptr};     // The exit Basic Block
+    BBVecTy containedBB;             // BasicBlock*s contained in this node
+    EdgeVecTy incomingForwardBR;     // The shared code of unstructured branch
+    RegionTy nodeType{Nil};          // The type of the node
+    bool isLoopHeader{false};
+    bool isBackEdge{false};
+    NodeTy *loopExitNode{nullptr};
+
+    void remove_predecessor(NodeTy *pred) {
+      predNode.erase(std::remove(predNode.begin(), predNode.end(), pred),
+                     predNode.end());
+    }
+    bool has_predecessor(NodeTy *pred) const {
+      const auto iter = std::find(predNode.cbegin(), predNode.cend(), pred);
+      return (iter != predNode.cend());
+    }
+    bool add_predecessor(NodeTy *pred) {
+      if (has_predecessor(pred)) {
+        return false;
+      }
+      predNode.emplace_back(pred);
+      return true;
+    }
+    void remove_successor(NodeTy *succ) {
+      succNode.erase(std::remove(succNode.begin(), succNode.end(), succ),
+                     succNode.end());
+    }
+    bool has_successor(NodeTy *succ) const {
+      const auto iter = std::find(succNode.cbegin(), succNode.cend(), succ);
+      return (iter != succNode.cend());
+    }
+    bool add_successor(NodeTy *succ) {
+      if (has_successor(succ)) {
+        return false;
+      }
+      succNode.emplace_back(succ);
+      return true;
+    }
+
+    ~NodeTy();
+  };
+
+  // NodeVecTy - used to holds nodes in a set
+  typedef std::vector<NodeTy *> NodeVecTy;
+  typedef std::unordered_map<BasicBlock *, NodeTy *> BB2NodeMapTy;
+  typedef std::pair<NodeTy *, NodeTy *> EdgeTy;
+  typedef std::map<EdgeTy, EdgeClass> Edge2ClassMapTy;
+  typedef std::set<EdgeTy> EdgeSetTy;
+  typedef std::set<NodeTy *> VisitSetTy;
+
+public:
+  ~StructuralAnalysis();
+  void analyze(Function &F);
+
+  // Get a text representation of the analysis
+  void write(llvm::raw_ostream &stream) const;
+
+public:
+  NodeVecTy Net;
+
+  // unstructuredBRVec - store the detected unstructured branches
+  EdgeVecTy unstructuredBRVec;
+
+  // BB2NodeMap - This var is used to find the Node from BasicBlock*
+  BB2NodeMapTy BB2NodeMap;
+
+  NodeVecTy unreachableNodeSet;
+
+private:
+  Function *_function;
+
+  // postorder traversal of the flowgraph
+  uint32_t postCtr, postMax, preMax;
+  std::map<uint32_t, NodeTy *> post;
+  VisitSetTy visit, visitPath;
+  std::map<NodeTy *, uint32_t> preTree, postTree;
+
+  // edge2ClassMap - map the edge to its class
+  Edge2ClassMapTy edge2ClassMap;
+
+public:
+  // buildSimpleCFG - Build a Simple CFG out of the LLVM CFG
+  void buildSimpleCFG(NodeVecTy &Nodes);
+
+  // structuralAnalysis - Follow Fig 7.39 of Muchnick book
+  void structuralAnalysis(NodeVecTy &Nodes, NodeTy *entry);
+
+  // DFSPostorder - Follow Fig 7.40 of Muchnick book
+  void DFSPostorder(const NodeVecTy &Nodes, NodeTy *x);
+
+  // acyclicRegionType - Follow Fig 7.41 of Muchnick book
+  RegionTy acyclicRegionType(const NodeVecTy &Nodes, NodeTy *node,
+                             NodeVecTy &nset, NodeTy **entryNode,
+                             NodeTy **exitNode, NodeTy *entry);
+
+  // cyclicRegionType - Follow Fig 7.42 of Muchnick book
+  RegionTy cyclicRegionType(const NodeVecTy &Nodes, NodeVecTy &nset,
+                            NodeTy *loopHeaderNode, NodeTy *backEdgeNode,
+                            NodeTy **exitNode, NodeTy *entry);
+
+  // reduce - Follow Fig 7.43 of Muchnick book
+  NodeTy *reduce(NodeVecTy &N, RegionTy rType, NodeVecTy &nodeSet,
+                 NodeTy *entryNode, NodeTy *exitNode);
+
+  // replace - Follow Fig 7.44 of Muchnick book
+  void replace(NodeVecTy &N, NodeTy *node, NodeVecTy &nodeSet);
+
+  // isImproper - Follow Fig 7.45 of Muchnick book
+  bool isImproper(const NodeVecTy &Nodes, NodeVecTy &nset,
+                  NodeTy *loopHeaderNode, NodeTy *backEdgeNode,
+                  NodeTy **exitNode, NodeTy *entry);
+
+  // pathBack - Check if there is a node k such that there is a path from
+  // m to k that does not pass through n and an edge k->n that is a back edge
+  NodeTy *pathBack(NodeTy *n, NodeVecTy &N, NodeVecTy &reachUnder);
+
+  // isCaseWithDefault - Check if node leads a case block
+  bool isCaseWithDefault(const NodeVecTy &Nodes, NodeTy *entryNode,
+                         NodeTy **exitNode, NodeTy *entry);
+
+  // isCaseWithoutDefault - Check if node leads a case block
+  bool isCaseWithoutDefault(const NodeVecTy &Nodes, NodeTy *entryNode,
+                            NodeTy **exitNode, NodeTy *entry);
+
+  // isImproperCaseWithDefault - Check if node leads
+  // a case block with incoming edges
+  bool isImproperCaseWithDefault(const NodeVecTy &Nodes, NodeTy *entryNode,
+                                 NodeTy *entry);
+
+  // isImproperCaseoutWithDefault - Check if node leads
+  //	a case block with incoming edges
+  bool isImproperCaseWithoutDefault(const NodeVecTy &Nodes, NodeTy *entryNode,
+                                    NodeTy **exitNode, NodeTy *entry);
+
+  // path(n, m, I) - Return true if there is a path from from n to m
+  // such that all the nodes in it are in I and false otherwise
+  bool path(NodeTy *n, NodeTy *m, const NodeVecTy &Nodes, NodeTy *esc);
+
+  // path(n, m, I, src, dst ) - Return true if there is a path from from
+  // n to m such that all the nodes in it are in I without going through edge
+  // src->dst and false otherwise
+  bool path(NodeTy *n, NodeTy *m, const NodeVecTy &Nodes, NodeTy *src,
+            NodeTy *dst);
+
+  // compact - Compact nodes in nset into n;
+  void compact(NodeVecTy &N, NodeTy *n, NodeVecTy &nset);
+
+  // mapNode2BB - Return the corresponding BasicBlock* of the node
+  BasicBlock *mapNode2BB(const NodeTy *node) const;
+
+  // mapBB2Node - Return the corresponding sturcture node of the basic block
+  NodeTy *mapBB2Node(BasicBlock *bb);
+
+  // dumpCTNode - Dump one Control Node
+  void dumpCTNode(llvm::raw_ostream &stream, NodeTy *n) const;
+
+  // dumpNode - Dump one node
+  void dumpNode(llvm::raw_ostream &stream, NodeTy *node) const;
+
+  // findUnstructuredBR - Record the branch and remove it from CFG
+  void findUnstructuredBR(NodeTy *srcNode, NodeTy *dstNode,
+                          bool needForwardCopy, bool isGoto);
+
+  // findBB - put all Basic Blocks in node into nodeVec
+  void findBB(NodeTy *node, BBVecTy &nodeVec) const;
+
+  // findEntryBB - find the entry Basic Block of the node
+  BasicBlock *findEntryBB(NodeTy *node);
+
+  // dumpUnstructuredBR - Dump all found unstructured branches
+  void dumpUnstructuredBR(llvm::raw_ostream &stream) const;
+
+  // dump all nodes/edges that need a forward copy
+  void dumpForwardCopy(llvm::raw_ostream &stream) const;
+
+  // dumps the CFG of the specified function as a .dot file
+  static void dumpCFGDot(const std::string &filename, const Function &F);
+
+  // dumps the IR of the specified function as a .ll file
+  static void dumpIR(const std::string &filename, const Function &F);
+
+  // isStillReachableFrom entry -Return true if after erasing
+  // edge src->dst, dst is still reachable from entry
+  bool isStillReachableFromEntry(const NodeVecTy &Nodes, NodeTy *entry,
+                                 NodeTy *dstNode, NodeTy *srcNode);
+
+  // clean - fill in the element of incoming branches and outgoing branches
+  void cleanup(NodeTy *node);
+
+  void cleanupUnreachable();
+
+  void reconstructUnreachable();
+
+  // deleteUnreachableNode - delete nodes that are no longer reachable from the
+  // entry
+  void deleteUnreachableNodes(NodeVecTy &Nodes, NodeTy *entry);
+
+  bool checkUnique(EdgeVecTy &edgeVec, BasicBlock *srcBB, BasicBlock *dstBB);
+
+  // returns true if "Nodes" contains "node"
+  static bool containsNode(const NodeVecTy &Nodes, const NodeTy *node);
+
+  // returns true if "BBs" contains "BB"
+  static bool containsBB(const BBVecTy &BBs, const BasicBlock *BB);
+
+  // inserts "node" into "Nodes", returns true if successful, false if it
+  // already exists
+  static bool insertNode(NodeVecTy &Nodes, NodeTy *node);
+
+  // removes "node" from "Nodes", returns true if successful
+  static bool eraseNode(NodeVecTy &Nodes, NodeTy *node);
+};
+}
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/StructuralTransform.h b/llvm/include/llvm/Transforms/LibFloor/StructuralTransform.h
new file mode 100644
index 000000000000..d4dafc581c8e
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/StructuralTransform.h
@@ -0,0 +1,165 @@
+//===- StructuralTransform.h - --------------------------------------------===//
+//
+// Copyright (c) 2015, Computer Architecture and Systems Laboratory at Georgia Tech
+// Copyright (c) 2016 - 2017, Florian Ziesche (LLVM port + general fixes/cleanup)
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice, this
+//   list of conditions and the following disclaimer.
+//
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+//
+// * Neither the name of gpuocelot nor the names of its
+//   contributors may be used to endorse or promote products derived from
+//   this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE
+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+//===----------------------------------------------------------------------===//
+// \author  Haicheng Wu <hwu36@gatech.edu>
+// \date    Monday April 4, 2011
+// \brief   The header file for the StructuralTransform pass.
+//===----------------------------------------------------------------------===//
+//
+// This file implements an Structural Transform based on Zhang's paper
+//
+// ref: "Using Hammock Graphs to Structure Programs",
+// Fubo Zhang and Erik H. DHollander
+// -> https://biblio.ugent.be/publication/291746/file/451220
+//
+// ref: "Characterization and Transformation of Unstructured Control Flow in GPU
+// Applications", Haicheng Wu, Gregory Diamos, Si Li, and Sudhakar Yalamanchili
+// -> http://www.gdiamos.net/papers/caches-paper.pdf
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_STRUCTURALTRANSFORM_H
+#define LLVM_TRANSFORMS_LIBFLOOR_STRUCTURALTRANSFORM_H
+
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/PostDominators.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+
+#include "StructuralAnalysis.h"
+
+#include <list>
+#include <unordered_map>
+
+namespace llvm {
+
+/// StructuralTransform - This class holds all the methods and data structures
+class StructuralTransform {
+public:
+  typedef StructuralAnalysis::BBVecTy BBVecTy;
+  typedef StructuralAnalysis::NodeTy NodeTy;
+
+public:
+  bool transform(Function &F);
+
+private:
+  struct BBMapper {
+    std::vector<std::pair<BasicBlock *, BasicBlock *>> bb_map;
+
+    BasicBlock *&operator[](const BasicBlock *key) {
+      for (auto &elem : bb_map) {
+        if (elem.first == key) {
+          return elem.second;
+        }
+      }
+      bb_map.emplace_back(const_cast<BasicBlock*>(key), (BasicBlock *)nullptr);
+      return bb_map.back().second;
+    }
+
+    auto begin() { return bb_map.begin(); }
+
+    auto cbegin() const { return bb_map.cbegin(); }
+
+    auto end() { return bb_map.end(); }
+
+    auto cend() const { return bb_map.cend(); }
+
+    auto find(const BasicBlock *bb) const {
+      for (auto iter = bb_map.begin(); iter != bb_map.end(); ++iter) {
+        if (iter->first == bb) {
+          return iter;
+        }
+      }
+      return bb_map.end();
+    }
+
+    auto find_value(const BasicBlock *bb) const {
+      for (auto iter = bb_map.begin(); iter != bb_map.end(); ++iter) {
+        if (iter->second == bb) {
+          return iter;
+        }
+      }
+      return bb_map.end();
+    }
+  };
+  BBMapper ClonedBBMap;
+  std::unordered_map<BasicBlock *, std::unique_ptr<ValueToValueMapTy>>
+      ClonedVMap;
+
+  // alloca insertion point
+  Instruction *alloca_insert{nullptr};
+
+  // insertion point for condition initialization
+  Instruction *condition_init_insert{nullptr};
+
+  // type used when creating new conditions (i1)
+  llvm::Type *condition_type{nullptr};
+
+  Function *_function;
+
+  // Algorithm 2 of Zhang's paper -- elimination of outgoing branches
+  bool Cut(NodeTy *N);
+
+  // Algorithm 3 of Zhang's paper -- elimination of backward branches
+  bool BackwardCopy(NodeTy *N);
+
+  // Algorithm 4 of Zhang's paper -- elimination of Forward branches
+  bool ForwardCopy(NodeTy *N, uint32_t level = 0);
+
+  bool stopCut;
+
+  StructuralAnalysis SA;
+
+  /// Get iterator to the basic block in the cfg
+  const BasicBlock *bb(NodeTy *node) const;
+
+  typedef std::list<const NodeTy *> NodeListTy;
+
+  /// Get the children (returns an ordered container)
+  const NodeListTy &children(const NodeTy *node) const;
+
+  /// Get condition node from IfThen and IfThenElse.
+  /// The last instruction of the condition node should be a branch.
+  const NodeTy *cond(const NodeTy *node) const;
+  /// Get if-true node from IfThen and IfThenElse
+  const NodeTy *ifTrue(const NodeTy *node) const;
+  /// Get if-false node from IfThenElse
+  const NodeTy *ifFalse(const NodeTy *node) const;
+};
+}
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/VulkanSampling.h b/llvm/include/llvm/Transforms/LibFloor/VulkanSampling.h
new file mode 100644
index 000000000000..8941bbf852ae
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/VulkanSampling.h
@@ -0,0 +1,79 @@
+//===- VulkanSampling.h - Vulkan-specific sampler/image info --------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// TODO
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_LIBFLOOR_VULKANSAMPLING_H
+#define LLVM_TRANSFORMS_LIBFLOOR_VULKANSAMPLING_H
+
+namespace vulkan_sampling {
+	// vulkan immutable/fixed sampler type
+	// -> libfloor: device/vulkan_image.hpp
+	struct sampler {
+		enum FILTER_MODE : uint32_t {
+			__FILTER_MODE_MASK			= (0x00000001u),
+			__FILTER_MODE_SHIFT			= (0u),
+			NEAREST						= (0u << __FILTER_MODE_SHIFT),
+			LINEAR						= (1u << __FILTER_MODE_SHIFT),
+		};
+		enum ADDRESS_MODE : uint32_t {
+			__ADDRESS_MODE_MASK			= (0x00000002u),
+			__ADDRESS_MODE_SHIFT		= (1u),
+			CLAMP_TO_EDGE				= (0u << __ADDRESS_MODE_SHIFT),
+			REPEAT						= (1u << __ADDRESS_MODE_SHIFT),
+		};
+		enum COMPARE_FUNCTION : uint32_t {
+			__COMPARE_FUNCTION_MASK		= (0x0000001Cu),
+			__COMPARE_FUNCTION_SHIFT	= (2u),
+			NEVER						= (0u << __COMPARE_FUNCTION_SHIFT),
+			LESS						= (1u << __COMPARE_FUNCTION_SHIFT),
+			EQUAL						= (2u << __COMPARE_FUNCTION_SHIFT),
+			LESS_OR_EQUAL				= (3u << __COMPARE_FUNCTION_SHIFT),
+			GREATER						= (4u << __COMPARE_FUNCTION_SHIFT),
+			NOT_EQUAL					= (5u << __COMPARE_FUNCTION_SHIFT),
+			GREATER_OR_EQUAL			= (6u << __COMPARE_FUNCTION_SHIFT),
+			ALWAYS						= (7u << __COMPARE_FUNCTION_SHIFT),
+		};
+		// NOTE: this should be the MSB, because we won't actually be creating samplers for pixel addressing
+		enum COORD_MODE : uint32_t {
+			__COORD_MODE_MASK			= (0x00000020u),
+			__COORD_MODE_SHIFT			= (5u),
+			NORMALIZED					= (0u << __COORD_MODE_SHIFT),
+			PIXEL						= (1u << __COORD_MODE_SHIFT),
+		};
+		
+		uint32_t value;
+	};
+	
+	enum class LOD_TYPE : uint32_t {
+		NO_LOD = 0,
+		IMPLICIT_LOD = 1, // fragment shader only
+		IMPLICIT_LOD_WITH_BIAS = 2, // fragment shader only
+		EXPLICIT_LOD = 3,
+		GRADIENT = 4,
+		__MAX_LOD_TYPE = GRADIENT,
+		INVALID = ~0u,
+	};
+	
+}
+
+#endif
diff --git a/llvm/include/llvm/Transforms/LibFloor/cfg/LICENSE b/llvm/include/llvm/Transforms/LibFloor/cfg/LICENSE
new file mode 100644
index 000000000000..4362b49151d7
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/cfg/LICENSE
@@ -0,0 +1,502 @@
+                  GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 2.1, February 1999
+
+ Copyright (C) 1991, 1999 Free Software Foundation, Inc.
+ 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+[This is the first released version of the Lesser GPL.  It also counts
+ as the successor of the GNU Library Public License, version 2, hence
+ the version number 2.1.]
+
+                            Preamble
+
+  The licenses for most software are designed to take away your
+freedom to share and change it.  By contrast, the GNU General Public
+Licenses are intended to guarantee your freedom to share and change
+free software--to make sure the software is free for all its users.
+
+  This license, the Lesser General Public License, applies to some
+specially designated software packages--typically libraries--of the
+Free Software Foundation and other authors who decide to use it.  You
+can use it too, but we suggest you first think carefully about whether
+this license or the ordinary General Public License is the better
+strategy to use in any particular case, based on the explanations below.
+
+  When we speak of free software, we are referring to freedom of use,
+not price.  Our General Public Licenses are designed to make sure that
+you have the freedom to distribute copies of free software (and charge
+for this service if you wish); that you receive source code or can get
+it if you want it; that you can change the software and use pieces of
+it in new free programs; and that you are informed that you can do
+these things.
+
+  To protect your rights, we need to make restrictions that forbid
+distributors to deny you these rights or to ask you to surrender these
+rights.  These restrictions translate to certain responsibilities for
+you if you distribute copies of the library or if you modify it.
+
+  For example, if you distribute copies of the library, whether gratis
+or for a fee, you must give the recipients all the rights that we gave
+you.  You must make sure that they, too, receive or can get the source
+code.  If you link other code with the library, you must provide
+complete object files to the recipients, so that they can relink them
+with the library after making changes to the library and recompiling
+it.  And you must show them these terms so they know their rights.
+
+  We protect your rights with a two-step method: (1) we copyright the
+library, and (2) we offer you this license, which gives you legal
+permission to copy, distribute and/or modify the library.
+
+  To protect each distributor, we want to make it very clear that
+there is no warranty for the free library.  Also, if the library is
+modified by someone else and passed on, the recipients should know
+that what they have is not the original version, so that the original
+author's reputation will not be affected by problems that might be
+introduced by others.
+
+  Finally, software patents pose a constant threat to the existence of
+any free program.  We wish to make sure that a company cannot
+effectively restrict the users of a free program by obtaining a
+restrictive license from a patent holder.  Therefore, we insist that
+any patent license obtained for a version of the library must be
+consistent with the full freedom of use specified in this license.
+
+  Most GNU software, including some libraries, is covered by the
+ordinary GNU General Public License.  This license, the GNU Lesser
+General Public License, applies to certain designated libraries, and
+is quite different from the ordinary General Public License.  We use
+this license for certain libraries in order to permit linking those
+libraries into non-free programs.
+
+  When a program is linked with a library, whether statically or using
+a shared library, the combination of the two is legally speaking a
+combined work, a derivative of the original library.  The ordinary
+General Public License therefore permits such linking only if the
+entire combination fits its criteria of freedom.  The Lesser General
+Public License permits more lax criteria for linking other code with
+the library.
+
+  We call this license the "Lesser" General Public License because it
+does Less to protect the user's freedom than the ordinary General
+Public License.  It also provides other free software developers Less
+of an advantage over competing non-free programs.  These disadvantages
+are the reason we use the ordinary General Public License for many
+libraries.  However, the Lesser license provides advantages in certain
+special circumstances.
+
+  For example, on rare occasions, there may be a special need to
+encourage the widest possible use of a certain library, so that it becomes
+a de-facto standard.  To achieve this, non-free programs must be
+allowed to use the library.  A more frequent case is that a free
+library does the same job as widely used non-free libraries.  In this
+case, there is little to gain by limiting the free library to free
+software only, so we use the Lesser General Public License.
+
+  In other cases, permission to use a particular library in non-free
+programs enables a greater number of people to use a large body of
+free software.  For example, permission to use the GNU C Library in
+non-free programs enables many more people to use the whole GNU
+operating system, as well as its variant, the GNU/Linux operating
+system.
+
+  Although the Lesser General Public License is Less protective of the
+users' freedom, it does ensure that the user of a program that is
+linked with the Library has the freedom and the wherewithal to run
+that program using a modified version of the Library.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.  Pay close attention to the difference between a
+"work based on the library" and a "work that uses the library".  The
+former contains code derived from the library, whereas the latter must
+be combined with the library in order to run.
+
+                  GNU LESSER GENERAL PUBLIC LICENSE
+   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+  0. This License Agreement applies to any software library or other
+program which contains a notice placed by the copyright holder or
+other authorized party saying it may be distributed under the terms of
+this Lesser General Public License (also called "this License").
+Each licensee is addressed as "you".
+
+  A "library" means a collection of software functions and/or data
+prepared so as to be conveniently linked with application programs
+(which use some of those functions and data) to form executables.
+
+  The "Library", below, refers to any such software library or work
+which has been distributed under these terms.  A "work based on the
+Library" means either the Library or any derivative work under
+copyright law: that is to say, a work containing the Library or a
+portion of it, either verbatim or with modifications and/or translated
+straightforwardly into another language.  (Hereinafter, translation is
+included without limitation in the term "modification".)
+
+  "Source code" for a work means the preferred form of the work for
+making modifications to it.  For a library, complete source code means
+all the source code for all modules it contains, plus any associated
+interface definition files, plus the scripts used to control compilation
+and installation of the library.
+
+  Activities other than copying, distribution and modification are not
+covered by this License; they are outside its scope.  The act of
+running a program using the Library is not restricted, and output from
+such a program is covered only if its contents constitute a work based
+on the Library (independent of the use of the Library in a tool for
+writing it).  Whether that is true depends on what the Library does
+and what the program that uses the Library does.
+
+  1. You may copy and distribute verbatim copies of the Library's
+complete source code as you receive it, in any medium, provided that
+you conspicuously and appropriately publish on each copy an
+appropriate copyright notice and disclaimer of warranty; keep intact
+all the notices that refer to this License and to the absence of any
+warranty; and distribute a copy of this License along with the
+Library.
+
+  You may charge a fee for the physical act of transferring a copy,
+and you may at your option offer warranty protection in exchange for a
+fee.
+
+  2. You may modify your copy or copies of the Library or any portion
+of it, thus forming a work based on the Library, and copy and
+distribute such modifications or work under the terms of Section 1
+above, provided that you also meet all of these conditions:
+
+    a) The modified work must itself be a software library.
+
+    b) You must cause the files modified to carry prominent notices
+    stating that you changed the files and the date of any change.
+
+    c) You must cause the whole of the work to be licensed at no
+    charge to all third parties under the terms of this License.
+
+    d) If a facility in the modified Library refers to a function or a
+    table of data to be supplied by an application program that uses
+    the facility, other than as an argument passed when the facility
+    is invoked, then you must make a good faith effort to ensure that,
+    in the event an application does not supply such function or
+    table, the facility still operates, and performs whatever part of
+    its purpose remains meaningful.
+
+    (For example, a function in a library to compute square roots has
+    a purpose that is entirely well-defined independent of the
+    application.  Therefore, Subsection 2d requires that any
+    application-supplied function or table used by this function must
+    be optional: if the application does not supply it, the square
+    root function must still compute square roots.)
+
+These requirements apply to the modified work as a whole.  If
+identifiable sections of that work are not derived from the Library,
+and can be reasonably considered independent and separate works in
+themselves, then this License, and its terms, do not apply to those
+sections when you distribute them as separate works.  But when you
+distribute the same sections as part of a whole which is a work based
+on the Library, the distribution of the whole must be on the terms of
+this License, whose permissions for other licensees extend to the
+entire whole, and thus to each and every part regardless of who wrote
+it.
+
+Thus, it is not the intent of this section to claim rights or contest
+your rights to work written entirely by you; rather, the intent is to
+exercise the right to control the distribution of derivative or
+collective works based on the Library.
+
+In addition, mere aggregation of another work not based on the Library
+with the Library (or with a work based on the Library) on a volume of
+a storage or distribution medium does not bring the other work under
+the scope of this License.
+
+  3. You may opt to apply the terms of the ordinary GNU General Public
+License instead of this License to a given copy of the Library.  To do
+this, you must alter all the notices that refer to this License, so
+that they refer to the ordinary GNU General Public License, version 2,
+instead of to this License.  (If a newer version than version 2 of the
+ordinary GNU General Public License has appeared, then you can specify
+that version instead if you wish.)  Do not make any other change in
+these notices.
+
+  Once this change is made in a given copy, it is irreversible for
+that copy, so the ordinary GNU General Public License applies to all
+subsequent copies and derivative works made from that copy.
+
+  This option is useful when you wish to copy part of the code of
+the Library into a program that is not a library.
+
+  4. You may copy and distribute the Library (or a portion or
+derivative of it, under Section 2) in object code or executable form
+under the terms of Sections 1 and 2 above provided that you accompany
+it with the complete corresponding machine-readable source code, which
+must be distributed under the terms of Sections 1 and 2 above on a
+medium customarily used for software interchange.
+
+  If distribution of object code is made by offering access to copy
+from a designated place, then offering equivalent access to copy the
+source code from the same place satisfies the requirement to
+distribute the source code, even though third parties are not
+compelled to copy the source along with the object code.
+
+  5. A program that contains no derivative of any portion of the
+Library, but is designed to work with the Library by being compiled or
+linked with it, is called a "work that uses the Library".  Such a
+work, in isolation, is not a derivative work of the Library, and
+therefore falls outside the scope of this License.
+
+  However, linking a "work that uses the Library" with the Library
+creates an executable that is a derivative of the Library (because it
+contains portions of the Library), rather than a "work that uses the
+library".  The executable is therefore covered by this License.
+Section 6 states terms for distribution of such executables.
+
+  When a "work that uses the Library" uses material from a header file
+that is part of the Library, the object code for the work may be a
+derivative work of the Library even though the source code is not.
+Whether this is true is especially significant if the work can be
+linked without the Library, or if the work is itself a library.  The
+threshold for this to be true is not precisely defined by law.
+
+  If such an object file uses only numerical parameters, data
+structure layouts and accessors, and small macros and small inline
+functions (ten lines or less in length), then the use of the object
+file is unrestricted, regardless of whether it is legally a derivative
+work.  (Executables containing this object code plus portions of the
+Library will still fall under Section 6.)
+
+  Otherwise, if the work is a derivative of the Library, you may
+distribute the object code for the work under the terms of Section 6.
+Any executables containing that work also fall under Section 6,
+whether or not they are linked directly with the Library itself.
+
+  6. As an exception to the Sections above, you may also combine or
+link a "work that uses the Library" with the Library to produce a
+work containing portions of the Library, and distribute that work
+under terms of your choice, provided that the terms permit
+modification of the work for the customer's own use and reverse
+engineering for debugging such modifications.
+
+  You must give prominent notice with each copy of the work that the
+Library is used in it and that the Library and its use are covered by
+this License.  You must supply a copy of this License.  If the work
+during execution displays copyright notices, you must include the
+copyright notice for the Library among them, as well as a reference
+directing the user to the copy of this License.  Also, you must do one
+of these things:
+
+    a) Accompany the work with the complete corresponding
+    machine-readable source code for the Library including whatever
+    changes were used in the work (which must be distributed under
+    Sections 1 and 2 above); and, if the work is an executable linked
+    with the Library, with the complete machine-readable "work that
+    uses the Library", as object code and/or source code, so that the
+    user can modify the Library and then relink to produce a modified
+    executable containing the modified Library.  (It is understood
+    that the user who changes the contents of definitions files in the
+    Library will not necessarily be able to recompile the application
+    to use the modified definitions.)
+
+    b) Use a suitable shared library mechanism for linking with the
+    Library.  A suitable mechanism is one that (1) uses at run time a
+    copy of the library already present on the user's computer system,
+    rather than copying library functions into the executable, and (2)
+    will operate properly with a modified version of the library, if
+    the user installs one, as long as the modified version is
+    interface-compatible with the version that the work was made with.
+
+    c) Accompany the work with a written offer, valid for at
+    least three years, to give the same user the materials
+    specified in Subsection 6a, above, for a charge no more
+    than the cost of performing this distribution.
+
+    d) If distribution of the work is made by offering access to copy
+    from a designated place, offer equivalent access to copy the above
+    specified materials from the same place.
+
+    e) Verify that the user has already received a copy of these
+    materials or that you have already sent this user a copy.
+
+  For an executable, the required form of the "work that uses the
+Library" must include any data and utility programs needed for
+reproducing the executable from it.  However, as a special exception,
+the materials to be distributed need not include anything that is
+normally distributed (in either source or binary form) with the major
+components (compiler, kernel, and so on) of the operating system on
+which the executable runs, unless that component itself accompanies
+the executable.
+
+  It may happen that this requirement contradicts the license
+restrictions of other proprietary libraries that do not normally
+accompany the operating system.  Such a contradiction means you cannot
+use both them and the Library together in an executable that you
+distribute.
+
+  7. You may place library facilities that are a work based on the
+Library side-by-side in a single library together with other library
+facilities not covered by this License, and distribute such a combined
+library, provided that the separate distribution of the work based on
+the Library and of the other library facilities is otherwise
+permitted, and provided that you do these two things:
+
+    a) Accompany the combined library with a copy of the same work
+    based on the Library, uncombined with any other library
+    facilities.  This must be distributed under the terms of the
+    Sections above.
+
+    b) Give prominent notice with the combined library of the fact
+    that part of it is a work based on the Library, and explaining
+    where to find the accompanying uncombined form of the same work.
+
+  8. You may not copy, modify, sublicense, link with, or distribute
+the Library except as expressly provided under this License.  Any
+attempt otherwise to copy, modify, sublicense, link with, or
+distribute the Library is void, and will automatically terminate your
+rights under this License.  However, parties who have received copies,
+or rights, from you under this License will not have their licenses
+terminated so long as such parties remain in full compliance.
+
+  9. You are not required to accept this License, since you have not
+signed it.  However, nothing else grants you permission to modify or
+distribute the Library or its derivative works.  These actions are
+prohibited by law if you do not accept this License.  Therefore, by
+modifying or distributing the Library (or any work based on the
+Library), you indicate your acceptance of this License to do so, and
+all its terms and conditions for copying, distributing or modifying
+the Library or works based on it.
+
+  10. Each time you redistribute the Library (or any work based on the
+Library), the recipient automatically receives a license from the
+original licensor to copy, distribute, link with or modify the Library
+subject to these terms and conditions.  You may not impose any further
+restrictions on the recipients' exercise of the rights granted herein.
+You are not responsible for enforcing compliance by third parties with
+this License.
+
+  11. If, as a consequence of a court judgment or allegation of patent
+infringement or for any other reason (not limited to patent issues),
+conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot
+distribute so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you
+may not distribute the Library at all.  For example, if a patent
+license would not permit royalty-free redistribution of the Library by
+all those who receive copies directly or indirectly through you, then
+the only way you could satisfy both it and this License would be to
+refrain entirely from distribution of the Library.
+
+If any portion of this section is held invalid or unenforceable under any
+particular circumstance, the balance of the section is intended to apply,
+and the section as a whole is intended to apply in other circumstances.
+
+It is not the purpose of this section to induce you to infringe any
+patents or other property right claims or to contest validity of any
+such claims; this section has the sole purpose of protecting the
+integrity of the free software distribution system which is
+implemented by public license practices.  Many people have made
+generous contributions to the wide range of software distributed
+through that system in reliance on consistent application of that
+system; it is up to the author/donor to decide if he or she is willing
+to distribute software through any other system and a licensee cannot
+impose that choice.
+
+This section is intended to make thoroughly clear what is believed to
+be a consequence of the rest of this License.
+
+  12. If the distribution and/or use of the Library is restricted in
+certain countries either by patents or by copyrighted interfaces, the
+original copyright holder who places the Library under this License may add
+an explicit geographical distribution limitation excluding those countries,
+so that distribution is permitted only in or among countries not thus
+excluded.  In such case, this License incorporates the limitation as if
+written in the body of this License.
+
+  13. The Free Software Foundation may publish revised and/or new
+versions of the Lesser General Public License from time to time.
+Such new versions will be similar in spirit to the present version,
+but may differ in detail to address new problems or concerns.
+
+Each version is given a distinguishing version number.  If the Library
+specifies a version number of this License which applies to it and
+"any later version", you have the option of following the terms and
+conditions either of that version or of any later version published by
+the Free Software Foundation.  If the Library does not specify a
+license version number, you may choose any version ever published by
+the Free Software Foundation.
+
+  14. If you wish to incorporate parts of the Library into other free
+programs whose distribution conditions are incompatible with these,
+write to the author to ask for permission.  For software which is
+copyrighted by the Free Software Foundation, write to the Free
+Software Foundation; we sometimes make exceptions for this.  Our
+decision will be guided by the two goals of preserving the free status
+of all derivatives of our free software and of promoting the sharing
+and reuse of software generally.
+
+                            NO WARRANTY
+
+  15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO
+WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW.
+EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR
+OTHER PARTIES PROVIDE THE LIBRARY "AS IS" WITHOUT WARRANTY OF ANY
+KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE
+LIBRARY IS WITH YOU.  SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME
+THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN
+WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
+AND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU
+FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR
+CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE
+LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING
+RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A
+FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF
+SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
+DAMAGES.
+
+                     END OF TERMS AND CONDITIONS
+
+           How to Apply These Terms to Your New Libraries
+
+  If you develop a new library, and you want it to be of the greatest
+possible use to the public, we recommend making it free software that
+everyone can redistribute and change.  You can do so by permitting
+redistribution under these terms (or, alternatively, under the terms of the
+ordinary General Public License).
+
+  To apply these terms, attach the following notices to the library.  It is
+safest to attach them to the start of each source file to most effectively
+convey the exclusion of warranty; and each file should have at least the
+"copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the library's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This library is free software; you can redistribute it and/or
+    modify it under the terms of the GNU Lesser General Public
+    License as published by the Free Software Foundation; either
+    version 2.1 of the License, or (at your option) any later version.
+
+    This library is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+    Lesser General Public License for more details.
+
+    You should have received a copy of the GNU Lesser General Public
+    License along with this library; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+
+Also add information on how to contact you by electronic and paper mail.
+
+You should also get your employer (if you work as a programmer) or your
+school, if any, to sign a "copyright disclaimer" for the library, if
+necessary.  Here is a sample; alter the names:
+
+  Yoyodyne, Inc., hereby disclaims all copyright interest in the
+  library `Frob' (a library for tweaking knobs) written by James Random Hacker.
+
+  <signature of Ty Coon>, 1 April 1990
+  Ty Coon, President of Vice
+
+That's all there is to it!
diff --git a/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_structurizer.hpp b/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_structurizer.hpp
new file mode 100644
index 000000000000..8da8b1504adf
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_structurizer.hpp
@@ -0,0 +1,189 @@
+/*
+ * Copyright 2019-2021 Hans-Kristian Arntzen for Valve Corporation
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// dxil-spirv CFG structurizer adopted for LLVM use
+// ref: https://github.com/HansKristian-Work/dxil-spirv
+// @ 189cc855b471591763d9951d63e51c72649037ab
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include <memory>
+#include <vector>
+#include <unordered_set>
+#include <unordered_map>
+#include <stdint.h>
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Transforms/LibFloor/cfg/node.hpp"
+#include "llvm/Transforms/LibFloor/cfg/ir.hpp"
+
+namespace llvm {
+class BlockEmissionInterface;
+class SPIRVModule;
+struct CFGNode;
+
+// TODO: remove this
+class BlockEmissionInterface {
+public:
+  virtual ~BlockEmissionInterface() = default;
+  virtual void emit_basic_block(CFGNode *node) = 0;
+  virtual void register_block(CFGNode *node) = 0;
+};
+
+class CFGStructurizer {
+public:
+  CFGStructurizer(CFGNode *entry, CFGNodePool &pool, Function &F_,
+                  LLVMContext &ctx_);
+  bool run();
+  void traverse(BlockEmissionInterface &iface);
+  CFGNode *get_entry_block() const;
+
+private:
+  CFGNode *entry_block;
+  CFGNode *exit_block;
+  CFGNodePool &pool;
+  [[maybe_unused]] Function &F;
+  LLVMContext &ctx;
+
+  // For dominance analysis.
+  std::vector<CFGNode *> forward_post_visit_order;
+  // For post-dominance analysis.
+  std::vector<CFGNode *> backward_post_visit_order;
+
+  std::vector<uint32_t> reachability_bitset;
+  unsigned reachability_stride = 0;
+
+  std::unordered_set<const CFGNode *> reachable_nodes;
+  std::unordered_set<const CFGNode *> structured_loop_merge_targets;
+  void visit(CFGNode &entry);
+  void backwards_visit();
+  void backwards_visit(CFGNode &entry);
+  void build_immediate_dominators();
+  void build_immediate_post_dominators();
+  void build_reachability();
+  void visit_reachability(const CFGNode &node);
+  bool query_reachability(const CFGNode &from, const CFGNode &to) const;
+  void structurize(unsigned pass);
+  void find_loops();
+  void split_merge_scopes();
+  void eliminate_degenerate_blocks();
+  void update_structured_loop_merge_targets();
+  void find_selection_merges(unsigned pass);
+  static bool
+  header_and_merge_block_have_entry_exit_relationship(CFGNode *header,
+                                                      CFGNode *merge);
+  void fixup_broken_selection_merges(unsigned pass);
+  bool find_switch_blocks(unsigned pass);
+
+  void split_merge_blocks();
+  static CFGNode *get_target_break_block_for_inner_header(const CFGNode *node,
+                                                          size_t header_index);
+  CFGNode *get_or_create_ladder_block(CFGNode *node, size_t header_index);
+  CFGNode *build_enclosing_break_target_for_loop_ladder(CFGNode *&node,
+                                                        CFGNode *loop_ladder);
+  CFGNode *build_ladder_block_for_escaping_edge_handling(
+      CFGNode *node, CFGNode *header, CFGNode *loop_ladder,
+      CFGNode *target_header, CFGNode *full_break_target,
+      const std::unordered_set<const CFGNode *> &normal_preds);
+
+  static CFGNode *
+  find_common_post_dominator(const std::vector<CFGNode *> &candidates);
+  static CFGNode *find_common_post_dominator_with_ignored_break(
+      std::vector<CFGNode *> candidates, const CFGNode *break_node);
+  CFGNode *find_break_target_for_selection_construct(CFGNode *idom,
+                                                     CFGNode *merge);
+  bool control_flow_is_escaping(const CFGNode *node,
+                                const CFGNode *merge) const;
+  bool block_is_load_bearing(const CFGNode *node, const CFGNode *merge) const;
+  static std::vector<CFGNode *> isolate_structured_sorted(const CFGNode *header,
+                                                          const CFGNode *merge);
+  static void isolate_structured(std::unordered_set<CFGNode *> &nodes,
+                                 const CFGNode *header, const CFGNode *merge);
+
+  static std::vector<IncomingValue>::const_iterator
+  find_incoming_value(const CFGNode *frontier_pred,
+                      const std::vector<IncomingValue> &incoming);
+
+  void rewrite_selection_breaks(CFGNode *header, CFGNode *ladder_to);
+
+  enum class LoopExitType {
+    Exit,
+    Merge,
+    Escape,
+    InnerLoopExit,
+    InnerLoopMerge,
+    InnerLoopFalsePositive
+  };
+  LoopExitType get_loop_exit_type(const CFGNode &header,
+                                  const CFGNode &node) const;
+  CFGNode *create_helper_pred_block(CFGNode *node);
+  CFGNode *create_helper_succ_block(CFGNode *node);
+  void reset_traversal();
+  void validate_structured();
+  void recompute_cfg();
+  void compute_dominance_frontier();
+  void compute_post_dominance_frontier();
+  void create_continue_block_ladders();
+  static void recompute_dominance_frontier(CFGNode *node);
+  static void recompute_post_dominance_frontier(CFGNode *node);
+  static void merge_to_succ(CFGNode *node, unsigned index);
+  void retarget_pred_from(CFGNode *new_node, CFGNode *old_succ);
+  void retarget_succ_from(CFGNode *new_node, CFGNode *old_pred);
+
+  CFGNode *get_post_dominance_frontier_with_cfg_subset_that_reaches(
+      const CFGNode *node, const CFGNode *must_reach,
+      const CFGNode *must_reach_frontier) const;
+  bool
+  exists_path_in_cfg_without_intermediate_node(const CFGNode *start_block,
+                                               const CFGNode *end_block,
+                                               const CFGNode *stop_block) const;
+
+  struct PHINode {
+    CFGNode *block;
+    unsigned phi_index;
+  };
+  std::vector<PHINode> phi_nodes;
+  void insert_phi();
+  void insert_phi(PHINode &node);
+  void fixup_phi(PHINode &node);
+  void cleanup_breaking_phi_constructs();
+  void eliminate_node_link_preds_to_succ(CFGNode *node);
+  void prune_dead_preds();
+
+  void fixup_broken_value_dominance();
+
+  std::unordered_map<Value *, CFGNode *> value_id_to_block;
+
+  void log_cfg(const char *tag) const;
+  void log_cfg_graphviz(const char *path) const;
+
+  static bool can_complete_phi_insertion(const PHI &phi,
+                                         const CFGNode *end_node);
+  bool query_reachability_through_back_edges(const CFGNode &from,
+                                             const CFGNode &to) const;
+  bool query_reachability_split_loop_header(const CFGNode &from,
+                                            const CFGNode &to,
+                                            const CFGNode &end_node) const;
+  bool phi_frontier_makes_forward_progress(const PHI &phi,
+                                           const CFGNode *frontier,
+                                           const CFGNode *end_node) const;
+};
+} // namespace llvm
diff --git a/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_translator.hpp b/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_translator.hpp
new file mode 100644
index 000000000000..1fadaeb85462
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/cfg/cfg_translator.hpp
@@ -0,0 +1,73 @@
+/*
+ * Copyright 2021 Florian Ziesche
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// LLVM compat/translator for the dxil-spirv CFG structurizer
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include <memory>
+#include <vector>
+#include <unordered_set>
+#include <unordered_map>
+#include <cstdint>
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Transforms/LibFloor/cfg/node.hpp"
+#include "llvm/Transforms/LibFloor/cfg/ir.hpp"
+
+namespace llvm {
+
+class cfg_translator {
+public:
+  //! initializes/connects the CFG from/with an LLVM function
+  cfg_translator(Function &F_, LLVMContext &ctx_, CFGNodePool &pool_);
+  CFGNode *get_entry_block() const { return entry; }
+
+  //! on CFG transform/structurization completion:
+  //! translate any CFG changes back to LLVM IR
+  //! since the entry block may change, "updated_entry_block" sets the new entry
+  //! block
+  void cfg_to_llvm_ir(CFGNode *updated_entry_block,
+                      const bool add_merge_annotations);
+
+protected:
+  Function &F;
+  Module &M;
+  LLVMContext &ctx;
+  CFGNodePool &pool;
+
+  CFGNode *entry = nullptr;
+  std::unordered_map<BasicBlock *, CFGNode *> bb_map;
+
+  void run();
+  void translate_bb(CFGNode &node);
+
+  void add_or_update_terminator(CFGNode &node);
+
+  CallInst *insert_merge_block_marker(BasicBlock *merge_block);
+  CallInst *insert_continue_block_marker(BasicBlock *continue_block);
+  void create_loop_merge(Instruction *insert_before, BasicBlock *bb_merge,
+                         BasicBlock *bb_continue);
+  void create_selection_merge(Instruction *insert_before,
+                              BasicBlock *merge_block);
+};
+
+} // namespace llvm
diff --git a/llvm/include/llvm/Transforms/LibFloor/cfg/ir.hpp b/llvm/include/llvm/Transforms/LibFloor/cfg/ir.hpp
new file mode 100644
index 000000000000..bcd37ec536ef
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/cfg/ir.hpp
@@ -0,0 +1,99 @@
+/*
+ * Copyright 2019-2021 Hans-Kristian Arntzen for Valve Corporation
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// dxil-spirv CFG structurizer adopted for LLVM use
+// ref: https://github.com/HansKristian-Work/dxil-spirv
+// @ 189cc855b471591763d9951d63e51c72649037ab
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include <assert.h>
+#include <initializer_list>
+#include <stdint.h>
+#include <vector>
+#include <optional>
+#include "llvm/IR/CFG.h"
+
+// A simple IR representation which allows the CFGStructurizer to do some simple
+// rewrites of blocks, PHI nodes in particular.
+
+namespace llvm {
+class ConstantInt;
+
+enum class MergeType { None, Loop, Selection };
+
+struct CFGNode;
+
+struct MergeInfo {
+  MergeType merge_type = MergeType::None;
+  CFGNode *merge_block = nullptr;
+  CFGNode *continue_block = nullptr;
+};
+
+struct IncomingValue {
+  CFGNode *block = nullptr;
+  Value *value = nullptr;
+};
+
+struct PHI {
+  PHINode *phi = nullptr;
+  std::vector<IncomingValue> incoming;
+};
+
+struct Terminator {
+  enum class Type { Unreachable, Branch, Condition, Switch, Return, Kill };
+
+  // NOTE: this may be nullptr for newly created conditions
+  Instruction *terminator = nullptr;
+
+  Type type = Type::Unreachable;
+
+  // Branch
+  CFGNode *direct_block = nullptr;
+
+  // Conditional Branch and Switch
+  Value *condition = nullptr;
+
+  // Condition
+  CFGNode *true_block = nullptr;
+  CFGNode *false_block = nullptr;
+
+  // Switch
+  struct Case {
+    CFGNode *node = nullptr;
+    uint64_t global_order = 0;
+    ConstantInt *value = nullptr;
+  };
+  std::vector<Case> cases;
+  CFGNode *default_node = nullptr;
+
+  // Return
+  Value *return_value = nullptr;
+};
+
+struct IRBlock {
+  std::vector<PHI> phi;
+  std::vector<Instruction *> operations;
+  MergeInfo merge_info;
+  Terminator terminator;
+};
+
+} // namespace llvm
diff --git a/llvm/include/llvm/Transforms/LibFloor/cfg/node.hpp b/llvm/include/llvm/Transforms/LibFloor/cfg/node.hpp
new file mode 100644
index 000000000000..4365292ff81b
--- /dev/null
+++ b/llvm/include/llvm/Transforms/LibFloor/cfg/node.hpp
@@ -0,0 +1,271 @@
+/*
+ * Copyright 2019-2021 Hans-Kristian Arntzen for Valve Corporation
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// dxil-spirv CFG structurizer adopted for LLVM use
+// ref: https://github.com/HansKristian-Work/dxil-spirv
+// @ 189cc855b471591763d9951d63e51c72649037ab
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "llvm/IR/CFG.h"
+#include "llvm/Transforms/LibFloor/cfg/ir.hpp"
+
+#include <algorithm>
+#include <stdint.h>
+#include <string>
+#include <unordered_set>
+
+namespace llvm {
+class CFGNodePool {
+public:
+  CFGNodePool(LLVMContext &ctx_, Function &F_) : ctx(ctx_), F(F_) {}
+  ~CFGNodePool() = default;
+
+  inline CFGNode *create_node(std::string name, BasicBlock *BB = nullptr) {
+    auto BB_ = (BB ? BB : BasicBlock::Create(ctx, name, &F));
+    auto node = std::make_unique<CFGNode>(*this, *BB_, BB_->getName().str());
+    auto *ret = node.get();
+    nodes.emplace_back(std::move(node));
+    return ret;
+  }
+
+  template <typename Op> inline void for_each_node(const Op &op) {
+    for (auto &node : nodes) {
+      op(*node);
+    }
+  }
+
+  bool remove_node(CFGNode &node) {
+    auto node_iter =
+        std::find_if(nodes.begin(), nodes.end(), [&node](const auto &node_ptr) {
+          return node_ptr.get() == &node;
+        });
+    if (node_iter == nodes.end()) {
+      return false;
+    }
+    nodes.erase(node_iter);
+    return true;
+  }
+
+private:
+  std::vector<std::unique_ptr<CFGNode>> nodes;
+  LLVMContext &ctx;
+  Function &F;
+};
+
+struct CFGNode {
+public:
+  BasicBlock &BB;
+  std::string name;
+  void *userdata = nullptr;
+  IRBlock ir;
+
+  void add_branch(CFGNode *to);
+  void add_fake_branch(CFGNode *to);
+
+  explicit CFGNode(CFGNodePool &pool, BasicBlock &BB, std::string name);
+
+private:
+  friend class CFGNodePool;
+  friend class CFGStructurizer;
+  friend class cfg_translator;
+  friend struct LoopBacktracer;
+  friend struct LoopMergeTracer;
+
+  CFGNodePool &pool;
+  uint32_t forward_post_visit_order = 0;
+  uint32_t backward_post_visit_order = 0;
+  bool visited = false;
+  bool backward_visited = false;
+  bool traversing = false;
+  bool freeze_structured_analysis = false;
+
+  MergeType merge = MergeType::None;
+  CFGNode *loop_merge_block = nullptr;
+  CFGNode *loop_ladder_block = nullptr;
+  CFGNode *selection_merge_block = nullptr;
+  // true if the selection merge is to be skipped because at least one BB exits
+  bool selection_merge_exit = false;
+  std::vector<CFGNode *> headers;
+
+  CFGNode *immediate_dominator = nullptr;
+  CFGNode *immediate_post_dominator = nullptr;
+  std::vector<CFGNode *> succ;
+  std::vector<CFGNode *> pred;
+
+  // Fake successors and predecessors which only serve to make the flipped CFG
+  // reducible. This makes post-domination analysis not strictly correct in all
+  // cases, but it is fine for the purposes we need post-domination analysis
+  // for. If a continue block is not reachable in the flipped CFG, we will add
+  // fake successors from the continue block.
+  std::vector<CFGNode *> fake_succ;
+  std::vector<CFGNode *> fake_pred;
+
+  CFGNode *pred_back_edge = nullptr;
+  CFGNode *succ_back_edge = nullptr;
+
+  void add_unique_succ(CFGNode *node);
+  void add_unique_pred(CFGNode *node);
+  void add_unique_fake_succ(CFGNode *node);
+  void add_unique_fake_pred(CFGNode *node);
+  void add_unique_header(CFGNode *node);
+  unsigned num_forward_preds() const;
+  bool has_pred_back_edges() const;
+  bool dominates(const CFGNode *other) const;
+  bool can_loop_merge_to(const CFGNode *other) const;
+  bool is_innermost_loop_header_for(const CFGNode *other) const;
+  const CFGNode *get_innermost_loop_header_for(const CFGNode *other) const;
+  bool branchless_path_to(const CFGNode *to) const;
+  bool post_dominates(const CFGNode *other) const;
+  bool dominates_all_reachable_exits() const;
+  static CFGNode *find_common_dominator(CFGNode *a, CFGNode *b);
+  static CFGNode *find_common_post_dominator(CFGNode *a, CFGNode *b);
+  CFGNode *get_immediate_dominator_loop_header();
+  bool can_backtrace_to(const CFGNode *parent) const;
+  bool can_backtrace_to(const CFGNode *parent,
+                        std::unordered_set<const CFGNode *> &node_cache) const;
+
+  void retarget_branch(CFGNode *to_prev, CFGNode *to_next);
+  void retarget_branch_with_intermediate_node(CFGNode *to_prev,
+                                              CFGNode *to_next);
+  void traverse_dominated_blocks_and_rewrite_branch(CFGNode *from, CFGNode *to);
+
+  template <typename Op>
+  void traverse_dominated_blocks_and_rewrite_branch(CFGNode *from, CFGNode *to,
+                                                    const Op &op);
+
+  void fixup_merge_info_after_branch_rewrite(CFGNode *from, CFGNode *to);
+
+  template <typename Op> void walk_cfg_from(const Op &op) const;
+
+  void recompute_immediate_dominator();
+  void recompute_immediate_post_dominator();
+
+  template <typename Op> void traverse_dominated_blocks(const Op &op);
+  CFGNode *get_outer_selection_dominator();
+  CFGNode *get_outer_header_dominator();
+
+  std::vector<CFGNode *> dominance_frontier;
+  std::vector<CFGNode *> post_dominance_frontier;
+
+private:
+  bool
+  dominates_all_reachable_exits(std::unordered_set<const CFGNode *> &completed,
+                                const CFGNode &header) const;
+  template <typename Op>
+  void traverse_dominated_blocks_and_rewrite_branch(const CFGNode &header,
+                                                    CFGNode *from, CFGNode *to,
+                                                    const Op &op);
+  template <typename Op>
+  void traverse_dominated_blocks_and_rewrite_branch(
+      const CFGNode &header, CFGNode *from, CFGNode *to, const Op &op,
+      std::unordered_set<const CFGNode *> &visitation_cache);
+  template <typename Op>
+  void traverse_dominated_blocks(const CFGNode &header, const Op &op);
+
+  void retarget_fake_succ(CFGNode *from, CFGNode *to);
+};
+
+template <typename Op> void CFGNode::walk_cfg_from(const Op &op) const {
+  if (!op(this))
+    return;
+
+  for (auto *s : succ)
+    s->walk_cfg_from(op);
+}
+
+template <typename Op>
+void CFGNode::traverse_dominated_blocks_and_rewrite_branch(CFGNode *from,
+                                                           CFGNode *to,
+                                                           const Op &op) {
+  traverse_dominated_blocks_and_rewrite_branch(*this, from, to, op);
+}
+
+template <typename Op>
+void CFGNode::traverse_dominated_blocks_and_rewrite_branch(
+    const CFGNode &header, CFGNode *from, CFGNode *to, const Op &op,
+    std::unordered_set<const CFGNode *> &visitation_cache) {
+  visitation_cache.insert(this);
+
+  if (from == to)
+    return;
+
+  for (auto *node : succ) {
+    if (!op(node))
+      continue;
+
+    if (node == from) {
+      // Don't introduce a cycle.
+      // We only retarget branches when we have "escape-like" edges.
+      if (!to->dominates(this)) {
+        // If we already have a branch to "to", need to branch there via an
+        // intermediate node. This way, we can distinguish between a normal
+        // branch and a rewritten branch.
+        if (std::find(succ.begin(), succ.end(), to) != succ.end())
+          retarget_branch_with_intermediate_node(from, to);
+        else
+          retarget_branch(from, to);
+      }
+    } else if (header.dominates(node) &&
+               node != to) // Do not traverse beyond the new branch target.
+    {
+      if (!visitation_cache.count(node))
+        node->traverse_dominated_blocks_and_rewrite_branch(header, from, to, op,
+                                                           visitation_cache);
+    }
+  }
+
+  // In case we are rewriting branches to a new merge block, we might
+  // change the immediate post dominator for continue blocks inside this loop
+  // construct. When analysing post dominance in these cases, we need to make
+  // sure that we merge to the new merge block, and not the old one. This avoids
+  // some redundant awkward loop constructs.
+  for (auto &fake_next : fake_succ) {
+    if (fake_next == from) {
+      retarget_fake_succ(from, to);
+      break;
+    }
+  }
+}
+
+template <typename Op>
+void CFGNode::traverse_dominated_blocks_and_rewrite_branch(
+    const CFGNode &header, CFGNode *from, CFGNode *to, const Op &op) {
+  std::unordered_set<const CFGNode *> visitation_cache;
+  traverse_dominated_blocks_and_rewrite_branch(header, from, to, op,
+                                               visitation_cache);
+}
+
+template <typename Op>
+void CFGNode::traverse_dominated_blocks(const CFGNode &header, const Op &op) {
+  for (auto *node : succ) {
+    if (header.dominates(node)) {
+      if (op(node))
+        node->traverse_dominated_blocks(header, op);
+    }
+  }
+}
+
+template <typename Op> void CFGNode::traverse_dominated_blocks(const Op &op) {
+  traverse_dominated_blocks(*this, op);
+}
+
+} // namespace llvm
diff --git a/llvm/include/llvm/Transforms/Scalar.h b/llvm/include/llvm/Transforms/Scalar.h
index d6228700aa9a..2ef1c206a7b7 100644
--- a/llvm/include/llvm/Transforms/Scalar.h
+++ b/llvm/include/llvm/Transforms/Scalar.h
@@ -79,7 +79,7 @@ FunctionPass *createCallSiteSplittingPass();
 // algorithm assumes instructions are dead until proven otherwise, which makes
 // it more successful are removing non-obviously dead instructions.
 //
-FunctionPass *createAggressiveDCEPass();
+FunctionPass *createAggressiveDCEPass(bool allow_cfg_removal = true);
 
 //===----------------------------------------------------------------------===//
 //
diff --git a/llvm/include/llvm/Transforms/Scalar/ADCE.h b/llvm/include/llvm/Transforms/Scalar/ADCE.h
index 7d8b7ae68c00..0c89ff682dbc 100644
--- a/llvm/include/llvm/Transforms/Scalar/ADCE.h
+++ b/llvm/include/llvm/Transforms/Scalar/ADCE.h
@@ -29,7 +29,7 @@ class Function;
 /// dead computations that other DCE passes do not catch, particularly involving
 /// loop computations.
 struct ADCEPass : PassInfoMixin<ADCEPass> {
-  PreservedAnalyses run(Function &F, FunctionAnalysisManager &);
+  PreservedAnalyses run(Function &F, FunctionAnalysisManager &, bool allow_cfg_removal = true);
 };
 
 } // end namespace llvm
diff --git a/llvm/include/llvm/Transforms/Vectorize.h b/llvm/include/llvm/Transforms/Vectorize.h
index bc7514267778..47b96d727427 100644
--- a/llvm/include/llvm/Transforms/Vectorize.h
+++ b/llvm/include/llvm/Transforms/Vectorize.h
@@ -142,7 +142,7 @@ Pass *createLoadStoreVectorizerPass();
 //
 // Optimize partial vector operations using target cost models.
 //
-Pass *createVectorCombinePass();
+Pass *createVectorCombinePass(bool isVulkan = false);
 
 } // End llvm namespace
 
diff --git a/llvm/include/llvm/Transforms/Vectorize/VectorCombine.h b/llvm/include/llvm/Transforms/Vectorize/VectorCombine.h
index a32f9fba967f..9f2cb2ca7348 100644
--- a/llvm/include/llvm/Transforms/Vectorize/VectorCombine.h
+++ b/llvm/include/llvm/Transforms/Vectorize/VectorCombine.h
@@ -29,7 +29,7 @@ public:
   VectorCombinePass(bool ScalarizationOnly = false)
       : ScalarizationOnly(ScalarizationOnly) {}
 
-  PreservedAnalyses run(Function &F, FunctionAnalysisManager &);
+  PreservedAnalyses run(Function &F, FunctionAnalysisManager &, bool isVulkan = false);
 };
 }
 #endif // LLVM_TRANSFORMS_VECTORIZE_VECTORCOMBINE_H
diff --git a/llvm/lib/Analysis/CodeMetrics.cpp b/llvm/lib/Analysis/CodeMetrics.cpp
index 27c52506352f..bd1ae45e9536 100644
--- a/llvm/lib/Analysis/CodeMetrics.cpp
+++ b/llvm/lib/Analysis/CodeMetrics.cpp
@@ -116,7 +116,8 @@ void CodeMetrics::collectEphemeralValues(
 /// block.
 void CodeMetrics::analyzeBasicBlock(
     const BasicBlock *BB, const TargetTransformInfo &TTI,
-    const SmallPtrSetImpl<const Value *> &EphValues, bool PrepareForLTO) {
+    const SmallPtrSetImpl<const Value *> &EphValues, bool PrepareForLTO,
+    const bool allow_duplicate) {
   ++NumBlocks;
   // Use a proxy variable for NumInsts of type InstructionCost, so that it can
   // use InstructionCost's arithmetic properties such as saturation when this
@@ -170,18 +171,18 @@ void CodeMetrics::analyzeBasicBlock(
     if (isa<ExtractElementInst>(I) || I.getType()->isVectorTy())
       ++NumVectorInsts;
 
-    if (I.getType()->isTokenTy() && I.isUsedOutsideOfBlock(BB))
+    if (!allow_duplicate && I.getType()->isTokenTy() && I.isUsedOutsideOfBlock(BB))
       notDuplicatable = true;
 
     if (const CallInst *CI = dyn_cast<CallInst>(&I)) {
-      if (CI->cannotDuplicate())
+      if (!allow_duplicate && CI->cannotDuplicate())
         notDuplicatable = true;
       if (CI->isConvergent())
         convergent = true;
     }
 
     if (const InvokeInst *InvI = dyn_cast<InvokeInst>(&I))
-      if (InvI->cannotDuplicate())
+      if (!allow_duplicate && InvI->cannotDuplicate())
         notDuplicatable = true;
 
     NumInstsProxy += TTI.getUserCost(&I, TargetTransformInfo::TCK_CodeSize);
diff --git a/llvm/lib/Analysis/ConstantFolding.cpp b/llvm/lib/Analysis/ConstantFolding.cpp
index 3ed3b8902343..a635d0c96db5 100644
--- a/llvm/lib/Analysis/ConstantFolding.cpp
+++ b/llvm/lib/Analysis/ConstantFolding.cpp
@@ -102,8 +102,10 @@ static Constant *foldConstVectorToAPInt(APInt &Result, Type *DestTy,
 /// This always returns a non-null constant, but it may be a
 /// ConstantExpr if unfoldable.
 Constant *FoldBitCast(Constant *C, Type *DestTy, const DataLayout &DL) {
+#if 0 // TODO/NOTE: disabled for now
   assert(CastInst::castIsValid(Instruction::BitCast, C, DestTy) &&
          "Invalid constantexpr bitcast!");
+#endif
 
   // Catch the obvious splat cases.
   if (C->isNullValue() && !DestTy->isX86_MMXTy() && !DestTy->isX86_AMXTy())
diff --git a/llvm/lib/Analysis/LoopInfo.cpp b/llvm/lib/Analysis/LoopInfo.cpp
index 081578e00442..fddc96b37f3a 100644
--- a/llvm/lib/Analysis/LoopInfo.cpp
+++ b/llvm/lib/Analysis/LoopInfo.cpp
@@ -492,10 +492,12 @@ bool Loop::isSafeToClone() const {
         isa<CallBrInst>(BB->getTerminator()))
       return false;
 
+#if 0 // TODO/NOTE: ignoring this for now, duplicates can very well exist in the same scope
     for (Instruction &I : *BB)
       if (auto *CB = dyn_cast<CallBase>(&I))
         if (CB->cannotDuplicate())
           return false;
+#endif
   }
   return true;
 }
diff --git a/llvm/lib/AsmParser/LLLexer.cpp b/llvm/lib/AsmParser/LLLexer.cpp
index 41fb0b9008be..ca62aaa17b4f 100644
--- a/llvm/lib/AsmParser/LLLexer.cpp
+++ b/llvm/lib/AsmParser/LLLexer.cpp
@@ -600,8 +600,10 @@ lltok::Kind LLLexer::LexIdentifier() {
   KEYWORD(avr_signalcc);
   KEYWORD(ptx_kernel);
   KEYWORD(ptx_device);
-  KEYWORD(spir_kernel);
-  KEYWORD(spir_func);
+  KEYWORD(floor_kernel);
+  KEYWORD(floor_vertex);
+  KEYWORD(floor_fragment);
+  KEYWORD(floor_func);
   KEYWORD(intel_ocl_bicc);
   KEYWORD(x86_64_sysvcc);
   KEYWORD(win64cc);
diff --git a/llvm/lib/AsmParser/LLParser.cpp b/llvm/lib/AsmParser/LLParser.cpp
index 5bce1eaa59a0..04494817890e 100644
--- a/llvm/lib/AsmParser/LLParser.cpp
+++ b/llvm/lib/AsmParser/LLParser.cpp
@@ -1787,8 +1787,10 @@ void LLParser::parseOptionalDLLStorageClass(unsigned &Res) {
 ///   ::= 'avr_signalcc'
 ///   ::= 'ptx_kernel'
 ///   ::= 'ptx_device'
-///   ::= 'spir_func'
-///   ::= 'spir_kernel'
+///   ::= 'floor_func'
+///   ::= 'floor_kernel'
+///   ::= 'floor_vertex'
+///   ::= 'floor_fragment'
 ///   ::= 'x86_64_sysvcc'
 ///   ::= 'win64cc'
 ///   ::= 'webkit_jscc'
@@ -1837,8 +1839,10 @@ bool LLParser::parseOptionalCallingConv(unsigned &CC) {
   case lltok::kw_avr_signalcc:   CC = CallingConv::AVR_SIGNAL; break;
   case lltok::kw_ptx_kernel:     CC = CallingConv::PTX_Kernel; break;
   case lltok::kw_ptx_device:     CC = CallingConv::PTX_Device; break;
-  case lltok::kw_spir_kernel:    CC = CallingConv::SPIR_KERNEL; break;
-  case lltok::kw_spir_func:      CC = CallingConv::SPIR_FUNC; break;
+  case lltok::kw_floor_kernel:   CC = CallingConv::FLOOR_KERNEL; break;
+  case lltok::kw_floor_vertex:   CC = CallingConv::FLOOR_VERTEX; break;
+  case lltok::kw_floor_fragment: CC = CallingConv::FLOOR_FRAGMENT; break;
+  case lltok::kw_floor_func:     CC = CallingConv::FLOOR_FUNC; break;
   case lltok::kw_intel_ocl_bicc: CC = CallingConv::Intel_OCL_BI; break;
   case lltok::kw_x86_64_sysvcc:  CC = CallingConv::X86_64_SysV; break;
   case lltok::kw_win64cc:        CC = CallingConv::Win64; break;
@@ -3309,10 +3313,12 @@ bool LLParser::parseValID(ValID &ID, PerFunctionState *PFS, Type *ExpectedTy) {
         parseType(DestTy) ||
         parseToken(lltok::rparen, "expected ')' at end of constantexpr cast"))
       return true;
+#if 0 // allow any (AS) cast (TODO: do this better)
     if (!CastInst::castIsValid((Instruction::CastOps)Opc, SrcVal, DestTy))
       return error(ID.Loc, "invalid cast opcode for cast from '" +
                                getTypeString(SrcVal->getType()) + "' to '" +
                                getTypeString(DestTy) + "'");
+#endif
     ID.ConstantVal = ConstantExpr::getCast((Instruction::CastOps)Opc,
                                                  SrcVal, DestTy);
     ID.Kind = ValID::t_Constant;
@@ -6735,12 +6741,14 @@ bool LLParser::parseCast(Instruction *&Inst, PerFunctionState &PFS,
       parseType(DestTy))
     return true;
 
+#if 0 // allow any (AS) cast (TODO: do this better)
   if (!CastInst::castIsValid((Instruction::CastOps)Opc, Op, DestTy)) {
     CastInst::castIsValid((Instruction::CastOps)Opc, Op, DestTy);
     return error(Loc, "invalid cast opcode for cast from '" +
                           getTypeString(Op->getType()) + "' to '" +
                           getTypeString(DestTy) + "'");
   }
+#endif
   Inst = CastInst::Create((Instruction::CastOps)Opc, Op, DestTy);
   return false;
 }
diff --git a/llvm/lib/Bitcode/CMakeLists.txt b/llvm/lib/Bitcode/CMakeLists.txt
index ff7e290cad1b..5c80c4d26545 100644
--- a/llvm/lib/Bitcode/CMakeLists.txt
+++ b/llvm/lib/Bitcode/CMakeLists.txt
@@ -1,2 +1,5 @@
 add_subdirectory(Reader)
 add_subdirectory(Writer)
+add_subdirectory(Writer32)
+add_subdirectory(Writer50)
+add_subdirectory(MetalLib)
diff --git a/llvm/lib/Bitcode/MetalLib/CMakeLists.txt b/llvm/lib/Bitcode/MetalLib/CMakeLists.txt
new file mode 100644
index 000000000000..81b842a5df3f
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/CMakeLists.txt
@@ -0,0 +1,23 @@
+add_llvm_component_library(LLVMMetalLib
+  MetalLibWriter.cpp
+  MetalLibWriterPass.cpp
+  bzip2/blocksort.c
+  bzip2/bzlib.c
+  bzip2/compress.c
+  bzip2/crctable.c
+  bzip2/decompress.c
+  bzip2/huffman.c
+  bzip2/randtable.c
+  tar/microtar.c
+
+  DEPENDS
+  intrinsics_gen
+  LLVMBitWriter50
+
+  LINK_COMPONENTS
+  Analysis
+  Core
+  MC
+  Object
+  Support
+  )
diff --git a/llvm/lib/Bitcode/MetalLib/MetalLibWriter.cpp b/llvm/lib/Bitcode/MetalLib/MetalLibWriter.cpp
new file mode 100644
index 000000000000..6e0e79cec35c
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/MetalLibWriter.cpp
@@ -0,0 +1,49 @@
+//===-- MetalLibWriter.cpp ------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm-c/BitWriter.h"
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Support/raw_ostream.h"
+using namespace llvm;
+
+/*===-- Operations on modules ---------------------------------------------===*/
+
+int LLVMWriteMetalLibToFile(LLVMModuleRef M, const char *Path) {
+  std::error_code EC;
+  raw_fd_ostream OS(Path, EC, sys::fs::OF_None);
+
+  if (EC)
+    return -1;
+
+  WriteMetalLibToFile(*unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteMetalLibToFD(LLVMModuleRef M, int FD, int ShouldClose,
+                          int Unbuffered) {
+  raw_fd_ostream OS(FD, ShouldClose, Unbuffered);
+
+  WriteMetalLibToFile(*unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteMetalLibToFileHandle(LLVMModuleRef M, int FileHandle) {
+  return LLVMWriteMetalLibToFD(M, FileHandle, true, false);
+}
+
+LLVMMemoryBufferRef LLVMWriteMetalLibToMemoryBuffer(LLVMModuleRef M) {
+  std::string Data;
+  raw_string_ostream OS(Data);
+
+  WriteMetalLibToFile(*unwrap(M), OS);
+  return wrap(MemoryBuffer::getMemBufferCopy(OS.str()).release());
+}
diff --git a/llvm/lib/Bitcode/MetalLib/MetalLibWriterPass.cpp b/llvm/lib/Bitcode/MetalLib/MetalLibWriterPass.cpp
new file mode 100644
index 000000000000..ffad0e7651b5
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/MetalLibWriterPass.cpp
@@ -0,0 +1,1100 @@
+//===- MetalLibWriterPass.cpp - Metal Library writing pass ----------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// MetalLibWriterPass implementation.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Triple.h"
+#include "llvm/Bitcode/BitcodeWriterPass.h"
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "../Writer50/ValueEnumerator50.h"
+#include "llvm/IR/DebugInfoMetadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/PassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Support/MemoryBuffer.h"
+#include "llvm/Support/Path.h"
+#include "llvm/Support/VersionTuple.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+#include "sha256.hpp"
+#define BZ_NO_STDIO 1
+#include "bzip2/bzlib.h"
+#include "tar/microtar.h"
+#include <string>
+#include <unordered_set>
+#include <unordered_map>
+#include <random>
+#include <fstream>
+#include <sstream>
+using namespace llvm;
+using namespace std;
+
+// workaround Windows stupidity ...
+#if defined(uuid_t)
+#undef uuid_t
+#endif
+
+PreservedAnalyses MetalLibWriterPass::run(Module &M, ModuleAnalysisManager &) {
+  WriteMetalLibToFile(M, OS);
+  return PreservedAnalyses::all();
+}
+
+namespace {
+class WriteMetalLibPass : public ModulePass {
+  raw_ostream &OS; // raw_ostream to print on
+
+public:
+  static char ID; // Pass identification, replacement for typeid
+  explicit WriteMetalLibPass(raw_ostream &o) : ModulePass(ID), OS(o) {}
+
+  StringRef getPassName() const override { return "Metal Library Writer"; }
+
+  bool runOnModule(Module &M) override {
+    WriteMetalLibToFile(M, OS);
+    return false;
+  }
+};
+} // namespace
+
+char WriteMetalLibPass::ID = 0;
+
+ModulePass *llvm::createMetalLibWriterPass(raw_ostream &Str) {
+  return new WriteMetalLibPass(Str);
+}
+
+//
+struct __attribute__((packed)) metallib_version {
+  // container/file version
+  uint16_t container_version_major : 15;
+  uint16_t is_macos_target : 1;
+  uint16_t container_version_minor;
+  uint16_t container_version_bugfix;
+
+  // flags
+  uint8_t file_type : 7;
+  uint8_t is_stub : 1;
+  uint8_t platform : 7;
+  uint8_t is_64_bit : 1;
+
+  // platform version
+  uint32_t platform_version_major : 16;
+  uint32_t platform_version_minor : 8;
+  uint32_t platform_version_update : 8;
+};
+static_assert(sizeof(metallib_version) == 12, "invalid version header length");
+
+struct __attribute__((packed)) metallib_header_control {
+  uint64_t programs_offset;
+  uint64_t programs_length;
+  uint64_t reflection_offset;
+  uint64_t reflection_length;
+  uint64_t debug_offset;
+  uint64_t debug_length;
+  uint64_t bitcode_offset;
+  uint64_t bitcode_length;
+};
+static_assert(sizeof(metallib_header_control) == 64,
+              "invalid program info length");
+
+struct __attribute__((packed)) metallib_header {
+  const char magic[4]; // == metallib_magic
+  const metallib_version version;
+  const uint64_t file_length;
+  const metallib_header_control header_control;
+};
+static_assert(sizeof(metallib_header) == 4 + sizeof(metallib_version) +
+                                             sizeof(uint64_t) +
+                                             sizeof(metallib_header_control),
+              "invalid metallib header size");
+
+struct metallib_program_info {
+// NOTE: tag types are always 32-bit
+// NOTE: tag types are always followed by a uint16_t that specifies the length
+// of the tag data
+#define make_tag_type(a, b, c, d)                                              \
+  ((uint32_t(d) << 24u) | (uint32_t(c) << 16u) | (uint32_t(b) << 8u) |         \
+   uint32_t(a))
+  enum TAG_TYPE : uint32_t {
+    // used in initial header section
+    NAME = make_tag_type('N', 'A', 'M', 'E'),
+    TYPE = make_tag_type('T', 'Y', 'P', 'E'),
+    HASH = make_tag_type('H', 'A', 'S', 'H'),
+    MD_SIZE = make_tag_type('M', 'D', 'S', 'Z'),
+    OFFSET = make_tag_type('O', 'F', 'F', 'T'),
+    VERSION = make_tag_type('V', 'E', 'R', 'S'),
+    SOFF = make_tag_type('S', 'O', 'F', 'F'),
+    // used in reflection section
+    CNST = make_tag_type('C', 'N', 'S', 'T'),
+    VATT = make_tag_type('V', 'A', 'T', 'T'),
+    VATY = make_tag_type('V', 'A', 'T', 'Y'),
+    RETR = make_tag_type('R', 'E', 'T', 'R'),
+    ARGR = make_tag_type('A', 'R', 'G', 'R'),
+    // used in debug section
+    DEBI = make_tag_type('D', 'E', 'B', 'I'),
+    DEPF = make_tag_type('D', 'E', 'P', 'F'),
+    // additional metadata
+    HSRD = make_tag_type('H', 'S', 'R', 'D'),
+    UUID = make_tag_type('U', 'U', 'I', 'D'),
+    // used for source code/archive
+    SARC = make_tag_type('S', 'A', 'R', 'C'),
+    // TODO/TBD
+    LAYR = make_tag_type('L', 'A', 'Y', 'R'),
+    TESS = make_tag_type('T', 'E', 'S', 'S'),
+    // generic end tag
+    END = make_tag_type('E', 'N', 'D', 'T'),
+  };
+#undef make_tag_type
+
+  enum class PROGRAM_TYPE : uint8_t {
+    VERTEX = 0,
+    FRAGMENT = 1,
+    KERNEL = 2,
+    // TODO: tessellation?
+    NONE = 255
+  };
+
+  struct version_info {
+    uint32_t major : 16;
+    uint32_t minor : 8;
+    uint32_t rev : 8;
+  };
+  static_assert(sizeof(version_info) == sizeof(uint32_t),
+                "invalid offset_info size");
+
+  struct offset_info {
+    // NOTE: these are all relative offsets -> add to metallib_header_control
+    // offsets to get absolute offsets
+    uint64_t reflection_offset;
+    uint64_t debug_offset;
+    uint64_t bitcode_offset;
+  };
+  static_assert(sizeof(offset_info) == 3 * sizeof(uint64_t),
+                "invalid offset_info size");
+
+  struct entry {
+    uint32_t length{0};
+
+    string name; // NOTE: limited to 65536 - 1 ('\0')
+
+    PROGRAM_TYPE type{PROGRAM_TYPE::NONE};
+
+    sha256_hash hash;
+
+    offset_info offset{0, 0, 0};
+
+    // we need a separate stream for the actual bitcode data, since we need to
+    // know
+    // the size of each module/file (no way to know this beforehand)
+    string bitcode_data{""}; // -> used via raw_string_ostream later on
+    uint64_t bitcode_size{0};
+
+    // same for reflection and debug data
+    string reflection_data{""};
+    uint64_t reflection_size{0};
+    string debug_data{""};
+    uint64_t debug_size{0};
+
+    version_info metal_version;
+    version_info metal_language_version;
+
+    bool emit_debug_info{false};
+    uint64_t debug_source_offset{0u};
+
+    // output in same order as Apple:
+    //  * NAME
+    //  * TYPE
+    //  * HASH
+    //  * OFFT
+    //  * VERS
+    //  * MDSZ
+    //  * SOFF
+    //  * ENDT
+    void update_length() {
+      length = 4;                     // length info itself
+      length += 7 * sizeof(TAG_TYPE); // 7 tags
+      length += 6 * sizeof(uint16_t); // tag lengths (except ENDT)
+      if (emit_debug_info) {
+        length += 1 * (sizeof(TAG_TYPE) + sizeof(uint16_t)); // SOFF tag
+      }
+
+      length += name.size() + 1;          // name length + \0
+      length += 1;                        // type
+      length += sizeof(sha256_hash);      // hash
+      length += sizeof(offset_info);      // offset
+      length += sizeof(version_info) * 2; // both versions
+      length += sizeof(uint64_t);         // module size, always 8 bytes
+      if (emit_debug_info) {
+        length += sizeof(uint64_t); // SOFF, always 8 bytes
+      }
+
+      bitcode_size = bitcode_data.size();
+      reflection_size = reflection_data.size();
+      debug_size = debug_data.size();
+    }
+    void update_offsets(uint64_t &running_refl_size, uint64_t &running_dbg_size,
+                        uint64_t &running_bc_size) {
+      offset.reflection_offset = running_refl_size;
+      offset.debug_offset = running_dbg_size;
+      offset.bitcode_offset = running_bc_size;
+
+      running_refl_size += reflection_size;
+      running_dbg_size += debug_size;
+      running_bc_size += bitcode_size;
+    }
+
+    template <typename data_type>
+    static inline void write_value(raw_ostream &OS, const data_type &value) {
+      OS.write((const char *)&value, sizeof(data_type));
+    }
+
+    void write_header(raw_ostream &OS) const {
+      write_value(OS, length);
+
+      // NAME
+      write_value(OS, TAG_TYPE::NAME);
+      write_value(OS, uint16_t(name.size() + 1));
+      OS << name << '\0';
+
+      // TYPE
+      write_value(OS, TAG_TYPE::TYPE);
+      write_value(OS, uint16_t(sizeof(uint8_t)));
+      write_value(OS, uint8_t(type));
+
+      // HASH
+      write_value(OS, TAG_TYPE::HASH);
+      write_value(OS, uint16_t(sizeof(sha256_hash)));
+      OS.write((const char *)&hash, sizeof(sha256_hash));
+
+      // OFFT
+      write_value(OS, TAG_TYPE::OFFSET);
+      write_value(OS, uint16_t(sizeof(offset_info)));
+      write_value(OS, offset.reflection_offset);
+      write_value(OS, offset.debug_offset);
+      write_value(OS, offset.bitcode_offset);
+
+      // VERS
+      write_value(OS, TAG_TYPE::VERSION);
+      write_value(OS, uint16_t(2 * sizeof(version_info)));
+      write_value(OS, *(const uint32_t *)&metal_version);
+      write_value(OS, *(const uint32_t *)&metal_language_version);
+
+      // MDSZ
+      write_value(OS, TAG_TYPE::MD_SIZE);
+      write_value(OS, uint16_t(sizeof(uint64_t)));
+      write_value(OS, uint64_t(bitcode_data.size()));
+
+      if (emit_debug_info) {
+        // SOFF
+        write_value(OS, TAG_TYPE::SOFF);
+        write_value(OS, uint16_t(sizeof(uint64_t)));
+        write_value(OS, debug_source_offset);
+      }
+
+      // ENDT
+      write_value(OS, TAG_TYPE::END);
+    }
+
+    void write_module(raw_ostream &OS) const {
+      OS.write(bitcode_data.data(), bitcode_data.size());
+    }
+
+    void write_reflection(raw_ostream &OS) const {
+      OS.write(reflection_data.data(), reflection_data.size());
+    }
+
+    void write_debug(raw_ostream &OS) const {
+      OS.write(debug_data.data(), debug_data.size());
+    }
+  };
+  vector<entry> entries;
+};
+
+//
+static bool is_used_in_function(const Function *F, const GlobalVariable *GV) {
+  for (const auto &user : GV->users()) {
+    if (const auto instr = dyn_cast<Instruction>(user)) {
+      if (instr->getParent()->getParent() == F) {
+        return true;
+      }
+    } else if (const auto const_expr = dyn_cast<ConstantExpr>(user)) {
+      for (const auto &ce_user : const_expr->users()) {
+        if (const auto ce_instr = dyn_cast<Instruction>(ce_user)) {
+          if (ce_instr->getParent()->getParent() == F) {
+            return true;
+          }
+        }
+      }
+    }
+  }
+  return false;
+}
+
+// version -> { AIR version, language version }
+static const unordered_map<uint32_t,
+                           pair<array<uint32_t, 3>, array<uint32_t, 3>>>
+    metal_versions{
+        {200, {{{2, 0, 0}}, {{2, 0, 0}}}}, {210, {{{2, 1, 0}}, {{2, 1, 0}}}},
+        {220, {{{2, 2, 0}}, {{2, 2, 0}}}}, {230, {{{2, 3, 0}}, {{2, 3, 0}}}},
+        {240, {{{2, 4, 0}}, {{2, 4, 0}}}},
+    };
+
+static std::string make_abs_file_name(const std::string &file_name_in) {
+  std::string file_name_out = file_name_in;
+  SmallVector<char> file_name(file_name_out.size());
+  file_name.assign(file_name_out.begin(), file_name_out.end());
+  sys::fs::make_absolute(file_name);
+  file_name_out.resize(file_name.size(), '\0');
+  file_name_out.assign(file_name.begin(), file_name.end());
+  return file_name_out;
+}
+
+struct tar_stream_t {
+  static int read(void *stream, void *data, uint32_t size) {
+    assert(false && "should not call this");
+    return 0;
+  }
+
+  static int seek(void *stream, uint32_t offset) {
+    assert(false && "should not call this");
+    return MTAR_ESEEKFAIL;
+  }
+
+  static int write(void *stream, const void *data, uint32_t size) {
+    auto &str = ((tar_stream_t *)stream)->str;
+    str.write((const char *)data, size);
+    return size;
+  }
+
+  static int close(void *stream) {
+    (void)stream; // nop
+    return MTAR_ESUCCESS;
+  }
+
+  static constexpr const mtar_ops_t ops{
+      .read = &read,
+      .write = &write,
+      .seek = &seek,
+      .close = &close,
+  };
+
+  std::stringstream str;
+};
+static std::string create_tar(
+    const std::unordered_map<std::string /* abs path */,
+                             std::unique_ptr<MemoryBuffer> /* data */> &files) {
+  // create tar
+  tar_stream_t tar_stream;
+  mtar_t tar;
+  mtar_init(&tar, MTAR_WRITE, &tar_stream_t::ops, &tar_stream);
+
+  for (const auto &file : files) {
+    const auto file_size = file.second->getBufferSize();
+    mtar_write_file_header(&tar, file.first.c_str(), file_size);
+    mtar_write_data(&tar, file.second->getBufferStart(), file_size);
+    mtar_end_data(&tar);
+  }
+
+  mtar_finalize(&tar);
+  mtar_close(&tar);
+
+  return tar_stream.str.str();
+}
+
+static std::pair<std::unique_ptr<uint8_t[]>, uint32_t>
+compress_source_archive(const void *archive_ptr, const uint32_t archive_size) {
+  auto dst_len = archive_size + (archive_size / 50u) +
+                 600u; // provision orig + 2% + advised 600 bytes
+  auto dst = make_unique<uint8_t[]>(dst_len);
+  const auto bz2_ret = BZ2_bzBuffToBuffCompress(
+      (char *)dst.get(), &dst_len,
+      const_cast<char *>((const char *)archive_ptr), archive_size, 6, 0, 30);
+  if (bz2_ret != BZ_OK) {
+    errs() << "failed to perform bz2 compression on source archive: " << bz2_ret
+           << '\n';
+    return {nullptr, 0};
+  }
+  return {move(dst), dst_len};
+}
+
+//
+void llvm::WriteMetalLibToFile(Module &M, raw_ostream &OS) {
+  // get metal version
+  Triple TT(M.getTargetTriple());
+  uint32_t target_air_version = 200;
+  if (TT.isiOS()) {
+    uint32_t ios_major, ios_minor, ios_micro;
+    TT.getiOSVersion(ios_major, ios_minor, ios_micro);
+    if (ios_major <= 11) {
+      target_air_version = 200;
+    } else if (ios_major == 12) {
+      target_air_version = 210;
+    } else if (ios_major == 13) {
+      target_air_version = 220;
+    } else if (ios_major >= 14) {
+      target_air_version = 230;
+    } else if (ios_major >= 15) {
+      target_air_version = 240;
+    }
+
+    M.setSDKVersion(VersionTuple{ios_major, ios_minor});
+  } else {
+    uint32_t osx_major, osx_minor, osx_micro;
+    TT.getMacOSXVersion(osx_major, osx_minor, osx_micro);
+    if (osx_major == 10 && osx_minor <= 13) {
+      target_air_version = 200;
+    } else if (osx_major == 10 && osx_minor == 14) {
+      target_air_version = 210;
+    } else if (osx_major == 10 && osx_minor == 15) {
+      target_air_version = 220;
+    } else if ((osx_major == 11 && osx_minor >= 0) ||
+               (osx_major == 10 && osx_minor >= 16)) {
+      target_air_version = 230;
+    } else if ((osx_major == 12 && osx_minor >= 0) || osx_major > 12) {
+      target_air_version = 240;
+    }
+
+    M.setSDKVersion(VersionTuple{osx_major, osx_minor});
+  }
+  const auto &metal_version = *metal_versions.find(target_air_version);
+
+  // gather entry point functions that we want to clone/emit
+  unordered_map<string, metallib_program_info::PROGRAM_TYPE> function_set;
+  // -> first pass to gather all entry points specified in metadata lists
+  for (uint32_t i = 0; i < 3; ++i) {
+    const auto func_type = (metallib_program_info::PROGRAM_TYPE)i;
+    const NamedMDNode *func_list = nullptr;
+    switch (func_type) {
+    case metallib_program_info::PROGRAM_TYPE::KERNEL:
+      func_list = M.getNamedMetadata("air.kernel");
+      break;
+    case metallib_program_info::PROGRAM_TYPE::VERTEX:
+      func_list = M.getNamedMetadata("air.vertex");
+      break;
+    case metallib_program_info::PROGRAM_TYPE::FRAGMENT:
+      func_list = M.getNamedMetadata("air.fragment");
+      break;
+    case metallib_program_info::PROGRAM_TYPE::NONE:
+      llvm_unreachable("invalid type");
+    }
+    if (func_list == nullptr) {
+      // no functions of this type
+      continue;
+    }
+
+    for (const auto &op : func_list->operands()) {
+      const auto &op_0 = op->getOperand(0);
+      if (auto const_md = dyn_cast<ConstantAsMetadata>(op_0)) {
+        if (auto func = dyn_cast<Function>(const_md->getValue())) {
+          function_set.emplace(func->getName().str(), func_type);
+        }
+      }
+    }
+  }
+  // -> second pass to actually gather all functions
+  // NOTE: we do it this way so that we maintain the order of functions
+  vector<pair<const Function *, metallib_program_info::PROGRAM_TYPE>> functions;
+  for (const auto &func : M.functions()) {
+    if (!func.hasName()) {
+      continue;
+    }
+
+    const auto func_iter = function_set.find(func.getName().str());
+    if (func_iter == function_set.end()) {
+      continue; // not an entry point
+    }
+    functions.emplace_back(&func, func_iter->second);
+  }
+  const uint32_t function_count = uint32_t(functions.size());
+
+  // program info
+  metallib_header_control ctrl;
+  metallib_program_info prog_info;
+  prog_info.entries.resize(function_count);
+
+  // absolute source file name
+  const std::string src_file_name = make_abs_file_name(M.getSourceFileName());
+  const uint32_t src_file_name_length = src_file_name.length() + 1u /* \0 */;
+
+  // if we're building with debug info, emit .metallib specific debug info
+  // NOTE: for compat, only do this for Metal 2.4+
+  const bool emit_debug_info =
+      (M.debug_compile_units_begin() != M.debug_compile_units_end() &&
+       target_air_version >= 240);
+
+  // handle source files / archive creation
+  std::pair<std::unique_ptr<uint8_t[]>, uint32_t> source_archive_data{nullptr,
+                                                                      0};
+  std::unordered_map<std::string /* abs path */,
+                     std::unique_ptr<MemoryBuffer> /* source code */>
+      source_files;
+  std::string working_dir;
+  std::string linker_cmd;
+  std::string dependent_bc_file_name = src_file_name + ".air";
+  if (emit_debug_info) {
+    // * get the working directory
+    if (auto working_dir_md = M.getNamedMetadata("llvm_utils.workingdir");
+        working_dir_md) {
+      if (working_dir_md->getNumOperands() >= 1) {
+        auto md_node = working_dir_md->getOperand(0);
+        if (md_node->getNumOperands() >= 1) {
+          if (const auto working_dir_str_md =
+                  dyn_cast_or_null<llvm::MDString>(md_node->getOperand(0));
+              working_dir_str_md) {
+            working_dir = working_dir_str_md->getString().str();
+          }
+        }
+      }
+    }
+
+    // * gather all source files
+    // * ensure that all DIFile metadata entries use absolute file names +
+    //   directory is the working directory
+    auto &Context = M.getContext();
+    // somewhat overkill, but I don't know of a better way
+    ValueEnumerator50 VE(M, false);
+    for (const auto &md : VE.getMetadataMap()) {
+      if (const DIFile *difile_node = dyn_cast_or_null<DIFile>(md.first);
+          difile_node) {
+        const auto orig_file_name = difile_node->getFilename().str();
+        auto abs_file_name = make_abs_file_name(orig_file_name);
+        source_files.emplace(abs_file_name, nullptr);
+        auto mod_difile_node = const_cast<DIFile *>(difile_node);
+        mod_difile_node->replaceOperandWith(
+            0, llvm::MDString::get(Context, abs_file_name));
+        const auto path_sep = abs_file_name.rfind('/');
+        if (path_sep != std::string::npos) {
+          abs_file_name.erase(path_sep, abs_file_name.size() - path_sep);
+        }
+        mod_difile_node->replaceOperandWith(
+            1, llvm::MDString::get(Context, working_dir));
+      }
+    }
+
+    // * read all source code (drop files that we can't read)
+    // * emit code for all valid source files ("recompile_info")
+    llvm::NamedMDNode *recompile_info =
+        M.getOrInsertNamedMetadata("recompile_info");
+    SmallVector<llvm::Metadata *, 8> recompile_info_list;
+    for (auto src_iter = source_files.begin();
+         src_iter != source_files.end();) {
+      auto source = MemoryBuffer::getFile(src_iter->first, true /* is text */,
+                                          true /* requires \0 */,
+                                          false /* not volatile */);
+      if (!source) {
+        src_iter = source_files.erase(src_iter);
+      } else {
+        src_iter->second = std::move(*source);
+        ++src_iter;
+      }
+    }
+    for (const auto &source_file : source_files) {
+      SmallVector<llvm::Metadata *, 2> recompile_info_file;
+      recompile_info_file.push_back(
+          llvm::MDString::get(Context, source_file.first));
+      recompile_info_file.push_back(
+          llvm::MDString::get(Context, source_file.second->getBuffer()));
+      recompile_info_list.push_back(
+          llvm::MDNode::get(Context, recompile_info_file));
+    }
+    recompile_info->addOperand(llvm::MDNode::get(Context, recompile_info_list));
+
+    // * create additional source archive entries
+    if (working_dir.empty()) {
+      errs() << "no valid 'llvm_utils.workingdir' metadata entry!\n";
+    } else {
+      source_files.emplace("metal-working-dir.txt",
+                           MemoryBuffer::getMemBuffer(working_dir));
+    }
+
+    StringRef cmd_line;
+    if (auto cmd_line_md = M.getNamedMetadata("llvm.commandline");
+        cmd_line_md) {
+      if (cmd_line_md->getNumOperands() >= 1) {
+        auto md_node = cmd_line_md->getOperand(0);
+        if (md_node->getNumOperands() >= 1) {
+          if (auto cmd_line_str_md =
+                  dyn_cast_or_null<llvm::MDString>(md_node->getOperand(0));
+              cmd_line_str_md) {
+            cmd_line = cmd_line_str_md->getString();
+          }
+        }
+      }
+    }
+    if (cmd_line.empty()) {
+      errs() << "no valid 'llvm.commandline' metadata entry!\n";
+    } else {
+      source_files.emplace("metal-options.txt",
+                           MemoryBuffer::getMemBuffer(cmd_line));
+      linker_cmd = cmd_line;
+    }
+
+    source_files.emplace("original-input-filename.txt",
+                         MemoryBuffer::getMemBuffer(dependent_bc_file_name));
+
+    // * create .tar.bz2
+    auto tar_data = create_tar(source_files);
+#if 0
+		{
+			error_code ec;
+			raw_fd_ostream tar_file(src_file_name + ".tar", ec, sys::fs::CreationDisposition::CD_CreateAlways);
+			tar_file.write(tar_data.data(), tar_data.size());
+		}
+#endif
+    source_archive_data =
+        compress_source_archive(tar_data.data(), tar_data.size());
+#if 0
+		{
+			error_code ec;
+			raw_fd_ostream bz2_file(src_file_name + ".tar.bz2", ec, sys::fs::CreationDisposition::CD_CreateAlways);
+			bz2_file.write((const char*)source_archive_data.first.get(), source_archive_data.second);
+		}
+#endif
+
+    // * export the original complete bitcode file as well
+    error_code ec;
+    raw_fd_ostream dependent_bc_file(
+        dependent_bc_file_name, ec,
+        sys::fs::CreationDisposition::CD_CreateAlways);
+    if (!ec) {
+      WriteBitcode50ToFile(&M, dependent_bc_file);
+      dependent_bc_file.flush();
+    } else {
+      errs() << "failed to write dependent debug file "
+             << dependent_bc_file_name << "\n";
+      dependent_bc_file_name = "";
+    }
+  }
+
+  // we can now remove any metadata that we no longer need
+  static constexpr array<const char *, 5> drop_mds{
+      "llvm_utils.workingdir", "recompile_info",   "air.source_file_name",
+      "llvm.linker.options",   "llvm.commandline",
+  };
+  for (const auto &drop_md : drop_mds) {
+    if (auto md = M.getNamedMetadata(drop_md); md) {
+      md->dropAllReferences();
+      md->eraseFromParent();
+    }
+  }
+
+  // create per-function modules and fill entries
+  uint64_t entries_size = 0;
+  uint64_t reflection_data_size = 0;
+  uint64_t debug_data_size = 0;
+  uint64_t bitcode_data_size = 0;
+  for (uint32_t i = 0; i < function_count; ++i) {
+    auto &entry = prog_info.entries[i];
+    auto &func = functions[i].first;
+
+    entry.type = functions[i].second;
+    entry.name = func->getName().str();
+    entry.metal_version.major = metal_version.second.first[0];
+    entry.metal_version.minor = metal_version.second.first[1];
+    entry.metal_version.rev = metal_version.second.first[2];
+    entry.metal_language_version.major = metal_version.second.second[0];
+    entry.metal_language_version.minor = metal_version.second.second[1];
+    entry.metal_language_version.rev = metal_version.second.second[2];
+    entry.emit_debug_info = emit_debug_info;
+
+    // clone the module with the current entry point function and any global
+    // vars that we need
+    ValueToValueMapTy VMap;
+    auto cloned_mod = CloneModule(M, VMap, [&func](const GlobalValue *GV) {
+      if (GV == func) {
+        return true;
+      }
+      // only clone global vars if they are needed in a specific function
+      if (const GlobalVariable *GVar = dyn_cast<GlobalVariable>(GV)) {
+        return is_used_in_function(func, GVar);
+      }
+      return false;
+    });
+
+    // metallib uses the function name as the source file name
+    cloned_mod->setSourceFileName(func->getName());
+
+    // update data layout
+    if (target_air_version >= 230) {
+      cloned_mod->setDataLayout(
+          "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-"
+          "f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:128:"
+          "128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
+          "1024-n8:16:32");
+    }
+
+    // remove all unused functions and global vars, since CloneModule only sets
+    // unused vars to external linkage and unused funcs are declarations only
+    // NOTE: this also removes entry points that are now unused (metadata is
+    // removed later)
+    for (auto I = cloned_mod->begin(), E = cloned_mod->end(); I != E;) {
+      Function &F = *I++;
+      if (F.isDeclaration() && F.use_empty()) {
+        F.eraseFromParent();
+        continue;
+      }
+    }
+
+    for (auto I = cloned_mod->global_begin(), E = cloned_mod->global_end();
+         I != E;) {
+      GlobalVariable &GV = *I++;
+      if (GV.isDeclaration() && GV.use_empty()) {
+        GV.eraseFromParent();
+        continue;
+      }
+    }
+
+    // clean up metadata
+    // * metadata of all entry points that no longer exist
+    static constexpr const std::array<const char *, 3> entry_point_md_names{{
+        "air.kernel",
+        "air.vertex",
+        "air.fragment",
+    }};
+    for (const auto &entry_point_md_name : entry_point_md_names) {
+      if (auto func_entries =
+              cloned_mod->getNamedMetadata(entry_point_md_name)) {
+        vector<MDNode *> kept_nodes;
+        for (auto op_iter = func_entries->op_begin();
+             op_iter != func_entries->op_end(); ++op_iter) {
+          MDNode *node = *op_iter;
+          if (node->getNumOperands() < 3) {
+            continue;
+          }
+          if (node->getOperand(0).get() == nullptr) {
+            continue;
+          }
+          kept_nodes.emplace_back(node);
+        }
+
+        // need to drop all references to existing nodes, b/c we can't directly
+        // remove operands
+        func_entries->dropAllReferences();
+        if (kept_nodes.empty()) {
+          // remove air.sampler_states altogether
+          func_entries->eraseFromParent();
+        } else {
+          // now, only add ones we want to keep
+          for (auto node : kept_nodes) {
+            func_entries->addOperand(node);
+          }
+        }
+      }
+    }
+    // * sample states of entry points that no longer exist
+    if (auto sampler_states =
+            cloned_mod->getNamedMetadata("air.sampler_states")) {
+      vector<MDNode *> kept_nodes;
+      for (auto op_iter = sampler_states->op_begin();
+           op_iter != sampler_states->op_end(); ++op_iter) {
+        MDNode *node = *op_iter;
+        if (node->getNumOperands() != 2 ||
+            node->getOperand(1).get() == nullptr) {
+          continue;
+        }
+        kept_nodes.emplace_back(node);
+      }
+
+      // need to drop all references to existing nodes, b/c we can't directly
+      // remove operands
+      sampler_states->dropAllReferences();
+      if (kept_nodes.empty()) {
+        // remove air.sampler_states altogether
+        sampler_states->eraseFromParent();
+      } else {
+        // now, only add ones we want to keep
+        for (auto node : kept_nodes) {
+          sampler_states->addOperand(node);
+        }
+      }
+    }
+    // * set fake compiler ident
+    if (target_air_version >= 230) {
+      if (auto llvm_ident = cloned_mod->getNamedMetadata("llvm.ident")) {
+        if (MDNode *ident_op = llvm_ident->getOperand(0)) {
+          static const std::unordered_map<uint32_t, const char*> ident_versions{
+              {230, "Apple LLVM version 31001.143 (metalfe-31001.143)"},
+              {240, "Apple metal version 31001.363 (metalfe-31001.363)"},
+          };
+          ident_op->replaceOperandWith(
+              0, llvm::MDString::get(cloned_mod->getContext(),
+                                     ident_versions.at(target_air_version)));
+        }
+      }
+    }
+
+    // modify local and constant memory GVs
+    const auto &DL = cloned_mod->getDataLayout();
+    for (auto I = cloned_mod->global_begin(), E = cloned_mod->global_end();
+         I != E;) {
+      GlobalVariable &GV = *I++;
+      if (GV.getAddressSpace() == 2 /* constant memory */ ||
+          GV.getAddressSpace() == 3 /* local memory */) {
+        auto value_type = GV.getValueType();
+        if (value_type && value_type->isSized() &&
+            DL.getTypeStoreSize(value_type) >= 16 && GV.getAlignment() < 16) {
+          // use at least 16-byte alignment
+          GV.setAlignment(MaybeAlign{16u});
+        }
+      }
+      if (GV.getAddressSpace() == 3 /* local memory */) {
+        // always use undef initializer (instead of zeroinitializer)
+        GV.setInitializer(UndefValue::get(GV.getValueType()));
+      }
+    }
+
+    // write module / bitcode
+    raw_string_ostream bitcode_stream{entry.bitcode_data};
+    WriteBitcode50ToFile(cloned_mod.get(), bitcode_stream);
+    bitcode_stream.flush();
+
+    // hash module
+    entry.hash = compute_sha256_hash((const uint8_t *)entry.bitcode_data.data(),
+                                     entry.bitcode_data.size());
+
+    // write reflection and debug data (just ENDT right now)
+    static const auto end_tag = metallib_program_info::TAG_TYPE::END;
+    static const auto dbg_tag = metallib_program_info::TAG_TYPE::DEBI;
+    static const auto dep_tag = metallib_program_info::TAG_TYPE::DEPF;
+    static const uint32_t tag_length = sizeof(metallib_program_info::TAG_TYPE);
+
+    raw_string_ostream refl_stream{entry.reflection_data};
+    const uint32_t refl_length = tag_length + sizeof(uint32_t);
+    refl_stream.write((const char *)&refl_length, sizeof(uint32_t));
+    refl_stream.write((const char *)&end_tag, tag_length);
+    refl_stream.flush();
+
+    raw_string_ostream dbg_stream{entry.debug_data};
+    if (emit_debug_info) {
+      const uint32_t dep_length =
+          (dependent_bc_file_name.empty()
+               ? 0
+               : tag_length + 2 + dependent_bc_file_name.size());
+      const uint32_t dbg_length = 4u /* len */ + tag_length + 2 + 4 /* line */ +
+                                  src_file_name_length + dep_length +
+                                  tag_length;
+      dbg_stream.write((const char *)&dbg_length, sizeof(dbg_length));
+      dbg_stream.write((const char *)&dbg_tag, tag_length);
+      const uint16_t src_file_info_len = uint16_t(4u + src_file_name_length);
+      dbg_stream.write((const char *)&src_file_info_len,
+                       sizeof(src_file_info_len));
+      uint32_t function_line = 1;
+      if (DISubprogram *sub_prog = func->getSubprogram(); sub_prog) {
+        function_line = sub_prog->getLine();
+      }
+      dbg_stream.write((const char *)&function_line, sizeof(function_line));
+      dbg_stream.write(src_file_name.c_str(), src_file_name_length - 1u);
+      dbg_stream.write('\0');
+      if (!dependent_bc_file_name.empty()) {
+        dbg_stream.write((const char *)&dep_tag, tag_length);
+        const uint16_t dep_info_length =
+            uint16_t(dependent_bc_file_name.size() + 1 /* \0 */);
+        dbg_stream.write((const char *)&dep_info_length,
+                         sizeof(dep_info_length));
+        dbg_stream.write(dependent_bc_file_name.c_str(), dep_info_length - 1u);
+        dbg_stream.write('\0');
+      }
+    } else {
+      const uint32_t dbg_length = 4u /* len */ + tag_length;
+      dbg_stream.write((const char *)&dbg_length, sizeof(dbg_length));
+    }
+    dbg_stream.write((const char *)&end_tag, tag_length);
+    dbg_stream.flush();
+
+    // finish
+    entry.update_length();
+    entries_size += entry.length;
+    reflection_data_size += entry.reflection_size;
+    debug_data_size += entry.debug_size;
+    bitcode_data_size += entry.bitcode_size;
+  }
+
+  // now that we have created all data/info, update all offsets
+  uint64_t running_refl_size = 0, running_dbg_size = 0, running_bc_size = 0;
+  for (uint32_t i = 0; i < function_count; ++i) {
+    auto &entry = prog_info.entries[i];
+    entry.update_offsets(running_refl_size, running_dbg_size, running_bc_size);
+  }
+
+  //// start writing
+  // header
+  OS.write("MTLB", 4);
+
+  metallib_version header{
+      .container_version_major = 1,
+      .is_macos_target = TT.isMacOSX(),
+      .container_version_minor = 2,
+      .container_version_bugfix = 6,
+      .file_type = 0,    // always "execute"
+      .is_stub = false,  // never stub
+      .is_64_bit = true, // always 64-bit
+  };
+  uint32_t platform_major = 0, platform_minor = 0, platform_update = 0;
+  if (TT.isMacOSX()) {
+    header.platform = 1u;
+    TT.getMacOSXVersion(platform_major, platform_minor, platform_update);
+  } else if (TT.isiOS()) {
+    header.platform = 2u;
+    TT.getiOSVersion(platform_major, platform_minor, platform_update);
+  } else if (TT.isTvOS()) {
+    header.platform = 3u;
+    TT.getiOSVersion(platform_major, platform_minor, platform_update);
+  } else if (TT.isWatchOS()) {
+    header.platform = 4u;
+    TT.getWatchOSVersion(platform_major, platform_minor, platform_update);
+  } else {
+    header.platform = 0u;
+  }
+  header.platform_version_major = platform_major;
+  header.platform_version_minor = platform_minor;
+  header.platform_version_update = platform_update;
+
+  OS.write((const char *)&header, sizeof(metallib_version));
+
+  // file length
+  uint64_t ext_program_md_size =
+      sizeof(metallib_program_info::TAG_TYPE) /* ENDT*/;
+  if (target_air_version >= 240) {
+    ext_program_md_size += (4 + 2 + 16) /* UUID */;
+  }
+  if (emit_debug_info) {
+    ext_program_md_size += (4 + 2 + 16) /* HSRD */;
+  }
+  const uint32_t src_archive_header_length =
+      (emit_debug_info
+           ? (sizeof(uint32_t) /* count */ + (linker_cmd.size() + 1) +
+              (working_dir.size() + 1) + sizeof(uint32_t) /* length */)
+           : 0u);
+  const uint32_t src_archive_length =
+      (emit_debug_info
+           ? (sizeof(metallib_program_info::TAG_TYPE) /* magic/tag */ +
+              sizeof(uint32_t) /* archive length */ +
+              sizeof(uint16_t) /* "0" */ + source_archive_data.second +
+              sizeof(metallib_program_info::TAG_TYPE) /* end tag */)
+           : 0u);
+  const uint64_t file_length =
+      (sizeof(metallib_header) + sizeof(uint32_t) /* #programs */ +
+       entries_size + ext_program_md_size + reflection_data_size +
+       debug_data_size + bitcode_data_size + src_archive_header_length +
+       src_archive_length);
+  OS.write((const char *)&file_length, sizeof(uint64_t));
+
+  // header control
+  ctrl.programs_offset = sizeof(metallib_header);
+  ctrl.programs_length = entries_size;
+  ctrl.reflection_offset = ctrl.programs_offset + sizeof(uint32_t) +
+                           ctrl.programs_length + ext_program_md_size;
+  ctrl.reflection_length = reflection_data_size;
+  ctrl.debug_offset = ctrl.reflection_offset + ctrl.reflection_length;
+  ctrl.debug_length = debug_data_size;
+  ctrl.bitcode_offset = ctrl.debug_offset + ctrl.debug_length;
+  ctrl.bitcode_length = bitcode_data_size;
+  OS.write((const char *)&ctrl, sizeof(metallib_header_control));
+
+  // write entry headers/info
+  OS.write((const char *)&function_count, sizeof(function_count));
+  for (auto &entry : prog_info.entries) {
+    if (emit_debug_info) {
+      entry.debug_source_offset = src_archive_header_length;
+    }
+    entry.write_header(OS);
+  }
+
+  // write additional program metadata
+  {
+    // write embedded source code archive metadata
+    if (emit_debug_info) {
+      const auto HSRD_tag = metallib_program_info::TAG_TYPE::HSRD;
+      OS.write((const char *)&HSRD_tag,
+               sizeof(metallib_program_info::TAG_TYPE));
+      OS.write(0x10);
+      OS.write(0x0);
+      const uint64_t src_archives_offset =
+          ctrl.bitcode_offset + bitcode_data_size;
+      const uint64_t src_archives_length =
+          (src_archive_header_length + src_archive_length);
+      OS.write((const char *)&src_archives_offset, sizeof(uint64_t));
+      OS.write((const char *)&src_archives_length, sizeof(uint64_t));
+    }
+
+    // write UUID for Metal 2.4+
+    if (target_air_version >= 240) {
+      // NOTE: Apple doesn't actually care about UUID variants and versions,
+      // so just fill this with random, but still signal variant 1 + version 4
+      random_device rd{};
+      mt19937 gen{rd()};
+      uniform_int_distribution<uint8_t> dist(0u, 0xFFu);
+      raw_ostream::uuid_t program_uuid;
+      for (auto &ch : program_uuid) {
+        ch = dist(gen);
+      }
+      program_uuid[6] = (4u /* version */ << 4u) | (program_uuid[6] & 0x0Fu);
+      program_uuid[8] = (0b10 /* variant */ << 6u) | (program_uuid[8] & 0x3Fu);
+
+      // write
+      const auto UUID_tag = metallib_program_info::TAG_TYPE::UUID;
+      OS.write((const char *)&UUID_tag,
+               sizeof(metallib_program_info::TAG_TYPE));
+      OS.write(0x10);
+      OS.write(0x0);
+      OS.write((const char *)&program_uuid, sizeof(program_uuid));
+    }
+
+    // write ENDT
+    // NOTE: this is not included by the "programs_length"
+    const auto END_tag = metallib_program_info::TAG_TYPE::END;
+    OS.write((const char *)&END_tag, sizeof(metallib_program_info::TAG_TYPE));
+  }
+
+  // write reflection data
+  for (const auto &entry : prog_info.entries) {
+    entry.write_reflection(OS);
+  }
+
+  // write debug data
+  for (const auto &entry : prog_info.entries) {
+    entry.write_debug(OS);
+  }
+
+  // write bitcode data
+  for (const auto &entry : prog_info.entries) {
+    entry.write_module(OS);
+  }
+
+  // write embedded source code archives
+  if (emit_debug_info) {
+    const uint32_t src_archive_count = 1;
+    OS.write((const char *)&src_archive_count, sizeof(src_archive_count));
+
+    // linker command + linker working dir (assume same as compiler)
+    OS.write(linker_cmd.c_str(), linker_cmd.size());
+    OS.write(0);
+    OS.write(working_dir.c_str(), working_dir.size());
+    OS.write(0);
+
+    OS.write((const char *)&src_archive_length, sizeof(src_archive_length));
+
+    const auto SARC_tag = metallib_program_info::TAG_TYPE::SARC;
+    OS.write((const char *)&SARC_tag, sizeof(metallib_program_info::TAG_TYPE));
+
+    const uint32_t source_archive_data_size =
+        source_archive_data.second + sizeof(uint16_t);
+    OS.write((const char *)&source_archive_data_size, sizeof(uint32_t));
+    const char archive_number[] = "0";
+    OS.write((const char *)&archive_number, 2);
+    OS.write((const char *)source_archive_data.first.get(),
+             source_archive_data.second);
+
+    const auto END_tag = metallib_program_info::TAG_TYPE::END;
+    OS.write((const char *)&END_tag, sizeof(metallib_program_info::TAG_TYPE));
+  }
+}
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/LICENSE b/llvm/lib/Bitcode/MetalLib/bzip2/LICENSE
new file mode 100644
index 000000000000..81a37eab7a5b
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/LICENSE
@@ -0,0 +1,42 @@
+
+--------------------------------------------------------------------------
+
+This program, "bzip2", the associated library "libbzip2", and all
+documentation, are copyright (C) 1996-2019 Julian R Seward.  All
+rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+
+2. The origin of this software must not be misrepresented; you must 
+   not claim that you wrote the original software.  If you use this 
+   software in a product, an acknowledgment in the product 
+   documentation would be appreciated but is not required.
+
+3. Altered source versions must be plainly marked as such, and must
+   not be misrepresented as being the original software.
+
+4. The name of the author may not be used to endorse or promote 
+   products derived from this software without specific prior written 
+   permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
+OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
+DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
+GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+Julian Seward, jseward@acm.org
+bzip2/libbzip2 version 1.0.8 of 13 July 2019
+
+--------------------------------------------------------------------------
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/blocksort.c b/llvm/lib/Bitcode/MetalLib/bzip2/blocksort.c
new file mode 100644
index 000000000000..92d81fe287ea
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/blocksort.c
@@ -0,0 +1,1094 @@
+
+/*-------------------------------------------------------------*/
+/*--- Block sorting machinery                               ---*/
+/*---                                           blocksort.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#include "bzlib_private.h"
+
+/*---------------------------------------------*/
+/*--- Fallback O(N log(N)^2) sorting        ---*/
+/*--- algorithm, for repetitive blocks      ---*/
+/*---------------------------------------------*/
+
+/*---------------------------------------------*/
+static 
+__inline__
+void fallbackSimpleSort ( UInt32* fmap, 
+                          UInt32* eclass, 
+                          Int32   lo, 
+                          Int32   hi )
+{
+   Int32 i, j, tmp;
+   UInt32 ec_tmp;
+
+   if (lo == hi) return;
+
+   if (hi - lo > 3) {
+      for ( i = hi-4; i >= lo; i-- ) {
+         tmp = fmap[i];
+         ec_tmp = eclass[tmp];
+         for ( j = i+4; j <= hi && ec_tmp > eclass[fmap[j]]; j += 4 )
+            fmap[j-4] = fmap[j];
+         fmap[j-4] = tmp;
+      }
+   }
+
+   for ( i = hi-1; i >= lo; i-- ) {
+      tmp = fmap[i];
+      ec_tmp = eclass[tmp];
+      for ( j = i+1; j <= hi && ec_tmp > eclass[fmap[j]]; j++ )
+         fmap[j-1] = fmap[j];
+      fmap[j-1] = tmp;
+   }
+}
+
+
+/*---------------------------------------------*/
+#define fswap(zz1, zz2) \
+   { Int32 zztmp = zz1; zz1 = zz2; zz2 = zztmp; }
+
+#define fvswap(zzp1, zzp2, zzn)       \
+{                                     \
+   Int32 yyp1 = (zzp1);               \
+   Int32 yyp2 = (zzp2);               \
+   Int32 yyn  = (zzn);                \
+   while (yyn > 0) {                  \
+      fswap(fmap[yyp1], fmap[yyp2]);  \
+      yyp1++; yyp2++; yyn--;          \
+   }                                  \
+}
+
+
+#define fmin(a,b) ((a) < (b)) ? (a) : (b)
+
+#define fpush(lz,hz) { stackLo[sp] = lz; \
+                       stackHi[sp] = hz; \
+                       sp++; }
+
+#define fpop(lz,hz) { sp--;              \
+                      lz = stackLo[sp];  \
+                      hz = stackHi[sp]; }
+
+#define FALLBACK_QSORT_SMALL_THRESH 10
+#define FALLBACK_QSORT_STACK_SIZE   100
+
+
+static
+void fallbackQSort3 ( UInt32* fmap, 
+                      UInt32* eclass,
+                      Int32   loSt, 
+                      Int32   hiSt )
+{
+   Int32 unLo, unHi, ltLo, gtHi, n, m;
+   Int32 sp, lo, hi;
+   UInt32 med, r, r3;
+   Int32 stackLo[FALLBACK_QSORT_STACK_SIZE];
+   Int32 stackHi[FALLBACK_QSORT_STACK_SIZE];
+
+   r = 0;
+
+   sp = 0;
+   fpush ( loSt, hiSt );
+
+   while (sp > 0) {
+
+      AssertH ( sp < FALLBACK_QSORT_STACK_SIZE - 1, 1004 );
+
+      fpop ( lo, hi );
+      if (hi - lo < FALLBACK_QSORT_SMALL_THRESH) {
+         fallbackSimpleSort ( fmap, eclass, lo, hi );
+         continue;
+      }
+
+      /* Random partitioning.  Median of 3 sometimes fails to
+         avoid bad cases.  Median of 9 seems to help but 
+         looks rather expensive.  This too seems to work but
+         is cheaper.  Guidance for the magic constants 
+         7621 and 32768 is taken from Sedgewick's algorithms
+         book, chapter 35.
+      */
+      r = ((r * 7621) + 1) % 32768;
+      r3 = r % 3;
+      if (r3 == 0) med = eclass[fmap[lo]]; else
+      if (r3 == 1) med = eclass[fmap[(lo+hi)>>1]]; else
+                   med = eclass[fmap[hi]];
+
+      unLo = ltLo = lo;
+      unHi = gtHi = hi;
+
+      while (1) {
+         while (1) {
+            if (unLo > unHi) break;
+            n = (Int32)eclass[fmap[unLo]] - (Int32)med;
+            if (n == 0) { 
+               fswap(fmap[unLo], fmap[ltLo]); 
+               ltLo++; unLo++; 
+               continue; 
+            };
+            if (n > 0) break;
+            unLo++;
+         }
+         while (1) {
+            if (unLo > unHi) break;
+            n = (Int32)eclass[fmap[unHi]] - (Int32)med;
+            if (n == 0) { 
+               fswap(fmap[unHi], fmap[gtHi]); 
+               gtHi--; unHi--; 
+               continue; 
+            };
+            if (n < 0) break;
+            unHi--;
+         }
+         if (unLo > unHi) break;
+         fswap(fmap[unLo], fmap[unHi]); unLo++; unHi--;
+      }
+
+      AssertD ( unHi == unLo-1, "fallbackQSort3(2)" );
+
+      if (gtHi < ltLo) continue;
+
+      n = fmin(ltLo-lo, unLo-ltLo); fvswap(lo, unLo-n, n);
+      m = fmin(hi-gtHi, gtHi-unHi); fvswap(unLo, hi-m+1, m);
+
+      n = lo + unLo - ltLo - 1;
+      m = hi - (gtHi - unHi) + 1;
+
+      if (n - lo > hi - m) {
+         fpush ( lo, n );
+         fpush ( m, hi );
+      } else {
+         fpush ( m, hi );
+         fpush ( lo, n );
+      }
+   }
+}
+
+#undef fmin
+#undef fpush
+#undef fpop
+#undef fswap
+#undef fvswap
+#undef FALLBACK_QSORT_SMALL_THRESH
+#undef FALLBACK_QSORT_STACK_SIZE
+
+
+/*---------------------------------------------*/
+/* Pre:
+      nblock > 0
+      eclass exists for [0 .. nblock-1]
+      ((UChar*)eclass) [0 .. nblock-1] holds block
+      ptr exists for [0 .. nblock-1]
+
+   Post:
+      ((UChar*)eclass) [0 .. nblock-1] holds block
+      All other areas of eclass destroyed
+      fmap [0 .. nblock-1] holds sorted order
+      bhtab [ 0 .. 2+(nblock/32) ] destroyed
+*/
+
+#define       SET_BH(zz)  bhtab[(zz) >> 5] |= ((UInt32)1 << ((zz) & 31))
+#define     CLEAR_BH(zz)  bhtab[(zz) >> 5] &= ~((UInt32)1 << ((zz) & 31))
+#define     ISSET_BH(zz)  (bhtab[(zz) >> 5] & ((UInt32)1 << ((zz) & 31)))
+#define      WORD_BH(zz)  bhtab[(zz) >> 5]
+#define UNALIGNED_BH(zz)  ((zz) & 0x01f)
+
+static
+void fallbackSort ( UInt32* fmap, 
+                    UInt32* eclass, 
+                    UInt32* bhtab,
+                    Int32   nblock,
+                    Int32   verb )
+{
+   Int32 ftab[257];
+   Int32 ftabCopy[256];
+   Int32 H, i, j, k, l, r, cc, cc1;
+   Int32 nNotDone;
+   Int32 nBhtab;
+   UChar* eclass8 = (UChar*)eclass;
+
+   /*--
+      Initial 1-char radix sort to generate
+      initial fmap and initial BH bits.
+   --*/
+   if (verb >= 4)
+      VPrintf0 ( "        bucket sorting ...\n" );
+   for (i = 0; i < 257;    i++) ftab[i] = 0;
+   for (i = 0; i < nblock; i++) ftab[eclass8[i]]++;
+   for (i = 0; i < 256;    i++) ftabCopy[i] = ftab[i];
+   for (i = 1; i < 257;    i++) ftab[i] += ftab[i-1];
+
+   for (i = 0; i < nblock; i++) {
+      j = eclass8[i];
+      k = ftab[j] - 1;
+      ftab[j] = k;
+      fmap[k] = i;
+   }
+
+   nBhtab = 2 + (nblock / 32);
+   for (i = 0; i < nBhtab; i++) bhtab[i] = 0;
+   for (i = 0; i < 256; i++) SET_BH(ftab[i]);
+
+   /*--
+      Inductively refine the buckets.  Kind-of an
+      "exponential radix sort" (!), inspired by the
+      Manber-Myers suffix array construction algorithm.
+   --*/
+
+   /*-- set sentinel bits for block-end detection --*/
+   for (i = 0; i < 32; i++) { 
+      SET_BH(nblock + 2*i);
+      CLEAR_BH(nblock + 2*i + 1);
+   }
+
+   /*-- the log(N) loop --*/
+   H = 1;
+   while (1) {
+
+      if (verb >= 4) 
+         VPrintf1 ( "        depth %6d has ", H );
+
+      j = 0;
+      for (i = 0; i < nblock; i++) {
+         if (ISSET_BH(i)) j = i;
+         k = fmap[i] - H; if (k < 0) k += nblock;
+         eclass[k] = j;
+      }
+
+      nNotDone = 0;
+      r = -1;
+      while (1) {
+
+	 /*-- find the next non-singleton bucket --*/
+         k = r + 1;
+         while (ISSET_BH(k) && UNALIGNED_BH(k)) k++;
+         if (ISSET_BH(k)) {
+            while (WORD_BH(k) == 0xffffffff) k += 32;
+            while (ISSET_BH(k)) k++;
+         }
+         l = k - 1;
+         if (l >= nblock) break;
+         while (!ISSET_BH(k) && UNALIGNED_BH(k)) k++;
+         if (!ISSET_BH(k)) {
+            while (WORD_BH(k) == 0x00000000) k += 32;
+            while (!ISSET_BH(k)) k++;
+         }
+         r = k - 1;
+         if (r >= nblock) break;
+
+         /*-- now [l, r] bracket current bucket --*/
+         if (r > l) {
+            nNotDone += (r - l + 1);
+            fallbackQSort3 ( fmap, eclass, l, r );
+
+            /*-- scan bucket and generate header bits-- */
+            cc = -1;
+            for (i = l; i <= r; i++) {
+               cc1 = eclass[fmap[i]];
+               if (cc != cc1) { SET_BH(i); cc = cc1; };
+            }
+         }
+      }
+
+      if (verb >= 4) 
+         VPrintf1 ( "%6d unresolved strings\n", nNotDone );
+
+      H *= 2;
+      if (H > nblock || nNotDone == 0) break;
+   }
+
+   /*-- 
+      Reconstruct the original block in
+      eclass8 [0 .. nblock-1], since the
+      previous phase destroyed it.
+   --*/
+   if (verb >= 4)
+      VPrintf0 ( "        reconstructing block ...\n" );
+   j = 0;
+   for (i = 0; i < nblock; i++) {
+      while (ftabCopy[j] == 0) j++;
+      ftabCopy[j]--;
+      eclass8[fmap[i]] = (UChar)j;
+   }
+   AssertH ( j < 256, 1005 );
+}
+
+#undef       SET_BH
+#undef     CLEAR_BH
+#undef     ISSET_BH
+#undef      WORD_BH
+#undef UNALIGNED_BH
+
+
+/*---------------------------------------------*/
+/*--- The main, O(N^2 log(N)) sorting       ---*/
+/*--- algorithm.  Faster for "normal"       ---*/
+/*--- non-repetitive blocks.                ---*/
+/*---------------------------------------------*/
+
+/*---------------------------------------------*/
+static
+__inline__
+Bool mainGtU ( UInt32  i1, 
+               UInt32  i2,
+               UChar*  block, 
+               UInt16* quadrant,
+               UInt32  nblock,
+               Int32*  budget )
+{
+   Int32  k;
+   UChar  c1, c2;
+   UInt16 s1, s2;
+
+   AssertD ( i1 != i2, "mainGtU" );
+   /* 1 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 2 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 3 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 4 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 5 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 6 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 7 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 8 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 9 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 10 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 11 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+   /* 12 */
+   c1 = block[i1]; c2 = block[i2];
+   if (c1 != c2) return (c1 > c2);
+   i1++; i2++;
+
+   k = nblock + 8;
+
+   do {
+      /* 1 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 2 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 3 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 4 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 5 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 6 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 7 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+      /* 8 */
+      c1 = block[i1]; c2 = block[i2];
+      if (c1 != c2) return (c1 > c2);
+      s1 = quadrant[i1]; s2 = quadrant[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1++; i2++;
+
+      if (i1 >= nblock) i1 -= nblock;
+      if (i2 >= nblock) i2 -= nblock;
+
+      k -= 8;
+      (*budget)--;
+   }
+      while (k >= 0);
+
+   return False;
+}
+
+
+/*---------------------------------------------*/
+/*--
+   Knuth's increments seem to work better
+   than Incerpi-Sedgewick here.  Possibly
+   because the number of elems to sort is
+   usually small, typically <= 20.
+--*/
+static
+Int32 incs[14] = { 1, 4, 13, 40, 121, 364, 1093, 3280,
+                   9841, 29524, 88573, 265720,
+                   797161, 2391484 };
+
+static
+void mainSimpleSort ( UInt32* ptr,
+                      UChar*  block,
+                      UInt16* quadrant,
+                      Int32   nblock,
+                      Int32   lo, 
+                      Int32   hi, 
+                      Int32   d,
+                      Int32*  budget )
+{
+   Int32 i, j, h, bigN, hp;
+   UInt32 v;
+
+   bigN = hi - lo + 1;
+   if (bigN < 2) return;
+
+   hp = 0;
+   while (incs[hp] < bigN) hp++;
+   hp--;
+
+   for (; hp >= 0; hp--) {
+      h = incs[hp];
+
+      i = lo + h;
+      while (True) {
+
+         /*-- copy 1 --*/
+         if (i > hi) break;
+         v = ptr[i];
+         j = i;
+         while ( mainGtU ( 
+                    ptr[j-h]+d, v+d, block, quadrant, nblock, budget 
+                 ) ) {
+            ptr[j] = ptr[j-h];
+            j = j - h;
+            if (j <= (lo + h - 1)) break;
+         }
+         ptr[j] = v;
+         i++;
+
+         /*-- copy 2 --*/
+         if (i > hi) break;
+         v = ptr[i];
+         j = i;
+         while ( mainGtU ( 
+                    ptr[j-h]+d, v+d, block, quadrant, nblock, budget 
+                 ) ) {
+            ptr[j] = ptr[j-h];
+            j = j - h;
+            if (j <= (lo + h - 1)) break;
+         }
+         ptr[j] = v;
+         i++;
+
+         /*-- copy 3 --*/
+         if (i > hi) break;
+         v = ptr[i];
+         j = i;
+         while ( mainGtU ( 
+                    ptr[j-h]+d, v+d, block, quadrant, nblock, budget 
+                 ) ) {
+            ptr[j] = ptr[j-h];
+            j = j - h;
+            if (j <= (lo + h - 1)) break;
+         }
+         ptr[j] = v;
+         i++;
+
+         if (*budget < 0) return;
+      }
+   }
+}
+
+
+/*---------------------------------------------*/
+/*--
+   The following is an implementation of
+   an elegant 3-way quicksort for strings,
+   described in a paper "Fast Algorithms for
+   Sorting and Searching Strings", by Robert
+   Sedgewick and Jon L. Bentley.
+--*/
+
+#define mswap(zz1, zz2) \
+   { Int32 zztmp = zz1; zz1 = zz2; zz2 = zztmp; }
+
+#define mvswap(zzp1, zzp2, zzn)       \
+{                                     \
+   Int32 yyp1 = (zzp1);               \
+   Int32 yyp2 = (zzp2);               \
+   Int32 yyn  = (zzn);                \
+   while (yyn > 0) {                  \
+      mswap(ptr[yyp1], ptr[yyp2]);    \
+      yyp1++; yyp2++; yyn--;          \
+   }                                  \
+}
+
+static 
+__inline__
+UChar mmed3 ( UChar a, UChar b, UChar c )
+{
+   UChar t;
+   if (a > b) { t = a; a = b; b = t; };
+   if (b > c) { 
+      b = c;
+      if (a > b) b = a;
+   }
+   return b;
+}
+
+#define mmin(a,b) ((a) < (b)) ? (a) : (b)
+
+#define mpush(lz,hz,dz) { stackLo[sp] = lz; \
+                          stackHi[sp] = hz; \
+                          stackD [sp] = dz; \
+                          sp++; }
+
+#define mpop(lz,hz,dz) { sp--;             \
+                         lz = stackLo[sp]; \
+                         hz = stackHi[sp]; \
+                         dz = stackD [sp]; }
+
+
+#define mnextsize(az) (nextHi[az]-nextLo[az])
+
+#define mnextswap(az,bz)                                        \
+   { Int32 tz;                                                  \
+     tz = nextLo[az]; nextLo[az] = nextLo[bz]; nextLo[bz] = tz; \
+     tz = nextHi[az]; nextHi[az] = nextHi[bz]; nextHi[bz] = tz; \
+     tz = nextD [az]; nextD [az] = nextD [bz]; nextD [bz] = tz; }
+
+
+#define MAIN_QSORT_SMALL_THRESH 20
+#define MAIN_QSORT_DEPTH_THRESH (BZ_N_RADIX + BZ_N_QSORT)
+#define MAIN_QSORT_STACK_SIZE 100
+
+static
+void mainQSort3 ( UInt32* ptr,
+                  UChar*  block,
+                  UInt16* quadrant,
+                  Int32   nblock,
+                  Int32   loSt, 
+                  Int32   hiSt, 
+                  Int32   dSt,
+                  Int32*  budget )
+{
+   Int32 unLo, unHi, ltLo, gtHi, n, m, med;
+   Int32 sp, lo, hi, d;
+
+   Int32 stackLo[MAIN_QSORT_STACK_SIZE];
+   Int32 stackHi[MAIN_QSORT_STACK_SIZE];
+   Int32 stackD [MAIN_QSORT_STACK_SIZE];
+
+   Int32 nextLo[3];
+   Int32 nextHi[3];
+   Int32 nextD [3];
+
+   sp = 0;
+   mpush ( loSt, hiSt, dSt );
+
+   while (sp > 0) {
+
+      AssertH ( sp < MAIN_QSORT_STACK_SIZE - 2, 1001 );
+
+      mpop ( lo, hi, d );
+      if (hi - lo < MAIN_QSORT_SMALL_THRESH || 
+          d > MAIN_QSORT_DEPTH_THRESH) {
+         mainSimpleSort ( ptr, block, quadrant, nblock, lo, hi, d, budget );
+         if (*budget < 0) return;
+         continue;
+      }
+
+      med = (Int32) 
+            mmed3 ( block[ptr[ lo         ]+d],
+                    block[ptr[ hi         ]+d],
+                    block[ptr[ (lo+hi)>>1 ]+d] );
+
+      unLo = ltLo = lo;
+      unHi = gtHi = hi;
+
+      while (True) {
+         while (True) {
+            if (unLo > unHi) break;
+            n = ((Int32)block[ptr[unLo]+d]) - med;
+            if (n == 0) { 
+               mswap(ptr[unLo], ptr[ltLo]); 
+               ltLo++; unLo++; continue; 
+            };
+            if (n >  0) break;
+            unLo++;
+         }
+         while (True) {
+            if (unLo > unHi) break;
+            n = ((Int32)block[ptr[unHi]+d]) - med;
+            if (n == 0) { 
+               mswap(ptr[unHi], ptr[gtHi]); 
+               gtHi--; unHi--; continue; 
+            };
+            if (n <  0) break;
+            unHi--;
+         }
+         if (unLo > unHi) break;
+         mswap(ptr[unLo], ptr[unHi]); unLo++; unHi--;
+      }
+
+      AssertD ( unHi == unLo-1, "mainQSort3(2)" );
+
+      if (gtHi < ltLo) {
+         mpush(lo, hi, d+1 );
+         continue;
+      }
+
+      n = mmin(ltLo-lo, unLo-ltLo); mvswap(lo, unLo-n, n);
+      m = mmin(hi-gtHi, gtHi-unHi); mvswap(unLo, hi-m+1, m);
+
+      n = lo + unLo - ltLo - 1;
+      m = hi - (gtHi - unHi) + 1;
+
+      nextLo[0] = lo;  nextHi[0] = n;   nextD[0] = d;
+      nextLo[1] = m;   nextHi[1] = hi;  nextD[1] = d;
+      nextLo[2] = n+1; nextHi[2] = m-1; nextD[2] = d+1;
+
+      if (mnextsize(0) < mnextsize(1)) mnextswap(0,1);
+      if (mnextsize(1) < mnextsize(2)) mnextswap(1,2);
+      if (mnextsize(0) < mnextsize(1)) mnextswap(0,1);
+
+      AssertD (mnextsize(0) >= mnextsize(1), "mainQSort3(8)" );
+      AssertD (mnextsize(1) >= mnextsize(2), "mainQSort3(9)" );
+
+      mpush (nextLo[0], nextHi[0], nextD[0]);
+      mpush (nextLo[1], nextHi[1], nextD[1]);
+      mpush (nextLo[2], nextHi[2], nextD[2]);
+   }
+}
+
+#undef mswap
+#undef mvswap
+#undef mpush
+#undef mpop
+#undef mmin
+#undef mnextsize
+#undef mnextswap
+#undef MAIN_QSORT_SMALL_THRESH
+#undef MAIN_QSORT_DEPTH_THRESH
+#undef MAIN_QSORT_STACK_SIZE
+
+
+/*---------------------------------------------*/
+/* Pre:
+      nblock > N_OVERSHOOT
+      block32 exists for [0 .. nblock-1 +N_OVERSHOOT]
+      ((UChar*)block32) [0 .. nblock-1] holds block
+      ptr exists for [0 .. nblock-1]
+
+   Post:
+      ((UChar*)block32) [0 .. nblock-1] holds block
+      All other areas of block32 destroyed
+      ftab [0 .. 65536 ] destroyed
+      ptr [0 .. nblock-1] holds sorted order
+      if (*budget < 0), sorting was abandoned
+*/
+
+#define BIGFREQ(b) (ftab[((b)+1) << 8] - ftab[(b) << 8])
+#define SETMASK (1 << 21)
+#define CLEARMASK (~(SETMASK))
+
+static
+void mainSort ( UInt32* ptr, 
+                UChar*  block,
+                UInt16* quadrant, 
+                UInt32* ftab,
+                Int32   nblock,
+                Int32   verb,
+                Int32*  budget )
+{
+   Int32  i, j, k, ss, sb;
+   Int32  runningOrder[256];
+   Bool   bigDone[256];
+   Int32  copyStart[256];
+   Int32  copyEnd  [256];
+   UChar  c1;
+   Int32  numQSorted;
+   UInt16 s;
+   if (verb >= 4) VPrintf0 ( "        main sort initialise ...\n" );
+
+   /*-- set up the 2-byte frequency table --*/
+   for (i = 65536; i >= 0; i--) ftab[i] = 0;
+
+   j = block[0] << 8;
+   i = nblock-1;
+   for (; i >= 3; i -= 4) {
+      quadrant[i] = 0;
+      j = (j >> 8) | ( ((UInt16)block[i]) << 8);
+      ftab[j]++;
+      quadrant[i-1] = 0;
+      j = (j >> 8) | ( ((UInt16)block[i-1]) << 8);
+      ftab[j]++;
+      quadrant[i-2] = 0;
+      j = (j >> 8) | ( ((UInt16)block[i-2]) << 8);
+      ftab[j]++;
+      quadrant[i-3] = 0;
+      j = (j >> 8) | ( ((UInt16)block[i-3]) << 8);
+      ftab[j]++;
+   }
+   for (; i >= 0; i--) {
+      quadrant[i] = 0;
+      j = (j >> 8) | ( ((UInt16)block[i]) << 8);
+      ftab[j]++;
+   }
+
+   /*-- (emphasises close relationship of block & quadrant) --*/
+   for (i = 0; i < BZ_N_OVERSHOOT; i++) {
+      block   [nblock+i] = block[i];
+      quadrant[nblock+i] = 0;
+   }
+
+   if (verb >= 4) VPrintf0 ( "        bucket sorting ...\n" );
+
+   /*-- Complete the initial radix sort --*/
+   for (i = 1; i <= 65536; i++) ftab[i] += ftab[i-1];
+
+   s = block[0] << 8;
+   i = nblock-1;
+   for (; i >= 3; i -= 4) {
+      s = (s >> 8) | (block[i] << 8);
+      j = ftab[s] -1;
+      ftab[s] = j;
+      ptr[j] = i;
+      s = (s >> 8) | (block[i-1] << 8);
+      j = ftab[s] -1;
+      ftab[s] = j;
+      ptr[j] = i-1;
+      s = (s >> 8) | (block[i-2] << 8);
+      j = ftab[s] -1;
+      ftab[s] = j;
+      ptr[j] = i-2;
+      s = (s >> 8) | (block[i-3] << 8);
+      j = ftab[s] -1;
+      ftab[s] = j;
+      ptr[j] = i-3;
+   }
+   for (; i >= 0; i--) {
+      s = (s >> 8) | (block[i] << 8);
+      j = ftab[s] -1;
+      ftab[s] = j;
+      ptr[j] = i;
+   }
+
+   /*--
+      Now ftab contains the first loc of every small bucket.
+      Calculate the running order, from smallest to largest
+      big bucket.
+   --*/
+   for (i = 0; i <= 255; i++) {
+      bigDone     [i] = False;
+      runningOrder[i] = i;
+   }
+
+   {
+      Int32 vv;
+      Int32 h = 1;
+      do h = 3 * h + 1; while (h <= 256);
+      do {
+         h = h / 3;
+         for (i = h; i <= 255; i++) {
+            vv = runningOrder[i];
+            j = i;
+            while ( BIGFREQ(runningOrder[j-h]) > BIGFREQ(vv) ) {
+               runningOrder[j] = runningOrder[j-h];
+               j = j - h;
+               if (j <= (h - 1)) goto zero;
+            }
+            zero:
+            runningOrder[j] = vv;
+         }
+      } while (h != 1);
+   }
+
+   /*--
+      The main sorting loop.
+   --*/
+
+   numQSorted = 0;
+
+   for (i = 0; i <= 255; i++) {
+
+      /*--
+         Process big buckets, starting with the least full.
+         Basically this is a 3-step process in which we call
+         mainQSort3 to sort the small buckets [ss, j], but
+         also make a big effort to avoid the calls if we can.
+      --*/
+      ss = runningOrder[i];
+
+      /*--
+         Step 1:
+         Complete the big bucket [ss] by quicksorting
+         any unsorted small buckets [ss, j], for j != ss.  
+         Hopefully previous pointer-scanning phases have already
+         completed many of the small buckets [ss, j], so
+         we don't have to sort them at all.
+      --*/
+      for (j = 0; j <= 255; j++) {
+         if (j != ss) {
+            sb = (ss << 8) + j;
+            if ( ! (ftab[sb] & SETMASK) ) {
+               Int32 lo = ftab[sb]   & CLEARMASK;
+               Int32 hi = (ftab[sb+1] & CLEARMASK) - 1;
+               if (hi > lo) {
+                  if (verb >= 4)
+                     VPrintf4 ( "        qsort [0x%x, 0x%x]   "
+                                "done %d   this %d\n",
+                                ss, j, numQSorted, hi - lo + 1 );
+                  mainQSort3 ( 
+                     ptr, block, quadrant, nblock, 
+                     lo, hi, BZ_N_RADIX, budget 
+                  );   
+                  numQSorted += (hi - lo + 1);
+                  if (*budget < 0) return;
+               }
+            }
+            ftab[sb] |= SETMASK;
+         }
+      }
+
+      AssertH ( !bigDone[ss], 1006 );
+
+      /*--
+         Step 2:
+         Now scan this big bucket [ss] so as to synthesise the
+         sorted order for small buckets [t, ss] for all t,
+         including, magically, the bucket [ss,ss] too.
+         This will avoid doing Real Work in subsequent Step 1's.
+      --*/
+      {
+         for (j = 0; j <= 255; j++) {
+            copyStart[j] =  ftab[(j << 8) + ss]     & CLEARMASK;
+            copyEnd  [j] = (ftab[(j << 8) + ss + 1] & CLEARMASK) - 1;
+         }
+         for (j = ftab[ss << 8] & CLEARMASK; j < copyStart[ss]; j++) {
+            k = ptr[j]-1; if (k < 0) k += nblock;
+            c1 = block[k];
+            if (!bigDone[c1])
+               ptr[ copyStart[c1]++ ] = k;
+         }
+         for (j = (ftab[(ss+1) << 8] & CLEARMASK) - 1; j > copyEnd[ss]; j--) {
+            k = ptr[j]-1; if (k < 0) k += nblock;
+            c1 = block[k];
+            if (!bigDone[c1]) 
+               ptr[ copyEnd[c1]-- ] = k;
+         }
+      }
+
+      AssertH ( (copyStart[ss]-1 == copyEnd[ss])
+                || 
+                /* Extremely rare case missing in bzip2-1.0.0 and 1.0.1.
+                   Necessity for this case is demonstrated by compressing 
+                   a sequence of approximately 48.5 million of character 
+                   251; 1.0.0/1.0.1 will then die here. */
+                (copyStart[ss] == 0 && copyEnd[ss] == nblock-1),
+                1007 )
+
+      for (j = 0; j <= 255; j++) ftab[(j << 8) + ss] |= SETMASK;
+
+      /*--
+         Step 3:
+         The [ss] big bucket is now done.  Record this fact,
+         and update the quadrant descriptors.  Remember to
+         update quadrants in the overshoot area too, if
+         necessary.  The "if (i < 255)" test merely skips
+         this updating for the last bucket processed, since
+         updating for the last bucket is pointless.
+
+         The quadrant array provides a way to incrementally
+         cache sort orderings, as they appear, so as to 
+         make subsequent comparisons in fullGtU() complete
+         faster.  For repetitive blocks this makes a big
+         difference (but not big enough to be able to avoid
+         the fallback sorting mechanism, exponential radix sort).
+
+         The precise meaning is: at all times:
+
+            for 0 <= i < nblock and 0 <= j <= nblock
+
+            if block[i] != block[j], 
+
+               then the relative values of quadrant[i] and 
+                    quadrant[j] are meaningless.
+
+               else {
+                  if quadrant[i] < quadrant[j]
+                     then the string starting at i lexicographically
+                     precedes the string starting at j
+
+                  else if quadrant[i] > quadrant[j]
+                     then the string starting at j lexicographically
+                     precedes the string starting at i
+
+                  else
+                     the relative ordering of the strings starting
+                     at i and j has not yet been determined.
+               }
+      --*/
+      bigDone[ss] = True;
+
+      if (i < 255) {
+         Int32 bbStart  = ftab[ss << 8] & CLEARMASK;
+         Int32 bbSize   = (ftab[(ss+1) << 8] & CLEARMASK) - bbStart;
+         Int32 shifts   = 0;
+
+         while ((bbSize >> shifts) > 65534) shifts++;
+
+         for (j = bbSize-1; j >= 0; j--) {
+            Int32 a2update     = ptr[bbStart + j];
+            UInt16 qVal        = (UInt16)(j >> shifts);
+            quadrant[a2update] = qVal;
+            if (a2update < BZ_N_OVERSHOOT)
+               quadrant[a2update + nblock] = qVal;
+         }
+         AssertH ( ((bbSize-1) >> shifts) <= 65535, 1002 );
+      }
+
+   }
+
+   if (verb >= 4)
+      VPrintf3 ( "        %d pointers, %d sorted, %d scanned\n",
+                 nblock, numQSorted, nblock - numQSorted );
+}
+
+#undef BIGFREQ
+#undef SETMASK
+#undef CLEARMASK
+
+
+/*---------------------------------------------*/
+/* Pre:
+      nblock > 0
+      arr2 exists for [0 .. nblock-1 +N_OVERSHOOT]
+      ((UChar*)arr2)  [0 .. nblock-1] holds block
+      arr1 exists for [0 .. nblock-1]
+
+   Post:
+      ((UChar*)arr2) [0 .. nblock-1] holds block
+      All other areas of block destroyed
+      ftab [ 0 .. 65536 ] destroyed
+      arr1 [0 .. nblock-1] holds sorted order
+*/
+void BZ2_blockSort ( EState* s )
+{
+   UInt32* ptr    = s->ptr; 
+   UChar*  block  = s->block;
+   UInt32* ftab   = s->ftab;
+   Int32   nblock = s->nblock;
+   Int32   verb   = s->verbosity;
+   Int32   wfact  = s->workFactor;
+   UInt16* quadrant;
+   Int32   budget;
+   Int32   budgetInit;
+   Int32   i;
+
+   if (nblock < 10000) {
+      fallbackSort ( s->arr1, s->arr2, ftab, nblock, verb );
+   } else {
+      /* Calculate the location for quadrant, remembering to get
+         the alignment right.  Assumes that &(block[0]) is at least
+         2-byte aligned -- this should be ok since block is really
+         the first section of arr2.
+      */
+      i = nblock+BZ_N_OVERSHOOT;
+      if (i & 1) i++;
+      quadrant = (UInt16*)(&(block[i]));
+
+      /* (wfact-1) / 3 puts the default-factor-30
+         transition point at very roughly the same place as 
+         with v0.1 and v0.9.0.  
+         Not that it particularly matters any more, since the
+         resulting compressed stream is now the same regardless
+         of whether or not we use the main sort or fallback sort.
+      */
+      if (wfact < 1  ) wfact = 1;
+      if (wfact > 100) wfact = 100;
+      budgetInit = nblock * ((wfact-1) / 3);
+      budget = budgetInit;
+
+      mainSort ( ptr, block, quadrant, ftab, nblock, verb, &budget );
+      if (verb >= 3) 
+         VPrintf3 ( "      %d work, %d block, ratio %5.2f\n",
+                    budgetInit - budget,
+                    nblock, 
+                    (float)(budgetInit - budget) /
+                    (float)(nblock==0 ? 1 : nblock) ); 
+      if (budget < 0) {
+         if (verb >= 2) 
+            VPrintf0 ( "    too repetitive; using fallback"
+                       " sorting algorithm\n" );
+         fallbackSort ( s->arr1, s->arr2, ftab, nblock, verb );
+      }
+   }
+
+   s->origPtr = -1;
+   for (i = 0; i < s->nblock; i++)
+      if (ptr[i] == 0)
+         { s->origPtr = i; break; };
+
+   AssertH( s->origPtr != -1, 1003 );
+}
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                       blocksort.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.c b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.c
new file mode 100644
index 000000000000..21786551b60b
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.c
@@ -0,0 +1,1572 @@
+
+/*-------------------------------------------------------------*/
+/*--- Library top-level functions.                          ---*/
+/*---                                               bzlib.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+/* CHANGES
+   0.9.0    -- original version.
+   0.9.0a/b -- no changes in this file.
+   0.9.0c   -- made zero-length BZ_FLUSH work correctly in bzCompress().
+     fixed bzWrite/bzRead to ignore zero-length requests.
+     fixed bzread to correctly handle read requests after EOF.
+     wrong parameter order in call to bzDecompressInit in
+     bzBuffToBuffDecompress.  Fixed.
+*/
+
+#include "bzlib_private.h"
+
+
+/*---------------------------------------------------*/
+/*--- Compression stuff                           ---*/
+/*---------------------------------------------------*/
+
+
+/*---------------------------------------------------*/
+#ifndef BZ_NO_STDIO
+void BZ2_bz__AssertH__fail ( int errcode )
+{
+   fprintf(stderr, 
+      "\n\nbzip2/libbzip2: internal error number %d.\n"
+      "This is a bug in bzip2/libbzip2, %s.\n"
+      "Please report it to: bzip2-devel@sourceware.org.  If this happened\n"
+      "when you were using some program which uses libbzip2 as a\n"
+      "component, you should also report this bug to the author(s)\n"
+      "of that program.  Please make an effort to report this bug;\n"
+      "timely and accurate bug reports eventually lead to higher\n"
+      "quality software.  Thanks.\n\n",
+      errcode,
+      BZ2_bzlibVersion()
+   );
+
+   if (errcode == 1007) {
+   fprintf(stderr,
+      "\n*** A special note about internal error number 1007 ***\n"
+      "\n"
+      "Experience suggests that a common cause of i.e. 1007\n"
+      "is unreliable memory or other hardware.  The 1007 assertion\n"
+      "just happens to cross-check the results of huge numbers of\n"
+      "memory reads/writes, and so acts (unintendedly) as a stress\n"
+      "test of your memory system.\n"
+      "\n"
+      "I suggest the following: try compressing the file again,\n"
+      "possibly monitoring progress in detail with the -vv flag.\n"
+      "\n"
+      "* If the error cannot be reproduced, and/or happens at different\n"
+      "  points in compression, you may have a flaky memory system.\n"
+      "  Try a memory-test program.  I have used Memtest86\n"
+      "  (www.memtest86.com).  At the time of writing it is free (GPLd).\n"
+      "  Memtest86 tests memory much more thorougly than your BIOSs\n"
+      "  power-on test, and may find failures that the BIOS doesn't.\n"
+      "\n"
+      "* If the error can be repeatably reproduced, this is a bug in\n"
+      "  bzip2, and I would very much like to hear about it.  Please\n"
+      "  let me know, and, ideally, save a copy of the file causing the\n"
+      "  problem -- without which I will be unable to investigate it.\n"
+      "\n"
+   );
+   }
+
+   exit(3);
+}
+#endif
+
+
+/*---------------------------------------------------*/
+static
+int bz_config_ok ( void )
+{
+   if (sizeof(int)   != 4) return 0;
+   if (sizeof(short) != 2) return 0;
+   if (sizeof(char)  != 1) return 0;
+   return 1;
+}
+
+
+/*---------------------------------------------------*/
+static
+void* default_bzalloc ( void* opaque, Int32 items, Int32 size )
+{
+   void* v = malloc ( items * size );
+   return v;
+}
+
+static
+void default_bzfree ( void* opaque, void* addr )
+{
+   if (addr != NULL) free ( addr );
+}
+
+
+/*---------------------------------------------------*/
+static
+void prepare_new_block ( EState* s )
+{
+   Int32 i;
+   s->nblock = 0;
+   s->numZ = 0;
+   s->state_out_pos = 0;
+   BZ_INITIALISE_CRC ( s->blockCRC );
+   for (i = 0; i < 256; i++) s->inUse[i] = False;
+   s->blockNo++;
+}
+
+
+/*---------------------------------------------------*/
+static
+void init_RL ( EState* s )
+{
+   s->state_in_ch  = 256;
+   s->state_in_len = 0;
+}
+
+
+static
+Bool isempty_RL ( EState* s )
+{
+   if (s->state_in_ch < 256 && s->state_in_len > 0)
+      return False; else
+      return True;
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzCompressInit) 
+                    ( bz_stream* strm, 
+                     int        blockSize100k,
+                     int        verbosity,
+                     int        workFactor )
+{
+   Int32   n;
+   EState* s;
+
+   if (!bz_config_ok()) return BZ_CONFIG_ERROR;
+
+   if (strm == NULL || 
+       blockSize100k < 1 || blockSize100k > 9 ||
+       workFactor < 0 || workFactor > 250)
+     return BZ_PARAM_ERROR;
+
+   if (workFactor == 0) workFactor = 30;
+   if (strm->bzalloc == NULL) strm->bzalloc = default_bzalloc;
+   if (strm->bzfree == NULL) strm->bzfree = default_bzfree;
+
+   s = BZALLOC( sizeof(EState) );
+   if (s == NULL) return BZ_MEM_ERROR;
+   s->strm = strm;
+
+   s->arr1 = NULL;
+   s->arr2 = NULL;
+   s->ftab = NULL;
+
+   n       = 100000 * blockSize100k;
+   s->arr1 = BZALLOC( n                  * sizeof(UInt32) );
+   s->arr2 = BZALLOC( (n+BZ_N_OVERSHOOT) * sizeof(UInt32) );
+   s->ftab = BZALLOC( 65537              * sizeof(UInt32) );
+
+   if (s->arr1 == NULL || s->arr2 == NULL || s->ftab == NULL) {
+      if (s->arr1 != NULL) BZFREE(s->arr1);
+      if (s->arr2 != NULL) BZFREE(s->arr2);
+      if (s->ftab != NULL) BZFREE(s->ftab);
+      if (s       != NULL) BZFREE(s);
+      return BZ_MEM_ERROR;
+   }
+
+   s->blockNo           = 0;
+   s->state             = BZ_S_INPUT;
+   s->mode              = BZ_M_RUNNING;
+   s->combinedCRC       = 0;
+   s->blockSize100k     = blockSize100k;
+   s->nblockMAX         = 100000 * blockSize100k - 19;
+   s->verbosity         = verbosity;
+   s->workFactor        = workFactor;
+
+   s->block             = (UChar*)s->arr2;
+   s->mtfv              = (UInt16*)s->arr1;
+   s->zbits             = NULL;
+   s->ptr               = (UInt32*)s->arr1;
+
+   strm->state          = s;
+   strm->total_in_lo32  = 0;
+   strm->total_in_hi32  = 0;
+   strm->total_out_lo32 = 0;
+   strm->total_out_hi32 = 0;
+   init_RL ( s );
+   prepare_new_block ( s );
+   return BZ_OK;
+}
+
+
+/*---------------------------------------------------*/
+static
+void add_pair_to_block ( EState* s )
+{
+   Int32 i;
+   UChar ch = (UChar)(s->state_in_ch);
+   for (i = 0; i < s->state_in_len; i++) {
+      BZ_UPDATE_CRC( s->blockCRC, ch );
+   }
+   s->inUse[s->state_in_ch] = True;
+   switch (s->state_in_len) {
+      case 1:
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         break;
+      case 2:
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         break;
+      case 3:
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         break;
+      default:
+         s->inUse[s->state_in_len-4] = True;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = (UChar)ch; s->nblock++;
+         s->block[s->nblock] = ((UChar)(s->state_in_len-4));
+         s->nblock++;
+         break;
+   }
+}
+
+
+/*---------------------------------------------------*/
+static
+void flush_RL ( EState* s )
+{
+   if (s->state_in_ch < 256) add_pair_to_block ( s );
+   init_RL ( s );
+}
+
+
+/*---------------------------------------------------*/
+#define ADD_CHAR_TO_BLOCK(zs,zchh0)               \
+{                                                 \
+   UInt32 zchh = (UInt32)(zchh0);                 \
+   /*-- fast track the common case --*/           \
+   if (zchh != zs->state_in_ch &&                 \
+       zs->state_in_len == 1) {                   \
+      UChar ch = (UChar)(zs->state_in_ch);        \
+      BZ_UPDATE_CRC( zs->blockCRC, ch );          \
+      zs->inUse[zs->state_in_ch] = True;          \
+      zs->block[zs->nblock] = (UChar)ch;          \
+      zs->nblock++;                               \
+      zs->state_in_ch = zchh;                     \
+   }                                              \
+   else                                           \
+   /*-- general, uncommon cases --*/              \
+   if (zchh != zs->state_in_ch ||                 \
+      zs->state_in_len == 255) {                  \
+      if (zs->state_in_ch < 256)                  \
+         add_pair_to_block ( zs );                \
+      zs->state_in_ch = zchh;                     \
+      zs->state_in_len = 1;                       \
+   } else {                                       \
+      zs->state_in_len++;                         \
+   }                                              \
+}
+
+
+/*---------------------------------------------------*/
+static
+Bool copy_input_until_stop ( EState* s )
+{
+   Bool progress_in = False;
+
+   if (s->mode == BZ_M_RUNNING) {
+
+      /*-- fast track the common case --*/
+      while (True) {
+         /*-- block full? --*/
+         if (s->nblock >= s->nblockMAX) break;
+         /*-- no input? --*/
+         if (s->strm->avail_in == 0) break;
+         progress_in = True;
+         ADD_CHAR_TO_BLOCK ( s, (UInt32)(*((UChar*)(s->strm->next_in))) ); 
+         s->strm->next_in++;
+         s->strm->avail_in--;
+         s->strm->total_in_lo32++;
+         if (s->strm->total_in_lo32 == 0) s->strm->total_in_hi32++;
+      }
+
+   } else {
+
+      /*-- general, uncommon case --*/
+      while (True) {
+         /*-- block full? --*/
+         if (s->nblock >= s->nblockMAX) break;
+         /*-- no input? --*/
+         if (s->strm->avail_in == 0) break;
+         /*-- flush/finish end? --*/
+         if (s->avail_in_expect == 0) break;
+         progress_in = True;
+         ADD_CHAR_TO_BLOCK ( s, (UInt32)(*((UChar*)(s->strm->next_in))) ); 
+         s->strm->next_in++;
+         s->strm->avail_in--;
+         s->strm->total_in_lo32++;
+         if (s->strm->total_in_lo32 == 0) s->strm->total_in_hi32++;
+         s->avail_in_expect--;
+      }
+   }
+   return progress_in;
+}
+
+
+/*---------------------------------------------------*/
+static
+Bool copy_output_until_stop ( EState* s )
+{
+   Bool progress_out = False;
+
+   while (True) {
+
+      /*-- no output space? --*/
+      if (s->strm->avail_out == 0) break;
+
+      /*-- block done? --*/
+      if (s->state_out_pos >= s->numZ) break;
+
+      progress_out = True;
+      *(s->strm->next_out) = s->zbits[s->state_out_pos];
+      s->state_out_pos++;
+      s->strm->avail_out--;
+      s->strm->next_out++;
+      s->strm->total_out_lo32++;
+      if (s->strm->total_out_lo32 == 0) s->strm->total_out_hi32++;
+   }
+
+   return progress_out;
+}
+
+
+/*---------------------------------------------------*/
+static
+Bool handle_compress ( bz_stream* strm )
+{
+   Bool progress_in  = False;
+   Bool progress_out = False;
+   EState* s = strm->state;
+   
+   while (True) {
+
+      if (s->state == BZ_S_OUTPUT) {
+         progress_out |= copy_output_until_stop ( s );
+         if (s->state_out_pos < s->numZ) break;
+         if (s->mode == BZ_M_FINISHING && 
+             s->avail_in_expect == 0 &&
+             isempty_RL(s)) break;
+         prepare_new_block ( s );
+         s->state = BZ_S_INPUT;
+         if (s->mode == BZ_M_FLUSHING && 
+             s->avail_in_expect == 0 &&
+             isempty_RL(s)) break;
+      }
+
+      if (s->state == BZ_S_INPUT) {
+         progress_in |= copy_input_until_stop ( s );
+         if (s->mode != BZ_M_RUNNING && s->avail_in_expect == 0) {
+            flush_RL ( s );
+            BZ2_compressBlock ( s, (Bool)(s->mode == BZ_M_FINISHING) );
+            s->state = BZ_S_OUTPUT;
+         }
+         else
+         if (s->nblock >= s->nblockMAX) {
+            BZ2_compressBlock ( s, False );
+            s->state = BZ_S_OUTPUT;
+         }
+         else
+         if (s->strm->avail_in == 0) {
+            break;
+         }
+      }
+
+   }
+
+   return progress_in || progress_out;
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzCompress) ( bz_stream *strm, int action )
+{
+   Bool progress;
+   EState* s;
+   if (strm == NULL) return BZ_PARAM_ERROR;
+   s = strm->state;
+   if (s == NULL) return BZ_PARAM_ERROR;
+   if (s->strm != strm) return BZ_PARAM_ERROR;
+
+   preswitch:
+   switch (s->mode) {
+
+      case BZ_M_IDLE:
+         return BZ_SEQUENCE_ERROR;
+
+      case BZ_M_RUNNING:
+         if (action == BZ_RUN) {
+            progress = handle_compress ( strm );
+            return progress ? BZ_RUN_OK : BZ_PARAM_ERROR;
+         } 
+         else
+	 if (action == BZ_FLUSH) {
+            s->avail_in_expect = strm->avail_in;
+            s->mode = BZ_M_FLUSHING;
+            goto preswitch;
+         }
+         else
+         if (action == BZ_FINISH) {
+            s->avail_in_expect = strm->avail_in;
+            s->mode = BZ_M_FINISHING;
+            goto preswitch;
+         }
+         else 
+            return BZ_PARAM_ERROR;
+
+      case BZ_M_FLUSHING:
+         if (action != BZ_FLUSH) return BZ_SEQUENCE_ERROR;
+         if (s->avail_in_expect != s->strm->avail_in) 
+            return BZ_SEQUENCE_ERROR;
+         progress = handle_compress ( strm );
+         if (s->avail_in_expect > 0 || !isempty_RL(s) ||
+             s->state_out_pos < s->numZ) return BZ_FLUSH_OK;
+         s->mode = BZ_M_RUNNING;
+         return BZ_RUN_OK;
+
+      case BZ_M_FINISHING:
+         if (action != BZ_FINISH) return BZ_SEQUENCE_ERROR;
+         if (s->avail_in_expect != s->strm->avail_in) 
+            return BZ_SEQUENCE_ERROR;
+         progress = handle_compress ( strm );
+         if (!progress) return BZ_SEQUENCE_ERROR;
+         if (s->avail_in_expect > 0 || !isempty_RL(s) ||
+             s->state_out_pos < s->numZ) return BZ_FINISH_OK;
+         s->mode = BZ_M_IDLE;
+         return BZ_STREAM_END;
+   }
+   return BZ_OK; /*--not reached--*/
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzCompressEnd)  ( bz_stream *strm )
+{
+   EState* s;
+   if (strm == NULL) return BZ_PARAM_ERROR;
+   s = strm->state;
+   if (s == NULL) return BZ_PARAM_ERROR;
+   if (s->strm != strm) return BZ_PARAM_ERROR;
+
+   if (s->arr1 != NULL) BZFREE(s->arr1);
+   if (s->arr2 != NULL) BZFREE(s->arr2);
+   if (s->ftab != NULL) BZFREE(s->ftab);
+   BZFREE(strm->state);
+
+   strm->state = NULL;   
+
+   return BZ_OK;
+}
+
+
+/*---------------------------------------------------*/
+/*--- Decompression stuff                         ---*/
+/*---------------------------------------------------*/
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzDecompressInit) 
+                     ( bz_stream* strm, 
+                       int        verbosity,
+                       int        small )
+{
+   DState* s;
+
+   if (!bz_config_ok()) return BZ_CONFIG_ERROR;
+
+   if (strm == NULL) return BZ_PARAM_ERROR;
+   if (small != 0 && small != 1) return BZ_PARAM_ERROR;
+   if (verbosity < 0 || verbosity > 4) return BZ_PARAM_ERROR;
+
+   if (strm->bzalloc == NULL) strm->bzalloc = default_bzalloc;
+   if (strm->bzfree == NULL) strm->bzfree = default_bzfree;
+
+   s = BZALLOC( sizeof(DState) );
+   if (s == NULL) return BZ_MEM_ERROR;
+   s->strm                  = strm;
+   strm->state              = s;
+   s->state                 = BZ_X_MAGIC_1;
+   s->bsLive                = 0;
+   s->bsBuff                = 0;
+   s->calculatedCombinedCRC = 0;
+   strm->total_in_lo32      = 0;
+   strm->total_in_hi32      = 0;
+   strm->total_out_lo32     = 0;
+   strm->total_out_hi32     = 0;
+   s->smallDecompress       = (Bool)small;
+   s->ll4                   = NULL;
+   s->ll16                  = NULL;
+   s->tt                    = NULL;
+   s->currBlockNo           = 0;
+   s->verbosity             = verbosity;
+
+   return BZ_OK;
+}
+
+
+/*---------------------------------------------------*/
+/* Return  True iff data corruption is discovered.
+   Returns False if there is no problem.
+*/
+static
+Bool unRLE_obuf_to_output_FAST ( DState* s )
+{
+   UChar k1;
+
+   if (s->blockRandomised) {
+
+      while (True) {
+         /* try to finish existing run */
+         while (True) {
+            if (s->strm->avail_out == 0) return False;
+            if (s->state_out_len == 0) break;
+            *( (UChar*)(s->strm->next_out) ) = s->state_out_ch;
+            BZ_UPDATE_CRC ( s->calculatedBlockCRC, s->state_out_ch );
+            s->state_out_len--;
+            s->strm->next_out++;
+            s->strm->avail_out--;
+            s->strm->total_out_lo32++;
+            if (s->strm->total_out_lo32 == 0) s->strm->total_out_hi32++;
+         }
+
+         /* can a new run be started? */
+         if (s->nblock_used == s->save_nblock+1) return False;
+               
+         /* Only caused by corrupt data stream? */
+         if (s->nblock_used > s->save_nblock+1)
+            return True;
+   
+         s->state_out_len = 1;
+         s->state_out_ch = s->k0;
+         BZ_GET_FAST(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 2;
+         BZ_GET_FAST(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 3;
+         BZ_GET_FAST(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         BZ_GET_FAST(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         s->state_out_len = ((Int32)k1) + 4;
+         BZ_GET_FAST(s->k0); BZ_RAND_UPD_MASK; 
+         s->k0 ^= BZ_RAND_MASK; s->nblock_used++;
+      }
+
+   } else {
+
+      /* restore */
+      UInt32        c_calculatedBlockCRC = s->calculatedBlockCRC;
+      UChar         c_state_out_ch       = s->state_out_ch;
+      Int32         c_state_out_len      = s->state_out_len;
+      Int32         c_nblock_used        = s->nblock_used;
+      Int32         c_k0                 = s->k0;
+      UInt32*       c_tt                 = s->tt;
+      UInt32        c_tPos               = s->tPos;
+      char*         cs_next_out          = s->strm->next_out;
+      unsigned int  cs_avail_out         = s->strm->avail_out;
+      Int32         ro_blockSize100k     = s->blockSize100k;
+      /* end restore */
+
+      UInt32       avail_out_INIT = cs_avail_out;
+      Int32        s_save_nblockPP = s->save_nblock+1;
+      unsigned int total_out_lo32_old;
+
+      while (True) {
+
+         /* try to finish existing run */
+         if (c_state_out_len > 0) {
+            while (True) {
+               if (cs_avail_out == 0) goto return_notr;
+               if (c_state_out_len == 1) break;
+               *( (UChar*)(cs_next_out) ) = c_state_out_ch;
+               BZ_UPDATE_CRC ( c_calculatedBlockCRC, c_state_out_ch );
+               c_state_out_len--;
+               cs_next_out++;
+               cs_avail_out--;
+            }
+            s_state_out_len_eq_one:
+            {
+               if (cs_avail_out == 0) { 
+                  c_state_out_len = 1; goto return_notr;
+               };
+               *( (UChar*)(cs_next_out) ) = c_state_out_ch;
+               BZ_UPDATE_CRC ( c_calculatedBlockCRC, c_state_out_ch );
+               cs_next_out++;
+               cs_avail_out--;
+            }
+         }   
+         /* Only caused by corrupt data stream? */
+         if (c_nblock_used > s_save_nblockPP)
+            return True;
+
+         /* can a new run be started? */
+         if (c_nblock_used == s_save_nblockPP) {
+            c_state_out_len = 0; goto return_notr;
+         };   
+         c_state_out_ch = c_k0;
+         BZ_GET_FAST_C(k1); c_nblock_used++;
+         if (k1 != c_k0) { 
+            c_k0 = k1; goto s_state_out_len_eq_one; 
+         };
+         if (c_nblock_used == s_save_nblockPP) 
+            goto s_state_out_len_eq_one;
+   
+         c_state_out_len = 2;
+         BZ_GET_FAST_C(k1); c_nblock_used++;
+         if (c_nblock_used == s_save_nblockPP) continue;
+         if (k1 != c_k0) { c_k0 = k1; continue; };
+   
+         c_state_out_len = 3;
+         BZ_GET_FAST_C(k1); c_nblock_used++;
+         if (c_nblock_used == s_save_nblockPP) continue;
+         if (k1 != c_k0) { c_k0 = k1; continue; };
+   
+         BZ_GET_FAST_C(k1); c_nblock_used++;
+         c_state_out_len = ((Int32)k1) + 4;
+         BZ_GET_FAST_C(c_k0); c_nblock_used++;
+      }
+
+      return_notr:
+      total_out_lo32_old = s->strm->total_out_lo32;
+      s->strm->total_out_lo32 += (avail_out_INIT - cs_avail_out);
+      if (s->strm->total_out_lo32 < total_out_lo32_old)
+         s->strm->total_out_hi32++;
+
+      /* save */
+      s->calculatedBlockCRC = c_calculatedBlockCRC;
+      s->state_out_ch       = c_state_out_ch;
+      s->state_out_len      = c_state_out_len;
+      s->nblock_used        = c_nblock_used;
+      s->k0                 = c_k0;
+      s->tt                 = c_tt;
+      s->tPos               = c_tPos;
+      s->strm->next_out     = cs_next_out;
+      s->strm->avail_out    = cs_avail_out;
+      /* end save */
+   }
+   return False;
+}
+
+
+
+/*---------------------------------------------------*/
+__inline__ Int32 BZ2_indexIntoF ( Int32 indx, Int32 *cftab )
+{
+   Int32 nb, na, mid;
+   nb = 0;
+   na = 256;
+   do {
+      mid = (nb + na) >> 1;
+      if (indx >= cftab[mid]) nb = mid; else na = mid;
+   }
+   while (na - nb != 1);
+   return nb;
+}
+
+
+/*---------------------------------------------------*/
+/* Return  True iff data corruption is discovered.
+   Returns False if there is no problem.
+*/
+static
+Bool unRLE_obuf_to_output_SMALL ( DState* s )
+{
+   UChar k1;
+
+   if (s->blockRandomised) {
+
+      while (True) {
+         /* try to finish existing run */
+         while (True) {
+            if (s->strm->avail_out == 0) return False;
+            if (s->state_out_len == 0) break;
+            *( (UChar*)(s->strm->next_out) ) = s->state_out_ch;
+            BZ_UPDATE_CRC ( s->calculatedBlockCRC, s->state_out_ch );
+            s->state_out_len--;
+            s->strm->next_out++;
+            s->strm->avail_out--;
+            s->strm->total_out_lo32++;
+            if (s->strm->total_out_lo32 == 0) s->strm->total_out_hi32++;
+         }
+   
+         /* can a new run be started? */
+         if (s->nblock_used == s->save_nblock+1) return False;
+
+         /* Only caused by corrupt data stream? */
+         if (s->nblock_used > s->save_nblock+1)
+            return True;
+   
+         s->state_out_len = 1;
+         s->state_out_ch = s->k0;
+         BZ_GET_SMALL(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 2;
+         BZ_GET_SMALL(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 3;
+         BZ_GET_SMALL(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         BZ_GET_SMALL(k1); BZ_RAND_UPD_MASK; 
+         k1 ^= BZ_RAND_MASK; s->nblock_used++;
+         s->state_out_len = ((Int32)k1) + 4;
+         BZ_GET_SMALL(s->k0); BZ_RAND_UPD_MASK; 
+         s->k0 ^= BZ_RAND_MASK; s->nblock_used++;
+      }
+
+   } else {
+
+      while (True) {
+         /* try to finish existing run */
+         while (True) {
+            if (s->strm->avail_out == 0) return False;
+            if (s->state_out_len == 0) break;
+            *( (UChar*)(s->strm->next_out) ) = s->state_out_ch;
+            BZ_UPDATE_CRC ( s->calculatedBlockCRC, s->state_out_ch );
+            s->state_out_len--;
+            s->strm->next_out++;
+            s->strm->avail_out--;
+            s->strm->total_out_lo32++;
+            if (s->strm->total_out_lo32 == 0) s->strm->total_out_hi32++;
+         }
+   
+         /* can a new run be started? */
+         if (s->nblock_used == s->save_nblock+1) return False;
+
+         /* Only caused by corrupt data stream? */
+         if (s->nblock_used > s->save_nblock+1)
+            return True;
+   
+         s->state_out_len = 1;
+         s->state_out_ch = s->k0;
+         BZ_GET_SMALL(k1); s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 2;
+         BZ_GET_SMALL(k1); s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         s->state_out_len = 3;
+         BZ_GET_SMALL(k1); s->nblock_used++;
+         if (s->nblock_used == s->save_nblock+1) continue;
+         if (k1 != s->k0) { s->k0 = k1; continue; };
+   
+         BZ_GET_SMALL(k1); s->nblock_used++;
+         s->state_out_len = ((Int32)k1) + 4;
+         BZ_GET_SMALL(s->k0); s->nblock_used++;
+      }
+
+   }
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzDecompress) ( bz_stream *strm )
+{
+   Bool    corrupt;
+   DState* s;
+   if (strm == NULL) return BZ_PARAM_ERROR;
+   s = strm->state;
+   if (s == NULL) return BZ_PARAM_ERROR;
+   if (s->strm != strm) return BZ_PARAM_ERROR;
+
+   while (True) {
+      if (s->state == BZ_X_IDLE) return BZ_SEQUENCE_ERROR;
+      if (s->state == BZ_X_OUTPUT) {
+         if (s->smallDecompress)
+            corrupt = unRLE_obuf_to_output_SMALL ( s ); else
+            corrupt = unRLE_obuf_to_output_FAST  ( s );
+         if (corrupt) return BZ_DATA_ERROR;
+         if (s->nblock_used == s->save_nblock+1 && s->state_out_len == 0) {
+            BZ_FINALISE_CRC ( s->calculatedBlockCRC );
+            if (s->verbosity >= 3) 
+               VPrintf2 ( " {0x%08x, 0x%08x}", s->storedBlockCRC, 
+                          s->calculatedBlockCRC );
+            if (s->verbosity >= 2) VPrintf0 ( "]" );
+            if (s->calculatedBlockCRC != s->storedBlockCRC)
+               return BZ_DATA_ERROR;
+            s->calculatedCombinedCRC 
+               = (s->calculatedCombinedCRC << 1) | 
+                    (s->calculatedCombinedCRC >> 31);
+            s->calculatedCombinedCRC ^= s->calculatedBlockCRC;
+            s->state = BZ_X_BLKHDR_1;
+         } else {
+            return BZ_OK;
+         }
+      }
+      if (s->state >= BZ_X_MAGIC_1) {
+         Int32 r = BZ2_decompress ( s );
+         if (r == BZ_STREAM_END) {
+            if (s->verbosity >= 3)
+               VPrintf2 ( "\n    combined CRCs: stored = 0x%08x, computed = 0x%08x", 
+                          s->storedCombinedCRC, s->calculatedCombinedCRC );
+            if (s->calculatedCombinedCRC != s->storedCombinedCRC)
+               return BZ_DATA_ERROR;
+            return r;
+         }
+         if (s->state != BZ_X_OUTPUT) return r;
+      }
+   }
+
+   AssertH ( 0, 6001 );
+
+   return 0;  /*NOTREACHED*/
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzDecompressEnd)  ( bz_stream *strm )
+{
+   DState* s;
+   if (strm == NULL) return BZ_PARAM_ERROR;
+   s = strm->state;
+   if (s == NULL) return BZ_PARAM_ERROR;
+   if (s->strm != strm) return BZ_PARAM_ERROR;
+
+   if (s->tt   != NULL) BZFREE(s->tt);
+   if (s->ll16 != NULL) BZFREE(s->ll16);
+   if (s->ll4  != NULL) BZFREE(s->ll4);
+
+   BZFREE(strm->state);
+   strm->state = NULL;
+
+   return BZ_OK;
+}
+
+
+#ifndef BZ_NO_STDIO
+/*---------------------------------------------------*/
+/*--- File I/O stuff                              ---*/
+/*---------------------------------------------------*/
+
+#define BZ_SETERR(eee)                    \
+{                                         \
+   if (bzerror != NULL) *bzerror = eee;   \
+   if (bzf != NULL) bzf->lastErr = eee;   \
+}
+
+typedef 
+   struct {
+      FILE*     handle;
+      Char      buf[BZ_MAX_UNUSED];
+      Int32     bufN;
+      Bool      writing;
+      bz_stream strm;
+      Int32     lastErr;
+      Bool      initialisedOk;
+   }
+   bzFile;
+
+
+/*---------------------------------------------*/
+static Bool myfeof ( FILE* f )
+{
+   Int32 c = fgetc ( f );
+   if (c == EOF) return True;
+   ungetc ( c, f );
+   return False;
+}
+
+
+/*---------------------------------------------------*/
+BZFILE* BZ_API(BZ2_bzWriteOpen) 
+                    ( int*  bzerror,      
+                      FILE* f, 
+                      int   blockSize100k, 
+                      int   verbosity,
+                      int   workFactor )
+{
+   Int32   ret;
+   bzFile* bzf = NULL;
+
+   BZ_SETERR(BZ_OK);
+
+   if (f == NULL ||
+       (blockSize100k < 1 || blockSize100k > 9) ||
+       (workFactor < 0 || workFactor > 250) ||
+       (verbosity < 0 || verbosity > 4))
+      { BZ_SETERR(BZ_PARAM_ERROR); return NULL; };
+
+   if (ferror(f))
+      { BZ_SETERR(BZ_IO_ERROR); return NULL; };
+
+   bzf = malloc ( sizeof(bzFile) );
+   if (bzf == NULL)
+      { BZ_SETERR(BZ_MEM_ERROR); return NULL; };
+
+   BZ_SETERR(BZ_OK);
+   bzf->initialisedOk = False;
+   bzf->bufN          = 0;
+   bzf->handle        = f;
+   bzf->writing       = True;
+   bzf->strm.bzalloc  = NULL;
+   bzf->strm.bzfree   = NULL;
+   bzf->strm.opaque   = NULL;
+
+   if (workFactor == 0) workFactor = 30;
+   ret = BZ2_bzCompressInit ( &(bzf->strm), blockSize100k, 
+                              verbosity, workFactor );
+   if (ret != BZ_OK)
+      { BZ_SETERR(ret); free(bzf); return NULL; };
+
+   bzf->strm.avail_in = 0;
+   bzf->initialisedOk = True;
+   return bzf;   
+}
+
+
+
+/*---------------------------------------------------*/
+void BZ_API(BZ2_bzWrite)
+             ( int*    bzerror, 
+               BZFILE* b, 
+               void*   buf, 
+               int     len )
+{
+   Int32 n, n2, ret;
+   bzFile* bzf = (bzFile*)b;
+
+   BZ_SETERR(BZ_OK);
+   if (bzf == NULL || buf == NULL || len < 0)
+      { BZ_SETERR(BZ_PARAM_ERROR); return; };
+   if (!(bzf->writing))
+      { BZ_SETERR(BZ_SEQUENCE_ERROR); return; };
+   if (ferror(bzf->handle))
+      { BZ_SETERR(BZ_IO_ERROR); return; };
+
+   if (len == 0)
+      { BZ_SETERR(BZ_OK); return; };
+
+   bzf->strm.avail_in = len;
+   bzf->strm.next_in  = buf;
+
+   while (True) {
+      bzf->strm.avail_out = BZ_MAX_UNUSED;
+      bzf->strm.next_out = bzf->buf;
+      ret = BZ2_bzCompress ( &(bzf->strm), BZ_RUN );
+      if (ret != BZ_RUN_OK)
+         { BZ_SETERR(ret); return; };
+
+      if (bzf->strm.avail_out < BZ_MAX_UNUSED) {
+         n = BZ_MAX_UNUSED - bzf->strm.avail_out;
+         n2 = fwrite ( (void*)(bzf->buf), sizeof(UChar), 
+                       n, bzf->handle );
+         if (n != n2 || ferror(bzf->handle))
+            { BZ_SETERR(BZ_IO_ERROR); return; };
+      }
+
+      if (bzf->strm.avail_in == 0)
+         { BZ_SETERR(BZ_OK); return; };
+   }
+}
+
+
+/*---------------------------------------------------*/
+void BZ_API(BZ2_bzWriteClose)
+                  ( int*          bzerror, 
+                    BZFILE*       b, 
+                    int           abandon,
+                    unsigned int* nbytes_in,
+                    unsigned int* nbytes_out )
+{
+   BZ2_bzWriteClose64 ( bzerror, b, abandon, 
+                        nbytes_in, NULL, nbytes_out, NULL );
+}
+
+
+void BZ_API(BZ2_bzWriteClose64)
+                  ( int*          bzerror, 
+                    BZFILE*       b, 
+                    int           abandon,
+                    unsigned int* nbytes_in_lo32,
+                    unsigned int* nbytes_in_hi32,
+                    unsigned int* nbytes_out_lo32,
+                    unsigned int* nbytes_out_hi32 )
+{
+   Int32   n, n2, ret;
+   bzFile* bzf = (bzFile*)b;
+
+   if (bzf == NULL)
+      { BZ_SETERR(BZ_OK); return; };
+   if (!(bzf->writing))
+      { BZ_SETERR(BZ_SEQUENCE_ERROR); return; };
+   if (ferror(bzf->handle))
+      { BZ_SETERR(BZ_IO_ERROR); return; };
+
+   if (nbytes_in_lo32 != NULL) *nbytes_in_lo32 = 0;
+   if (nbytes_in_hi32 != NULL) *nbytes_in_hi32 = 0;
+   if (nbytes_out_lo32 != NULL) *nbytes_out_lo32 = 0;
+   if (nbytes_out_hi32 != NULL) *nbytes_out_hi32 = 0;
+
+   if ((!abandon) && bzf->lastErr == BZ_OK) {
+      while (True) {
+         bzf->strm.avail_out = BZ_MAX_UNUSED;
+         bzf->strm.next_out = bzf->buf;
+         ret = BZ2_bzCompress ( &(bzf->strm), BZ_FINISH );
+         if (ret != BZ_FINISH_OK && ret != BZ_STREAM_END)
+            { BZ_SETERR(ret); return; };
+
+         if (bzf->strm.avail_out < BZ_MAX_UNUSED) {
+            n = BZ_MAX_UNUSED - bzf->strm.avail_out;
+            n2 = fwrite ( (void*)(bzf->buf), sizeof(UChar), 
+                          n, bzf->handle );
+            if (n != n2 || ferror(bzf->handle))
+               { BZ_SETERR(BZ_IO_ERROR); return; };
+         }
+
+         if (ret == BZ_STREAM_END) break;
+      }
+   }
+
+   if ( !abandon && !ferror ( bzf->handle ) ) {
+      fflush ( bzf->handle );
+      if (ferror(bzf->handle))
+         { BZ_SETERR(BZ_IO_ERROR); return; };
+   }
+
+   if (nbytes_in_lo32 != NULL)
+      *nbytes_in_lo32 = bzf->strm.total_in_lo32;
+   if (nbytes_in_hi32 != NULL)
+      *nbytes_in_hi32 = bzf->strm.total_in_hi32;
+   if (nbytes_out_lo32 != NULL)
+      *nbytes_out_lo32 = bzf->strm.total_out_lo32;
+   if (nbytes_out_hi32 != NULL)
+      *nbytes_out_hi32 = bzf->strm.total_out_hi32;
+
+   BZ_SETERR(BZ_OK);
+   BZ2_bzCompressEnd ( &(bzf->strm) );
+   free ( bzf );
+}
+
+
+/*---------------------------------------------------*/
+BZFILE* BZ_API(BZ2_bzReadOpen) 
+                   ( int*  bzerror, 
+                     FILE* f, 
+                     int   verbosity,
+                     int   small,
+                     void* unused,
+                     int   nUnused )
+{
+   bzFile* bzf = NULL;
+   int     ret;
+
+   BZ_SETERR(BZ_OK);
+
+   if (f == NULL || 
+       (small != 0 && small != 1) ||
+       (verbosity < 0 || verbosity > 4) ||
+       (unused == NULL && nUnused != 0) ||
+       (unused != NULL && (nUnused < 0 || nUnused > BZ_MAX_UNUSED)))
+      { BZ_SETERR(BZ_PARAM_ERROR); return NULL; };
+
+   if (ferror(f))
+      { BZ_SETERR(BZ_IO_ERROR); return NULL; };
+
+   bzf = malloc ( sizeof(bzFile) );
+   if (bzf == NULL) 
+      { BZ_SETERR(BZ_MEM_ERROR); return NULL; };
+
+   BZ_SETERR(BZ_OK);
+
+   bzf->initialisedOk = False;
+   bzf->handle        = f;
+   bzf->bufN          = 0;
+   bzf->writing       = False;
+   bzf->strm.bzalloc  = NULL;
+   bzf->strm.bzfree   = NULL;
+   bzf->strm.opaque   = NULL;
+   
+   while (nUnused > 0) {
+      bzf->buf[bzf->bufN] = *((UChar*)(unused)); bzf->bufN++;
+      unused = ((void*)( 1 + ((UChar*)(unused))  ));
+      nUnused--;
+   }
+
+   ret = BZ2_bzDecompressInit ( &(bzf->strm), verbosity, small );
+   if (ret != BZ_OK)
+      { BZ_SETERR(ret); free(bzf); return NULL; };
+
+   bzf->strm.avail_in = bzf->bufN;
+   bzf->strm.next_in  = bzf->buf;
+
+   bzf->initialisedOk = True;
+   return bzf;   
+}
+
+
+/*---------------------------------------------------*/
+void BZ_API(BZ2_bzReadClose) ( int *bzerror, BZFILE *b )
+{
+   bzFile* bzf = (bzFile*)b;
+
+   BZ_SETERR(BZ_OK);
+   if (bzf == NULL)
+      { BZ_SETERR(BZ_OK); return; };
+
+   if (bzf->writing)
+      { BZ_SETERR(BZ_SEQUENCE_ERROR); return; };
+
+   if (bzf->initialisedOk)
+      (void)BZ2_bzDecompressEnd ( &(bzf->strm) );
+   free ( bzf );
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzRead) 
+           ( int*    bzerror, 
+             BZFILE* b, 
+             void*   buf, 
+             int     len )
+{
+   Int32   n, ret;
+   bzFile* bzf = (bzFile*)b;
+
+   BZ_SETERR(BZ_OK);
+
+   if (bzf == NULL || buf == NULL || len < 0)
+      { BZ_SETERR(BZ_PARAM_ERROR); return 0; };
+
+   if (bzf->writing)
+      { BZ_SETERR(BZ_SEQUENCE_ERROR); return 0; };
+
+   if (len == 0)
+      { BZ_SETERR(BZ_OK); return 0; };
+
+   bzf->strm.avail_out = len;
+   bzf->strm.next_out = buf;
+
+   while (True) {
+
+      if (ferror(bzf->handle)) 
+         { BZ_SETERR(BZ_IO_ERROR); return 0; };
+
+      if (bzf->strm.avail_in == 0 && !myfeof(bzf->handle)) {
+         n = fread ( bzf->buf, sizeof(UChar), 
+                     BZ_MAX_UNUSED, bzf->handle );
+         if (ferror(bzf->handle))
+            { BZ_SETERR(BZ_IO_ERROR); return 0; };
+         bzf->bufN = n;
+         bzf->strm.avail_in = bzf->bufN;
+         bzf->strm.next_in = bzf->buf;
+      }
+
+      ret = BZ2_bzDecompress ( &(bzf->strm) );
+
+      if (ret != BZ_OK && ret != BZ_STREAM_END)
+         { BZ_SETERR(ret); return 0; };
+
+      if (ret == BZ_OK && myfeof(bzf->handle) && 
+          bzf->strm.avail_in == 0 && bzf->strm.avail_out > 0)
+         { BZ_SETERR(BZ_UNEXPECTED_EOF); return 0; };
+
+      if (ret == BZ_STREAM_END)
+         { BZ_SETERR(BZ_STREAM_END);
+           return len - bzf->strm.avail_out; };
+      if (bzf->strm.avail_out == 0)
+         { BZ_SETERR(BZ_OK); return len; };
+      
+   }
+
+   return 0; /*not reached*/
+}
+
+
+/*---------------------------------------------------*/
+void BZ_API(BZ2_bzReadGetUnused) 
+                     ( int*    bzerror, 
+                       BZFILE* b, 
+                       void**  unused, 
+                       int*    nUnused )
+{
+   bzFile* bzf = (bzFile*)b;
+   if (bzf == NULL)
+      { BZ_SETERR(BZ_PARAM_ERROR); return; };
+   if (bzf->lastErr != BZ_STREAM_END)
+      { BZ_SETERR(BZ_SEQUENCE_ERROR); return; };
+   if (unused == NULL || nUnused == NULL)
+      { BZ_SETERR(BZ_PARAM_ERROR); return; };
+
+   BZ_SETERR(BZ_OK);
+   *nUnused = bzf->strm.avail_in;
+   *unused = bzf->strm.next_in;
+}
+#endif
+
+
+/*---------------------------------------------------*/
+/*--- Misc convenience stuff                      ---*/
+/*---------------------------------------------------*/
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzBuffToBuffCompress) 
+                         ( char*         dest, 
+                           unsigned int* destLen,
+                           char*         source, 
+                           unsigned int  sourceLen,
+                           int           blockSize100k, 
+                           int           verbosity, 
+                           int           workFactor )
+{
+   bz_stream strm;
+   int ret;
+
+   if (dest == NULL || destLen == NULL || 
+       source == NULL ||
+       blockSize100k < 1 || blockSize100k > 9 ||
+       verbosity < 0 || verbosity > 4 ||
+       workFactor < 0 || workFactor > 250) 
+      return BZ_PARAM_ERROR;
+
+   if (workFactor == 0) workFactor = 30;
+   strm.bzalloc = NULL;
+   strm.bzfree = NULL;
+   strm.opaque = NULL;
+   ret = BZ2_bzCompressInit ( &strm, blockSize100k, 
+                              verbosity, workFactor );
+   if (ret != BZ_OK) return ret;
+
+   strm.next_in = source;
+   strm.next_out = dest;
+   strm.avail_in = sourceLen;
+   strm.avail_out = *destLen;
+
+   ret = BZ2_bzCompress ( &strm, BZ_FINISH );
+   if (ret == BZ_FINISH_OK) goto output_overflow;
+   if (ret != BZ_STREAM_END) goto errhandler;
+
+   /* normal termination */
+   *destLen -= strm.avail_out;   
+   BZ2_bzCompressEnd ( &strm );
+   return BZ_OK;
+
+   output_overflow:
+   BZ2_bzCompressEnd ( &strm );
+   return BZ_OUTBUFF_FULL;
+
+   errhandler:
+   BZ2_bzCompressEnd ( &strm );
+   return ret;
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzBuffToBuffDecompress) 
+                           ( char*         dest, 
+                             unsigned int* destLen,
+                             char*         source, 
+                             unsigned int  sourceLen,
+                             int           small,
+                             int           verbosity )
+{
+   bz_stream strm;
+   int ret;
+
+   if (dest == NULL || destLen == NULL || 
+       source == NULL ||
+       (small != 0 && small != 1) ||
+       verbosity < 0 || verbosity > 4) 
+          return BZ_PARAM_ERROR;
+
+   strm.bzalloc = NULL;
+   strm.bzfree = NULL;
+   strm.opaque = NULL;
+   ret = BZ2_bzDecompressInit ( &strm, verbosity, small );
+   if (ret != BZ_OK) return ret;
+
+   strm.next_in = source;
+   strm.next_out = dest;
+   strm.avail_in = sourceLen;
+   strm.avail_out = *destLen;
+
+   ret = BZ2_bzDecompress ( &strm );
+   if (ret == BZ_OK) goto output_overflow_or_eof;
+   if (ret != BZ_STREAM_END) goto errhandler;
+
+   /* normal termination */
+   *destLen -= strm.avail_out;
+   BZ2_bzDecompressEnd ( &strm );
+   return BZ_OK;
+
+   output_overflow_or_eof:
+   if (strm.avail_out > 0) {
+      BZ2_bzDecompressEnd ( &strm );
+      return BZ_UNEXPECTED_EOF;
+   } else {
+      BZ2_bzDecompressEnd ( &strm );
+      return BZ_OUTBUFF_FULL;
+   };      
+
+   errhandler:
+   BZ2_bzDecompressEnd ( &strm );
+   return ret; 
+}
+
+
+/*---------------------------------------------------*/
+/*--
+   Code contributed by Yoshioka Tsuneo (tsuneo@rr.iij4u.or.jp)
+   to support better zlib compatibility.
+   This code is not _officially_ part of libbzip2 (yet);
+   I haven't tested it, documented it, or considered the
+   threading-safeness of it.
+   If this code breaks, please contact both Yoshioka and me.
+--*/
+/*---------------------------------------------------*/
+
+/*---------------------------------------------------*/
+/*--
+   return version like "0.9.5d, 4-Sept-1999".
+--*/
+const char * BZ_API(BZ2_bzlibVersion)(void)
+{
+   return BZ_VERSION;
+}
+
+
+#ifndef BZ_NO_STDIO
+/*---------------------------------------------------*/
+
+#if defined(_WIN32) || defined(OS2) || defined(MSDOS)
+#   include <fcntl.h>
+#   include <io.h>
+#   define SET_BINARY_MODE(file) setmode(fileno(file),O_BINARY)
+#else
+#   define SET_BINARY_MODE(file)
+#endif
+static
+BZFILE * bzopen_or_bzdopen
+               ( const char *path,   /* no use when bzdopen */
+                 int fd,             /* no use when bzdopen */
+                 const char *mode,
+                 int open_mode)      /* bzopen: 0, bzdopen:1 */
+{
+   int    bzerr;
+   char   unused[BZ_MAX_UNUSED];
+   int    blockSize100k = 9;
+   int    writing       = 0;
+   char   mode2[10]     = "";
+   FILE   *fp           = NULL;
+   BZFILE *bzfp         = NULL;
+   int    verbosity     = 0;
+   int    workFactor    = 30;
+   int    smallMode     = 0;
+   int    nUnused       = 0; 
+
+   if (mode == NULL) return NULL;
+   while (*mode) {
+      switch (*mode) {
+      case 'r':
+         writing = 0; break;
+      case 'w':
+         writing = 1; break;
+      case 's':
+         smallMode = 1; break;
+      default:
+         if (isdigit((int)(*mode))) {
+            blockSize100k = *mode-BZ_HDR_0;
+         }
+      }
+      mode++;
+   }
+   strcat(mode2, writing ? "w" : "r" );
+   strcat(mode2,"b");   /* binary mode */
+
+   if (open_mode==0) {
+      if (path==NULL || strcmp(path,"")==0) {
+        fp = (writing ? stdout : stdin);
+        SET_BINARY_MODE(fp);
+      } else {
+        fp = fopen(path,mode2);
+      }
+   } else {
+#ifdef BZ_STRICT_ANSI
+      fp = NULL;
+#else
+      fp = fdopen(fd,mode2);
+#endif
+   }
+   if (fp == NULL) return NULL;
+
+   if (writing) {
+      /* Guard against total chaos and anarchy -- JRS */
+      if (blockSize100k < 1) blockSize100k = 1;
+      if (blockSize100k > 9) blockSize100k = 9; 
+      bzfp = BZ2_bzWriteOpen(&bzerr,fp,blockSize100k,
+                             verbosity,workFactor);
+   } else {
+      bzfp = BZ2_bzReadOpen(&bzerr,fp,verbosity,smallMode,
+                            unused,nUnused);
+   }
+   if (bzfp == NULL) {
+      if (fp != stdin && fp != stdout) fclose(fp);
+      return NULL;
+   }
+   return bzfp;
+}
+
+
+/*---------------------------------------------------*/
+/*--
+   open file for read or write.
+      ex) bzopen("file","w9")
+      case path="" or NULL => use stdin or stdout.
+--*/
+BZFILE * BZ_API(BZ2_bzopen)
+               ( const char *path,
+                 const char *mode )
+{
+   return bzopen_or_bzdopen(path,-1,mode,/*bzopen*/0);
+}
+
+
+/*---------------------------------------------------*/
+BZFILE * BZ_API(BZ2_bzdopen)
+               ( int fd,
+                 const char *mode )
+{
+   return bzopen_or_bzdopen(NULL,fd,mode,/*bzdopen*/1);
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzread) (BZFILE* b, void* buf, int len )
+{
+   int bzerr, nread;
+   if (((bzFile*)b)->lastErr == BZ_STREAM_END) return 0;
+   nread = BZ2_bzRead(&bzerr,b,buf,len);
+   if (bzerr == BZ_OK || bzerr == BZ_STREAM_END) {
+      return nread;
+   } else {
+      return -1;
+   }
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzwrite) (BZFILE* b, void* buf, int len )
+{
+   int bzerr;
+
+   BZ2_bzWrite(&bzerr,b,buf,len);
+   if(bzerr == BZ_OK){
+      return len;
+   }else{
+      return -1;
+   }
+}
+
+
+/*---------------------------------------------------*/
+int BZ_API(BZ2_bzflush) (BZFILE *b)
+{
+   /* do nothing now... */
+   return 0;
+}
+
+
+/*---------------------------------------------------*/
+void BZ_API(BZ2_bzclose) (BZFILE* b)
+{
+   int bzerr;
+   FILE *fp;
+   
+   if (b==NULL) {return;}
+   fp = ((bzFile *)b)->handle;
+   if(((bzFile*)b)->writing){
+      BZ2_bzWriteClose(&bzerr,b,0,NULL,NULL);
+      if(bzerr != BZ_OK){
+         BZ2_bzWriteClose(NULL,b,1,NULL,NULL);
+      }
+   }else{
+      BZ2_bzReadClose(&bzerr,b);
+   }
+   if(fp!=stdin && fp!=stdout){
+      fclose(fp);
+   }
+}
+
+
+/*---------------------------------------------------*/
+/*--
+   return last error code 
+--*/
+static const char *bzerrorstrings[] = {
+       "OK"
+      ,"SEQUENCE_ERROR"
+      ,"PARAM_ERROR"
+      ,"MEM_ERROR"
+      ,"DATA_ERROR"
+      ,"DATA_ERROR_MAGIC"
+      ,"IO_ERROR"
+      ,"UNEXPECTED_EOF"
+      ,"OUTBUFF_FULL"
+      ,"CONFIG_ERROR"
+      ,"???"   /* for future */
+      ,"???"   /* for future */
+      ,"???"   /* for future */
+      ,"???"   /* for future */
+      ,"???"   /* for future */
+      ,"???"   /* for future */
+};
+
+
+const char * BZ_API(BZ2_bzerror) (BZFILE *b, int *errnum)
+{
+   int err = ((bzFile *)b)->lastErr;
+
+   if(err>0) err = 0;
+   *errnum = err;
+   return bzerrorstrings[err*-1];
+}
+#endif
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                           bzlib.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.h b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.h
new file mode 100644
index 000000000000..8966a6c5804e
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib.h
@@ -0,0 +1,282 @@
+
+/*-------------------------------------------------------------*/
+/*--- Public header file for the library.                   ---*/
+/*---                                               bzlib.h ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#ifndef _BZLIB_H
+#define _BZLIB_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define BZ_RUN               0
+#define BZ_FLUSH             1
+#define BZ_FINISH            2
+
+#define BZ_OK                0
+#define BZ_RUN_OK            1
+#define BZ_FLUSH_OK          2
+#define BZ_FINISH_OK         3
+#define BZ_STREAM_END        4
+#define BZ_SEQUENCE_ERROR    (-1)
+#define BZ_PARAM_ERROR       (-2)
+#define BZ_MEM_ERROR         (-3)
+#define BZ_DATA_ERROR        (-4)
+#define BZ_DATA_ERROR_MAGIC  (-5)
+#define BZ_IO_ERROR          (-6)
+#define BZ_UNEXPECTED_EOF    (-7)
+#define BZ_OUTBUFF_FULL      (-8)
+#define BZ_CONFIG_ERROR      (-9)
+
+typedef 
+   struct {
+      char *next_in;
+      unsigned int avail_in;
+      unsigned int total_in_lo32;
+      unsigned int total_in_hi32;
+
+      char *next_out;
+      unsigned int avail_out;
+      unsigned int total_out_lo32;
+      unsigned int total_out_hi32;
+
+      void *state;
+
+      void *(*bzalloc)(void *,int,int);
+      void (*bzfree)(void *,void *);
+      void *opaque;
+   } 
+   bz_stream;
+
+
+#ifndef BZ_IMPORT
+#define BZ_EXPORT
+#endif
+
+#ifndef BZ_NO_STDIO
+/* Need a definitition for FILE */
+#include <stdio.h>
+#endif
+
+#ifdef _WIN32
+#   include <windows.h>
+#   ifdef small
+      /* windows.h define small to char */
+#      undef small
+#   endif
+#   ifdef BZ_EXPORT
+#   define BZ_API(func) WINAPI func
+#   define BZ_EXTERN extern
+#   else
+   /* import windows dll dynamically */
+#   define BZ_API(func) (WINAPI * func)
+#   define BZ_EXTERN
+#   endif
+#else
+#   define BZ_API(func) func
+#   define BZ_EXTERN extern
+#endif
+
+
+/*-- Core (low-level) library functions --*/
+
+BZ_EXTERN int BZ_API(BZ2_bzCompressInit) ( 
+      bz_stream* strm, 
+      int        blockSize100k, 
+      int        verbosity, 
+      int        workFactor 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzCompress) ( 
+      bz_stream* strm, 
+      int action 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzCompressEnd) ( 
+      bz_stream* strm 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzDecompressInit) ( 
+      bz_stream *strm, 
+      int       verbosity, 
+      int       small
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzDecompress) ( 
+      bz_stream* strm 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzDecompressEnd) ( 
+      bz_stream *strm 
+   );
+
+
+
+/*-- High(er) level library functions --*/
+
+#ifndef BZ_NO_STDIO
+#define BZ_MAX_UNUSED 5000
+
+typedef void BZFILE;
+
+BZ_EXTERN BZFILE* BZ_API(BZ2_bzReadOpen) ( 
+      int*  bzerror,   
+      FILE* f, 
+      int   verbosity, 
+      int   small,
+      void* unused,    
+      int   nUnused 
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzReadClose) ( 
+      int*    bzerror, 
+      BZFILE* b 
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzReadGetUnused) ( 
+      int*    bzerror, 
+      BZFILE* b, 
+      void**  unused,  
+      int*    nUnused 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzRead) ( 
+      int*    bzerror, 
+      BZFILE* b, 
+      void*   buf, 
+      int     len 
+   );
+
+BZ_EXTERN BZFILE* BZ_API(BZ2_bzWriteOpen) ( 
+      int*  bzerror,      
+      FILE* f, 
+      int   blockSize100k, 
+      int   verbosity, 
+      int   workFactor 
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzWrite) ( 
+      int*    bzerror, 
+      BZFILE* b, 
+      void*   buf, 
+      int     len 
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzWriteClose) ( 
+      int*          bzerror, 
+      BZFILE*       b, 
+      int           abandon, 
+      unsigned int* nbytes_in, 
+      unsigned int* nbytes_out 
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzWriteClose64) ( 
+      int*          bzerror, 
+      BZFILE*       b, 
+      int           abandon, 
+      unsigned int* nbytes_in_lo32, 
+      unsigned int* nbytes_in_hi32, 
+      unsigned int* nbytes_out_lo32, 
+      unsigned int* nbytes_out_hi32
+   );
+#endif
+
+
+/*-- Utility functions --*/
+
+BZ_EXTERN int BZ_API(BZ2_bzBuffToBuffCompress) ( 
+      char*         dest, 
+      unsigned int* destLen,
+      char*         source, 
+      unsigned int  sourceLen,
+      int           blockSize100k, 
+      int           verbosity, 
+      int           workFactor 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzBuffToBuffDecompress) ( 
+      char*         dest, 
+      unsigned int* destLen,
+      char*         source, 
+      unsigned int  sourceLen,
+      int           small, 
+      int           verbosity 
+   );
+
+
+/*--
+   Code contributed by Yoshioka Tsuneo (tsuneo@rr.iij4u.or.jp)
+   to support better zlib compatibility.
+   This code is not _officially_ part of libbzip2 (yet);
+   I haven't tested it, documented it, or considered the
+   threading-safeness of it.
+   If this code breaks, please contact both Yoshioka and me.
+--*/
+
+BZ_EXTERN const char * BZ_API(BZ2_bzlibVersion) (
+      void
+   );
+
+#ifndef BZ_NO_STDIO
+BZ_EXTERN BZFILE * BZ_API(BZ2_bzopen) (
+      const char *path,
+      const char *mode
+   );
+
+BZ_EXTERN BZFILE * BZ_API(BZ2_bzdopen) (
+      int        fd,
+      const char *mode
+   );
+         
+BZ_EXTERN int BZ_API(BZ2_bzread) (
+      BZFILE* b, 
+      void* buf, 
+      int len 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzwrite) (
+      BZFILE* b, 
+      void*   buf, 
+      int     len 
+   );
+
+BZ_EXTERN int BZ_API(BZ2_bzflush) (
+      BZFILE* b
+   );
+
+BZ_EXTERN void BZ_API(BZ2_bzclose) (
+      BZFILE* b
+   );
+
+BZ_EXTERN const char * BZ_API(BZ2_bzerror) (
+      BZFILE *b, 
+      int    *errnum
+   );
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
+
+/*-------------------------------------------------------------*/
+/*--- end                                           bzlib.h ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/bzlib_private.h b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib_private.h
new file mode 100644
index 000000000000..3755a6f701ec
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/bzlib_private.h
@@ -0,0 +1,509 @@
+
+/*-------------------------------------------------------------*/
+/*--- Private header file for the library.                  ---*/
+/*---                                       bzlib_private.h ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#ifndef _BZLIB_PRIVATE_H
+#define _BZLIB_PRIVATE_H
+
+#include <stdlib.h>
+
+#ifndef BZ_NO_STDIO
+#include <stdio.h>
+#include <ctype.h>
+#include <string.h>
+#endif
+
+#include "bzlib.h"
+
+
+
+/*-- General stuff. --*/
+
+#define BZ_VERSION  "1.0.8, 13-Jul-2019"
+
+typedef char            Char;
+typedef unsigned char   Bool;
+typedef unsigned char   UChar;
+typedef int             Int32;
+typedef unsigned int    UInt32;
+typedef short           Int16;
+typedef unsigned short  UInt16;
+
+#define True  ((Bool)1)
+#define False ((Bool)0)
+
+#ifndef __GNUC__
+#define __inline__  /* */
+#endif 
+
+#ifndef BZ_NO_STDIO
+
+extern void BZ2_bz__AssertH__fail ( int errcode );
+#define AssertH(cond,errcode) \
+   { if (!(cond)) BZ2_bz__AssertH__fail ( errcode ); }
+
+#if BZ_DEBUG
+#define AssertD(cond,msg) \
+   { if (!(cond)) {       \
+      fprintf ( stderr,   \
+        "\n\nlibbzip2(debug build): internal error\n\t%s\n", msg );\
+      exit(1); \
+   }}
+#else
+#define AssertD(cond,msg) /* */
+#endif
+
+#define VPrintf0(zf) \
+   fprintf(stderr,zf)
+#define VPrintf1(zf,za1) \
+   fprintf(stderr,zf,za1)
+#define VPrintf2(zf,za1,za2) \
+   fprintf(stderr,zf,za1,za2)
+#define VPrintf3(zf,za1,za2,za3) \
+   fprintf(stderr,zf,za1,za2,za3)
+#define VPrintf4(zf,za1,za2,za3,za4) \
+   fprintf(stderr,zf,za1,za2,za3,za4)
+#define VPrintf5(zf,za1,za2,za3,za4,za5) \
+   fprintf(stderr,zf,za1,za2,za3,za4,za5)
+
+#else
+
+extern void bz_internal_error ( int errcode );
+#define AssertH(cond,errcode) \
+   { if (!(cond)) bz_internal_error ( errcode ); }
+#define AssertD(cond,msg)                do { } while (0)
+#define VPrintf0(zf)                     do { } while (0)
+#define VPrintf1(zf,za1)                 do { } while (0)
+#define VPrintf2(zf,za1,za2)             do { } while (0)
+#define VPrintf3(zf,za1,za2,za3)         do { } while (0)
+#define VPrintf4(zf,za1,za2,za3,za4)     do { } while (0)
+#define VPrintf5(zf,za1,za2,za3,za4,za5) do { } while (0)
+
+#endif
+
+
+#define BZALLOC(nnn) (strm->bzalloc)(strm->opaque,(nnn),1)
+#define BZFREE(ppp)  (strm->bzfree)(strm->opaque,(ppp))
+
+
+/*-- Header bytes. --*/
+
+#define BZ_HDR_B 0x42   /* 'B' */
+#define BZ_HDR_Z 0x5a   /* 'Z' */
+#define BZ_HDR_h 0x68   /* 'h' */
+#define BZ_HDR_0 0x30   /* '0' */
+  
+/*-- Constants for the back end. --*/
+
+#define BZ_MAX_ALPHA_SIZE 258
+#define BZ_MAX_CODE_LEN    23
+
+#define BZ_RUNA 0
+#define BZ_RUNB 1
+
+#define BZ_N_GROUPS 6
+#define BZ_G_SIZE   50
+#define BZ_N_ITERS  4
+
+#define BZ_MAX_SELECTORS (2 + (900000 / BZ_G_SIZE))
+
+
+
+/*-- Stuff for randomising repetitive blocks. --*/
+
+extern Int32 BZ2_rNums[512];
+
+#define BZ_RAND_DECLS                          \
+   Int32 rNToGo;                               \
+   Int32 rTPos                                 \
+
+#define BZ_RAND_INIT_MASK                      \
+   s->rNToGo = 0;                              \
+   s->rTPos  = 0                               \
+
+#define BZ_RAND_MASK ((s->rNToGo == 1) ? 1 : 0)
+
+#define BZ_RAND_UPD_MASK                       \
+   if (s->rNToGo == 0) {                       \
+      s->rNToGo = BZ2_rNums[s->rTPos];         \
+      s->rTPos++;                              \
+      if (s->rTPos == 512) s->rTPos = 0;       \
+   }                                           \
+   s->rNToGo--;
+
+
+
+/*-- Stuff for doing CRCs. --*/
+
+extern UInt32 BZ2_crc32Table[256];
+
+#define BZ_INITIALISE_CRC(crcVar)              \
+{                                              \
+   crcVar = 0xffffffffL;                       \
+}
+
+#define BZ_FINALISE_CRC(crcVar)                \
+{                                              \
+   crcVar = ~(crcVar);                         \
+}
+
+#define BZ_UPDATE_CRC(crcVar,cha)              \
+{                                              \
+   crcVar = (crcVar << 8) ^                    \
+            BZ2_crc32Table[(crcVar >> 24) ^    \
+                           ((UChar)cha)];      \
+}
+
+
+
+/*-- States and modes for compression. --*/
+
+#define BZ_M_IDLE      1
+#define BZ_M_RUNNING   2
+#define BZ_M_FLUSHING  3
+#define BZ_M_FINISHING 4
+
+#define BZ_S_OUTPUT    1
+#define BZ_S_INPUT     2
+
+#define BZ_N_RADIX 2
+#define BZ_N_QSORT 12
+#define BZ_N_SHELL 18
+#define BZ_N_OVERSHOOT (BZ_N_RADIX + BZ_N_QSORT + BZ_N_SHELL + 2)
+
+
+
+
+/*-- Structure holding all the compression-side stuff. --*/
+
+typedef
+   struct {
+      /* pointer back to the struct bz_stream */
+      bz_stream* strm;
+
+      /* mode this stream is in, and whether inputting */
+      /* or outputting data */
+      Int32    mode;
+      Int32    state;
+
+      /* remembers avail_in when flush/finish requested */
+      UInt32   avail_in_expect;
+
+      /* for doing the block sorting */
+      UInt32*  arr1;
+      UInt32*  arr2;
+      UInt32*  ftab;
+      Int32    origPtr;
+
+      /* aliases for arr1 and arr2 */
+      UInt32*  ptr;
+      UChar*   block;
+      UInt16*  mtfv;
+      UChar*   zbits;
+
+      /* for deciding when to use the fallback sorting algorithm */
+      Int32    workFactor;
+
+      /* run-length-encoding of the input */
+      UInt32   state_in_ch;
+      Int32    state_in_len;
+      BZ_RAND_DECLS;
+
+      /* input and output limits and current posns */
+      Int32    nblock;
+      Int32    nblockMAX;
+      Int32    numZ;
+      Int32    state_out_pos;
+
+      /* map of bytes used in block */
+      Int32    nInUse;
+      Bool     inUse[256];
+      UChar    unseqToSeq[256];
+
+      /* the buffer for bit stream creation */
+      UInt32   bsBuff;
+      Int32    bsLive;
+
+      /* block and combined CRCs */
+      UInt32   blockCRC;
+      UInt32   combinedCRC;
+
+      /* misc administratium */
+      Int32    verbosity;
+      Int32    blockNo;
+      Int32    blockSize100k;
+
+      /* stuff for coding the MTF values */
+      Int32    nMTF;
+      Int32    mtfFreq    [BZ_MAX_ALPHA_SIZE];
+      UChar    selector   [BZ_MAX_SELECTORS];
+      UChar    selectorMtf[BZ_MAX_SELECTORS];
+
+      UChar    len     [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      Int32    code    [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      Int32    rfreq   [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      /* second dimension: only 3 needed; 4 makes index calculations faster */
+      UInt32   len_pack[BZ_MAX_ALPHA_SIZE][4];
+
+   }
+   EState;
+
+
+
+/*-- externs for compression. --*/
+
+extern void 
+BZ2_blockSort ( EState* );
+
+extern void 
+BZ2_compressBlock ( EState*, Bool );
+
+extern void 
+BZ2_bsInitWrite ( EState* );
+
+extern void 
+BZ2_hbAssignCodes ( Int32*, UChar*, Int32, Int32, Int32 );
+
+extern void 
+BZ2_hbMakeCodeLengths ( UChar*, Int32*, Int32, Int32 );
+
+
+
+/*-- states for decompression. --*/
+
+#define BZ_X_IDLE        1
+#define BZ_X_OUTPUT      2
+
+#define BZ_X_MAGIC_1     10
+#define BZ_X_MAGIC_2     11
+#define BZ_X_MAGIC_3     12
+#define BZ_X_MAGIC_4     13
+#define BZ_X_BLKHDR_1    14
+#define BZ_X_BLKHDR_2    15
+#define BZ_X_BLKHDR_3    16
+#define BZ_X_BLKHDR_4    17
+#define BZ_X_BLKHDR_5    18
+#define BZ_X_BLKHDR_6    19
+#define BZ_X_BCRC_1      20
+#define BZ_X_BCRC_2      21
+#define BZ_X_BCRC_3      22
+#define BZ_X_BCRC_4      23
+#define BZ_X_RANDBIT     24
+#define BZ_X_ORIGPTR_1   25
+#define BZ_X_ORIGPTR_2   26
+#define BZ_X_ORIGPTR_3   27
+#define BZ_X_MAPPING_1   28
+#define BZ_X_MAPPING_2   29
+#define BZ_X_SELECTOR_1  30
+#define BZ_X_SELECTOR_2  31
+#define BZ_X_SELECTOR_3  32
+#define BZ_X_CODING_1    33
+#define BZ_X_CODING_2    34
+#define BZ_X_CODING_3    35
+#define BZ_X_MTF_1       36
+#define BZ_X_MTF_2       37
+#define BZ_X_MTF_3       38
+#define BZ_X_MTF_4       39
+#define BZ_X_MTF_5       40
+#define BZ_X_MTF_6       41
+#define BZ_X_ENDHDR_2    42
+#define BZ_X_ENDHDR_3    43
+#define BZ_X_ENDHDR_4    44
+#define BZ_X_ENDHDR_5    45
+#define BZ_X_ENDHDR_6    46
+#define BZ_X_CCRC_1      47
+#define BZ_X_CCRC_2      48
+#define BZ_X_CCRC_3      49
+#define BZ_X_CCRC_4      50
+
+
+
+/*-- Constants for the fast MTF decoder. --*/
+
+#define MTFA_SIZE 4096
+#define MTFL_SIZE 16
+
+
+
+/*-- Structure holding all the decompression-side stuff. --*/
+
+typedef
+   struct {
+      /* pointer back to the struct bz_stream */
+      bz_stream* strm;
+
+      /* state indicator for this stream */
+      Int32    state;
+
+      /* for doing the final run-length decoding */
+      UChar    state_out_ch;
+      Int32    state_out_len;
+      Bool     blockRandomised;
+      BZ_RAND_DECLS;
+
+      /* the buffer for bit stream reading */
+      UInt32   bsBuff;
+      Int32    bsLive;
+
+      /* misc administratium */
+      Int32    blockSize100k;
+      Bool     smallDecompress;
+      Int32    currBlockNo;
+      Int32    verbosity;
+
+      /* for undoing the Burrows-Wheeler transform */
+      Int32    origPtr;
+      UInt32   tPos;
+      Int32    k0;
+      Int32    unzftab[256];
+      Int32    nblock_used;
+      Int32    cftab[257];
+      Int32    cftabCopy[257];
+
+      /* for undoing the Burrows-Wheeler transform (FAST) */
+      UInt32   *tt;
+
+      /* for undoing the Burrows-Wheeler transform (SMALL) */
+      UInt16   *ll16;
+      UChar    *ll4;
+
+      /* stored and calculated CRCs */
+      UInt32   storedBlockCRC;
+      UInt32   storedCombinedCRC;
+      UInt32   calculatedBlockCRC;
+      UInt32   calculatedCombinedCRC;
+
+      /* map of bytes used in block */
+      Int32    nInUse;
+      Bool     inUse[256];
+      Bool     inUse16[16];
+      UChar    seqToUnseq[256];
+
+      /* for decoding the MTF values */
+      UChar    mtfa   [MTFA_SIZE];
+      Int32    mtfbase[256 / MTFL_SIZE];
+      UChar    selector   [BZ_MAX_SELECTORS];
+      UChar    selectorMtf[BZ_MAX_SELECTORS];
+      UChar    len  [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+
+      Int32    limit  [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      Int32    base   [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      Int32    perm   [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+      Int32    minLens[BZ_N_GROUPS];
+
+      /* save area for scalars in the main decompress code */
+      Int32    save_i;
+      Int32    save_j;
+      Int32    save_t;
+      Int32    save_alphaSize;
+      Int32    save_nGroups;
+      Int32    save_nSelectors;
+      Int32    save_EOB;
+      Int32    save_groupNo;
+      Int32    save_groupPos;
+      Int32    save_nextSym;
+      Int32    save_nblockMAX;
+      Int32    save_nblock;
+      Int32    save_es;
+      Int32    save_N;
+      Int32    save_curr;
+      Int32    save_zt;
+      Int32    save_zn; 
+      Int32    save_zvec;
+      Int32    save_zj;
+      Int32    save_gSel;
+      Int32    save_gMinlen;
+      Int32*   save_gLimit;
+      Int32*   save_gBase;
+      Int32*   save_gPerm;
+
+   }
+   DState;
+
+
+
+/*-- Macros for decompression. --*/
+
+#define BZ_GET_FAST(cccc)                     \
+    /* c_tPos is unsigned, hence test < 0 is pointless. */ \
+    if (s->tPos >= (UInt32)100000 * (UInt32)s->blockSize100k) return True; \
+    s->tPos = s->tt[s->tPos];                 \
+    cccc = (UChar)(s->tPos & 0xff);           \
+    s->tPos >>= 8;
+
+#define BZ_GET_FAST_C(cccc)                   \
+    /* c_tPos is unsigned, hence test < 0 is pointless. */ \
+    if (c_tPos >= (UInt32)100000 * (UInt32)ro_blockSize100k) return True; \
+    c_tPos = c_tt[c_tPos];                    \
+    cccc = (UChar)(c_tPos & 0xff);            \
+    c_tPos >>= 8;
+
+#define SET_LL4(i,n)                                          \
+   { if (((i) & 0x1) == 0)                                    \
+        s->ll4[(i) >> 1] = (s->ll4[(i) >> 1] & 0xf0) | (n); else    \
+        s->ll4[(i) >> 1] = (s->ll4[(i) >> 1] & 0x0f) | ((n) << 4);  \
+   }
+
+#define GET_LL4(i)                             \
+   ((((UInt32)(s->ll4[(i) >> 1])) >> (((i) << 2) & 0x4)) & 0xF)
+
+#define SET_LL(i,n)                          \
+   { s->ll16[i] = (UInt16)(n & 0x0000ffff);  \
+     SET_LL4(i, n >> 16);                    \
+   }
+
+#define GET_LL(i) \
+   (((UInt32)s->ll16[i]) | (GET_LL4(i) << 16))
+
+#define BZ_GET_SMALL(cccc)                            \
+    /* c_tPos is unsigned, hence test < 0 is pointless. */ \
+    if (s->tPos >= (UInt32)100000 * (UInt32)s->blockSize100k) return True; \
+    cccc = BZ2_indexIntoF ( s->tPos, s->cftab );    \
+    s->tPos = GET_LL(s->tPos);
+
+
+/*-- externs for decompression. --*/
+
+extern Int32 
+BZ2_indexIntoF ( Int32, Int32* );
+
+extern Int32 
+BZ2_decompress ( DState* );
+
+extern void 
+BZ2_hbCreateDecodeTables ( Int32*, Int32*, Int32*, UChar*,
+                           Int32,  Int32, Int32 );
+
+
+#endif
+
+
+/*-- BZ_NO_STDIO seems to make NULL disappear on some platforms. --*/
+
+#ifdef BZ_NO_STDIO
+#ifndef NULL
+#define NULL 0
+#endif
+#endif
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                   bzlib_private.h ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/compress.c b/llvm/lib/Bitcode/MetalLib/bzip2/compress.c
new file mode 100644
index 000000000000..5dfa00231b09
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/compress.c
@@ -0,0 +1,672 @@
+
+/*-------------------------------------------------------------*/
+/*--- Compression machinery (not incl block sorting)        ---*/
+/*---                                            compress.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+/* CHANGES
+    0.9.0    -- original version.
+    0.9.0a/b -- no changes in this file.
+    0.9.0c   -- changed setting of nGroups in sendMTFValues() 
+                so as to do a bit better on small files
+*/
+
+#include "bzlib_private.h"
+
+
+/*---------------------------------------------------*/
+/*--- Bit stream I/O                              ---*/
+/*---------------------------------------------------*/
+
+/*---------------------------------------------------*/
+void BZ2_bsInitWrite ( EState* s )
+{
+   s->bsLive = 0;
+   s->bsBuff = 0;
+}
+
+
+/*---------------------------------------------------*/
+static
+void bsFinishWrite ( EState* s )
+{
+   while (s->bsLive > 0) {
+      s->zbits[s->numZ] = (UChar)(s->bsBuff >> 24);
+      s->numZ++;
+      s->bsBuff <<= 8;
+      s->bsLive -= 8;
+   }
+}
+
+
+/*---------------------------------------------------*/
+#define bsNEEDW(nz)                           \
+{                                             \
+   while (s->bsLive >= 8) {                   \
+      s->zbits[s->numZ]                       \
+         = (UChar)(s->bsBuff >> 24);          \
+      s->numZ++;                              \
+      s->bsBuff <<= 8;                        \
+      s->bsLive -= 8;                         \
+   }                                          \
+}
+
+
+/*---------------------------------------------------*/
+static
+__inline__
+void bsW ( EState* s, Int32 n, UInt32 v )
+{
+   bsNEEDW ( n );
+   s->bsBuff |= (v << (32 - s->bsLive - n));
+   s->bsLive += n;
+}
+
+
+/*---------------------------------------------------*/
+static
+void bsPutUInt32 ( EState* s, UInt32 u )
+{
+   bsW ( s, 8, (u >> 24) & 0xffL );
+   bsW ( s, 8, (u >> 16) & 0xffL );
+   bsW ( s, 8, (u >>  8) & 0xffL );
+   bsW ( s, 8,  u        & 0xffL );
+}
+
+
+/*---------------------------------------------------*/
+static
+void bsPutUChar ( EState* s, UChar c )
+{
+   bsW( s, 8, (UInt32)c );
+}
+
+
+/*---------------------------------------------------*/
+/*--- The back end proper                         ---*/
+/*---------------------------------------------------*/
+
+/*---------------------------------------------------*/
+static
+void makeMaps_e ( EState* s )
+{
+   Int32 i;
+   s->nInUse = 0;
+   for (i = 0; i < 256; i++)
+      if (s->inUse[i]) {
+         s->unseqToSeq[i] = s->nInUse;
+         s->nInUse++;
+      }
+}
+
+
+/*---------------------------------------------------*/
+static
+void generateMTFValues ( EState* s )
+{
+   UChar   yy[256];
+   Int32   i, j;
+   Int32   zPend;
+   Int32   wr;
+   Int32   EOB;
+
+   /* 
+      After sorting (eg, here),
+         s->arr1 [ 0 .. s->nblock-1 ] holds sorted order,
+         and
+         ((UChar*)s->arr2) [ 0 .. s->nblock-1 ] 
+         holds the original block data.
+
+      The first thing to do is generate the MTF values,
+      and put them in
+         ((UInt16*)s->arr1) [ 0 .. s->nblock-1 ].
+      Because there are strictly fewer or equal MTF values
+      than block values, ptr values in this area are overwritten
+      with MTF values only when they are no longer needed.
+
+      The final compressed bitstream is generated into the
+      area starting at
+         (UChar*) (&((UChar*)s->arr2)[s->nblock])
+
+      These storage aliases are set up in bzCompressInit(),
+      except for the last one, which is arranged in 
+      compressBlock().
+   */
+   UInt32* ptr   = s->ptr;
+   UChar* block  = s->block;
+   UInt16* mtfv  = s->mtfv;
+
+   makeMaps_e ( s );
+   EOB = s->nInUse+1;
+
+   for (i = 0; i <= EOB; i++) s->mtfFreq[i] = 0;
+
+   wr = 0;
+   zPend = 0;
+   for (i = 0; i < s->nInUse; i++) yy[i] = (UChar) i;
+
+   for (i = 0; i < s->nblock; i++) {
+      UChar ll_i;
+      AssertD ( wr <= i, "generateMTFValues(1)" );
+      j = ptr[i]-1; if (j < 0) j += s->nblock;
+      ll_i = s->unseqToSeq[block[j]];
+      AssertD ( ll_i < s->nInUse, "generateMTFValues(2a)" );
+
+      if (yy[0] == ll_i) { 
+         zPend++;
+      } else {
+
+         if (zPend > 0) {
+            zPend--;
+            while (True) {
+               if (zPend & 1) {
+                  mtfv[wr] = BZ_RUNB; wr++; 
+                  s->mtfFreq[BZ_RUNB]++; 
+               } else {
+                  mtfv[wr] = BZ_RUNA; wr++; 
+                  s->mtfFreq[BZ_RUNA]++; 
+               }
+               if (zPend < 2) break;
+               zPend = (zPend - 2) / 2;
+            };
+            zPend = 0;
+         }
+         {
+            register UChar  rtmp;
+            register UChar* ryy_j;
+            register UChar  rll_i;
+            rtmp  = yy[1];
+            yy[1] = yy[0];
+            ryy_j = &(yy[1]);
+            rll_i = ll_i;
+            while ( rll_i != rtmp ) {
+               register UChar rtmp2;
+               ryy_j++;
+               rtmp2  = rtmp;
+               rtmp   = *ryy_j;
+               *ryy_j = rtmp2;
+            };
+            yy[0] = rtmp;
+            j = ryy_j - &(yy[0]);
+            mtfv[wr] = j+1; wr++; s->mtfFreq[j+1]++;
+         }
+
+      }
+   }
+
+   if (zPend > 0) {
+      zPend--;
+      while (True) {
+         if (zPend & 1) {
+            mtfv[wr] = BZ_RUNB; wr++; 
+            s->mtfFreq[BZ_RUNB]++; 
+         } else {
+            mtfv[wr] = BZ_RUNA; wr++; 
+            s->mtfFreq[BZ_RUNA]++; 
+         }
+         if (zPend < 2) break;
+         zPend = (zPend - 2) / 2;
+      };
+      zPend = 0;
+   }
+
+   mtfv[wr] = EOB; wr++; s->mtfFreq[EOB]++;
+
+   s->nMTF = wr;
+}
+
+
+/*---------------------------------------------------*/
+#define BZ_LESSER_ICOST  0
+#define BZ_GREATER_ICOST 15
+
+static
+void sendMTFValues ( EState* s )
+{
+   Int32 v, t, i, j, gs, ge, totc, bt, bc, iter;
+   Int32 nSelectors, alphaSize, minLen, maxLen, selCtr;
+   Int32 nGroups, nBytes;
+
+   /*--
+   UChar  len [BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+   is a global since the decoder also needs it.
+
+   Int32  code[BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+   Int32  rfreq[BZ_N_GROUPS][BZ_MAX_ALPHA_SIZE];
+   are also globals only used in this proc.
+   Made global to keep stack frame size small.
+   --*/
+
+
+   UInt16 cost[BZ_N_GROUPS];
+   Int32  fave[BZ_N_GROUPS];
+
+   UInt16* mtfv = s->mtfv;
+
+   if (s->verbosity >= 3)
+      VPrintf3( "      %d in block, %d after MTF & 1-2 coding, "
+                "%d+2 syms in use\n", 
+                s->nblock, s->nMTF, s->nInUse );
+
+   alphaSize = s->nInUse+2;
+   for (t = 0; t < BZ_N_GROUPS; t++)
+      for (v = 0; v < alphaSize; v++)
+         s->len[t][v] = BZ_GREATER_ICOST;
+
+   /*--- Decide how many coding tables to use ---*/
+   AssertH ( s->nMTF > 0, 3001 );
+   if (s->nMTF < 200)  nGroups = 2; else
+   if (s->nMTF < 600)  nGroups = 3; else
+   if (s->nMTF < 1200) nGroups = 4; else
+   if (s->nMTF < 2400) nGroups = 5; else
+                       nGroups = 6;
+
+   /*--- Generate an initial set of coding tables ---*/
+   { 
+      Int32 nPart, remF, tFreq, aFreq;
+
+      nPart = nGroups;
+      remF  = s->nMTF;
+      gs = 0;
+      while (nPart > 0) {
+         tFreq = remF / nPart;
+         ge = gs-1;
+         aFreq = 0;
+         while (aFreq < tFreq && ge < alphaSize-1) {
+            ge++;
+            aFreq += s->mtfFreq[ge];
+         }
+
+         if (ge > gs 
+             && nPart != nGroups && nPart != 1 
+             && ((nGroups-nPart) % 2 == 1)) {
+            aFreq -= s->mtfFreq[ge];
+            ge--;
+         }
+
+         if (s->verbosity >= 3)
+            VPrintf5( "      initial group %d, [%d .. %d], "
+                      "has %d syms (%4.1f%%)\n",
+                      nPart, gs, ge, aFreq, 
+                      (100.0 * (float)aFreq) / (float)(s->nMTF) );
+ 
+         for (v = 0; v < alphaSize; v++)
+            if (v >= gs && v <= ge) 
+               s->len[nPart-1][v] = BZ_LESSER_ICOST; else
+               s->len[nPart-1][v] = BZ_GREATER_ICOST;
+ 
+         nPart--;
+         gs = ge+1;
+         remF -= aFreq;
+      }
+   }
+
+   /*--- 
+      Iterate up to BZ_N_ITERS times to improve the tables.
+   ---*/
+   for (iter = 0; iter < BZ_N_ITERS; iter++) {
+
+      for (t = 0; t < nGroups; t++) fave[t] = 0;
+
+      for (t = 0; t < nGroups; t++)
+         for (v = 0; v < alphaSize; v++)
+            s->rfreq[t][v] = 0;
+
+      /*---
+        Set up an auxiliary length table which is used to fast-track
+	the common case (nGroups == 6). 
+      ---*/
+      if (nGroups == 6) {
+         for (v = 0; v < alphaSize; v++) {
+            s->len_pack[v][0] = (s->len[1][v] << 16) | s->len[0][v];
+            s->len_pack[v][1] = (s->len[3][v] << 16) | s->len[2][v];
+            s->len_pack[v][2] = (s->len[5][v] << 16) | s->len[4][v];
+	 }
+      }
+
+      nSelectors = 0;
+      totc = 0;
+      gs = 0;
+      while (True) {
+
+         /*--- Set group start & end marks. --*/
+         if (gs >= s->nMTF) break;
+         ge = gs + BZ_G_SIZE - 1; 
+         if (ge >= s->nMTF) ge = s->nMTF-1;
+
+         /*-- 
+            Calculate the cost of this group as coded
+            by each of the coding tables.
+         --*/
+         for (t = 0; t < nGroups; t++) cost[t] = 0;
+
+         if (nGroups == 6 && 50 == ge-gs+1) {
+            /*--- fast track the common case ---*/
+            register UInt32 cost01, cost23, cost45;
+            register UInt16 icv;
+            cost01 = cost23 = cost45 = 0;
+
+#           define BZ_ITER(nn)                \
+               icv = mtfv[gs+(nn)];           \
+               cost01 += s->len_pack[icv][0]; \
+               cost23 += s->len_pack[icv][1]; \
+               cost45 += s->len_pack[icv][2]; \
+
+            BZ_ITER(0);  BZ_ITER(1);  BZ_ITER(2);  BZ_ITER(3);  BZ_ITER(4);
+            BZ_ITER(5);  BZ_ITER(6);  BZ_ITER(7);  BZ_ITER(8);  BZ_ITER(9);
+            BZ_ITER(10); BZ_ITER(11); BZ_ITER(12); BZ_ITER(13); BZ_ITER(14);
+            BZ_ITER(15); BZ_ITER(16); BZ_ITER(17); BZ_ITER(18); BZ_ITER(19);
+            BZ_ITER(20); BZ_ITER(21); BZ_ITER(22); BZ_ITER(23); BZ_ITER(24);
+            BZ_ITER(25); BZ_ITER(26); BZ_ITER(27); BZ_ITER(28); BZ_ITER(29);
+            BZ_ITER(30); BZ_ITER(31); BZ_ITER(32); BZ_ITER(33); BZ_ITER(34);
+            BZ_ITER(35); BZ_ITER(36); BZ_ITER(37); BZ_ITER(38); BZ_ITER(39);
+            BZ_ITER(40); BZ_ITER(41); BZ_ITER(42); BZ_ITER(43); BZ_ITER(44);
+            BZ_ITER(45); BZ_ITER(46); BZ_ITER(47); BZ_ITER(48); BZ_ITER(49);
+
+#           undef BZ_ITER
+
+            cost[0] = cost01 & 0xffff; cost[1] = cost01 >> 16;
+            cost[2] = cost23 & 0xffff; cost[3] = cost23 >> 16;
+            cost[4] = cost45 & 0xffff; cost[5] = cost45 >> 16;
+
+         } else {
+	    /*--- slow version which correctly handles all situations ---*/
+            for (i = gs; i <= ge; i++) { 
+               UInt16 icv = mtfv[i];
+               for (t = 0; t < nGroups; t++) cost[t] += s->len[t][icv];
+            }
+         }
+ 
+         /*-- 
+            Find the coding table which is best for this group,
+            and record its identity in the selector table.
+         --*/
+         bc = 999999999; bt = -1;
+         for (t = 0; t < nGroups; t++)
+            if (cost[t] < bc) { bc = cost[t]; bt = t; };
+         totc += bc;
+         fave[bt]++;
+         s->selector[nSelectors] = bt;
+         nSelectors++;
+
+         /*-- 
+            Increment the symbol frequencies for the selected table.
+          --*/
+         if (nGroups == 6 && 50 == ge-gs+1) {
+            /*--- fast track the common case ---*/
+
+#           define BZ_ITUR(nn) s->rfreq[bt][ mtfv[gs+(nn)] ]++
+
+            BZ_ITUR(0);  BZ_ITUR(1);  BZ_ITUR(2);  BZ_ITUR(3);  BZ_ITUR(4);
+            BZ_ITUR(5);  BZ_ITUR(6);  BZ_ITUR(7);  BZ_ITUR(8);  BZ_ITUR(9);
+            BZ_ITUR(10); BZ_ITUR(11); BZ_ITUR(12); BZ_ITUR(13); BZ_ITUR(14);
+            BZ_ITUR(15); BZ_ITUR(16); BZ_ITUR(17); BZ_ITUR(18); BZ_ITUR(19);
+            BZ_ITUR(20); BZ_ITUR(21); BZ_ITUR(22); BZ_ITUR(23); BZ_ITUR(24);
+            BZ_ITUR(25); BZ_ITUR(26); BZ_ITUR(27); BZ_ITUR(28); BZ_ITUR(29);
+            BZ_ITUR(30); BZ_ITUR(31); BZ_ITUR(32); BZ_ITUR(33); BZ_ITUR(34);
+            BZ_ITUR(35); BZ_ITUR(36); BZ_ITUR(37); BZ_ITUR(38); BZ_ITUR(39);
+            BZ_ITUR(40); BZ_ITUR(41); BZ_ITUR(42); BZ_ITUR(43); BZ_ITUR(44);
+            BZ_ITUR(45); BZ_ITUR(46); BZ_ITUR(47); BZ_ITUR(48); BZ_ITUR(49);
+
+#           undef BZ_ITUR
+
+         } else {
+	    /*--- slow version which correctly handles all situations ---*/
+            for (i = gs; i <= ge; i++)
+               s->rfreq[bt][ mtfv[i] ]++;
+         }
+
+         gs = ge+1;
+      }
+      if (s->verbosity >= 3) {
+         VPrintf2 ( "      pass %d: size is %d, grp uses are ", 
+                   iter+1, totc/8 );
+         for (t = 0; t < nGroups; t++)
+            VPrintf1 ( "%d ", fave[t] );
+         VPrintf0 ( "\n" );
+      }
+
+      /*--
+        Recompute the tables based on the accumulated frequencies.
+      --*/
+      /* maxLen was changed from 20 to 17 in bzip2-1.0.3.  See 
+         comment in huffman.c for details. */
+      for (t = 0; t < nGroups; t++)
+         BZ2_hbMakeCodeLengths ( &(s->len[t][0]), &(s->rfreq[t][0]), 
+                                 alphaSize, 17 /*20*/ );
+   }
+
+
+   AssertH( nGroups < 8, 3002 );
+   AssertH( nSelectors < 32768 &&
+            nSelectors <= BZ_MAX_SELECTORS,
+            3003 );
+
+
+   /*--- Compute MTF values for the selectors. ---*/
+   {
+      UChar pos[BZ_N_GROUPS], ll_i, tmp2, tmp;
+      for (i = 0; i < nGroups; i++) pos[i] = i;
+      for (i = 0; i < nSelectors; i++) {
+         ll_i = s->selector[i];
+         j = 0;
+         tmp = pos[j];
+         while ( ll_i != tmp ) {
+            j++;
+            tmp2 = tmp;
+            tmp = pos[j];
+            pos[j] = tmp2;
+         };
+         pos[0] = tmp;
+         s->selectorMtf[i] = j;
+      }
+   };
+
+   /*--- Assign actual codes for the tables. --*/
+   for (t = 0; t < nGroups; t++) {
+      minLen = 32;
+      maxLen = 0;
+      for (i = 0; i < alphaSize; i++) {
+         if (s->len[t][i] > maxLen) maxLen = s->len[t][i];
+         if (s->len[t][i] < minLen) minLen = s->len[t][i];
+      }
+      AssertH ( !(maxLen > 17 /*20*/ ), 3004 );
+      AssertH ( !(minLen < 1),  3005 );
+      BZ2_hbAssignCodes ( &(s->code[t][0]), &(s->len[t][0]), 
+                          minLen, maxLen, alphaSize );
+   }
+
+   /*--- Transmit the mapping table. ---*/
+   { 
+      Bool inUse16[16];
+      for (i = 0; i < 16; i++) {
+          inUse16[i] = False;
+          for (j = 0; j < 16; j++)
+             if (s->inUse[i * 16 + j]) inUse16[i] = True;
+      }
+     
+      nBytes = s->numZ;
+      for (i = 0; i < 16; i++)
+         if (inUse16[i]) bsW(s,1,1); else bsW(s,1,0);
+
+      for (i = 0; i < 16; i++)
+         if (inUse16[i])
+            for (j = 0; j < 16; j++) {
+               if (s->inUse[i * 16 + j]) bsW(s,1,1); else bsW(s,1,0);
+            }
+
+      if (s->verbosity >= 3) 
+         VPrintf1( "      bytes: mapping %d, ", s->numZ-nBytes );
+   }
+
+   /*--- Now the selectors. ---*/
+   nBytes = s->numZ;
+   bsW ( s, 3, nGroups );
+   bsW ( s, 15, nSelectors );
+   for (i = 0; i < nSelectors; i++) { 
+      for (j = 0; j < s->selectorMtf[i]; j++) bsW(s,1,1);
+      bsW(s,1,0);
+   }
+   if (s->verbosity >= 3)
+      VPrintf1( "selectors %d, ", s->numZ-nBytes );
+
+   /*--- Now the coding tables. ---*/
+   nBytes = s->numZ;
+
+   for (t = 0; t < nGroups; t++) {
+      Int32 curr = s->len[t][0];
+      bsW ( s, 5, curr );
+      for (i = 0; i < alphaSize; i++) {
+         while (curr < s->len[t][i]) { bsW(s,2,2); curr++; /* 10 */ };
+         while (curr > s->len[t][i]) { bsW(s,2,3); curr--; /* 11 */ };
+         bsW ( s, 1, 0 );
+      }
+   }
+
+   if (s->verbosity >= 3)
+      VPrintf1 ( "code lengths %d, ", s->numZ-nBytes );
+
+   /*--- And finally, the block data proper ---*/
+   nBytes = s->numZ;
+   selCtr = 0;
+   gs = 0;
+   while (True) {
+      if (gs >= s->nMTF) break;
+      ge = gs + BZ_G_SIZE - 1; 
+      if (ge >= s->nMTF) ge = s->nMTF-1;
+      AssertH ( s->selector[selCtr] < nGroups, 3006 );
+
+      if (nGroups == 6 && 50 == ge-gs+1) {
+            /*--- fast track the common case ---*/
+            UInt16 mtfv_i;
+            UChar* s_len_sel_selCtr 
+               = &(s->len[s->selector[selCtr]][0]);
+            Int32* s_code_sel_selCtr
+               = &(s->code[s->selector[selCtr]][0]);
+
+#           define BZ_ITAH(nn)                      \
+               mtfv_i = mtfv[gs+(nn)];              \
+               bsW ( s,                             \
+                     s_len_sel_selCtr[mtfv_i],      \
+                     s_code_sel_selCtr[mtfv_i] )
+
+            BZ_ITAH(0);  BZ_ITAH(1);  BZ_ITAH(2);  BZ_ITAH(3);  BZ_ITAH(4);
+            BZ_ITAH(5);  BZ_ITAH(6);  BZ_ITAH(7);  BZ_ITAH(8);  BZ_ITAH(9);
+            BZ_ITAH(10); BZ_ITAH(11); BZ_ITAH(12); BZ_ITAH(13); BZ_ITAH(14);
+            BZ_ITAH(15); BZ_ITAH(16); BZ_ITAH(17); BZ_ITAH(18); BZ_ITAH(19);
+            BZ_ITAH(20); BZ_ITAH(21); BZ_ITAH(22); BZ_ITAH(23); BZ_ITAH(24);
+            BZ_ITAH(25); BZ_ITAH(26); BZ_ITAH(27); BZ_ITAH(28); BZ_ITAH(29);
+            BZ_ITAH(30); BZ_ITAH(31); BZ_ITAH(32); BZ_ITAH(33); BZ_ITAH(34);
+            BZ_ITAH(35); BZ_ITAH(36); BZ_ITAH(37); BZ_ITAH(38); BZ_ITAH(39);
+            BZ_ITAH(40); BZ_ITAH(41); BZ_ITAH(42); BZ_ITAH(43); BZ_ITAH(44);
+            BZ_ITAH(45); BZ_ITAH(46); BZ_ITAH(47); BZ_ITAH(48); BZ_ITAH(49);
+
+#           undef BZ_ITAH
+
+      } else {
+	 /*--- slow version which correctly handles all situations ---*/
+         for (i = gs; i <= ge; i++) {
+            bsW ( s, 
+                  s->len  [s->selector[selCtr]] [mtfv[i]],
+                  s->code [s->selector[selCtr]] [mtfv[i]] );
+         }
+      }
+
+
+      gs = ge+1;
+      selCtr++;
+   }
+   AssertH( selCtr == nSelectors, 3007 );
+
+   if (s->verbosity >= 3)
+      VPrintf1( "codes %d\n", s->numZ-nBytes );
+}
+
+
+/*---------------------------------------------------*/
+void BZ2_compressBlock ( EState* s, Bool is_last_block )
+{
+   if (s->nblock > 0) {
+
+      BZ_FINALISE_CRC ( s->blockCRC );
+      s->combinedCRC = (s->combinedCRC << 1) | (s->combinedCRC >> 31);
+      s->combinedCRC ^= s->blockCRC;
+      if (s->blockNo > 1) s->numZ = 0;
+
+      if (s->verbosity >= 2)
+         VPrintf4( "    block %d: crc = 0x%08x, "
+                   "combined CRC = 0x%08x, size = %d\n",
+                   s->blockNo, s->blockCRC, s->combinedCRC, s->nblock );
+
+      BZ2_blockSort ( s );
+   }
+
+   s->zbits = (UChar*) (&((UChar*)s->arr2)[s->nblock]);
+
+   /*-- If this is the first block, create the stream header. --*/
+   if (s->blockNo == 1) {
+      BZ2_bsInitWrite ( s );
+      bsPutUChar ( s, BZ_HDR_B );
+      bsPutUChar ( s, BZ_HDR_Z );
+      bsPutUChar ( s, BZ_HDR_h );
+      bsPutUChar ( s, (UChar)(BZ_HDR_0 + s->blockSize100k) );
+   }
+
+   if (s->nblock > 0) {
+
+      bsPutUChar ( s, 0x31 ); bsPutUChar ( s, 0x41 );
+      bsPutUChar ( s, 0x59 ); bsPutUChar ( s, 0x26 );
+      bsPutUChar ( s, 0x53 ); bsPutUChar ( s, 0x59 );
+
+      /*-- Now the block's CRC, so it is in a known place. --*/
+      bsPutUInt32 ( s, s->blockCRC );
+
+      /*-- 
+         Now a single bit indicating (non-)randomisation. 
+         As of version 0.9.5, we use a better sorting algorithm
+         which makes randomisation unnecessary.  So always set
+         the randomised bit to 'no'.  Of course, the decoder
+         still needs to be able to handle randomised blocks
+         so as to maintain backwards compatibility with
+         older versions of bzip2.
+      --*/
+      bsW(s,1,0);
+
+      bsW ( s, 24, s->origPtr );
+      generateMTFValues ( s );
+      sendMTFValues ( s );
+   }
+
+
+   /*-- If this is the last block, add the stream trailer. --*/
+   if (is_last_block) {
+
+      bsPutUChar ( s, 0x17 ); bsPutUChar ( s, 0x72 );
+      bsPutUChar ( s, 0x45 ); bsPutUChar ( s, 0x38 );
+      bsPutUChar ( s, 0x50 ); bsPutUChar ( s, 0x90 );
+      bsPutUInt32 ( s, s->combinedCRC );
+      if (s->verbosity >= 2)
+         VPrintf1( "    final combined CRC = 0x%08x\n   ", s->combinedCRC );
+      bsFinishWrite ( s );
+   }
+}
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                        compress.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/crctable.c b/llvm/lib/Bitcode/MetalLib/bzip2/crctable.c
new file mode 100644
index 000000000000..2b33c2535338
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/crctable.c
@@ -0,0 +1,104 @@
+
+/*-------------------------------------------------------------*/
+/*--- Table for doing CRCs                                  ---*/
+/*---                                            crctable.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#include "bzlib_private.h"
+
+/*--
+  I think this is an implementation of the AUTODIN-II,
+  Ethernet & FDDI 32-bit CRC standard.  Vaguely derived
+  from code by Rob Warnock, in Section 51 of the
+  comp.compression FAQ.
+--*/
+
+UInt32 BZ2_crc32Table[256] = {
+
+   /*-- Ugly, innit? --*/
+
+   0x00000000L, 0x04c11db7L, 0x09823b6eL, 0x0d4326d9L,
+   0x130476dcL, 0x17c56b6bL, 0x1a864db2L, 0x1e475005L,
+   0x2608edb8L, 0x22c9f00fL, 0x2f8ad6d6L, 0x2b4bcb61L,
+   0x350c9b64L, 0x31cd86d3L, 0x3c8ea00aL, 0x384fbdbdL,
+   0x4c11db70L, 0x48d0c6c7L, 0x4593e01eL, 0x4152fda9L,
+   0x5f15adacL, 0x5bd4b01bL, 0x569796c2L, 0x52568b75L,
+   0x6a1936c8L, 0x6ed82b7fL, 0x639b0da6L, 0x675a1011L,
+   0x791d4014L, 0x7ddc5da3L, 0x709f7b7aL, 0x745e66cdL,
+   0x9823b6e0L, 0x9ce2ab57L, 0x91a18d8eL, 0x95609039L,
+   0x8b27c03cL, 0x8fe6dd8bL, 0x82a5fb52L, 0x8664e6e5L,
+   0xbe2b5b58L, 0xbaea46efL, 0xb7a96036L, 0xb3687d81L,
+   0xad2f2d84L, 0xa9ee3033L, 0xa4ad16eaL, 0xa06c0b5dL,
+   0xd4326d90L, 0xd0f37027L, 0xddb056feL, 0xd9714b49L,
+   0xc7361b4cL, 0xc3f706fbL, 0xceb42022L, 0xca753d95L,
+   0xf23a8028L, 0xf6fb9d9fL, 0xfbb8bb46L, 0xff79a6f1L,
+   0xe13ef6f4L, 0xe5ffeb43L, 0xe8bccd9aL, 0xec7dd02dL,
+   0x34867077L, 0x30476dc0L, 0x3d044b19L, 0x39c556aeL,
+   0x278206abL, 0x23431b1cL, 0x2e003dc5L, 0x2ac12072L,
+   0x128e9dcfL, 0x164f8078L, 0x1b0ca6a1L, 0x1fcdbb16L,
+   0x018aeb13L, 0x054bf6a4L, 0x0808d07dL, 0x0cc9cdcaL,
+   0x7897ab07L, 0x7c56b6b0L, 0x71159069L, 0x75d48ddeL,
+   0x6b93dddbL, 0x6f52c06cL, 0x6211e6b5L, 0x66d0fb02L,
+   0x5e9f46bfL, 0x5a5e5b08L, 0x571d7dd1L, 0x53dc6066L,
+   0x4d9b3063L, 0x495a2dd4L, 0x44190b0dL, 0x40d816baL,
+   0xaca5c697L, 0xa864db20L, 0xa527fdf9L, 0xa1e6e04eL,
+   0xbfa1b04bL, 0xbb60adfcL, 0xb6238b25L, 0xb2e29692L,
+   0x8aad2b2fL, 0x8e6c3698L, 0x832f1041L, 0x87ee0df6L,
+   0x99a95df3L, 0x9d684044L, 0x902b669dL, 0x94ea7b2aL,
+   0xe0b41de7L, 0xe4750050L, 0xe9362689L, 0xedf73b3eL,
+   0xf3b06b3bL, 0xf771768cL, 0xfa325055L, 0xfef34de2L,
+   0xc6bcf05fL, 0xc27dede8L, 0xcf3ecb31L, 0xcbffd686L,
+   0xd5b88683L, 0xd1799b34L, 0xdc3abdedL, 0xd8fba05aL,
+   0x690ce0eeL, 0x6dcdfd59L, 0x608edb80L, 0x644fc637L,
+   0x7a089632L, 0x7ec98b85L, 0x738aad5cL, 0x774bb0ebL,
+   0x4f040d56L, 0x4bc510e1L, 0x46863638L, 0x42472b8fL,
+   0x5c007b8aL, 0x58c1663dL, 0x558240e4L, 0x51435d53L,
+   0x251d3b9eL, 0x21dc2629L, 0x2c9f00f0L, 0x285e1d47L,
+   0x36194d42L, 0x32d850f5L, 0x3f9b762cL, 0x3b5a6b9bL,
+   0x0315d626L, 0x07d4cb91L, 0x0a97ed48L, 0x0e56f0ffL,
+   0x1011a0faL, 0x14d0bd4dL, 0x19939b94L, 0x1d528623L,
+   0xf12f560eL, 0xf5ee4bb9L, 0xf8ad6d60L, 0xfc6c70d7L,
+   0xe22b20d2L, 0xe6ea3d65L, 0xeba91bbcL, 0xef68060bL,
+   0xd727bbb6L, 0xd3e6a601L, 0xdea580d8L, 0xda649d6fL,
+   0xc423cd6aL, 0xc0e2d0ddL, 0xcda1f604L, 0xc960ebb3L,
+   0xbd3e8d7eL, 0xb9ff90c9L, 0xb4bcb610L, 0xb07daba7L,
+   0xae3afba2L, 0xaafbe615L, 0xa7b8c0ccL, 0xa379dd7bL,
+   0x9b3660c6L, 0x9ff77d71L, 0x92b45ba8L, 0x9675461fL,
+   0x8832161aL, 0x8cf30badL, 0x81b02d74L, 0x857130c3L,
+   0x5d8a9099L, 0x594b8d2eL, 0x5408abf7L, 0x50c9b640L,
+   0x4e8ee645L, 0x4a4ffbf2L, 0x470cdd2bL, 0x43cdc09cL,
+   0x7b827d21L, 0x7f436096L, 0x7200464fL, 0x76c15bf8L,
+   0x68860bfdL, 0x6c47164aL, 0x61043093L, 0x65c52d24L,
+   0x119b4be9L, 0x155a565eL, 0x18197087L, 0x1cd86d30L,
+   0x029f3d35L, 0x065e2082L, 0x0b1d065bL, 0x0fdc1becL,
+   0x3793a651L, 0x3352bbe6L, 0x3e119d3fL, 0x3ad08088L,
+   0x2497d08dL, 0x2056cd3aL, 0x2d15ebe3L, 0x29d4f654L,
+   0xc5a92679L, 0xc1683bceL, 0xcc2b1d17L, 0xc8ea00a0L,
+   0xd6ad50a5L, 0xd26c4d12L, 0xdf2f6bcbL, 0xdbee767cL,
+   0xe3a1cbc1L, 0xe760d676L, 0xea23f0afL, 0xeee2ed18L,
+   0xf0a5bd1dL, 0xf464a0aaL, 0xf9278673L, 0xfde69bc4L,
+   0x89b8fd09L, 0x8d79e0beL, 0x803ac667L, 0x84fbdbd0L,
+   0x9abc8bd5L, 0x9e7d9662L, 0x933eb0bbL, 0x97ffad0cL,
+   0xafb010b1L, 0xab710d06L, 0xa6322bdfL, 0xa2f33668L,
+   0xbcb4666dL, 0xb8757bdaL, 0xb5365d03L, 0xb1f740b4L
+};
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                        crctable.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/decompress.c b/llvm/lib/Bitcode/MetalLib/bzip2/decompress.c
new file mode 100644
index 000000000000..a1a0bac89227
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/decompress.c
@@ -0,0 +1,652 @@
+
+/*-------------------------------------------------------------*/
+/*--- Decompression machinery                               ---*/
+/*---                                          decompress.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#include "bzlib_private.h"
+
+
+/*---------------------------------------------------*/
+static
+void makeMaps_d ( DState* s )
+{
+   Int32 i;
+   s->nInUse = 0;
+   for (i = 0; i < 256; i++)
+      if (s->inUse[i]) {
+         s->seqToUnseq[s->nInUse] = i;
+         s->nInUse++;
+      }
+}
+
+
+/*---------------------------------------------------*/
+#define RETURN(rrr)                               \
+   { retVal = rrr; goto save_state_and_return; };
+
+#define GET_BITS(lll,vvv,nnn)                     \
+   case lll: s->state = lll;                      \
+   while (True) {                                 \
+      if (s->bsLive >= nnn) {                     \
+         UInt32 v;                                \
+         v = (s->bsBuff >>                        \
+             (s->bsLive-nnn)) & ((1 << nnn)-1);   \
+         s->bsLive -= nnn;                        \
+         vvv = v;                                 \
+         break;                                   \
+      }                                           \
+      if (s->strm->avail_in == 0) RETURN(BZ_OK);  \
+      s->bsBuff                                   \
+         = (s->bsBuff << 8) |                     \
+           ((UInt32)                              \
+              (*((UChar*)(s->strm->next_in))));   \
+      s->bsLive += 8;                             \
+      s->strm->next_in++;                         \
+      s->strm->avail_in--;                        \
+      s->strm->total_in_lo32++;                   \
+      if (s->strm->total_in_lo32 == 0)            \
+         s->strm->total_in_hi32++;                \
+   }
+
+#define GET_UCHAR(lll,uuu)                        \
+   GET_BITS(lll,uuu,8)
+
+#define GET_BIT(lll,uuu)                          \
+   GET_BITS(lll,uuu,1)
+
+/*---------------------------------------------------*/
+#define GET_MTF_VAL(label1,label2,lval)           \
+{                                                 \
+   if (groupPos == 0) {                           \
+      groupNo++;                                  \
+      if (groupNo >= nSelectors)                  \
+         RETURN(BZ_DATA_ERROR);                   \
+      groupPos = BZ_G_SIZE;                       \
+      gSel = s->selector[groupNo];                \
+      gMinlen = s->minLens[gSel];                 \
+      gLimit = &(s->limit[gSel][0]);              \
+      gPerm = &(s->perm[gSel][0]);                \
+      gBase = &(s->base[gSel][0]);                \
+   }                                              \
+   groupPos--;                                    \
+   zn = gMinlen;                                  \
+   GET_BITS(label1, zvec, zn);                    \
+   while (1) {                                    \
+      if (zn > 20 /* the longest code */)         \
+         RETURN(BZ_DATA_ERROR);                   \
+      if (zvec <= gLimit[zn]) break;              \
+      zn++;                                       \
+      GET_BIT(label2, zj);                        \
+      zvec = (zvec << 1) | zj;                    \
+   };                                             \
+   if (zvec - gBase[zn] < 0                       \
+       || zvec - gBase[zn] >= BZ_MAX_ALPHA_SIZE)  \
+      RETURN(BZ_DATA_ERROR);                      \
+   lval = gPerm[zvec - gBase[zn]];                \
+}
+
+
+/*---------------------------------------------------*/
+Int32 BZ2_decompress ( DState* s )
+{
+   UChar      uc;
+   Int32      retVal;
+   Int32      minLen, maxLen;
+   bz_stream* strm = s->strm;
+
+   /* stuff that needs to be saved/restored */
+   Int32  i;
+   Int32  j;
+   Int32  t;
+   Int32  alphaSize;
+   Int32  nGroups;
+   Int32  nSelectors;
+   Int32  EOB;
+   Int32  groupNo;
+   Int32  groupPos;
+   Int32  nextSym;
+   Int32  nblockMAX;
+   Int32  nblock;
+   Int32  es;
+   Int32  N;
+   Int32  curr;
+   Int32  zt;
+   Int32  zn; 
+   Int32  zvec;
+   Int32  zj;
+   Int32  gSel;
+   Int32  gMinlen;
+   Int32* gLimit;
+   Int32* gBase;
+   Int32* gPerm;
+
+   if (s->state == BZ_X_MAGIC_1) {
+      /*initialise the save area*/
+      s->save_i           = 0;
+      s->save_j           = 0;
+      s->save_t           = 0;
+      s->save_alphaSize   = 0;
+      s->save_nGroups     = 0;
+      s->save_nSelectors  = 0;
+      s->save_EOB         = 0;
+      s->save_groupNo     = 0;
+      s->save_groupPos    = 0;
+      s->save_nextSym     = 0;
+      s->save_nblockMAX   = 0;
+      s->save_nblock      = 0;
+      s->save_es          = 0;
+      s->save_N           = 0;
+      s->save_curr        = 0;
+      s->save_zt          = 0;
+      s->save_zn          = 0;
+      s->save_zvec        = 0;
+      s->save_zj          = 0;
+      s->save_gSel        = 0;
+      s->save_gMinlen     = 0;
+      s->save_gLimit      = NULL;
+      s->save_gBase       = NULL;
+      s->save_gPerm       = NULL;
+   }
+
+   /*restore from the save area*/
+   i           = s->save_i;
+   j           = s->save_j;
+   t           = s->save_t;
+   alphaSize   = s->save_alphaSize;
+   nGroups     = s->save_nGroups;
+   nSelectors  = s->save_nSelectors;
+   EOB         = s->save_EOB;
+   groupNo     = s->save_groupNo;
+   groupPos    = s->save_groupPos;
+   nextSym     = s->save_nextSym;
+   nblockMAX   = s->save_nblockMAX;
+   nblock      = s->save_nblock;
+   es          = s->save_es;
+   N           = s->save_N;
+   curr        = s->save_curr;
+   zt          = s->save_zt;
+   zn          = s->save_zn; 
+   zvec        = s->save_zvec;
+   zj          = s->save_zj;
+   gSel        = s->save_gSel;
+   gMinlen     = s->save_gMinlen;
+   gLimit      = s->save_gLimit;
+   gBase       = s->save_gBase;
+   gPerm       = s->save_gPerm;
+
+   retVal = BZ_OK;
+
+   switch (s->state) {
+
+      GET_UCHAR(BZ_X_MAGIC_1, uc);
+      if (uc != BZ_HDR_B) RETURN(BZ_DATA_ERROR_MAGIC);
+
+      GET_UCHAR(BZ_X_MAGIC_2, uc);
+      if (uc != BZ_HDR_Z) RETURN(BZ_DATA_ERROR_MAGIC);
+
+      GET_UCHAR(BZ_X_MAGIC_3, uc)
+      if (uc != BZ_HDR_h) RETURN(BZ_DATA_ERROR_MAGIC);
+
+      GET_BITS(BZ_X_MAGIC_4, s->blockSize100k, 8)
+      if (s->blockSize100k < (BZ_HDR_0 + 1) || 
+          s->blockSize100k > (BZ_HDR_0 + 9)) RETURN(BZ_DATA_ERROR_MAGIC);
+      s->blockSize100k -= BZ_HDR_0;
+
+      if (s->smallDecompress) {
+         s->ll16 = BZALLOC( s->blockSize100k * 100000 * sizeof(UInt16) );
+         s->ll4  = BZALLOC( 
+                      ((1 + s->blockSize100k * 100000) >> 1) * sizeof(UChar) 
+                   );
+         if (s->ll16 == NULL || s->ll4 == NULL) RETURN(BZ_MEM_ERROR);
+      } else {
+         s->tt  = BZALLOC( s->blockSize100k * 100000 * sizeof(Int32) );
+         if (s->tt == NULL) RETURN(BZ_MEM_ERROR);
+      }
+
+      GET_UCHAR(BZ_X_BLKHDR_1, uc);
+
+      if (uc == 0x17) goto endhdr_2;
+      if (uc != 0x31) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_BLKHDR_2, uc);
+      if (uc != 0x41) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_BLKHDR_3, uc);
+      if (uc != 0x59) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_BLKHDR_4, uc);
+      if (uc != 0x26) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_BLKHDR_5, uc);
+      if (uc != 0x53) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_BLKHDR_6, uc);
+      if (uc != 0x59) RETURN(BZ_DATA_ERROR);
+
+      s->currBlockNo++;
+      if (s->verbosity >= 2)
+         VPrintf1 ( "\n    [%d: huff+mtf ", s->currBlockNo );
+ 
+      s->storedBlockCRC = 0;
+      GET_UCHAR(BZ_X_BCRC_1, uc);
+      s->storedBlockCRC = (s->storedBlockCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_BCRC_2, uc);
+      s->storedBlockCRC = (s->storedBlockCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_BCRC_3, uc);
+      s->storedBlockCRC = (s->storedBlockCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_BCRC_4, uc);
+      s->storedBlockCRC = (s->storedBlockCRC << 8) | ((UInt32)uc);
+
+      GET_BITS(BZ_X_RANDBIT, s->blockRandomised, 1);
+
+      s->origPtr = 0;
+      GET_UCHAR(BZ_X_ORIGPTR_1, uc);
+      s->origPtr = (s->origPtr << 8) | ((Int32)uc);
+      GET_UCHAR(BZ_X_ORIGPTR_2, uc);
+      s->origPtr = (s->origPtr << 8) | ((Int32)uc);
+      GET_UCHAR(BZ_X_ORIGPTR_3, uc);
+      s->origPtr = (s->origPtr << 8) | ((Int32)uc);
+
+      if (s->origPtr < 0)
+         RETURN(BZ_DATA_ERROR);
+      if (s->origPtr > 10 + 100000*s->blockSize100k) 
+         RETURN(BZ_DATA_ERROR);
+
+      /*--- Receive the mapping table ---*/
+      for (i = 0; i < 16; i++) {
+         GET_BIT(BZ_X_MAPPING_1, uc);
+         if (uc == 1) 
+            s->inUse16[i] = True; else 
+            s->inUse16[i] = False;
+      }
+
+      for (i = 0; i < 256; i++) s->inUse[i] = False;
+
+      for (i = 0; i < 16; i++)
+         if (s->inUse16[i])
+            for (j = 0; j < 16; j++) {
+               GET_BIT(BZ_X_MAPPING_2, uc);
+               if (uc == 1) s->inUse[i * 16 + j] = True;
+            }
+      makeMaps_d ( s );
+      if (s->nInUse == 0) RETURN(BZ_DATA_ERROR);
+      alphaSize = s->nInUse+2;
+
+      /*--- Now the selectors ---*/
+      GET_BITS(BZ_X_SELECTOR_1, nGroups, 3);
+      if (nGroups < 2 || nGroups > BZ_N_GROUPS) RETURN(BZ_DATA_ERROR);
+      GET_BITS(BZ_X_SELECTOR_2, nSelectors, 15);
+      if (nSelectors < 1) RETURN(BZ_DATA_ERROR);
+      for (i = 0; i < nSelectors; i++) {
+         j = 0;
+         while (True) {
+            GET_BIT(BZ_X_SELECTOR_3, uc);
+            if (uc == 0) break;
+            j++;
+            if (j >= nGroups) RETURN(BZ_DATA_ERROR);
+         }
+         /* Having more than BZ_MAX_SELECTORS doesn't make much sense
+            since they will never be used, but some implementations might
+            "round up" the number of selectors, so just ignore those. */
+         if (i < BZ_MAX_SELECTORS)
+           s->selectorMtf[i] = j;
+      }
+      if (nSelectors > BZ_MAX_SELECTORS)
+        nSelectors = BZ_MAX_SELECTORS;
+
+      /*--- Undo the MTF values for the selectors. ---*/
+      {
+         UChar pos[BZ_N_GROUPS], tmp, v;
+         for (v = 0; v < nGroups; v++) pos[v] = v;
+   
+         for (i = 0; i < nSelectors; i++) {
+            v = s->selectorMtf[i];
+            tmp = pos[v];
+            while (v > 0) { pos[v] = pos[v-1]; v--; }
+            pos[0] = tmp;
+            s->selector[i] = tmp;
+         }
+      }
+
+      /*--- Now the coding tables ---*/
+      for (t = 0; t < nGroups; t++) {
+         GET_BITS(BZ_X_CODING_1, curr, 5);
+         for (i = 0; i < alphaSize; i++) {
+            while (True) {
+               if (curr < 1 || curr > 20) RETURN(BZ_DATA_ERROR);
+               GET_BIT(BZ_X_CODING_2, uc);
+               if (uc == 0) break;
+               GET_BIT(BZ_X_CODING_3, uc);
+               if (uc == 0) curr++; else curr--;
+            }
+            s->len[t][i] = curr;
+         }
+      }
+
+      /*--- Create the Huffman decoding tables ---*/
+      for (t = 0; t < nGroups; t++) {
+         minLen = 32;
+         maxLen = 0;
+         for (i = 0; i < alphaSize; i++) {
+            if (s->len[t][i] > maxLen) maxLen = s->len[t][i];
+            if (s->len[t][i] < minLen) minLen = s->len[t][i];
+         }
+         BZ2_hbCreateDecodeTables ( 
+            &(s->limit[t][0]), 
+            &(s->base[t][0]), 
+            &(s->perm[t][0]), 
+            &(s->len[t][0]),
+            minLen, maxLen, alphaSize
+         );
+         s->minLens[t] = minLen;
+      }
+
+      /*--- Now the MTF values ---*/
+
+      EOB      = s->nInUse+1;
+      nblockMAX = 100000 * s->blockSize100k;
+      groupNo  = -1;
+      groupPos = 0;
+
+      for (i = 0; i <= 255; i++) s->unzftab[i] = 0;
+
+      /*-- MTF init --*/
+      {
+         Int32 ii, jj, kk;
+         kk = MTFA_SIZE-1;
+         for (ii = 256 / MTFL_SIZE - 1; ii >= 0; ii--) {
+            for (jj = MTFL_SIZE-1; jj >= 0; jj--) {
+               s->mtfa[kk] = (UChar)(ii * MTFL_SIZE + jj);
+               kk--;
+            }
+            s->mtfbase[ii] = kk + 1;
+         }
+      }
+      /*-- end MTF init --*/
+
+      nblock = 0;
+      GET_MTF_VAL(BZ_X_MTF_1, BZ_X_MTF_2, nextSym);
+
+      while (True) {
+
+         if (nextSym == EOB) break;
+
+         if (nextSym == BZ_RUNA || nextSym == BZ_RUNB) {
+
+            es = -1;
+            N = 1;
+            do {
+               /* Check that N doesn't get too big, so that es doesn't
+                  go negative.  The maximum value that can be
+                  RUNA/RUNB encoded is equal to the block size (post
+                  the initial RLE), viz, 900k, so bounding N at 2
+                  million should guard against overflow without
+                  rejecting any legitimate inputs. */
+               if (N >= 2*1024*1024) RETURN(BZ_DATA_ERROR);
+               if (nextSym == BZ_RUNA) es = es + (0+1) * N; else
+               if (nextSym == BZ_RUNB) es = es + (1+1) * N;
+               N = N * 2;
+               GET_MTF_VAL(BZ_X_MTF_3, BZ_X_MTF_4, nextSym);
+            }
+               while (nextSym == BZ_RUNA || nextSym == BZ_RUNB);
+
+            es++;
+            uc = s->seqToUnseq[ s->mtfa[s->mtfbase[0]] ];
+            s->unzftab[uc] += es;
+
+            if (s->smallDecompress)
+               while (es > 0) {
+                  if (nblock >= nblockMAX) RETURN(BZ_DATA_ERROR);
+                  s->ll16[nblock] = (UInt16)uc;
+                  nblock++;
+                  es--;
+               }
+            else
+               while (es > 0) {
+                  if (nblock >= nblockMAX) RETURN(BZ_DATA_ERROR);
+                  s->tt[nblock] = (UInt32)uc;
+                  nblock++;
+                  es--;
+               };
+
+            continue;
+
+         } else {
+
+            if (nblock >= nblockMAX) RETURN(BZ_DATA_ERROR);
+
+            /*-- uc = MTF ( nextSym-1 ) --*/
+            {
+               Int32 ii, jj, kk, pp, lno, off;
+               UInt32 nn;
+               nn = (UInt32)(nextSym - 1);
+
+               if (nn < MTFL_SIZE) {
+                  /* avoid general-case expense */
+                  pp = s->mtfbase[0];
+                  uc = s->mtfa[pp+nn];
+                  while (nn > 3) {
+                     Int32 z = pp+nn;
+                     s->mtfa[(z)  ] = s->mtfa[(z)-1];
+                     s->mtfa[(z)-1] = s->mtfa[(z)-2];
+                     s->mtfa[(z)-2] = s->mtfa[(z)-3];
+                     s->mtfa[(z)-3] = s->mtfa[(z)-4];
+                     nn -= 4;
+                  }
+                  while (nn > 0) { 
+                     s->mtfa[(pp+nn)] = s->mtfa[(pp+nn)-1]; nn--; 
+                  };
+                  s->mtfa[pp] = uc;
+               } else { 
+                  /* general case */
+                  lno = nn / MTFL_SIZE;
+                  off = nn % MTFL_SIZE;
+                  pp = s->mtfbase[lno] + off;
+                  uc = s->mtfa[pp];
+                  while (pp > s->mtfbase[lno]) { 
+                     s->mtfa[pp] = s->mtfa[pp-1]; pp--; 
+                  };
+                  s->mtfbase[lno]++;
+                  while (lno > 0) {
+                     s->mtfbase[lno]--;
+                     s->mtfa[s->mtfbase[lno]] 
+                        = s->mtfa[s->mtfbase[lno-1] + MTFL_SIZE - 1];
+                     lno--;
+                  }
+                  s->mtfbase[0]--;
+                  s->mtfa[s->mtfbase[0]] = uc;
+                  if (s->mtfbase[0] == 0) {
+                     kk = MTFA_SIZE-1;
+                     for (ii = 256 / MTFL_SIZE-1; ii >= 0; ii--) {
+                        for (jj = MTFL_SIZE-1; jj >= 0; jj--) {
+                           s->mtfa[kk] = s->mtfa[s->mtfbase[ii] + jj];
+                           kk--;
+                        }
+                        s->mtfbase[ii] = kk + 1;
+                     }
+                  }
+               }
+            }
+            /*-- end uc = MTF ( nextSym-1 ) --*/
+
+            s->unzftab[s->seqToUnseq[uc]]++;
+            if (s->smallDecompress)
+               s->ll16[nblock] = (UInt16)(s->seqToUnseq[uc]); else
+               s->tt[nblock]   = (UInt32)(s->seqToUnseq[uc]);
+            nblock++;
+
+            GET_MTF_VAL(BZ_X_MTF_5, BZ_X_MTF_6, nextSym);
+            continue;
+         }
+      }
+
+      /* Now we know what nblock is, we can do a better sanity
+         check on s->origPtr.
+      */
+      if (s->origPtr < 0 || s->origPtr >= nblock)
+         RETURN(BZ_DATA_ERROR);
+
+      /*-- Set up cftab to facilitate generation of T^(-1) --*/
+      /* Check: unzftab entries in range. */
+      for (i = 0; i <= 255; i++) {
+         if (s->unzftab[i] < 0 || s->unzftab[i] > nblock)
+            RETURN(BZ_DATA_ERROR);
+      }
+      /* Actually generate cftab. */
+      s->cftab[0] = 0;
+      for (i = 1; i <= 256; i++) s->cftab[i] = s->unzftab[i-1];
+      for (i = 1; i <= 256; i++) s->cftab[i] += s->cftab[i-1];
+      /* Check: cftab entries in range. */
+      for (i = 0; i <= 256; i++) {
+         if (s->cftab[i] < 0 || s->cftab[i] > nblock) {
+            /* s->cftab[i] can legitimately be == nblock */
+            RETURN(BZ_DATA_ERROR);
+         }
+      }
+      /* Check: cftab entries non-descending. */
+      for (i = 1; i <= 256; i++) {
+         if (s->cftab[i-1] > s->cftab[i]) {
+            RETURN(BZ_DATA_ERROR);
+         }
+      }
+
+      s->state_out_len = 0;
+      s->state_out_ch  = 0;
+      BZ_INITIALISE_CRC ( s->calculatedBlockCRC );
+      s->state = BZ_X_OUTPUT;
+      if (s->verbosity >= 2) VPrintf0 ( "rt+rld" );
+
+      if (s->smallDecompress) {
+
+         /*-- Make a copy of cftab, used in generation of T --*/
+         for (i = 0; i <= 256; i++) s->cftabCopy[i] = s->cftab[i];
+
+         /*-- compute the T vector --*/
+         for (i = 0; i < nblock; i++) {
+            uc = (UChar)(s->ll16[i]);
+            SET_LL(i, s->cftabCopy[uc]);
+            s->cftabCopy[uc]++;
+         }
+
+         /*-- Compute T^(-1) by pointer reversal on T --*/
+         i = s->origPtr;
+         j = GET_LL(i);
+         do {
+            Int32 tmp = GET_LL(j);
+            SET_LL(j, i);
+            i = j;
+            j = tmp;
+         }
+            while (i != s->origPtr);
+
+         s->tPos = s->origPtr;
+         s->nblock_used = 0;
+         if (s->blockRandomised) {
+            BZ_RAND_INIT_MASK;
+            BZ_GET_SMALL(s->k0); s->nblock_used++;
+            BZ_RAND_UPD_MASK; s->k0 ^= BZ_RAND_MASK; 
+         } else {
+            BZ_GET_SMALL(s->k0); s->nblock_used++;
+         }
+
+      } else {
+
+         /*-- compute the T^(-1) vector --*/
+         for (i = 0; i < nblock; i++) {
+            uc = (UChar)(s->tt[i] & 0xff);
+            s->tt[s->cftab[uc]] |= (i << 8);
+            s->cftab[uc]++;
+         }
+
+         s->tPos = s->tt[s->origPtr] >> 8;
+         s->nblock_used = 0;
+         if (s->blockRandomised) {
+            BZ_RAND_INIT_MASK;
+            BZ_GET_FAST(s->k0); s->nblock_used++;
+            BZ_RAND_UPD_MASK; s->k0 ^= BZ_RAND_MASK; 
+         } else {
+            BZ_GET_FAST(s->k0); s->nblock_used++;
+         }
+
+      }
+
+      RETURN(BZ_OK);
+
+
+
+    endhdr_2:
+
+      GET_UCHAR(BZ_X_ENDHDR_2, uc);
+      if (uc != 0x72) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_ENDHDR_3, uc);
+      if (uc != 0x45) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_ENDHDR_4, uc);
+      if (uc != 0x38) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_ENDHDR_5, uc);
+      if (uc != 0x50) RETURN(BZ_DATA_ERROR);
+      GET_UCHAR(BZ_X_ENDHDR_6, uc);
+      if (uc != 0x90) RETURN(BZ_DATA_ERROR);
+
+      s->storedCombinedCRC = 0;
+      GET_UCHAR(BZ_X_CCRC_1, uc);
+      s->storedCombinedCRC = (s->storedCombinedCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_CCRC_2, uc);
+      s->storedCombinedCRC = (s->storedCombinedCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_CCRC_3, uc);
+      s->storedCombinedCRC = (s->storedCombinedCRC << 8) | ((UInt32)uc);
+      GET_UCHAR(BZ_X_CCRC_4, uc);
+      s->storedCombinedCRC = (s->storedCombinedCRC << 8) | ((UInt32)uc);
+
+      s->state = BZ_X_IDLE;
+      RETURN(BZ_STREAM_END);
+
+      default: AssertH ( False, 4001 );
+   }
+
+   AssertH ( False, 4002 );
+
+   save_state_and_return:
+
+   s->save_i           = i;
+   s->save_j           = j;
+   s->save_t           = t;
+   s->save_alphaSize   = alphaSize;
+   s->save_nGroups     = nGroups;
+   s->save_nSelectors  = nSelectors;
+   s->save_EOB         = EOB;
+   s->save_groupNo     = groupNo;
+   s->save_groupPos    = groupPos;
+   s->save_nextSym     = nextSym;
+   s->save_nblockMAX   = nblockMAX;
+   s->save_nblock      = nblock;
+   s->save_es          = es;
+   s->save_N           = N;
+   s->save_curr        = curr;
+   s->save_zt          = zt;
+   s->save_zn          = zn;
+   s->save_zvec        = zvec;
+   s->save_zj          = zj;
+   s->save_gSel        = gSel;
+   s->save_gMinlen     = gMinlen;
+   s->save_gLimit      = gLimit;
+   s->save_gBase       = gBase;
+   s->save_gPerm       = gPerm;
+
+   return retVal;   
+}
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                      decompress.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/huffman.c b/llvm/lib/Bitcode/MetalLib/bzip2/huffman.c
new file mode 100644
index 000000000000..43a1899e4688
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/huffman.c
@@ -0,0 +1,205 @@
+
+/*-------------------------------------------------------------*/
+/*--- Huffman coding low-level stuff                        ---*/
+/*---                                             huffman.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#include "bzlib_private.h"
+
+/*---------------------------------------------------*/
+#define WEIGHTOF(zz0)  ((zz0) & 0xffffff00)
+#define DEPTHOF(zz1)   ((zz1) & 0x000000ff)
+#define MYMAX(zz2,zz3) ((zz2) > (zz3) ? (zz2) : (zz3))
+
+#define ADDWEIGHTS(zw1,zw2)                           \
+   (WEIGHTOF(zw1)+WEIGHTOF(zw2)) |                    \
+   (1 + MYMAX(DEPTHOF(zw1),DEPTHOF(zw2)))
+
+#define UPHEAP(z)                                     \
+{                                                     \
+   Int32 zz, tmp;                                     \
+   zz = z; tmp = heap[zz];                            \
+   while (weight[tmp] < weight[heap[zz >> 1]]) {      \
+      heap[zz] = heap[zz >> 1];                       \
+      zz >>= 1;                                       \
+   }                                                  \
+   heap[zz] = tmp;                                    \
+}
+
+#define DOWNHEAP(z)                                   \
+{                                                     \
+   Int32 zz, yy, tmp;                                 \
+   zz = z; tmp = heap[zz];                            \
+   while (True) {                                     \
+      yy = zz << 1;                                   \
+      if (yy > nHeap) break;                          \
+      if (yy < nHeap &&                               \
+          weight[heap[yy+1]] < weight[heap[yy]])      \
+         yy++;                                        \
+      if (weight[tmp] < weight[heap[yy]]) break;      \
+      heap[zz] = heap[yy];                            \
+      zz = yy;                                        \
+   }                                                  \
+   heap[zz] = tmp;                                    \
+}
+
+
+/*---------------------------------------------------*/
+void BZ2_hbMakeCodeLengths ( UChar *len, 
+                             Int32 *freq,
+                             Int32 alphaSize,
+                             Int32 maxLen )
+{
+   /*--
+      Nodes and heap entries run from 1.  Entry 0
+      for both the heap and nodes is a sentinel.
+   --*/
+   Int32 nNodes, nHeap, n1, n2, i, j, k;
+   Bool  tooLong;
+
+   Int32 heap   [ BZ_MAX_ALPHA_SIZE + 2 ];
+   Int32 weight [ BZ_MAX_ALPHA_SIZE * 2 ];
+   Int32 parent [ BZ_MAX_ALPHA_SIZE * 2 ]; 
+
+   for (i = 0; i < alphaSize; i++)
+      weight[i+1] = (freq[i] == 0 ? 1 : freq[i]) << 8;
+
+   while (True) {
+
+      nNodes = alphaSize;
+      nHeap = 0;
+
+      heap[0] = 0;
+      weight[0] = 0;
+      parent[0] = -2;
+
+      for (i = 1; i <= alphaSize; i++) {
+         parent[i] = -1;
+         nHeap++;
+         heap[nHeap] = i;
+         UPHEAP(nHeap);
+      }
+
+      AssertH( nHeap < (BZ_MAX_ALPHA_SIZE+2), 2001 );
+   
+      while (nHeap > 1) {
+         n1 = heap[1]; heap[1] = heap[nHeap]; nHeap--; DOWNHEAP(1);
+         n2 = heap[1]; heap[1] = heap[nHeap]; nHeap--; DOWNHEAP(1);
+         nNodes++;
+         parent[n1] = parent[n2] = nNodes;
+         weight[nNodes] = ADDWEIGHTS(weight[n1], weight[n2]);
+         parent[nNodes] = -1;
+         nHeap++;
+         heap[nHeap] = nNodes;
+         UPHEAP(nHeap);
+      }
+
+      AssertH( nNodes < (BZ_MAX_ALPHA_SIZE * 2), 2002 );
+
+      tooLong = False;
+      for (i = 1; i <= alphaSize; i++) {
+         j = 0;
+         k = i;
+         while (parent[k] >= 0) { k = parent[k]; j++; }
+         len[i-1] = j;
+         if (j > maxLen) tooLong = True;
+      }
+      
+      if (! tooLong) break;
+
+      /* 17 Oct 04: keep-going condition for the following loop used
+         to be 'i < alphaSize', which missed the last element,
+         theoretically leading to the possibility of the compressor
+         looping.  However, this count-scaling step is only needed if
+         one of the generated Huffman code words is longer than
+         maxLen, which up to and including version 1.0.2 was 20 bits,
+         which is extremely unlikely.  In version 1.0.3 maxLen was
+         changed to 17 bits, which has minimal effect on compression
+         ratio, but does mean this scaling step is used from time to
+         time, enough to verify that it works.
+
+         This means that bzip2-1.0.3 and later will only produce
+         Huffman codes with a maximum length of 17 bits.  However, in
+         order to preserve backwards compatibility with bitstreams
+         produced by versions pre-1.0.3, the decompressor must still
+         handle lengths of up to 20. */
+
+      for (i = 1; i <= alphaSize; i++) {
+         j = weight[i] >> 8;
+         j = 1 + (j / 2);
+         weight[i] = j << 8;
+      }
+   }
+}
+
+
+/*---------------------------------------------------*/
+void BZ2_hbAssignCodes ( Int32 *code,
+                         UChar *length,
+                         Int32 minLen,
+                         Int32 maxLen,
+                         Int32 alphaSize )
+{
+   Int32 n, vec, i;
+
+   vec = 0;
+   for (n = minLen; n <= maxLen; n++) {
+      for (i = 0; i < alphaSize; i++)
+         if (length[i] == n) { code[i] = vec; vec++; };
+      vec <<= 1;
+   }
+}
+
+
+/*---------------------------------------------------*/
+void BZ2_hbCreateDecodeTables ( Int32 *limit,
+                                Int32 *base,
+                                Int32 *perm,
+                                UChar *length,
+                                Int32 minLen,
+                                Int32 maxLen,
+                                Int32 alphaSize )
+{
+   Int32 pp, i, j, vec;
+
+   pp = 0;
+   for (i = minLen; i <= maxLen; i++)
+      for (j = 0; j < alphaSize; j++)
+         if (length[j] == i) { perm[pp] = j; pp++; };
+
+   for (i = 0; i < BZ_MAX_CODE_LEN; i++) base[i] = 0;
+   for (i = 0; i < alphaSize; i++) base[length[i]+1]++;
+
+   for (i = 1; i < BZ_MAX_CODE_LEN; i++) base[i] += base[i-1];
+
+   for (i = 0; i < BZ_MAX_CODE_LEN; i++) limit[i] = 0;
+   vec = 0;
+
+   for (i = minLen; i <= maxLen; i++) {
+      vec += (base[i+1] - base[i]);
+      limit[i] = vec-1;
+      vec <<= 1;
+   }
+   for (i = minLen + 1; i <= maxLen; i++)
+      base[i] = ((limit[i-1] + 1) << 1) - base[i];
+}
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                         huffman.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/bzip2/randtable.c b/llvm/lib/Bitcode/MetalLib/bzip2/randtable.c
new file mode 100644
index 000000000000..bdc6d4a4cc9a
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/bzip2/randtable.c
@@ -0,0 +1,84 @@
+
+/*-------------------------------------------------------------*/
+/*--- Table for randomising repetitive blocks               ---*/
+/*---                                           randtable.c ---*/
+/*-------------------------------------------------------------*/
+
+/* ------------------------------------------------------------------
+   This file is part of bzip2/libbzip2, a program and library for
+   lossless, block-sorting data compression.
+
+   bzip2/libbzip2 version 1.0.8 of 13 July 2019
+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>
+
+   Please read the WARNING, DISCLAIMER and PATENTS sections in the 
+   README file.
+
+   This program is released under the terms of the license contained
+   in the file LICENSE.
+   ------------------------------------------------------------------ */
+
+
+#include "bzlib_private.h"
+
+
+/*---------------------------------------------*/
+Int32 BZ2_rNums[512] = { 
+   619, 720, 127, 481, 931, 816, 813, 233, 566, 247, 
+   985, 724, 205, 454, 863, 491, 741, 242, 949, 214, 
+   733, 859, 335, 708, 621, 574, 73, 654, 730, 472, 
+   419, 436, 278, 496, 867, 210, 399, 680, 480, 51, 
+   878, 465, 811, 169, 869, 675, 611, 697, 867, 561, 
+   862, 687, 507, 283, 482, 129, 807, 591, 733, 623, 
+   150, 238, 59, 379, 684, 877, 625, 169, 643, 105, 
+   170, 607, 520, 932, 727, 476, 693, 425, 174, 647, 
+   73, 122, 335, 530, 442, 853, 695, 249, 445, 515, 
+   909, 545, 703, 919, 874, 474, 882, 500, 594, 612, 
+   641, 801, 220, 162, 819, 984, 589, 513, 495, 799, 
+   161, 604, 958, 533, 221, 400, 386, 867, 600, 782, 
+   382, 596, 414, 171, 516, 375, 682, 485, 911, 276, 
+   98, 553, 163, 354, 666, 933, 424, 341, 533, 870, 
+   227, 730, 475, 186, 263, 647, 537, 686, 600, 224, 
+   469, 68, 770, 919, 190, 373, 294, 822, 808, 206, 
+   184, 943, 795, 384, 383, 461, 404, 758, 839, 887, 
+   715, 67, 618, 276, 204, 918, 873, 777, 604, 560, 
+   951, 160, 578, 722, 79, 804, 96, 409, 713, 940, 
+   652, 934, 970, 447, 318, 353, 859, 672, 112, 785, 
+   645, 863, 803, 350, 139, 93, 354, 99, 820, 908, 
+   609, 772, 154, 274, 580, 184, 79, 626, 630, 742, 
+   653, 282, 762, 623, 680, 81, 927, 626, 789, 125, 
+   411, 521, 938, 300, 821, 78, 343, 175, 128, 250, 
+   170, 774, 972, 275, 999, 639, 495, 78, 352, 126, 
+   857, 956, 358, 619, 580, 124, 737, 594, 701, 612, 
+   669, 112, 134, 694, 363, 992, 809, 743, 168, 974, 
+   944, 375, 748, 52, 600, 747, 642, 182, 862, 81, 
+   344, 805, 988, 739, 511, 655, 814, 334, 249, 515, 
+   897, 955, 664, 981, 649, 113, 974, 459, 893, 228, 
+   433, 837, 553, 268, 926, 240, 102, 654, 459, 51, 
+   686, 754, 806, 760, 493, 403, 415, 394, 687, 700, 
+   946, 670, 656, 610, 738, 392, 760, 799, 887, 653, 
+   978, 321, 576, 617, 626, 502, 894, 679, 243, 440, 
+   680, 879, 194, 572, 640, 724, 926, 56, 204, 700, 
+   707, 151, 457, 449, 797, 195, 791, 558, 945, 679, 
+   297, 59, 87, 824, 713, 663, 412, 693, 342, 606, 
+   134, 108, 571, 364, 631, 212, 174, 643, 304, 329, 
+   343, 97, 430, 751, 497, 314, 983, 374, 822, 928, 
+   140, 206, 73, 263, 980, 736, 876, 478, 430, 305, 
+   170, 514, 364, 692, 829, 82, 855, 953, 676, 246, 
+   369, 970, 294, 750, 807, 827, 150, 790, 288, 923, 
+   804, 378, 215, 828, 592, 281, 565, 555, 710, 82, 
+   896, 831, 547, 261, 524, 462, 293, 465, 502, 56, 
+   661, 821, 976, 991, 658, 869, 905, 758, 745, 193, 
+   768, 550, 608, 933, 378, 286, 215, 979, 792, 961, 
+   61, 688, 793, 644, 986, 403, 106, 366, 905, 644, 
+   372, 567, 466, 434, 645, 210, 389, 550, 919, 135, 
+   780, 773, 635, 389, 707, 100, 626, 958, 165, 504, 
+   920, 176, 193, 713, 857, 265, 203, 50, 668, 108, 
+   645, 990, 626, 197, 510, 357, 358, 850, 858, 364, 
+   936, 638
+};
+
+
+/*-------------------------------------------------------------*/
+/*--- end                                       randtable.c ---*/
+/*-------------------------------------------------------------*/
diff --git a/llvm/lib/Bitcode/MetalLib/sha256.hpp b/llvm/lib/Bitcode/MetalLib/sha256.hpp
new file mode 100644
index 000000000000..286db0315734
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/sha256.hpp
@@ -0,0 +1,194 @@
+/*********************************************************************
+* Authors:    Brad Conte (original code, brad AT bradconte.com)
+*             Florian Ziesche (C++, LLVM/MetalLib integration)
+* Copyright:
+* Disclaimer: This code is presented "as is" without any guarantees.
+* Details:    Implementation of the SHA-256 hashing algorithm.
+              SHA-256 is one of the three algorithms in the SHA2
+              specification. The others, SHA-384 and SHA-512, are not
+              offered in this implementation.
+              Algorithm specification can be found here:
+               *
+http://csrc.nist.gov/publications/fips/fips180-2/fips180-2withchangenotice.pdf
+              This implementation uses little endian byte order.
+ * Original:  https://github.com/B-Con/crypto-algorithms
+*********************************************************************/
+
+#ifndef __METALLIB_SHA256_HPP__
+#define __METALLIB_SHA256_HPP__
+
+/*************************** HEADER FILES ***************************/
+#include <stddef.h>
+#include <stdlib.h>
+#include <memory.h>
+
+/****************************** MACROS ******************************/
+#define SHA256_BLOCK_SIZE 32 // SHA256 outputs a 32 byte digest
+
+/**************************** DATA TYPES ****************************/
+
+struct sha256_hash {
+  uint8_t hash[SHA256_BLOCK_SIZE]{
+      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+  };
+};
+static_assert(sizeof(sha256_hash) == SHA256_BLOCK_SIZE, "invalid hash size");
+
+/****************************** MACROS ******************************/
+#define SHA256_ROTLEFT(a, b) (((a) << (b)) | ((a) >> (32 - (b))))
+#define SHA256_ROTRIGHT(a, b) (((a) >> (b)) | ((a) << (32 - (b))))
+
+#define SHA256_CH(x, y, z) (((x) & (y)) ^ (~(x) & (z)))
+#define SHA256_MAJ(x, y, z) (((x) & (y)) ^ ((x) & (z)) ^ ((y) & (z)))
+#define SHA256_EP0(x)                                                          \
+  (SHA256_ROTRIGHT(x, 2) ^ SHA256_ROTRIGHT(x, 13) ^ SHA256_ROTRIGHT(x, 22))
+#define SHA256_EP1(x)                                                          \
+  (SHA256_ROTRIGHT(x, 6) ^ SHA256_ROTRIGHT(x, 11) ^ SHA256_ROTRIGHT(x, 25))
+#define SHA256_SIG0(x)                                                         \
+  (SHA256_ROTRIGHT(x, 7) ^ SHA256_ROTRIGHT(x, 18) ^ ((x) >> 3))
+#define SHA256_SIG1(x)                                                         \
+  (SHA256_ROTRIGHT(x, 17) ^ SHA256_ROTRIGHT(x, 19) ^ ((x) >> 10))
+
+/**************************** VARIABLES *****************************/
+static constexpr const uint32_t k[64]{
+    0x428A2F98, 0x71374491, 0xB5C0FBCF, 0xE9B5DBA5, 0x3956C25B, 0x59F111F1,
+    0x923F82A4, 0xAB1C5ED5, 0xD807AA98, 0x12835B01, 0x243185BE, 0x550C7DC3,
+    0x72BE5D74, 0x80DEB1FE, 0x9BDC06A7, 0xC19BF174, 0xE49B69C1, 0xEFBE4786,
+    0x0FC19DC6, 0x240CA1CC, 0x2DE92C6F, 0x4A7484AA, 0x5CB0A9DC, 0x76F988DA,
+    0x983E5152, 0xA831C66D, 0xB00327C8, 0xBF597FC7, 0xC6E00BF3, 0xD5A79147,
+    0x06CA6351, 0x14292967, 0x27B70A85, 0x2E1B2138, 0x4D2C6DFC, 0x53380D13,
+    0x650A7354, 0x766A0ABB, 0x81C2C92E, 0x92722C85, 0xA2BFE8A1, 0xA81A664B,
+    0xC24B8B70, 0xC76C51A3, 0xD192E819, 0xD6990624, 0xF40E3585, 0x106AA070,
+    0x19A4C116, 0x1E376C08, 0x2748774C, 0x34B0BCB5, 0x391C0CB3, 0x4ED8AA4A,
+    0x5B9CCA4F, 0x682E6FF3, 0x748F82EE, 0x78A5636F, 0x84C87814, 0x8CC70208,
+    0x90BEFFFA, 0xA4506CEB, 0xBEF9A3F7, 0xC67178F2};
+
+/*********************** FUNCTION DEFINITIONS ***********************/
+static inline sha256_hash compute_sha256_hash(const uint8_t *data,
+                                              const size_t size) {
+  // init
+  struct {
+    uint8_t data[64];
+    uint32_t datalen{0};
+    uint64_t bitlen{0};
+    uint32_t state[8]{0x6a09e667, 0xbb67ae85, 0x3c6ef372, 0xa54ff53a,
+                      0x510e527f, 0x9b05688c, 0x1f83d9ab, 0x5be0cd19};
+  } ctx;
+
+  // transform
+  const auto sha256_transform = [&ctx]() {
+    uint32_t a, b, c, d, e, f, g, h, i, j, t1, t2, m[64];
+
+    for (i = 0, j = 0; i < 16; ++i, j += 4) {
+      m[i] = (uint32_t(ctx.data[j]) << 24u) |
+             (uint32_t(ctx.data[j + 1]) << 16u) |
+             (uint32_t(ctx.data[j + 2]) << 8u) | uint32_t(ctx.data[j + 3]);
+    }
+    for (; i < 64; ++i) {
+      m[i] =
+          SHA256_SIG1(m[i - 2]) + m[i - 7] + SHA256_SIG0(m[i - 15]) + m[i - 16];
+    }
+
+    a = ctx.state[0];
+    b = ctx.state[1];
+    c = ctx.state[2];
+    d = ctx.state[3];
+    e = ctx.state[4];
+    f = ctx.state[5];
+    g = ctx.state[6];
+    h = ctx.state[7];
+
+    for (i = 0; i < 64; ++i) {
+      t1 = h + SHA256_EP1(e) + SHA256_CH(e, f, g) + k[i] + m[i];
+      t2 = SHA256_EP0(a) + SHA256_MAJ(a, b, c);
+      h = g;
+      g = f;
+      f = e;
+      e = d + t1;
+      d = c;
+      c = b;
+      b = a;
+      a = t1 + t2;
+    }
+
+    ctx.state[0] += a;
+    ctx.state[1] += b;
+    ctx.state[2] += c;
+    ctx.state[3] += d;
+    ctx.state[4] += e;
+    ctx.state[5] += f;
+    ctx.state[6] += g;
+    ctx.state[7] += h;
+  };
+
+  // update
+  for (size_t i = 0; i < size; ++i) {
+    ctx.data[ctx.datalen] = data[i];
+    ++ctx.datalen;
+    if (ctx.datalen == 64) {
+      sha256_transform();
+      ctx.bitlen += 512;
+      ctx.datalen = 0;
+    }
+  }
+
+  // final
+  auto pad_idx = ctx.datalen;
+
+  // Pad whatever data is left in the buffer.
+  if (ctx.datalen < 56) {
+    ctx.data[pad_idx++] = 0x80;
+    while (pad_idx < 56) {
+      ctx.data[pad_idx++] = 0x00;
+    }
+  } else {
+    ctx.data[pad_idx++] = 0x80;
+    while (pad_idx < 64) {
+      ctx.data[pad_idx++] = 0x00;
+    }
+    sha256_transform();
+    memset(ctx.data, 0, 56);
+  }
+
+  // Append to the padding the total message's length in bits and transform.
+  ctx.bitlen += ctx.datalen * 8;
+  ctx.data[63] = uint8_t(ctx.bitlen & 0xFFull);
+  ctx.data[62] = uint8_t((ctx.bitlen >> 8ull) & 0xFFull);
+  ctx.data[61] = uint8_t((ctx.bitlen >> 16ull) & 0xFFull);
+  ctx.data[60] = uint8_t((ctx.bitlen >> 24ull) & 0xFFull);
+  ctx.data[59] = uint8_t((ctx.bitlen >> 32ull) & 0xFFull);
+  ctx.data[58] = uint8_t((ctx.bitlen >> 40ull) & 0xFFull);
+  ctx.data[57] = uint8_t((ctx.bitlen >> 48ull) & 0xFFull);
+  ctx.data[56] = uint8_t((ctx.bitlen >> 56ull) & 0xFFull);
+  sha256_transform();
+
+  // Since this implementation uses little endian byte ordering and SHA uses
+  // big endian, reverse all the bytes when copying the final state to the
+  // output hash.
+  sha256_hash ret;
+  for (uint32_t i = 0; i < 4; ++i) {
+    ret.hash[i] = (ctx.state[0] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 4] = (ctx.state[1] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 8] = (ctx.state[2] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 12] = (ctx.state[3] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 16] = (ctx.state[4] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 20] = (ctx.state[5] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 24] = (ctx.state[6] >> (24 - i * 8)) & 0x000000FF;
+    ret.hash[i + 28] = (ctx.state[7] >> (24 - i * 8)) & 0x000000FF;
+  }
+
+  return ret;
+}
+
+// cleanup
+#undef SHA256_ROTLEFT
+#undef SHA256_ROTRIGHT
+#undef SHA256_CH
+#undef SHA256_MAJ
+#undef SHA256_EP0
+#undef SHA256_EP1
+#undef SHA256_SIG0
+#undef SHA256_SIG1
+
+#endif
diff --git a/llvm/lib/Bitcode/MetalLib/tar/LICENSE b/llvm/lib/Bitcode/MetalLib/tar/LICENSE
new file mode 100644
index 000000000000..72cf80d63bb5
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/tar/LICENSE
@@ -0,0 +1,20 @@
+Copyright (c) 2017 rxi
+Copyright (c) 2021 Aidan MacDonald
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
+of the Software, and to permit persons to whom the Software is furnished to do
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/llvm/lib/Bitcode/MetalLib/tar/microtar.c b/llvm/lib/Bitcode/MetalLib/tar/microtar.c
new file mode 100644
index 000000000000..bc3dd878e6f5
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/tar/microtar.c
@@ -0,0 +1,737 @@
+/*
+ * Copyright (c) 2017 rxi
+ * Copyright (c) 2021 Aidan MacDonald
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#include "microtar.h"
+#include <limits.h>
+#include <string.h>
+
+enum {
+    S_HEADER_VALID   = 1 << 0,
+    S_WROTE_HEADER   = 1 << 1,
+    S_WROTE_DATA     = 1 << 2,
+    S_WROTE_DATA_EOF = 1 << 3,
+    S_WROTE_FINALIZE = 1 << 4,
+};
+
+enum {
+    NAME_OFF        = 0,                                NAME_LEN = 100,
+    MODE_OFF        = NAME_OFF+NAME_LEN,                MODE_LEN = 8,
+    OWNER_OFF       = MODE_OFF+MODE_LEN,                OWNER_LEN = 8,
+    GROUP_OFF       = OWNER_OFF+OWNER_LEN,              GROUP_LEN = 8,
+    SIZE_OFF        = GROUP_OFF+GROUP_LEN,              SIZE_LEN = 12,
+    MTIME_OFF       = SIZE_OFF+SIZE_LEN,                MTIME_LEN = 12,
+    CHKSUM_OFF      = MTIME_OFF+MTIME_LEN,              CHKSUM_LEN = 8,
+    TYPE_OFF        = CHKSUM_OFF+CHKSUM_LEN,
+    LINKNAME_OFF    = TYPE_OFF+1,                       LINKNAME_LEN = 100,
+    
+    // UStar extension
+    USTAR_OFF       = LINKNAME_OFF+LINKNAME_LEN,        USTAR_LEN = 6,
+    USTAR_VER_OFF   = USTAR_OFF+USTAR_LEN,              USTAR_VER_LEN = 2,
+    OWNER_NAME_OFF  = USTAR_VER_OFF+USTAR_VER_LEN,      OWNER_NAME_LEN = 32,
+    OWNER_GROUP_OFF = OWNER_NAME_OFF+OWNER_NAME_LEN,    OWNER_GROUP_LEN = 32,
+    DEV_MAJOR_OFF   = OWNER_GROUP_OFF+OWNER_GROUP_LEN,  DEV_MAJOR_LEN = 8,
+    DEV_MINOR_OFF   = DEV_MAJOR_OFF+DEV_MAJOR_LEN,      DEV_MINOR_LEN = 8,
+    NAME_PREFIX_OFF = DEV_MINOR_OFF+DEV_MINOR_LEN,      NAME_PREFIX_LEN = 155,
+
+    HEADER_LEN   = 512,
+};
+
+static int parse_octal(const char* str, size_t len, unsigned* ret)
+{
+    unsigned n = 0;
+
+    while(len-- > 0 && *str != 0) {
+        if(*str < '0' || *str > '9')
+            return MTAR_EOVERFLOW;
+
+        if(n > UINT_MAX/8)
+            return MTAR_EOVERFLOW;
+        else
+            n *= 8;
+
+        char r = *str++ - '0';
+
+        if(n > UINT_MAX - r)
+            return MTAR_EOVERFLOW;
+        else
+            n += r;
+    }
+
+    *ret = n;
+    return MTAR_ESUCCESS;
+}
+
+static int print_octal(char* str, size_t len, unsigned value, unsigned print_double_term)
+{
+    /* move backwards over the output string */
+    char* ptr = str + len - 1;
+    *ptr = (print_double_term ? '\0' : ' ');
+    
+    if (print_double_term && ptr != str) {
+        --ptr;
+        *ptr = ' ';
+    }
+
+    /* output the significant digits */
+    while(value > 0) {
+        if(ptr == str)
+            return MTAR_EOVERFLOW;
+
+        --ptr;
+        *ptr = '0' + (value % 8);
+        value /= 8;
+    }
+
+    /* pad the remainder of the field with zeros */
+    while(ptr != str) {
+        --ptr;
+        *ptr = '0';
+    }
+
+    return MTAR_ESUCCESS;
+}
+
+static unsigned round_up_512(unsigned n)
+{
+    return (n + 511u) & ~511u;
+}
+
+static int tread(mtar_t* tar, void* data, unsigned size)
+{
+    int ret = tar->ops->read(tar->stream, data, size);
+    if(ret >= 0)
+        tar->pos += ret;
+
+    return ret;
+}
+
+static int twrite(mtar_t* tar, const void* data, unsigned size)
+{
+    int ret = tar->ops->write(tar->stream, data, size);
+    if(ret >= 0)
+        tar->pos += ret;
+
+    return ret;
+}
+
+static int tseek(mtar_t* tar, unsigned pos)
+{
+    int err = tar->ops->seek(tar->stream, pos);
+    tar->pos = pos;
+    return err;
+}
+
+static int write_null_bytes(mtar_t* tar, size_t count)
+{
+    int ret;
+    size_t n;
+
+    memset(tar->buffer, 0, sizeof(tar->buffer));
+    while(count > 0) {
+        n = count < sizeof(tar->buffer) ? count : sizeof(tar->buffer);
+        ret = twrite(tar, tar->buffer, n);
+        if(ret < 0)
+            return ret;
+        if(ret != (int)n)
+            return MTAR_EWRITEFAIL;
+
+        count -= n;
+    }
+
+    return MTAR_ESUCCESS;
+}
+
+static unsigned checksum(const char* raw)
+{
+    unsigned i;
+    unsigned char* p = (unsigned char*)raw;
+    unsigned res = 256;
+
+    for(i = 0; i < CHKSUM_OFF; i++)
+        res += p[i];
+    for(i = TYPE_OFF; i < HEADER_LEN; i++)
+        res += p[i];
+
+    return res;
+}
+
+static int raw_to_header(mtar_header_t* h, const char* raw)
+{
+    unsigned chksum;
+    int rc;
+
+    /* If the checksum starts with a null byte we assume the record is NULL */
+    if(raw[CHKSUM_OFF] == '\0')
+        return MTAR_ENULLRECORD;
+
+    /* Compare the checksum */
+    if((rc = parse_octal(&raw[CHKSUM_OFF], CHKSUM_LEN, &chksum)))
+       return rc;
+    if(chksum != checksum(raw))
+        return MTAR_EBADCHKSUM;
+
+    /* Load raw header into header */
+    if((rc = parse_octal(&raw[MODE_OFF], MODE_LEN, &h->mode)))
+        return rc;
+    if((rc = parse_octal(&raw[OWNER_OFF], OWNER_LEN, &h->owner)))
+        return rc;
+    if((rc = parse_octal(&raw[GROUP_OFF], GROUP_LEN, &h->group)))
+        return rc;
+    if((rc = parse_octal(&raw[SIZE_OFF], SIZE_LEN, &h->size)))
+        return rc;
+    if((rc = parse_octal(&raw[MTIME_OFF], MTIME_LEN, &h->mtime)))
+        return rc;
+
+    h->type = raw[TYPE_OFF];
+    if(!h->type)
+        h->type = MTAR_TREG;
+
+    memcpy(h->name, &raw[NAME_OFF], NAME_LEN);
+    h->name[sizeof(h->name) - 1] = 0;
+
+    memcpy(h->linkname, &raw[LINKNAME_OFF], LINKNAME_LEN);
+    h->linkname[sizeof(h->linkname) - 1] = 0;
+
+    return MTAR_ESUCCESS;
+}
+
+static int header_to_raw(char* raw, const mtar_header_t* h)
+{
+    unsigned chksum;
+    int rc;
+
+    memset(raw, 0, HEADER_LEN);
+
+    /* Load header into raw header */
+    if((rc = print_octal(&raw[MODE_OFF], MODE_LEN, h->mode, 1)))
+        return rc;
+    if((rc = print_octal(&raw[OWNER_OFF], OWNER_LEN, h->owner, 1)))
+        return rc;
+    if((rc = print_octal(&raw[GROUP_OFF], GROUP_LEN, h->group, 1)))
+        return rc;
+    if((rc = print_octal(&raw[SIZE_OFF], SIZE_LEN, h->size, 0)))
+        return rc;
+    if((rc = print_octal(&raw[MTIME_OFF], MTIME_LEN, h->mtime, 0)))
+        return rc;
+    if((rc = print_octal(&raw[DEV_MAJOR_OFF], DEV_MAJOR_LEN, 0, 1)))
+        return rc;
+    if((rc = print_octal(&raw[DEV_MINOR_OFF], DEV_MINOR_LEN, 0, 1)))
+        return rc;
+
+    raw[TYPE_OFF] = h->type ? h->type : MTAR_TREG;
+    
+    raw[USTAR_VER_OFF] = 0x30;
+    raw[USTAR_VER_OFF + 1] = 0x30;
+
+#if defined(__GNUC__) && (__GNUC__ >= 8)
+/* Sigh. GCC wrongly assumes the output of strncpy() is supposed to be
+ * a null-terminated string -- which it is not, and we are relying on
+ * that fact here -- and tries to warn about 'string truncation' because
+ * the null terminator might not be copied. Just suppress the warning. */
+# pragma GCC diagnostic push
+# pragma GCC diagnostic ignored "-Wstringop-truncation"
+#endif
+
+    strncpy(&raw[NAME_OFF], h->name, NAME_LEN);
+    strncpy(&raw[LINKNAME_OFF], h->linkname, LINKNAME_LEN);
+    strncpy(&raw[NAME_PREFIX_OFF], h->name_prefix, NAME_PREFIX_LEN);
+    
+    static const char ustar_magic[] = "ustar";
+    strncpy(&raw[USTAR_OFF], ustar_magic, USTAR_LEN);
+
+#if defined(__GNUC__) && (__GNUC__ >= 8)
+# pragma GCC diagnostic pop
+#endif
+    
+    // NOTE: won't write owner name and group (keep it zero)
+
+    /* Calculate and write checksum */
+    chksum = checksum(raw);
+    if((rc = print_octal(&raw[CHKSUM_OFF], CHKSUM_LEN-1, chksum, 0)))
+        return rc;
+
+    raw[CHKSUM_OFF + CHKSUM_LEN - 2] = '\0';
+    raw[CHKSUM_OFF + CHKSUM_LEN - 1] = ' ';
+
+    return MTAR_ESUCCESS;
+}
+
+static unsigned data_beg_pos(const mtar_t* tar)
+{
+    return tar->header_pos + HEADER_LEN;
+}
+
+static unsigned data_end_pos(const mtar_t* tar)
+{
+    return tar->end_pos;
+}
+
+static int ensure_header(mtar_t* tar)
+{
+    int ret, err;
+
+    if(tar->state & S_HEADER_VALID)
+        return MTAR_ESUCCESS;
+
+    if(tar->pos > UINT_MAX - HEADER_LEN)
+        return MTAR_EOVERFLOW;
+
+    tar->header_pos = tar->pos;
+    tar->end_pos = data_beg_pos(tar);
+
+    ret = tread(tar, tar->buffer, HEADER_LEN);
+    if(ret < 0)
+        return ret;
+    if(ret != HEADER_LEN)
+        return MTAR_EREADFAIL;
+
+    err = raw_to_header(&tar->header, tar->buffer);
+    if(err)
+        return err;
+
+    if(tar->end_pos > UINT_MAX - tar->header.size)
+        return MTAR_EOVERFLOW;
+    tar->end_pos += tar->header.size;
+
+    tar->state |= S_HEADER_VALID;
+    return MTAR_ESUCCESS;
+}
+
+const char* mtar_strerror(int err)
+{
+    switch(err) {
+    case MTAR_ESUCCESS:     return "success";
+    case MTAR_EFAILURE:     return "failure";
+    case MTAR_EOPENFAIL:    return "could not open";
+    case MTAR_EREADFAIL:    return "could not read";
+    case MTAR_EWRITEFAIL:   return "could not write";
+    case MTAR_ESEEKFAIL:    return "could not seek";
+    case MTAR_ESEEKRANGE:   return "seek out of bounds";
+    case MTAR_EBADCHKSUM:   return "bad checksum";
+    case MTAR_ENULLRECORD:  return "null record";
+    case MTAR_ENOTFOUND:    return "file not found";
+    case MTAR_EOVERFLOW:    return "overflow";
+    case MTAR_EAPI:         return "API usage error";
+    case MTAR_ENAMETOOLONG: return "name too long";
+    case MTAR_EWRONGSIZE:   return "wrong amount of data written";
+    case MTAR_EACCESS:      return "wrong access mode";
+    default:                return "unknown error";
+    }
+}
+
+void mtar_init(mtar_t* tar, int access, const mtar_ops_t* ops, void* stream)
+{
+    memset(tar, 0, sizeof(mtar_t));
+    tar->access = access;
+    tar->ops = ops;
+    tar->stream = stream;
+}
+
+int mtar_close(mtar_t* tar)
+{
+    int err = tar->ops->close(tar->stream);
+    tar->ops = NULL;
+    tar->stream = NULL;
+    return err;
+}
+
+int mtar_is_open(mtar_t* tar)
+{
+    return (tar->ops != NULL) ? 1 : 0;
+}
+
+mtar_header_t* mtar_get_header(mtar_t* tar)
+{
+    if(tar->state & S_HEADER_VALID)
+        return &tar->header;
+    else
+        return NULL;
+}
+
+int mtar_access_mode(const mtar_t* tar)
+{
+    return tar->access;
+}
+
+int mtar_rewind(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_READ)
+        return MTAR_EAPI;
+#endif
+
+    int err = tseek(tar, 0);
+    tar->state = 0;
+    return err;
+}
+
+int mtar_next(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_READ)
+        return MTAR_EACCESS;
+#endif
+
+    if(tar->state & S_HEADER_VALID) {
+        tar->state &= ~S_HEADER_VALID;
+
+        /* seek to the next header */
+        int err = tseek(tar, round_up_512(data_end_pos(tar)));
+        if(err)
+            return err;
+    }
+
+    return ensure_header(tar);
+}
+
+int mtar_foreach(mtar_t* tar, mtar_foreach_cb cb, void* arg)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_READ)
+        return MTAR_EACCESS;
+#endif
+
+    int err = mtar_rewind(tar);
+    if(err)
+        return err;
+
+    while((err = mtar_next(tar)) == MTAR_ESUCCESS)
+        if((err = cb(tar, &tar->header, arg)) != 0)
+            return err;
+
+    if(err == MTAR_ENULLRECORD)
+        err = MTAR_ESUCCESS;
+
+    return err;
+}
+
+static int find_foreach_cb(mtar_t* tar, const mtar_header_t* h, void* arg)
+{
+    (void)tar;
+    const char* name = (const char*)arg;
+    return strcmp(name, h->name) ? 0 : 1;
+}
+
+int mtar_find(mtar_t* tar, const char* name)
+{
+    int err = mtar_foreach(tar, find_foreach_cb, (void*)name);
+    if(err == 1)
+        err = MTAR_ESUCCESS;
+    else if(err == MTAR_ESUCCESS)
+        err = MTAR_ENOTFOUND;
+
+    return err;
+}
+
+int mtar_read_data(mtar_t* tar, void* ptr, unsigned size)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(!(tar->state & S_HEADER_VALID))
+        return MTAR_EAPI;
+#endif
+
+    /* have we reached end of file? */
+    unsigned data_end = data_end_pos(tar);
+    if(tar->pos >= data_end)
+        return 0;
+
+    /* truncate the read if it would go beyond EOF */
+    unsigned data_left = data_end - tar->pos;
+    if(data_left < size)
+        size = data_left;
+
+    return tread(tar, ptr, size);
+}
+
+int mtar_seek_data(mtar_t* tar, int offset, int whence)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(!(tar->state & S_HEADER_VALID))
+        return MTAR_EAPI;
+#endif
+
+    unsigned data_beg = data_beg_pos(tar);
+    unsigned data_end = data_end_pos(tar);
+    unsigned newpos;
+
+    switch(whence) {
+    case SEEK_SET:
+        if(offset < 0)
+            return MTAR_ESEEKRANGE;
+
+        newpos = data_beg + offset;
+        break;
+
+    case SEEK_CUR:
+        if((offset > 0 && (unsigned) offset > data_end - tar->pos) ||
+           (offset < 0 && (unsigned)-offset > tar->pos - data_beg))
+            return MTAR_ESEEKRANGE;
+
+        newpos = tar->pos + offset;
+        break;
+
+    case SEEK_END:
+        if(offset > 0)
+            return MTAR_ESEEKRANGE;
+
+        newpos = data_end + offset;
+        break;
+
+    default:
+        return MTAR_EAPI;
+    }
+
+    return tseek(tar, newpos);
+}
+
+unsigned mtar_tell_data(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(!(tar->state & S_HEADER_VALID))
+        return MTAR_EAPI;
+#endif
+
+    return tar->pos - data_beg_pos(tar);
+}
+
+int mtar_eof_data(mtar_t* tar)
+{
+    /* API usage error, but just claim EOF. */
+    if(!(tar->state & S_HEADER_VALID))
+        return 1;
+
+    return tar->pos >= data_end_pos(tar) ? 1 : 0;
+}
+
+int mtar_write_header(mtar_t* tar, const mtar_header_t* h)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(((tar->state & S_WROTE_DATA) && !(tar->state & S_WROTE_DATA_EOF)) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    tar->state &= ~(S_HEADER_VALID | S_WROTE_HEADER |
+                    S_WROTE_DATA | S_WROTE_DATA_EOF);
+
+    /* ensure we have enough space to write the declared amount of data */
+    if(tar->pos > UINT_MAX - HEADER_LEN - round_up_512(h->size))
+        return MTAR_EOVERFLOW;
+
+    tar->header_pos = tar->pos;
+    tar->end_pos = data_beg_pos(tar);
+
+    if(h != &tar->header)
+        tar->header = *h;
+
+    int err = header_to_raw(tar->buffer, &tar->header);
+    if(err)
+        return err;
+
+    int ret = twrite(tar, tar->buffer, HEADER_LEN);
+    if(ret < 0)
+        return ret;
+    if(ret != HEADER_LEN)
+        return MTAR_EWRITEFAIL;
+
+    tar->state |= (S_HEADER_VALID | S_WROTE_HEADER);
+    return MTAR_ESUCCESS;
+}
+
+int mtar_update_header(mtar_t* tar, const mtar_header_t* h)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(!(tar->state & S_WROTE_HEADER) ||
+       (tar->state & S_WROTE_DATA_EOF) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    unsigned beg_pos = data_beg_pos(tar);
+    if(beg_pos > UINT_MAX - h->size)
+        return MTAR_EOVERFLOW;
+
+    unsigned old_pos = tar->pos;
+    int err = tseek(tar, tar->header_pos);
+    if(err)
+        return err;
+
+    if(h != &tar->header)
+        tar->header = *h;
+
+    err = header_to_raw(tar->buffer, &tar->header);
+    if(err)
+        return err;
+
+    int len = twrite(tar, tar->buffer, HEADER_LEN);
+    if(len < 0)
+        return len;
+    if(len != HEADER_LEN)
+        return MTAR_EWRITEFAIL;
+
+    return tseek(tar, old_pos);
+}
+
+int mtar_write_file_header(mtar_t* tar, const char* name, unsigned size)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(((tar->state & S_WROTE_DATA) && !(tar->state & S_WROTE_DATA_EOF)) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    size_t namelen = strlen(name);
+    if(namelen > NAME_LEN)
+        return MTAR_ENAMETOOLONG;
+
+    tar->header.mode = 0644;
+    tar->header.owner = 0;
+    tar->header.group = 0;
+    tar->header.size = size;
+    tar->header.mtime = 0;
+    tar->header.type = MTAR_TREG;
+    memcpy(tar->header.name, name, namelen + 1);
+    tar->header.linkname[0] = '\0';
+
+    return mtar_write_header(tar, &tar->header);
+}
+
+int mtar_write_dir_header(mtar_t* tar, const char* name)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(((tar->state & S_WROTE_DATA) && !(tar->state & S_WROTE_DATA_EOF)) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    size_t namelen = strlen(name);
+    if(namelen > NAME_LEN)
+        return MTAR_ENAMETOOLONG;
+
+    tar->header.mode = 0755;
+    tar->header.owner = 0;
+    tar->header.group = 0;
+    tar->header.size = 0;
+    tar->header.mtime = 0;
+    tar->header.type = MTAR_TDIR;
+    memcpy(tar->header.name, name, namelen + 1);
+    tar->header.linkname[0] = '\0';
+
+    return mtar_write_header(tar, &tar->header);
+}
+
+int mtar_write_data(mtar_t* tar, const void* ptr, unsigned size)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(!(tar->state & S_WROTE_HEADER) ||
+       (tar->state & S_WROTE_DATA_EOF) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    tar->state |= S_WROTE_DATA;
+
+    int err = twrite(tar, ptr, size);
+    if(tar->pos > tar->end_pos)
+        tar->end_pos = tar->pos;
+
+    return err;
+}
+
+int mtar_update_file_size(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(!(tar->state & S_WROTE_HEADER) ||
+       (tar->state & S_WROTE_DATA_EOF) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    unsigned new_size = data_end_pos(tar) - data_beg_pos(tar);
+    if(new_size == tar->header.size)
+        return MTAR_ESUCCESS;
+    else {
+        tar->header.size = new_size;
+        return mtar_update_header(tar, &tar->header);
+    }
+}
+
+int mtar_end_data(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(!(tar->state & S_WROTE_HEADER) ||
+       (tar->state & S_WROTE_DATA_EOF) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    int err;
+
+    /* ensure the caller wrote the correct amount of data */
+    unsigned expected_end = data_beg_pos(tar) + tar->header.size;
+    if(tar->end_pos != expected_end)
+        return MTAR_EWRONGSIZE;
+
+    /* ensure we're positioned at the end of the stream */
+    if(tar->pos != tar->end_pos) {
+        err = tseek(tar, tar->end_pos);
+        if(err)
+            return err;
+    }
+
+    /* write remainder of the 512-byte record */
+    err = write_null_bytes(tar, round_up_512(tar->pos) - tar->pos);
+    if(err)
+        return err;
+
+    tar->state |= S_WROTE_DATA_EOF;
+    return MTAR_ESUCCESS;
+}
+
+int mtar_finalize(mtar_t* tar)
+{
+#ifndef MICROTAR_DISABLE_API_CHECKS
+    if(tar->access != MTAR_WRITE)
+        return MTAR_EACCESS;
+    if(((tar->state & S_WROTE_DATA) && !(tar->state & S_WROTE_DATA_EOF)) ||
+       (tar->state & S_WROTE_FINALIZE))
+        return MTAR_EAPI;
+#endif
+
+    tar->state |= S_WROTE_FINALIZE;
+    return write_null_bytes(tar, 1024);
+}
diff --git a/llvm/lib/Bitcode/MetalLib/tar/microtar.h b/llvm/lib/Bitcode/MetalLib/tar/microtar.h
new file mode 100644
index 000000000000..86f17f896ea3
--- /dev/null
+++ b/llvm/lib/Bitcode/MetalLib/tar/microtar.h
@@ -0,0 +1,136 @@
+/*
+ * Copyright (c) 2017 rxi
+ * Copyright (c) 2021 Aidan MacDonald
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef MICROTAR_H
+#define MICROTAR_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include <stdio.h>  /* SEEK_SET et al. */
+
+enum mtar_error {
+    MTAR_ESUCCESS     =  0,
+    MTAR_EFAILURE     = -1,
+    MTAR_EOPENFAIL    = -2,
+    MTAR_EREADFAIL    = -3,
+    MTAR_EWRITEFAIL   = -4,
+    MTAR_ESEEKFAIL    = -5,
+    MTAR_ESEEKRANGE   = -6,
+    MTAR_EBADCHKSUM   = -7,
+    MTAR_ENULLRECORD  = -8,
+    MTAR_ENOTFOUND    = -9,
+    MTAR_EOVERFLOW    = -10,
+    MTAR_EAPI         = -11,
+    MTAR_ENAMETOOLONG = -12,
+    MTAR_EWRONGSIZE   = -13,
+    MTAR_EACCESS      = -14,
+};
+
+enum mtar_type {
+    MTAR_TREG   = '0',
+    MTAR_TLNK   = '1',
+    MTAR_TSYM   = '2',
+    MTAR_TCHR   = '3',
+    MTAR_TBLK   = '4',
+    MTAR_TDIR   = '5',
+    MTAR_TFIFO  = '6',
+};
+
+enum mtar_access {
+    MTAR_READ,
+    MTAR_WRITE,
+};
+
+typedef struct mtar_header mtar_header_t;
+typedef struct mtar mtar_t;
+typedef struct mtar_ops mtar_ops_t;
+
+typedef int(*mtar_foreach_cb)(mtar_t*, const mtar_header_t*, void*);
+
+struct mtar_header {
+    unsigned mode;
+    unsigned owner;
+    unsigned group;
+    unsigned size;
+    unsigned mtime;
+    unsigned type;
+    char name[100 + 1]; /* +1 byte in order to ensure null termination */
+    char linkname[101];
+	char name_prefix[155 + 1];
+};
+
+struct mtar_ops {
+    int(*read)(void* stream, void* data, unsigned size);
+    int(*write)(void* stream, const void* data, unsigned size);
+    int(*seek)(void* stream, unsigned pos);
+    int(*close)(void* stream);
+};
+
+struct mtar {
+    char buffer[512];       /* IO buffer, put first to allow library users to
+                             * control its alignment */
+    int state;              /* Used to simplify the API and verify API usage */
+    int access;             /* Access mode */
+    unsigned pos;           /* Current position in file */
+    unsigned end_pos;       /* End position of the current file */
+    unsigned header_pos;    /* Position of the current header */
+    mtar_header_t header;   /* Most recently parsed header */
+    const mtar_ops_t* ops;  /* Stream operations */
+    void* stream;           /* Stream handle */
+};
+
+const char* mtar_strerror(int err);
+
+void mtar_init(mtar_t* tar, int access, const mtar_ops_t* ops, void* stream);
+int mtar_close(mtar_t* tar);
+int mtar_is_open(mtar_t* tar);
+
+mtar_header_t* mtar_get_header(mtar_t* tar);
+int mtar_access_mode(const mtar_t* tar);
+
+int mtar_rewind(mtar_t* tar);
+int mtar_next(mtar_t* tar);
+int mtar_foreach(mtar_t* tar, mtar_foreach_cb cb, void* arg);
+int mtar_find(mtar_t* tar, const char* name);
+
+int mtar_read_data(mtar_t* tar, void* ptr, unsigned size);
+int mtar_seek_data(mtar_t* tar, int offset, int whence);
+unsigned mtar_tell_data(mtar_t* tar);
+int mtar_eof_data(mtar_t* tar);
+
+int mtar_write_header(mtar_t* tar, const mtar_header_t* h);
+int mtar_update_header(mtar_t* tar, const mtar_header_t* h);
+int mtar_write_file_header(mtar_t* tar, const char* name, unsigned size);
+int mtar_write_dir_header(mtar_t* tar, const char* name);
+int mtar_write_data(mtar_t* tar, const void* ptr, unsigned size);
+int mtar_update_file_size(mtar_t* tar);
+int mtar_end_data(mtar_t* tar);
+int mtar_finalize(mtar_t* tar);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/llvm/lib/Bitcode/Writer32/BitWriter32.cpp b/llvm/lib/Bitcode/Writer32/BitWriter32.cpp
new file mode 100644
index 000000000000..3873c0b2d169
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/BitWriter32.cpp
@@ -0,0 +1,50 @@
+//===-- BitWriter32.cpp ---------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm-c/BitWriter.h"
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Support/raw_ostream.h"
+using namespace llvm;
+
+
+/*===-- Operations on modules ---------------------------------------------===*/
+
+int LLVMWriteBitcode32ToFile(LLVMModuleRef M, const char *Path) {
+  std::error_code EC;
+  raw_fd_ostream OS(Path, EC, sys::fs::OF_None);
+
+  if (EC)
+    return -1;
+
+  WriteBitcode32ToFile(unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteBitcode32ToFD(LLVMModuleRef M, int FD, int ShouldClose,
+                           int Unbuffered) {
+  raw_fd_ostream OS(FD, ShouldClose, Unbuffered);
+
+  WriteBitcode32ToFile(unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteBitcode32ToFileHandle(LLVMModuleRef M, int FileHandle) {
+  return LLVMWriteBitcode32ToFD(M, FileHandle, true, false);
+}
+
+LLVMMemoryBufferRef LLVMWriteBitcode32ToMemoryBuffer(LLVMModuleRef M) {
+  std::string Data;
+  raw_string_ostream OS(Data);
+
+  WriteBitcode32ToFile(unwrap(M), OS);
+  return wrap(MemoryBuffer::getMemBufferCopy(OS.str()).release());
+}
diff --git a/llvm/lib/Bitcode/Writer32/BitcodeWriter32.cpp b/llvm/lib/Bitcode/Writer32/BitcodeWriter32.cpp
new file mode 100644
index 000000000000..541396f6a145
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/BitcodeWriter32.cpp
@@ -0,0 +1,2538 @@
+//===--- Bitcode/Writer32/BitcodeWriter32.cpp - Bitcode 3.2 Writer --------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Bitcode 3.2 writer implementation.
+//
+//===----------------------------------------------------------------------===//
+
+// enable errors when using > 3.2 bitcode enums from LLVMBitCodes.h
+#define LLVM_BITCODE_32 1
+
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "ValueEnumerator32.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Bitstream/BitstreamWriter.h"
+#include "llvm/Bitcode/LLVMBitCodes.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DebugInfoMetadata.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Operator.h"
+#include "llvm/IR/UseListOrder.h"
+#include "llvm/IR/ValueSymbolTable.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/Program.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/BinaryFormat/Dwarf.h"
+#include <cctype>
+#include <map>
+using namespace llvm;
+
+#define SPIR32_TRIPLE "spir-unknown-unknown"
+#define SPIR64_TRIPLE "spir64-unknown-unknown"
+#define SPIR32_DATALAYOUT                                         \
+  "e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-"     \
+  "f32:32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-"  \
+  "v64:64:64-v96:128:128-v128:128:128-v192:256:256-v256:256:256-" \
+  "v512:512:512-v1024:1024:1024"
+#define SPIR64_DATALAYOUT                                         \
+  "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-"     \
+  "f32:32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-"  \
+  "v64:64:64-v96:128:128-v128:128:128-v192:256:256-v256:256:256-" \
+  "v512:512:512-v1024:1024:1024"
+
+/// These are manifest constants used by the bitcode writer. They do not need to
+/// be kept in sync with the reader, but need to be consistent within this file.
+enum {
+  // VALUE_SYMTAB_BLOCK abbrev id's.
+  VST_ENTRY_8_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  VST_ENTRY_7_ABBREV,
+  VST_ENTRY_6_ABBREV,
+  VST_BBENTRY_6_ABBREV,
+
+  // CONSTANTS_BLOCK abbrev id's.
+  CONSTANTS_SETTYPE_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  CONSTANTS_INTEGER_ABBREV,
+  CONSTANTS_CE_CAST_Abbrev,
+  CONSTANTS_NULL_Abbrev,
+
+  // FUNCTION_BLOCK abbrev id's.
+  FUNCTION_INST_LOAD_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  FUNCTION_INST_BINOP_ABBREV,
+  FUNCTION_INST_BINOP_FLAGS_ABBREV,
+  FUNCTION_INST_CAST_ABBREV,
+  FUNCTION_INST_RET_VOID_ABBREV,
+  FUNCTION_INST_RET_VAL_ABBREV,
+  FUNCTION_INST_UNREACHABLE_ABBREV,
+  FUNCTION_INST_GEP_ABBREV BC38,
+
+  // SwitchInst Magic
+  SWITCH_INST_MAGIC = 0x4B5 // May 2012 => 1205 => Hex
+};
+
+static unsigned GetEncodedCastOpcode(unsigned Opcode) {
+  switch (Opcode) {
+  default: llvm_unreachable("Unknown cast instruction!");
+  case Instruction::Trunc   : return bitc::CAST_TRUNC;
+  case Instruction::ZExt    : return bitc::CAST_ZEXT;
+  case Instruction::SExt    : return bitc::CAST_SEXT;
+  case Instruction::FPToUI  : return bitc::CAST_FPTOUI;
+  case Instruction::FPToSI  : return bitc::CAST_FPTOSI;
+  case Instruction::UIToFP  : return bitc::CAST_UITOFP;
+  case Instruction::SIToFP  : return bitc::CAST_SITOFP;
+  case Instruction::FPTrunc : return bitc::CAST_FPTRUNC;
+  case Instruction::FPExt   : return bitc::CAST_FPEXT;
+  case Instruction::PtrToInt: return bitc::CAST_PTRTOINT;
+  case Instruction::IntToPtr: return bitc::CAST_INTTOPTR;
+  case Instruction::BitCast : return bitc::CAST_BITCAST;
+  case Instruction::AddrSpaceCast:
+    assert(false && "addrspacecast instructions must be filtered out before writing 3.2 bitcode");
+    llvm_unreachable("addrspacecast instructions must be filtered out before writing 3.2 bitcode");
+  }
+}
+
+static unsigned GetEncodedBinaryOpcode(unsigned Opcode) {
+  switch (Opcode) {
+  default: llvm_unreachable("Unknown binary instruction!");
+  case Instruction::Add:
+  case Instruction::FAdd: return bitc::BINOP_ADD;
+  case Instruction::Sub:
+  case Instruction::FSub: return bitc::BINOP_SUB;
+  case Instruction::Mul:
+  case Instruction::FMul: return bitc::BINOP_MUL;
+  case Instruction::UDiv: return bitc::BINOP_UDIV;
+  case Instruction::FDiv:
+  case Instruction::SDiv: return bitc::BINOP_SDIV;
+  case Instruction::URem: return bitc::BINOP_UREM;
+  case Instruction::FRem:
+  case Instruction::SRem: return bitc::BINOP_SREM;
+  case Instruction::Shl:  return bitc::BINOP_SHL;
+  case Instruction::LShr: return bitc::BINOP_LSHR;
+  case Instruction::AShr: return bitc::BINOP_ASHR;
+  case Instruction::And:  return bitc::BINOP_AND;
+  case Instruction::Or:   return bitc::BINOP_OR;
+  case Instruction::Xor:  return bitc::BINOP_XOR;
+  }
+}
+
+static unsigned GetEncodedRMWOperation(AtomicRMWInst::BinOp Op) {
+  switch (Op) {
+  default: llvm_unreachable("Unknown RMW operation!");
+  case AtomicRMWInst::Xchg: return bitc::RMW_XCHG;
+  case AtomicRMWInst::Add: return bitc::RMW_ADD;
+  case AtomicRMWInst::Sub: return bitc::RMW_SUB;
+  case AtomicRMWInst::And: return bitc::RMW_AND;
+  case AtomicRMWInst::Nand: return bitc::RMW_NAND;
+  case AtomicRMWInst::Or: return bitc::RMW_OR;
+  case AtomicRMWInst::Xor: return bitc::RMW_XOR;
+  case AtomicRMWInst::Max: return bitc::RMW_MAX;
+  case AtomicRMWInst::Min: return bitc::RMW_MIN;
+  case AtomicRMWInst::UMax: return bitc::RMW_UMAX;
+  case AtomicRMWInst::UMin: return bitc::RMW_UMIN;
+  }
+}
+
+static unsigned GetEncodedOrdering(AtomicOrdering Ordering) {
+  switch (Ordering) {
+  case AtomicOrdering::NotAtomic: return bitc::ORDERING_NOTATOMIC;
+  case AtomicOrdering::Unordered: return bitc::ORDERING_UNORDERED;
+  case AtomicOrdering::Monotonic: return bitc::ORDERING_MONOTONIC;
+  case AtomicOrdering::Acquire: return bitc::ORDERING_ACQUIRE;
+  case AtomicOrdering::Release: return bitc::ORDERING_RELEASE;
+  case AtomicOrdering::AcquireRelease: return bitc::ORDERING_ACQREL;
+  case AtomicOrdering::SequentiallyConsistent: return bitc::ORDERING_SEQCST;
+  }
+  llvm_unreachable("Invalid ordering");
+}
+
+static unsigned GetEncodedSynchScope(SyncScope::ID SSID) {
+  switch (SSID) {
+    case SyncScope::SingleThread: return bitc::SYNCHSCOPE_SINGLETHREAD_OLD;
+    case SyncScope::System: return bitc::SYNCHSCOPE_CROSSTHREAD_OLD;
+  }
+  llvm_unreachable("Invalid synch scope");
+}
+
+static void WriteStringRecord(unsigned Code, StringRef Str,
+                              unsigned AbbrevToUse, BitstreamWriter &Stream) {
+  SmallVector<unsigned, 64> Vals;
+
+  // Code: [strchar x N]
+  for (unsigned i = 0, e = Str.size(); i != e; ++i) {
+    if (AbbrevToUse && !BitCodeAbbrevOp::isChar6(Str[i]))
+      AbbrevToUse = 0;
+    Vals.push_back(Str[i]);
+  }
+
+  // Emit the finished record.
+  Stream.EmitRecord(Code, Vals, AbbrevToUse);
+}
+
+uint64_t getAttrKindEncodingBC32(Attribute::AttrKind Kind); // for use in ValueEnumerator32 as well
+uint64_t getAttrKindEncodingBC32(Attribute::AttrKind Kind) {
+  // NOTE: these are the only parameter attributes supported by SPIR 1.2 (see "3.9 Parameter Attributes")
+  // NOTE: nocapture is technically supported as well, but leads to problems on certain implementations
+  switch (Kind) {
+  case Attribute::None:            return 0;
+  case Attribute::ZExt:            return 1 << 0;
+  case Attribute::SExt:            return 1 << 1;
+  case Attribute::StructRet:       return 1 << 4;
+  case Attribute::ByVal:           return 1 << 7;
+  default: return 0; // just drop the attr
+  }
+}
+
+static void WriteAttributeTable(const ValueEnumerator32 &VE,
+                                BitstreamWriter &Stream) {
+  const std::vector<AttributeList> &Attrs = VE.getAttributeLists();
+  if (Attrs.empty()) return;
+
+  Stream.EnterSubblock(bitc::PARAMATTR_BLOCK_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+  for (unsigned i = 0, e = Attrs.size(); i != e; ++i) {
+    AttributeList AL = Attrs[i];
+    for (unsigned i : AL.indexes()) {
+      // manual A.raw(i) so that we don't hit any asserts or unreachables (for attrs that have no raw representation)
+      // also: ignore 3.5+ style alignment, 3.2 style alignment is handled after this
+      uint64_t raw_attrs = 0;
+      AttributeSet attrs = AL.getAttributes(i);
+      for (auto Attr : attrs) {
+        // can't handle non enum/int attrs
+        if (!Attr.isEnumAttribute() && !Attr.isIntAttribute()) continue;
+
+        Attribute::AttrKind Kind = Attr.getKindAsEnum();
+        if (Kind == Attribute::Alignment) { /* drop it */ }
+        else if (Kind == Attribute::StackAlignment) { /* drop it */ }
+        else if (Kind == Attribute::Dereferenceable) { /* drop it */ }
+        else raw_attrs |= getAttrKindEncodingBC32(Kind);
+      }
+
+      // Taken from LLVM 3.2 Attributes::encodeLLVMAttributesForBitcode
+      uint64_t EncodedAttrs = raw_attrs & 0xffff;
+      if (AL.hasAttributeAtIndex(i, Attribute::Alignment))
+        EncodedAttrs |= AL.getAttributeAtIndex(i, Attribute::Alignment).getAlignment().getValue().value() << 16;
+      EncodedAttrs |= (raw_attrs & (0xffffULL << 21)) << 11;
+
+      Record.push_back(i);
+      Record.push_back(EncodedAttrs);
+    }
+
+    Stream.EmitRecord(bitc::PARAMATTR_CODE_ENTRY_OLD, Record);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+/// WriteTypeTable - Write out the type table for a module.
+static void WriteTypeTable(const ValueEnumerator32 &VE, BitstreamWriter &Stream) {
+  const ValueEnumerator32::TypeList &TypeList = VE.getTypes();
+
+  Stream.EnterSubblock(bitc::TYPE_BLOCK_ID_NEW, 4 /*count from # abbrevs */);
+  SmallVector<uint64_t, 64> TypeVals;
+
+  uint64_t NumBits = VE.computeBitsRequiredForTypeIndicies();
+
+  // Abbrev for TYPE_CODE_POINTER.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_POINTER));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+  Abbv->Add(BitCodeAbbrevOp(0));  // Addrspace = 0
+  unsigned PtrAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_FUNCTION.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_FUNCTION));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // isvararg
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned FunctionAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_ANON.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_ANON));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // ispacked
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned StructAnonAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_NAME.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_NAME));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+  unsigned StructNameAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_NAMED.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_NAMED));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // ispacked
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned StructNamedAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_ARRAY.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_ARRAY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // size
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned ArrayAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // don't emit token types for 3.2
+  unsigned int ignored_types = 0;
+  for (const auto& type : TypeList) {
+    if (type->getTypeID() == Type::TokenTyID) {
+      ++ignored_types;
+    }
+  }
+
+  // Emit an entry count so the reader can reserve space.
+  TypeVals.push_back(TypeList.size() - ignored_types);
+  Stream.EmitRecord(bitc::TYPE_CODE_NUMENTRY, TypeVals);
+  TypeVals.clear();
+
+  // Loop over all of the types, emitting each in turn.
+  for (unsigned i = 0, e = TypeList.size(); i != e; ++i) {
+    Type *T = TypeList[i];
+    int AbbrevToUse = 0;
+    unsigned Code = 0;
+
+    // not in 3.2
+    if (T->getTypeID() == Type::TokenTyID) continue;
+
+    switch (T->getTypeID()) {
+    case Type::VoidTyID:      Code = bitc::TYPE_CODE_VOID;      break;
+    case Type::HalfTyID:      Code = bitc::TYPE_CODE_HALF;      break;
+    case Type::FloatTyID:     Code = bitc::TYPE_CODE_FLOAT;     break;
+    case Type::DoubleTyID:    Code = bitc::TYPE_CODE_DOUBLE;    break;
+    case Type::X86_FP80TyID:  Code = bitc::TYPE_CODE_X86_FP80;  break;
+    case Type::FP128TyID:     Code = bitc::TYPE_CODE_FP128;     break;
+    case Type::PPC_FP128TyID: Code = bitc::TYPE_CODE_PPC_FP128; break;
+    case Type::LabelTyID:     Code = bitc::TYPE_CODE_LABEL;     break;
+    case Type::MetadataTyID:  Code = bitc::TYPE_CODE_METADATA;  break;
+    case Type::X86_MMXTyID:   Code = bitc::TYPE_CODE_X86_MMX;   break;
+    case Type::TokenTyID: break; // already handled
+    case Type::IntegerTyID:
+      // INTEGER: [width]
+      Code = bitc::TYPE_CODE_INTEGER;
+      TypeVals.push_back(cast<IntegerType>(T)->getBitWidth());
+      break;
+    case Type::PointerTyID: {
+      PointerType *PTy = cast<PointerType>(T);
+      // POINTER: [pointee type, address space]
+      Code = bitc::TYPE_CODE_POINTER;
+      TypeVals.push_back(VE.getTypeID(PTy->getElementType()));
+      unsigned AddressSpace = PTy->getAddressSpace();
+      TypeVals.push_back(AddressSpace);
+      if (AddressSpace == 0) AbbrevToUse = PtrAbbrev;
+      break;
+    }
+    case Type::FunctionTyID: {
+      FunctionType *FT = cast<FunctionType>(T);
+      // FUNCTION: [isvararg, retty, paramty x N]
+      Code = bitc::TYPE_CODE_FUNCTION;
+      TypeVals.push_back(FT->isVarArg());
+      TypeVals.push_back(VE.getTypeID(FT->getReturnType()));
+      for (unsigned i = 0, e = FT->getNumParams(); i != e; ++i)
+        TypeVals.push_back(VE.getTypeID(FT->getParamType(i)));
+      AbbrevToUse = FunctionAbbrev;
+      break;
+    }
+    case Type::StructTyID: {
+      StructType *ST = cast<StructType>(T);
+      // STRUCT: [ispacked, eltty x N]
+      TypeVals.push_back(ST->isPacked());
+      // Output all of the element types.
+      for (StructType::element_iterator I = ST->element_begin(),
+           E = ST->element_end(); I != E; ++I)
+        TypeVals.push_back(VE.getTypeID(*I));
+
+      if (ST->isLiteral()) {
+        Code = bitc::TYPE_CODE_STRUCT_ANON;
+        AbbrevToUse = StructAnonAbbrev;
+      } else {
+        if (ST->isOpaque()) {
+          Code = bitc::TYPE_CODE_OPAQUE;
+        } else {
+          Code = bitc::TYPE_CODE_STRUCT_NAMED;
+          AbbrevToUse = StructNamedAbbrev;
+        }
+
+        // Emit the name if it is present.
+        if (!ST->getName().empty())
+          WriteStringRecord(bitc::TYPE_CODE_STRUCT_NAME, ST->getName(),
+                            StructNameAbbrev, Stream);
+      }
+      break;
+    }
+    case Type::ArrayTyID: {
+      ArrayType *AT = cast<ArrayType>(T);
+      // ARRAY: [numelts, eltty]
+      Code = bitc::TYPE_CODE_ARRAY;
+      TypeVals.push_back(AT->getNumElements());
+      TypeVals.push_back(VE.getTypeID(AT->getElementType()));
+      AbbrevToUse = ArrayAbbrev;
+      break;
+    }
+    case Type::FixedVectorTyID: {
+      FixedVectorType *VT = cast<FixedVectorType>(T);
+      // VECTOR [numelts, eltty]
+      Code = bitc::TYPE_CODE_VECTOR;
+      TypeVals.push_back(VT->getNumElements());
+      TypeVals.push_back(VE.getTypeID(VT->getElementType()));
+      break;
+    }
+    case Type::ScalableVectorTyID:
+      report_fatal_error("scalar vector types are not supported with LLVM 3.2");
+      break;
+    case Type::BFloatTyID:
+      report_fatal_error("bfloat16 type is not supported with LLVM 3.2");
+      break;
+    case Type::X86_AMXTyID:
+      report_fatal_error("AMX types are not supported with LLVM 3.2");
+      break;
+    }
+
+    // Emit the finished record.
+    Stream.EmitRecord(Code, TypeVals, AbbrevToUse);
+    TypeVals.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static unsigned getEncodedLinkage(const GlobalValue &GV) {
+  switch (GV.getLinkage()) {
+  case GlobalValue::ExternalLinkage:                 return 0;
+  case GlobalValue::WeakAnyLinkage:                  return 1;
+  case GlobalValue::AppendingLinkage:                return 2;
+  case GlobalValue::InternalLinkage:                 return 3;
+  case GlobalValue::LinkOnceAnyLinkage:              return 4;
+  case GlobalValue::ExternalWeakLinkage:             return 7;
+  case GlobalValue::CommonLinkage:                   return 8;
+  case GlobalValue::PrivateLinkage:                  return 9;
+  case GlobalValue::WeakODRLinkage:                  return 10;
+  case GlobalValue::LinkOnceODRLinkage:              return 11;
+  case GlobalValue::AvailableExternallyLinkage:      return 12;
+  }
+  llvm_unreachable("Invalid linkage");
+}
+
+static unsigned getEncodedVisibility(const GlobalValue &GV) {
+  switch (GV.getVisibility()) {
+  case GlobalValue::DefaultVisibility:   return 0;
+  case GlobalValue::HiddenVisibility:    return 1;
+  case GlobalValue::ProtectedVisibility: return 2;
+  }
+  llvm_unreachable("Invalid visibility");
+}
+
+static unsigned getEncodedThreadLocalMode(const GlobalValue &GV) {
+  switch (GV.getThreadLocalMode()) {
+    case GlobalVariable::NotThreadLocal:         return 0;
+    case GlobalVariable::GeneralDynamicTLSModel: return 1;
+    case GlobalVariable::LocalDynamicTLSModel:   return 2;
+    case GlobalVariable::InitialExecTLSModel:    return 3;
+    case GlobalVariable::LocalExecTLSModel:      return 4;
+  }
+  llvm_unreachable("Invalid TLS model");
+}
+
+/// Emit top-level description of module, including target triple, inline asm,
+/// descriptors for global variables, and function prototype info.
+/// Returns the bit offset to backpatch with the location of the real VST.
+static uint64_t WriteModuleInfo(const Module *M, const ValueEnumerator32 &VE,
+                                BitstreamWriter &Stream) {
+  // always override target triple and data layout
+  const bool is_64_bit = (M->getTargetTriple().compare(0, 6, "spir64") == 0);
+  WriteStringRecord(bitc::MODULE_CODE_TRIPLE,
+                    is_64_bit ? SPIR64_TRIPLE : SPIR32_TRIPLE,
+                    0, Stream);
+  WriteStringRecord(bitc::MODULE_CODE_DATALAYOUT,
+                    is_64_bit ? SPIR64_DATALAYOUT : SPIR32_DATALAYOUT,
+                    0, Stream);
+
+  if (!M->getModuleInlineAsm().empty())
+    WriteStringRecord(bitc::MODULE_CODE_ASM, M->getModuleInlineAsm(),
+                      0/*TODO*/, Stream);
+
+  // Emit information about sections and GC, computing how many there are. Also
+  // compute the maximum alignment value.
+  std::map<std::string, unsigned> SectionMap;
+  std::map<std::string, unsigned> GCMap;
+  MaybeAlign MaxAlignment;
+  unsigned MaxGlobalType = 0;
+  const auto UpdateMaxAlignment = [&MaxAlignment](const MaybeAlign A) {
+    if (A)
+      MaxAlignment = !MaxAlignment ? *A : std::max(*MaxAlignment, *A);
+  };
+  for (const GlobalVariable &GV : M->globals()) {
+    UpdateMaxAlignment(GV.getAlign());
+    MaxGlobalType = std::max(MaxGlobalType, VE.getTypeID(GV.getType()));
+    if (GV.hasSection()) {
+      // Give section names unique ID's.
+      unsigned &Entry = SectionMap[std::string(GV.getSection())];
+      if (!Entry) {
+        WriteStringRecord(bitc::MODULE_CODE_SECTIONNAME, GV.getSection(),
+                          0/*TODO*/, Stream);
+        Entry = SectionMap.size();
+      }
+    }
+  }
+  for (const Function &F : *M) {
+    UpdateMaxAlignment(F.getAlign());
+    if (F.hasSection()) {
+      // Give section names unique ID's.
+      unsigned &Entry = SectionMap[std::string(F.getSection())];
+      if (!Entry) {
+        WriteStringRecord(bitc::MODULE_CODE_SECTIONNAME, F.getSection(),
+                          0/*TODO*/, Stream);
+        Entry = SectionMap.size();
+      }
+    }
+    if (F.hasGC()) {
+      // Same for GC names.
+      unsigned &Entry = GCMap[F.getGC()];
+      if (!Entry) {
+        WriteStringRecord(bitc::MODULE_CODE_GCNAME, F.getGC(),
+                          0/*TODO*/, Stream);
+        Entry = GCMap.size();
+      }
+    }
+  }
+
+  // Emit abbrev for globals, now that we know # sections and max alignment.
+  unsigned SimpleGVarAbbrev = 0;
+  if (!M->global_empty()) {
+    // Add an abbrev for common globals with no visibility or thread localness.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::MODULE_CODE_GLOBALVAR));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                              Log2_32_Ceil(MaxGlobalType+1)));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));      // Constant.
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));        // Initializer.
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4));      // Linkage.
+    if (!MaxAlignment)                                         // Alignment.
+      Abbv->Add(BitCodeAbbrevOp(0));
+    else {
+      unsigned MaxEncAlignment = Log2_32(MaxAlignment.getValue().value())+1;
+      Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                               Log2_32_Ceil(MaxEncAlignment+1)));
+    }
+    if (SectionMap.empty())                                    // Section.
+      Abbv->Add(BitCodeAbbrevOp(0));
+    else
+      Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                               Log2_32_Ceil(SectionMap.size()+1)));
+    // Don't bother emitting vis + thread local.
+    SimpleGVarAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  }
+
+  // Emit the global variable information.
+  SmallVector<unsigned, 64> Vals;
+  for (const GlobalVariable &GV : M->globals()) {
+    unsigned AbbrevToUse = 0;
+
+    // GLOBALVAR: [type, isconst, initid,
+    //             linkage, alignment, section, visibility, threadlocal,
+    //             unnamed_addr]
+    Vals.push_back(VE.getTypeID(GV.getType()));
+    Vals.push_back(GV.isConstant());
+    Vals.push_back(GV.isDeclaration() ? 0 :
+                   (VE.getValueID(GV.getInitializer()) + 1));
+    Vals.push_back(getEncodedLinkage(GV));
+    Vals.push_back(Log2_32(GV.getAlignment())+1);
+    Vals.push_back(GV.hasSection() ? SectionMap[std::string(GV.getSection())] : 0);
+    if (GV.isThreadLocal() ||
+        GV.getVisibility() != GlobalValue::DefaultVisibility ||
+        GV.hasGlobalUnnamedAddr()) {
+      Vals.push_back(getEncodedVisibility(GV));
+      Vals.push_back(getEncodedThreadLocalMode(GV));
+      Vals.push_back(GV.hasGlobalUnnamedAddr());
+    } else {
+      AbbrevToUse = SimpleGVarAbbrev;
+    }
+
+    Stream.EmitRecord(bitc::MODULE_CODE_GLOBALVAR, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  // Emit the function proto information.
+  for (const Function &F : *M) {
+    // FUNCTION:  [type, callingconv, isproto, linkage, paramattrs, alignment,
+    //             section, visibility, gc, unnamed_addr]
+    Vals.push_back(VE.getTypeID(F.getType()));
+    Vals.push_back(F.getCallingConv());
+    Vals.push_back(F.isDeclaration());
+    Vals.push_back(getEncodedLinkage(F));
+    Vals.push_back(VE.getAttributeListID(F.getAttributes()));
+    Vals.push_back(Log2_32(F.getAlignment())+1);
+    Vals.push_back(F.hasSection() ? SectionMap[std::string(F.getSection())] : 0);
+    Vals.push_back(getEncodedVisibility(F));
+    Vals.push_back(F.hasGC() ? GCMap[F.getGC()] : 0);
+    Vals.push_back(F.hasGlobalUnnamedAddr());
+
+    unsigned AbbrevToUse = 0;
+    Stream.EmitRecord(bitc::MODULE_CODE_FUNCTION, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  // Emit the alias information.
+  for (const GlobalAlias &A : M->aliases()) {
+    // ALIAS: [alias type, aliasee val#, linkage, visibility]
+    Vals.push_back(VE.getTypeID(A.getType()));
+    Vals.push_back(VE.getValueID(A.getAliasee()));
+    Vals.push_back(getEncodedLinkage(A));
+    Vals.push_back(getEncodedVisibility(A));
+    unsigned AbbrevToUse = 0;
+    Stream.EmitRecord(bitc::MODULE_CODE_ALIAS_OLD, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  return 0;
+}
+
+static uint64_t GetOptimizationFlags(const Value *V) {
+  uint64_t Flags = 0;
+
+  if (const auto *OBO = dyn_cast<OverflowingBinaryOperator>(V)) {
+    if (OBO->hasNoSignedWrap())
+      Flags |= 1 << bitc::OBO_NO_SIGNED_WRAP;
+    if (OBO->hasNoUnsignedWrap())
+      Flags |= 1 << bitc::OBO_NO_UNSIGNED_WRAP;
+  } else if (const auto *PEO = dyn_cast<PossiblyExactOperator>(V)) {
+    if (PEO->isExact())
+      Flags |= 1 << bitc::PEO_EXACT;
+  }
+  // no fast math flags in 3.2
+
+  return Flags;
+}
+
+static void WriteValueAsMetadata(const ValueAsMetadata *MD,
+                                 const ValueEnumerator32 &VE,
+                                 BitstreamWriter &Stream,
+                                 SmallVectorImpl<uint64_t> &Record,
+                                 const bool func_local = false) {
+  // Mimic an MDNode with a value as one operand.
+  Value *V = MD->getValue();
+  Record.push_back(VE.getTypeID(V->getType()));
+  Record.push_back(VE.getValueID(V));
+  Stream.EmitRecord(!func_local ? bitc::METADATA_OLD_NODE : bitc::METADATA_OLD_FN_NODE, Record, 0);
+  Record.clear();
+}
+
+// NOTE: similar to WriteMDNode from 3.2 (however, no function-local in here!)
+static void WriteMDTuple(const MDTuple *N, const ValueEnumerator32 &VE,
+                         BitstreamWriter &Stream,
+                         SmallVectorImpl<uint64_t> &Record, unsigned Abbrev) {
+  for (unsigned i = 0, e = N->getNumOperands(); i != e; ++i) {
+    const Metadata* op = N->getOperand(i);
+    if (op == nullptr) {
+      Record.push_back(VE.getTypeID(Type::getVoidTy(N->getContext())));
+      Record.push_back(0);
+      continue;
+    }
+    
+    switch (op->getMetadataID()) {
+      case Metadata::LocalAsMetadataKind:
+        assert(false && "Unexpected function-local metadata");
+        break;
+      case Metadata::ConstantAsMetadataKind: {
+        auto V = dyn_cast<ConstantAsMetadata>(op)->getValue();
+        Record.push_back(VE.getTypeID(V->getType()));
+        Record.push_back(VE.getValueID(V));
+        break;
+      }
+      default:
+        Record.push_back(VE.getTypeID(Type::getMetadataTy(N->getContext())));
+        Record.push_back(VE.getMetadataID(op));
+        break;
+    }
+  }
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+// DI* helper functions/macros
+static void WriteDI_UNIMPLEMENTED(BitstreamWriter &Stream) {
+  SmallVector<uint64_t, 1> empty_record;
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, empty_record, 0);
+}
+
+#define DI_TYPE_I1() Record.push_back(VE.getTypeID(Type::getInt1Ty(N->getContext())))
+#define DI_I1(val) { DI_TYPE_I1(); Record.push_back(VE.getValueID(ConstantInt::get(llvm::Type::getInt1Ty(N->getContext()), val))); }
+
+#define DI_TYPE_I32() Record.push_back(VE.getTypeID(Type::getInt32Ty(N->getContext())))
+#define DI_I32(val) { DI_TYPE_I32(); Record.push_back(VE.getValueID(ConstantInt::get(llvm::Type::getInt32Ty(N->getContext()), val))); }
+
+#define DI_TYPE_I64() Record.push_back(VE.getTypeID(Type::getInt64Ty(N->getContext())))
+#define DI_I64(val) { DI_TYPE_I64(); Record.push_back(VE.getValueID(ConstantInt::get(llvm::Type::getInt64Ty(N->getContext()), val))); }
+
+#define DI_TYPE_META() Record.push_back(VE.getTypeID(Type::getMetadataTy(N->getContext())))
+#define DI_META(val) { DI_TYPE_META(); Record.push_back(VE.getMetadataID(val)); }
+
+#define DI_TYPE_VOID() Record.push_back(VE.getTypeID(Type::getVoidTy(N->getContext())))
+#define DI_NULL() { DI_TYPE_VOID(); Record.push_back(0); }
+
+#define DI_FUNC(func) { Record.push_back(VE.getTypeID(func->getType())); Record.push_back(VE.getValueID(func)); }
+
+#define DI_TAG(tag) { DI_TYPE_I32(); Record.push_back(VE.getValueID(GetTagConstant(N->getContext(), tag))); }
+
+#define DI_META_OR_NULL(val) if(val) DI_META(val) else DI_NULL()
+
+// from 3.2 DIBuilder.cpp
+static Constant *GetTagConstant(LLVMContext &VMContext, unsigned Tag) {
+  assert((Tag & 0xffff0000 /* LLVMDebugVersionMask */) == 0 &&
+         "Tag too large for debug encoding!");
+  return ConstantInt::get(Type::getInt32Ty(VMContext), Tag | (12 << 16) /* LLVMDebugVersion */);
+}
+
+static void WriteDILocation(const DILocation *N, const ValueEnumerator32 &VE,
+                            BitstreamWriter &Stream,
+                            SmallVectorImpl<uint64_t> &Record,
+                            unsigned Abbrev) {
+  DI_I32(N->getLine());
+  DI_I32(N->getColumn());
+  DI_META(N->getScope());
+  DI_META_OR_NULL(N->getInlinedAt());
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+// NOTE: not in 3.2
+static void WriteGenericDINode(const GenericDINode *,
+                               const ValueEnumerator32 &,
+                               BitstreamWriter &Stream,
+                               SmallVectorImpl<uint64_t> &,
+                               unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+#if 0
+static uint64_t rotateSign(int64_t I) {
+  uint64_t U = I;
+  return I < 0 ? ~(U << 1) : U << 1;
+}
+#endif
+
+static void WriteDISubrange(const DISubrange *N, const ValueEnumerator32 &,
+                            BitstreamWriter &Stream,
+                            SmallVectorImpl<uint64_t> &Record,
+                            unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getCount());
+  Record.push_back(rotateSign(N->getLowerBound()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDIEnumerator(const DIEnumerator *N, const ValueEnumerator32 &VE,
+                              BitstreamWriter &Stream,
+                              SmallVectorImpl<uint64_t> &Record,
+                              unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(rotateSign(N->getValue()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDIBasicType(const DIBasicType *N, const ValueEnumerator32 &VE,
+                             BitstreamWriter &Stream,
+                             SmallVectorImpl<uint64_t> &Record,
+                             unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getEncoding());
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDIDerivedType(const DIDerivedType *N,
+                               const ValueEnumerator32 &VE,
+                               BitstreamWriter &Stream,
+                               SmallVectorImpl<uint64_t> &Record,
+                               unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getBaseType()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getOffsetInBits());
+  Record.push_back(N->getFlags());
+  Record.push_back(VE.getMetadataOrNullID(N->getExtraData()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDICompositeType(const DICompositeType *N,
+                                 const ValueEnumerator32 &VE,
+                                 BitstreamWriter &Stream,
+                                 SmallVectorImpl<uint64_t> &Record,
+                                 unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getBaseType()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getOffsetInBits());
+  Record.push_back(N->getFlags());
+  Record.push_back(VE.getMetadataOrNullID(N->getElements().get()));
+  Record.push_back(N->getRuntimeLang());
+  Record.push_back(VE.getMetadataOrNullID(N->getVTableHolder()));
+  Record.push_back(VE.getMetadataOrNullID(N->getTemplateParams().get()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawIdentifier()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDISubroutineType(const DISubroutineType *N,
+                                  const ValueEnumerator32 &VE,
+                                  BitstreamWriter &Stream,
+                                  SmallVectorImpl<uint64_t> &Record,
+                                  unsigned Abbrev) {
+  DI_TAG(dwarf::DW_TAG_subroutine_type);
+
+  DI_I32(0);
+  DI_NULL();
+  auto empty_str_node = MDString::get(N->getContext(), "");
+  DI_META(empty_str_node);
+  DI_I32(0);
+  DI_I64(0);
+  DI_I64(0);
+  DI_I64(0);
+  DI_I32(N->getFlags());
+  DI_NULL();
+  DI_META_OR_NULL(N->getTypeArray().get());
+  DI_I32(0);
+  DI_NULL();
+  DI_NULL();
+  DI_NULL();
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+static void WriteDIFile(const DIFile *N, const ValueEnumerator32 &VE,
+                        BitstreamWriter &Stream,
+                        SmallVectorImpl<uint64_t> &Record, unsigned Abbrev) {
+  // NOTE: { file, dir } node will already have been written
+  DI_TAG(dwarf::DW_TAG_file_type);
+
+  DI_META(N->contained_node);
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+static void WriteDICompileUnit(const DICompileUnit *N,
+                               const ValueEnumerator32 &VE,
+                               BitstreamWriter &Stream,
+                               SmallVectorImpl<uint64_t> &Record,
+                               unsigned Abbrev) {
+  assert(N->isDistinct() && "Expected distinct compile units");
+
+  DI_TAG(dwarf::DW_TAG_compile_unit);
+
+  if (N->getFile()) {
+    // not an actual DIFile node, but directly points to a { file, dir } node
+    DI_META(N->getFile()->contained_node);
+  } else DI_NULL()
+
+  DI_I32(N->getSourceLanguage());
+  DI_META_OR_NULL(N->getRawProducer());
+  DI_I1(N->isOptimized());
+  DI_META_OR_NULL(N->getRawFlags());
+  DI_I32(N->getRuntimeVersion());
+  DI_META_OR_NULL(N->getEnumTypes().get());
+  DI_META_OR_NULL(N->getRetainedTypes().get());
+  DI_META_OR_NULL(/*N->getSubprograms().get()*/ nullptr); // TODO: fix this, subprogram <-> cu ownership switched in 3.9
+  DI_META_OR_NULL(N->getGlobalVariables().get());
+  DI_META_OR_NULL(N->getImportedEntities().get());
+  DI_META_OR_NULL(N->getRawSplitDebugFilename());
+  DI_I32(N->getEmissionKind());
+  // NOTE: no macros or dwarf id
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+static void WriteDISubprogram(const DISubprogram *N, const ValueEnumerator32 &VE,
+                              BitstreamWriter &Stream,
+                              SmallVectorImpl<uint64_t> &Record,
+                              unsigned Abbrev) {
+  DI_TAG(dwarf::DW_TAG_subprogram);
+
+  DI_META_OR_NULL(N->getFile());
+  DI_META_OR_NULL(N->getScope());
+  DI_META_OR_NULL(N->getRawName());
+  DI_META_OR_NULL(N->getRawName());
+  DI_META_OR_NULL(N->getRawLinkageName());
+  DI_I32(N->getLine());
+  DI_META_OR_NULL(N->getType());
+  DI_I1(N->isLocalToUnit());
+  DI_I1(N->isDefinition());
+  DI_I32(N->getVirtuality());
+  DI_I32(N->getVirtualIndex());
+  DI_META_OR_NULL(N->getContainingType());
+  DI_I32(N->getFlags());
+  DI_I1(N->isOptimized());
+  if (N->associated_function) {
+    DI_FUNC(N->associated_function);
+  }
+  else DI_NULL();
+  DI_META_OR_NULL(N->getTemplateParams().get());
+  DI_META_OR_NULL(N->getDeclaration());
+  // TODO: always pointing to an empty node if non-existent?
+  if (N->getRetainedNodes()) {
+    DI_META(N->getRetainedNodes().get());
+  }
+  else {
+    auto empty_node = MDTuple::getTemporary(N->getContext(), {});
+    DI_META(empty_node.get());
+  }
+  DI_I32(N->getScopeLine());
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+static void WriteDILexicalBlock(const DILexicalBlock *N,
+                                const ValueEnumerator32 &VE,
+                                BitstreamWriter &Stream,
+                                SmallVectorImpl<uint64_t> &Record,
+                                unsigned Abbrev) {
+  DI_TAG(dwarf::DW_TAG_lexical_block);
+
+  static unsigned int unique_id = 0;
+  DI_META_OR_NULL(N->getFile());
+  DI_META_OR_NULL(N->getScope());
+  DI_I32(N->getLine());
+  DI_I32(N->getColumn());
+  DI_I32(0); // NOTE: no discriminator (also 0 in 3.2)
+  DI_I32(unique_id++);
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+}
+
+static void WriteDILexicalBlockFile(const DILexicalBlockFile *N,
+                                    const ValueEnumerator32 &VE,
+                                    BitstreamWriter &Stream,
+                                    SmallVectorImpl<uint64_t> &Record,
+                                    unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getDiscriminator());
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDINamespace(const DINamespace *N, const ValueEnumerator32 &VE,
+                             BitstreamWriter &Stream,
+                             SmallVectorImpl<uint64_t> &Record,
+                             unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(N->getLine());
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+// NOTE: not in 3.2
+static void WriteDIMacro(const DIMacro *, const ValueEnumerator32 &,
+                         BitstreamWriter &Stream,
+                         SmallVectorImpl<uint64_t> &, unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIMacroFile(const DIMacroFile *, const ValueEnumerator32 &,
+                             BitstreamWriter &Stream,
+                             SmallVectorImpl<uint64_t> &,
+                             unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIModule(const DIModule *, const ValueEnumerator32 &,
+                          BitstreamWriter &Stream,
+                          SmallVectorImpl<uint64_t> &, unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDICommonBlock(const DICommonBlock *, const ValueEnumerator32 &,
+                               BitstreamWriter &Stream,
+                               SmallVectorImpl<uint64_t> &,
+                               unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIArgList(const DIArgList *, const ValueEnumerator32 &,
+                           BitstreamWriter &Stream, SmallVectorImpl<uint64_t> &,
+                           unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIStringType(const DIStringType *, const ValueEnumerator32 &,
+                              BitstreamWriter &Stream,
+                              SmallVectorImpl<uint64_t> &, unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIGenericSubrange(const DIGenericSubrange *, const ValueEnumerator32 &,
+                                   BitstreamWriter &Stream,
+                                   SmallVectorImpl<uint64_t> &,
+                                   unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+static void WriteDITemplateTypeParameter(const DITemplateTypeParameter *N,
+                                         const ValueEnumerator32 &VE,
+                                         BitstreamWriter &Stream,
+                                         SmallVectorImpl<uint64_t> &Record,
+                                         unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDITemplateValueParameter(const DITemplateValueParameter *N,
+                                          const ValueEnumerator32 &VE,
+                                          BitstreamWriter &Stream,
+                                          SmallVectorImpl<uint64_t> &Record,
+                                          unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(VE.getMetadataOrNullID(N->getValue()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteDIGlobalVariable(const DIGlobalVariable *N,
+                                  const ValueEnumerator32 &VE,
+                                  BitstreamWriter &Stream,
+                                  SmallVectorImpl<uint64_t> &Record,
+                                  unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawLinkageName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(N->isLocalToUnit());
+  Record.push_back(N->isDefinition());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawVariable()));
+  Record.push_back(VE.getMetadataOrNullID(N->getStaticDataMemberDeclaration()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+// NOTE: not in 3.2
+static void WriteDIGlobalVariableExpression(const DIGlobalVariableExpression *,
+                                            const ValueEnumerator32 &,
+                                            BitstreamWriter &Stream,
+                                            SmallVectorImpl<uint64_t> &,
+                                            unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDILabel(const DILabel *N,
+                         const ValueEnumerator32 &,
+                         BitstreamWriter &Stream,
+                         SmallVectorImpl<uint64_t> &Record,
+                         unsigned Abbrev) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDILocalVariable(const DILocalVariable *,
+                                 const ValueEnumerator32 &,
+                                 BitstreamWriter &Stream,
+                                 SmallVectorImpl<uint64_t> &,
+                                 unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not in 3.2
+static void WriteDIExpression(const DIExpression *, const ValueEnumerator32 &,
+                              BitstreamWriter &Stream,
+                              SmallVectorImpl<uint64_t> &,
+                              unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+// NOTE: not supported, since there is no objective-c
+static void WriteDIObjCProperty(const DIObjCProperty *,
+                                const ValueEnumerator32 &,
+                                BitstreamWriter &Stream,
+                                SmallVectorImpl<uint64_t> &,
+                                unsigned) {
+  WriteDI_UNIMPLEMENTED(Stream);
+}
+
+static void WriteDIImportedEntity(const DIImportedEntity *N,
+                                  const ValueEnumerator32 &VE,
+                                  BitstreamWriter &Stream,
+                                  SmallVectorImpl<uint64_t> &Record,
+                                  unsigned Abbrev) {
+#if 1
+  WriteDI_UNIMPLEMENTED(Stream);
+#else
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getEntity()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+
+  Stream.EmitRecord(bitc::METADATA_OLD_NODE, Record, Abbrev);
+  Record.clear();
+#endif
+}
+
+static void WriteModuleMetadata(const Module *M,
+                                const ValueEnumerator32 &VE,
+                                BitstreamWriter &Stream) {
+  const auto &MDs = VE.getMDs();
+  if (MDs.empty() && M->named_metadata_empty())
+    return;
+
+  // NOTE: always present with AIR/SPIR (no need for StartedMetadataBlock)
+  Stream.EnterSubblock(bitc::METADATA_BLOCK_ID, 3);
+
+  unsigned MDSAbbrev = 0;
+  if (VE.hasMDString()) {
+    // Abbrev for METADATA_STRING_OLD.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_STRING_OLD));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+    MDSAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  }
+
+  SmallVector<uint64_t, 64> Record;
+  for (const Metadata *MD : MDs) {
+    if (const MDNode *N = dyn_cast<MDNode>(MD)) {
+      assert(N->isResolved() && "Expected forward references to be resolved");
+
+      // NOTE: no function-local in here
+      // NOTE: no abbreviations in here
+      switch (N->getMetadataID()) {
+      default:
+        llvm_unreachable("Invalid MDNode subclass");
+#define HANDLE_MDNODE_LEAF(CLASS)                                              \
+  case Metadata::CLASS##Kind:                                                  \
+    Write##CLASS(cast<CLASS>(N), VE, Stream, Record, 0);                       \
+    continue;
+#include "llvm/IR/Metadata.def"
+      }
+    } else if (const auto *MDC = dyn_cast<ConstantAsMetadata>(MD)) {
+      WriteValueAsMetadata(MDC, VE, Stream, Record);
+    } else if (const MDString *MDS = dyn_cast<MDString>(MD)) {
+      // Code: [strchar x N]
+      Record.append(MDS->bytes_begin(), MDS->bytes_end());
+
+      // Emit the finished record.
+      Stream.EmitRecord(bitc::METADATA_STRING_OLD, Record, MDSAbbrev);
+      Record.clear();
+    } else {
+      assert(false && "unhandled MD type");
+    }
+  }
+
+  // Write named metadata.
+  for (const NamedMDNode &NMD : M->named_metadata()) {
+    // Write name.
+    StringRef Str = NMD.getName();
+    Record.append(Str.bytes_begin(), Str.bytes_end());
+    Stream.EmitRecord(bitc::METADATA_NAME, Record, 0);
+    Record.clear();
+
+    // Write named metadata operands.
+    for (const MDNode *N : NMD.operands())
+      Record.push_back(VE.getMetadataID(N));
+    Stream.EmitRecord(bitc::METADATA_NAMED_NODE, Record, 0);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static void WriteFunctionLocalMetadata(const Function &F,
+                                       const ValueEnumerator32 &VE,
+                                       BitstreamWriter &Stream) {
+  bool StartedMetadataBlock = false;
+  SmallVector<uint64_t, 64> Record;
+  const SmallVectorImpl<const LocalAsMetadata *> &MDs =
+      VE.getFunctionLocalMDs();
+  for (unsigned i = 0, e = MDs.size(); i != e; ++i) {
+    assert(MDs[i] && "Expected valid function-local metadata");
+    if (!StartedMetadataBlock) {
+      Stream.EnterSubblock(bitc::METADATA_BLOCK_ID, 3);
+      StartedMetadataBlock = true;
+    }
+    WriteValueAsMetadata(MDs[i], VE, Stream, Record, true);
+  }
+
+  if (StartedMetadataBlock)
+    Stream.ExitBlock();
+}
+
+static void WriteMetadataAttachment(const Function &F,
+                                    const ValueEnumerator32 &VE,
+                                    BitstreamWriter &Stream) {
+  Stream.EnterSubblock(bitc::METADATA_ATTACHMENT_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+
+  // Write metadata attachments
+  // METADATA_ATTACHMENT - [m x [value, [n x [id, mdnode]]]
+  SmallVector<std::pair<unsigned, MDNode *>, 4> MDs;
+
+  for (const BasicBlock &BB : F)
+    for (const Instruction &I : BB) {
+      MDs.clear();
+      I.getAllMetadataOtherThanDebugLoc(MDs);
+
+      // If no metadata, ignore instruction.
+      if (MDs.empty()) continue;
+
+      Record.push_back(VE.getInstructionID(&I));
+
+      for (unsigned i = 0, e = MDs.size(); i != e; ++i) {
+        Record.push_back(MDs[i].first);
+        Record.push_back(VE.getMetadataID(MDs[i].second));
+      }
+      Stream.EmitRecord(bitc::METADATA_ATTACHMENT, Record, 0);
+      Record.clear();
+    }
+
+  Stream.ExitBlock();
+}
+
+static void WriteModuleMetadataStore(const Module *M, BitstreamWriter &Stream) {
+  SmallVector<uint64_t, 64> Record;
+
+  // Write metadata kinds
+  // METADATA_KIND - [n x [id, name]]
+  SmallVector<StringRef, 8> Names;
+  M->getMDKindNames(Names);
+
+  if (Names.empty()) return;
+
+  Stream.EnterSubblock(bitc::METADATA_BLOCK_ID, 3);
+
+  for (unsigned MDKindID = 0, e = Names.size(); MDKindID != e; ++MDKindID) {
+    Record.push_back(MDKindID);
+    StringRef KName = Names[MDKindID];
+    Record.append(KName.begin(), KName.end());
+
+    Stream.EmitRecord(bitc::METADATA_KIND, Record, 0);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static void emitSignedInt64(SmallVectorImpl<uint64_t> &Vals, uint64_t V) {
+  if ((int64_t)V >= 0)
+    Vals.push_back(V << 1);
+  else
+    Vals.push_back((-V << 1) | 1);
+}
+
+static void EmitAPInt(SmallVectorImpl<uint64_t> &Vals,
+                      unsigned &Code, unsigned &AbbrevToUse, const APInt &Val,
+                      bool EmitSizeForWideNumbers = false) {
+  if (Val.getBitWidth() <= 64) {
+    uint64_t V = Val.getSExtValue();
+    emitSignedInt64(Vals, V);
+    Code = bitc::CST_CODE_INTEGER;
+    AbbrevToUse = CONSTANTS_INTEGER_ABBREV;
+  } else {
+    // Wide integers, > 64 bits in size.
+    // We have an arbitrary precision integer value to write whose
+    // bit width is > 64. However, in canonical unsigned integer
+    // format it is likely that the high bits are going to be zero.
+    // So, we only write the number of active words.
+    unsigned NWords = Val.getActiveWords();
+
+    if (EmitSizeForWideNumbers)
+      Vals.push_back(NWords);
+
+    const uint64_t *RawWords = Val.getRawData();
+    for (unsigned i = 0; i != NWords; ++i) {
+      emitSignedInt64(Vals, RawWords[i]);
+    }
+    Code = bitc::CST_CODE_WIDE_INTEGER;
+  }
+}
+
+static void WriteConstants(unsigned FirstVal, unsigned LastVal,
+                           const ValueEnumerator32 &VE,
+                           BitstreamWriter &Stream, bool isGlobal) {
+  if (FirstVal == LastVal) return;
+
+  Stream.EnterSubblock(bitc::CONSTANTS_BLOCK_ID, 4);
+
+  unsigned AggregateAbbrev = 0;
+  unsigned String8Abbrev = 0;
+  unsigned CString7Abbrev = 0;
+  unsigned CString6Abbrev = 0;
+  // If this is a constant pool for the module, emit module-specific abbrevs.
+  if (isGlobal) {
+    // Abbrev for CST_CODE_AGGREGATE.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_AGGREGATE));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, Log2_32_Ceil(LastVal+1)));
+    AggregateAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+    // Abbrev for CST_CODE_STRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_STRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+    String8Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+    // Abbrev for CST_CODE_CSTRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CSTRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7));
+    CString7Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+    // Abbrev for CST_CODE_CSTRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CSTRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    CString6Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+  }
+
+  SmallVector<uint64_t, 64> Record;
+
+  const ValueEnumerator32::ValueList &Vals = VE.getValues();
+  Type *LastTy = nullptr;
+  for (unsigned i = FirstVal; i != LastVal; ++i) {
+    const Value *V = Vals[i].first;
+    // If we need to switch types, do so now.
+    if (V->getType() != LastTy) {
+      LastTy = V->getType();
+      Record.push_back(VE.getTypeID(LastTy));
+      Stream.EmitRecord(bitc::CST_CODE_SETTYPE, Record,
+                        CONSTANTS_SETTYPE_ABBREV);
+      Record.clear();
+    }
+
+    if (const InlineAsm *IA = dyn_cast<InlineAsm>(V)) {
+      Record.push_back(unsigned(IA->hasSideEffects()) |
+                       unsigned(IA->isAlignStack()) << 1 |
+                       unsigned(IA->getDialect()&1) << 2);
+
+      // Add the asm string.
+      const std::string &AsmStr = IA->getAsmString();
+      Record.push_back(AsmStr.size());
+      Record.append(AsmStr.begin(), AsmStr.end());
+
+      // Add the constraint string.
+      const std::string &ConstraintStr = IA->getConstraintString();
+      Record.push_back(ConstraintStr.size());
+      Record.append(ConstraintStr.begin(), ConstraintStr.end());
+      Stream.EmitRecord(bitc::CST_CODE_INLINEASM_OLD2, Record);
+      Record.clear();
+      continue;
+    }
+    const Constant *C = cast<Constant>(V);
+    unsigned Code = -1U;
+    unsigned AbbrevToUse = 0;
+    if (C->isNullValue()) {
+      Code = bitc::CST_CODE_NULL;
+    } else if (isa<UndefValue>(C)) {
+      Code = bitc::CST_CODE_UNDEF;
+    } else if (const ConstantInt *IV = dyn_cast<ConstantInt>(C)) {
+      EmitAPInt(Record, Code, AbbrevToUse, IV->getValue());
+    } else if (const ConstantFP *CFP = dyn_cast<ConstantFP>(C)) {
+      Code = bitc::CST_CODE_FLOAT;
+      Type *Ty = CFP->getType();
+      if (Ty->isHalfTy() || Ty->isFloatTy() || Ty->isDoubleTy()) {
+        Record.push_back(CFP->getValueAPF().bitcastToAPInt().getZExtValue());
+      } else if (Ty->isX86_FP80Ty()) {
+        // api needed to prevent premature destruction
+        // bits are not in the same order as a normal i80 APInt, compensate.
+        APInt api = CFP->getValueAPF().bitcastToAPInt();
+        const uint64_t *p = api.getRawData();
+        Record.push_back((p[1] << 48) | (p[0] >> 16));
+        Record.push_back(p[0] & 0xffffLL);
+      } else if (Ty->isFP128Ty() || Ty->isPPC_FP128Ty()) {
+        APInt api = CFP->getValueAPF().bitcastToAPInt();
+        const uint64_t *p = api.getRawData();
+        Record.push_back(p[0]);
+        Record.push_back(p[1]);
+      } else {
+        assert (0 && "Unknown FP type!");
+      }
+    } else if (isa<ConstantDataSequential>(C) &&
+               cast<ConstantDataSequential>(C)->isString()) {
+      const ConstantDataSequential *Str = cast<ConstantDataSequential>(C);
+      // Emit constant strings specially.
+      unsigned NumElts = Str->getNumElements();
+      // If this is a null-terminated string, use the denser CSTRING encoding.
+      if (Str->isCString()) {
+        Code = bitc::CST_CODE_CSTRING;
+        --NumElts;  // Don't encode the null, which isn't allowed by char6.
+      } else {
+        Code = bitc::CST_CODE_STRING;
+        AbbrevToUse = String8Abbrev;
+      }
+      bool isCStr7 = Code == bitc::CST_CODE_CSTRING;
+      bool isCStrChar6 = Code == bitc::CST_CODE_CSTRING;
+      for (unsigned i = 0; i != NumElts; ++i) {
+        unsigned char V = Str->getElementAsInteger(i);
+        Record.push_back(V);
+        isCStr7 &= (V & 128) == 0;
+        if (isCStrChar6)
+          isCStrChar6 = BitCodeAbbrevOp::isChar6(V);
+      }
+
+      if (isCStrChar6)
+        AbbrevToUse = CString6Abbrev;
+      else if (isCStr7)
+        AbbrevToUse = CString7Abbrev;
+    } else if (const ConstantDataSequential *CDS =
+                  dyn_cast<ConstantDataSequential>(C)) {
+      Code = bitc::CST_CODE_DATA;
+      Type *EltTy = CDS->getElementType();
+      if (isa<IntegerType>(EltTy)) {
+        for (unsigned i = 0, e = CDS->getNumElements(); i != e; ++i)
+          Record.push_back(CDS->getElementAsInteger(i));
+      } else if (EltTy->isFloatTy()) {
+        for (unsigned i = 0, e = CDS->getNumElements(); i != e; ++i) {
+          union { float F; uint32_t I; };
+          F = CDS->getElementAsFloat(i);
+          Record.push_back(I);
+        }
+      } else {
+        assert(EltTy->isDoubleTy() && "Unknown ConstantData element type");
+        for (unsigned i = 0, e = CDS->getNumElements(); i != e; ++i) {
+          union { double F; uint64_t I; };
+          F = CDS->getElementAsDouble(i);
+          Record.push_back(I);
+        }
+      }
+    } else if (isa<ConstantArray>(C) || isa<ConstantStruct>(C) ||
+               isa<ConstantVector>(C)) {
+      Code = bitc::CST_CODE_AGGREGATE;
+      for (const Value *Op : C->operands())
+        Record.push_back(VE.getValueID(Op));
+      AbbrevToUse = AggregateAbbrev;
+    } else if (const ConstantExpr *CE = dyn_cast<ConstantExpr>(C)) {
+      switch (CE->getOpcode()) {
+      default:
+        if (Instruction::isCast(CE->getOpcode())) {
+          Code = bitc::CST_CODE_CE_CAST;
+          Record.push_back(GetEncodedCastOpcode(CE->getOpcode()));
+          Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+          Record.push_back(VE.getValueID(C->getOperand(0)));
+          AbbrevToUse = CONSTANTS_CE_CAST_Abbrev;
+        } else {
+          assert(CE->getNumOperands() == 2 && "Unknown constant expr!");
+          Code = bitc::CST_CODE_CE_BINOP;
+          Record.push_back(GetEncodedBinaryOpcode(CE->getOpcode()));
+          Record.push_back(VE.getValueID(C->getOperand(0)));
+          Record.push_back(VE.getValueID(C->getOperand(1)));
+          uint64_t Flags = GetOptimizationFlags(CE);
+          if (Flags != 0)
+            Record.push_back(Flags);
+        }
+        break;
+      case Instruction::GetElementPtr: {
+        Code = bitc::CST_CODE_CE_GEP;
+        const auto *GO = cast<GEPOperator>(C);
+        if (GO->isInBounds())
+          Code = bitc::CST_CODE_CE_INBOUNDS_GEP;
+        for (unsigned i = 0, e = CE->getNumOperands(); i != e; ++i) {
+          Record.push_back(VE.getTypeID(C->getOperand(i)->getType()));
+          Record.push_back(VE.getValueID(C->getOperand(i)));
+        }
+        break;
+      }
+      case Instruction::Select:
+        Code = bitc::CST_CODE_CE_SELECT;
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ExtractElement:
+        Code = bitc::CST_CODE_CE_EXTRACTELT;
+        Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        break;
+      case Instruction::InsertElement:
+        Code = bitc::CST_CODE_CE_INSERTELT;
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ShuffleVector:
+        // If the return type and argument types are the same, this is a
+        // standard shufflevector instruction.  If the types are different,
+        // then the shuffle is widening or truncating the input vectors, and
+        // the argument type must also be encoded.
+        if (C->getType() == C->getOperand(0)->getType()) {
+          Code = bitc::CST_CODE_CE_SHUFFLEVEC;
+        } else {
+          Code = bitc::CST_CODE_CE_SHUFVEC_EX;
+          Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        }
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ICmp:
+      case Instruction::FCmp:
+        Code = bitc::CST_CODE_CE_CMP;
+        Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(CE->getPredicate());
+        break;
+      }
+    } else if (const BlockAddress *BA = dyn_cast<BlockAddress>(C)) {
+      Code = bitc::CST_CODE_BLOCKADDRESS;
+      Record.push_back(VE.getTypeID(BA->getFunction()->getType()));
+      Record.push_back(VE.getValueID(BA->getFunction()));
+      Record.push_back(VE.getGlobalBasicBlockID(BA->getBasicBlock()));
+    } else {
+#ifndef NDEBUG
+      C->dump();
+#endif
+      llvm_unreachable("Unknown constant!");
+    }
+    Stream.EmitRecord(Code, Record, AbbrevToUse);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static void WriteModuleConstants(const ValueEnumerator32 &VE,
+                                 BitstreamWriter &Stream) {
+  const ValueEnumerator32::ValueList &Vals = VE.getValues();
+
+  // Find the first constant to emit, which is the first non-globalvalue value.
+  // We know globalvalues have been emitted by WriteModuleInfo.
+  for (unsigned i = 0, e = Vals.size(); i != e; ++i) {
+    if (!isa<GlobalValue>(Vals[i].first)) {
+      WriteConstants(i, Vals.size(), VE, Stream, true);
+      return;
+    }
+  }
+}
+
+/// PushValueAndType - The file has to encode both the value and type id for
+/// many values, because we need to know what type to create for forward
+/// references.  However, most operands are not forward references, so this type
+/// field is not needed.
+///
+/// This function adds V's value ID to Vals.  If the value ID is higher than the
+/// instruction ID, then it is a forward reference, and it also includes the
+/// type ID.  The value ID that is written is encoded relative to the InstID.
+static bool PushValueAndType(const Value *V, unsigned InstID,
+                             SmallVectorImpl<unsigned> &Vals,
+                             ValueEnumerator32 &VE) {
+  unsigned ValID = VE.getValueID(V);
+  // Make encoding relative to the InstID.
+  Vals.push_back(InstID - ValID);
+  if (ValID >= InstID) {
+    Vals.push_back(VE.getTypeID(V->getType()));
+    return true;
+  }
+  return false;
+}
+
+/// pushValue - Like PushValueAndType, but where the type of the value is
+/// omitted (perhaps it was already encoded in an earlier operand).
+static void pushValue(const Value *V, unsigned InstID,
+                      SmallVectorImpl<unsigned> &Vals,
+                      ValueEnumerator32 &VE) {
+  unsigned ValID = VE.getValueID(V);
+  Vals.push_back(InstID - ValID);
+}
+
+static void pushValue64(const Value *V, unsigned InstID,
+                        SmallVector<uint64_t, 128> &Vals,
+                        ValueEnumerator32 &VE) {
+  uint64_t ValID = VE.getValueID(V);
+  Vals.push_back(InstID - ValID);
+}
+
+static void pushValueSigned(const Value *V, unsigned InstID,
+                            SmallVectorImpl<uint64_t> &Vals,
+                            ValueEnumerator32 &VE) {
+  unsigned ValID = VE.getValueID(V);
+  int64_t diff = ((int32_t)InstID - (int32_t)ValID);
+  emitSignedInt64(Vals, diff);
+}
+
+/// WriteInstruction - Emit an instruction to the specified stream.
+static void WriteInstruction(const Instruction &I, unsigned InstID,
+                             ValueEnumerator32 &VE, BitstreamWriter &Stream,
+                             SmallVectorImpl<unsigned> &Vals) {
+  unsigned Code = 0;
+  unsigned AbbrevToUse = 0;
+  VE.setInstructionID(&I);
+  switch (I.getOpcode()) {
+  default:
+    if (Instruction::isCast(I.getOpcode())) {
+      Code = bitc::FUNC_CODE_INST_CAST;
+      if (!PushValueAndType(I.getOperand(0), InstID, Vals, VE))
+        AbbrevToUse = FUNCTION_INST_CAST_ABBREV;
+      Vals.push_back(VE.getTypeID(I.getType()));
+      Vals.push_back(GetEncodedCastOpcode(I.getOpcode()));
+    } else {
+      assert(isa<BinaryOperator>(I) && "Unknown instruction!");
+      Code = bitc::FUNC_CODE_INST_BINOP;
+      if (!PushValueAndType(I.getOperand(0), InstID, Vals, VE))
+        AbbrevToUse = FUNCTION_INST_BINOP_ABBREV;
+      pushValue(I.getOperand(1), InstID, Vals, VE);
+      Vals.push_back(GetEncodedBinaryOpcode(I.getOpcode()));
+      uint64_t Flags = GetOptimizationFlags(&I);
+      if (Flags != 0) {
+        if (AbbrevToUse == FUNCTION_INST_BINOP_ABBREV)
+          AbbrevToUse = FUNCTION_INST_BINOP_FLAGS_ABBREV;
+        Vals.push_back(Flags);
+      }
+    }
+    break;
+
+  case Instruction::FNeg: {
+    // emit as "fsub -0, value"
+    Code = bitc::FUNC_CODE_INST_BINOP;
+    pushValue(ConstantFP::get(I.getOperand(0)->getType(), -0.0), InstID, Vals, VE);
+    if (!PushValueAndType(I.getOperand(0), InstID, Vals, VE))
+      AbbrevToUse = FUNCTION_INST_BINOP_ABBREV;
+    Vals.push_back(GetEncodedBinaryOpcode(Instruction::FSub));
+    uint64_t Flags = GetOptimizationFlags(&I);
+    if (Flags != 0) {
+      if (AbbrevToUse == FUNCTION_INST_BINOP_ABBREV)
+        AbbrevToUse = FUNCTION_INST_BINOP_FLAGS_ABBREV;
+      Vals.push_back(Flags);
+    }
+    break;
+  }
+  case Instruction::GetElementPtr: {
+    Code = bitc::FUNC_CODE_INST_GEP_OLD;
+    //AbbrevToUse = FUNCTION_INST_GEP_ABBREV;
+    auto &GEPInst = cast<GetElementPtrInst>(I);
+    if (GEPInst.isInBounds())
+      Code = bitc::FUNC_CODE_INST_INBOUNDS_GEP_OLD;
+    for (unsigned i = 0, e = I.getNumOperands(); i != e; ++i)
+      PushValueAndType(I.getOperand(i), InstID, Vals, VE);
+    break;
+  }
+  case Instruction::ExtractValue: {
+    Code = bitc::FUNC_CODE_INST_EXTRACTVAL;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    const ExtractValueInst *EVI = cast<ExtractValueInst>(&I);
+    Vals.append(EVI->idx_begin(), EVI->idx_end());
+    break;
+  }
+  case Instruction::InsertValue: {
+    Code = bitc::FUNC_CODE_INST_INSERTVAL;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    PushValueAndType(I.getOperand(1), InstID, Vals, VE);
+    const InsertValueInst *IVI = cast<InsertValueInst>(&I);
+    Vals.append(IVI->idx_begin(), IVI->idx_end());
+    break;
+  }
+  case Instruction::Select:
+    Code = bitc::FUNC_CODE_INST_VSELECT;
+    PushValueAndType(I.getOperand(1), InstID, Vals, VE);
+    pushValue(I.getOperand(2), InstID, Vals, VE);
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    break;
+  case Instruction::ExtractElement:
+    Code = bitc::FUNC_CODE_INST_EXTRACTELT;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    pushValue(I.getOperand(1), InstID, Vals, VE);
+    break;
+  case Instruction::InsertElement:
+    Code = bitc::FUNC_CODE_INST_INSERTELT;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    pushValue(I.getOperand(1), InstID, Vals, VE);
+    pushValue(I.getOperand(2), InstID, Vals, VE);
+    break;
+  case Instruction::ShuffleVector:
+    Code = bitc::FUNC_CODE_INST_SHUFFLEVEC;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    pushValue(I.getOperand(1), InstID, Vals, VE);
+    //pushValue(I.getOperand(2), InstID, Vals, VE);
+    pushValue(cast<ShuffleVectorInst>(I).getShuffleMaskForBitcode(), InstID,
+              Vals, VE);
+    break;
+  case Instruction::ICmp:
+  case Instruction::FCmp: {
+    // compare returning Int1Ty or vector of Int1Ty
+    Code = bitc::FUNC_CODE_INST_CMP2;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    pushValue(I.getOperand(1), InstID, Vals, VE);
+    Vals.push_back(cast<CmpInst>(I).getPredicate());
+    break;
+  }
+
+  case Instruction::Ret:
+    {
+      Code = bitc::FUNC_CODE_INST_RET;
+      unsigned NumOperands = I.getNumOperands();
+      if (NumOperands == 0)
+        AbbrevToUse = FUNCTION_INST_RET_VOID_ABBREV;
+      else if (NumOperands == 1) {
+        if (!PushValueAndType(I.getOperand(0), InstID, Vals, VE))
+          AbbrevToUse = FUNCTION_INST_RET_VAL_ABBREV;
+      } else {
+        for (unsigned i = 0, e = NumOperands; i != e; ++i)
+          PushValueAndType(I.getOperand(i), InstID, Vals, VE);
+      }
+    }
+    break;
+  case Instruction::Br:
+    {
+      Code = bitc::FUNC_CODE_INST_BR;
+      const BranchInst &II = cast<BranchInst>(I);
+      Vals.push_back(VE.getValueID(II.getSuccessor(0)));
+      if (II.isConditional()) {
+        Vals.push_back(VE.getValueID(II.getSuccessor(1)));
+        pushValue(II.getCondition(), InstID, Vals, VE);
+      }
+    }
+    break;
+  case Instruction::Switch:
+    {
+      // Redefine Vals, since here we need to use 64 bit values
+      // explicitly to store large APInt numbers.
+      SmallVector<uint64_t, 128> Vals64;
+
+      Code = bitc::FUNC_CODE_INST_SWITCH;
+      const SwitchInst &SI = cast<SwitchInst>(I);
+
+      // Compute hash (LLVM 3.2 SwitchInst::Hash)
+      uint32_t NumberOfCases = (uint32_t)SI.getNumCases();
+      uint16_t Hash = (0xFFFF & NumberOfCases) ^ (NumberOfCases >> 16);
+      for (SwitchInst::ConstCaseIt i = SI.case_begin(), e = SI.case_end();
+           i != e; ++i) {
+        uint32_t NumItems = 1;
+        Hash = (Hash << 1) ^ (0xFFFF & NumItems) ^ (NumItems >> 16);
+      }
+
+      uint32_t SwitchRecordHeader = Hash | (SWITCH_INST_MAGIC << 16);
+      Vals64.push_back(SwitchRecordHeader);
+
+      Vals64.push_back(VE.getTypeID(SI.getCondition()->getType()));
+      pushValue64(SI.getCondition(), InstID, Vals64, VE);
+      Vals64.push_back(VE.getValueID(SI.getDefaultDest()));
+      Vals64.push_back(SI.getNumCases());
+      for (auto Case : SI.cases()) {
+        unsigned Code, Abbrev; // will unused.
+        Vals64.push_back(1/*NumItems = 1*/);
+        Vals64.push_back(true/*IsSingleNumber = true*/);
+        EmitAPInt(Vals64, Code, Abbrev, Case.getCaseValue()->getValue(), true);
+        Vals64.push_back(VE.getValueID(Case.getCaseSuccessor()));
+      }
+
+      Stream.EmitRecord(Code, Vals64, AbbrevToUse);
+
+      // Also do expected action - clear external Vals collection:
+      Vals.clear();
+      return;
+    }
+    break;
+  case Instruction::IndirectBr:
+    Code = bitc::FUNC_CODE_INST_INDIRECTBR;
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));
+    // Encode the address operand as relative, but not the basic blocks.
+    pushValue(I.getOperand(0), InstID, Vals, VE);
+    for (unsigned i = 1, e = I.getNumOperands(); i != e; ++i)
+      Vals.push_back(VE.getValueID(I.getOperand(i)));
+    break;
+
+  case Instruction::Invoke: {
+    const InvokeInst *II = cast<InvokeInst>(&I);
+    const Value *Callee = II->getCalledOperand();
+    FunctionType *FTy = II->getFunctionType();
+
+    Code = bitc::FUNC_CODE_INST_INVOKE;
+
+    Vals.push_back(VE.getAttributeListID(II->getAttributes()));
+    Vals.push_back(II->getCallingConv());
+    Vals.push_back(VE.getValueID(II->getNormalDest()));
+    Vals.push_back(VE.getValueID(II->getUnwindDest()));
+    PushValueAndType(Callee, InstID, Vals, VE);
+
+    // Emit value #'s for the fixed parameters.
+    for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i)
+      pushValue(I.getOperand(i), InstID, Vals, VE);  // fixed param.
+
+    // Emit type/value pairs for varargs params.
+    if (FTy->isVarArg()) {
+      for (unsigned i = FTy->getNumParams(), e = I.getNumOperands()-3;
+           i != e; ++i)
+        PushValueAndType(I.getOperand(i), InstID, Vals, VE); // vararg
+    }
+    break;
+  }
+  case Instruction::Resume:
+    Code = bitc::FUNC_CODE_INST_RESUME;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    break;
+  case Instruction::CleanupRet:
+  case Instruction::CatchRet:
+  case Instruction::CleanupPad:
+  case Instruction::CatchPad:
+  case Instruction::CatchSwitch:
+    // all unsupported in 3.2
+    assert(false && "encountered unsupported instruction (these need to be filtered out before writing 3.2 bitcode)");
+    return;
+  case Instruction::Unreachable:
+    Code = bitc::FUNC_CODE_INST_UNREACHABLE;
+    AbbrevToUse = FUNCTION_INST_UNREACHABLE_ABBREV;
+    break;
+
+  case Instruction::PHI: {
+    const PHINode &PN = cast<PHINode>(I);
+    Code = bitc::FUNC_CODE_INST_PHI;
+    // With the newer instruction encoding, forward references could give
+    // negative valued IDs.  This is most common for PHIs, so we use
+    // signed VBRs.
+    SmallVector<uint64_t, 128> Vals64;
+    Vals64.push_back(VE.getTypeID(PN.getType()));
+    for (unsigned i = 0, e = PN.getNumIncomingValues(); i != e; ++i) {
+      pushValueSigned(PN.getIncomingValue(i), InstID, Vals64, VE);
+      Vals64.push_back(VE.getValueID(PN.getIncomingBlock(i)));
+    }
+    // Emit a Vals64 vector and exit.
+    Stream.EmitRecord(Code, Vals64, AbbrevToUse);
+    Vals64.clear();
+    return;
+  }
+
+  case Instruction::LandingPad: {
+    const LandingPadInst &LP = cast<LandingPadInst>(I);
+    Code = bitc::FUNC_CODE_INST_LANDINGPAD_OLD;
+    Vals.push_back(VE.getTypeID(LP.getType()));
+    PushValueAndType(LP.getFunction()->getPersonalityFn(), InstID, Vals, VE);
+    Vals.push_back(LP.isCleanup());
+    Vals.push_back(LP.getNumClauses());
+    for (unsigned I = 0, E = LP.getNumClauses(); I != E; ++I) {
+      if (LP.isCatch(I))
+        Vals.push_back(LandingPadInst::Catch);
+      else
+        Vals.push_back(LandingPadInst::Filter);
+      PushValueAndType(LP.getClause(I), InstID, Vals, VE);
+    }
+    break;
+  }
+
+  case Instruction::Alloca: {
+    Code = bitc::FUNC_CODE_INST_ALLOCA;
+    const AllocaInst &AI = cast<AllocaInst>(I);
+    Vals.push_back(VE.getTypeID(I.getType()));
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));
+    Vals.push_back(VE.getValueID(I.getOperand(0))); // size.
+    unsigned AlignRecord = Log2_32(AI.getAlignment()) + 1;
+    assert(Log2_32(AI.getAlignment()) + 1 < 1 << 5 &&
+           "not enough bits for maximum alignment");
+    assert(AlignRecord < 1 << 5 && "alignment greater than 1 << 64");
+    Vals.push_back(AlignRecord);
+    break;
+  }
+
+  case Instruction::Load:
+    if (cast<LoadInst>(I).isAtomic()) {
+      Code = bitc::FUNC_CODE_INST_LOADATOMIC;
+      PushValueAndType(I.getOperand(0), InstID, Vals, VE);
+    } else {
+      Code = bitc::FUNC_CODE_INST_LOAD;
+      if (!PushValueAndType(I.getOperand(0), InstID, Vals, VE))  // ptr
+        AbbrevToUse = FUNCTION_INST_LOAD_ABBREV;
+    }
+    Vals.push_back(Log2_32(cast<LoadInst>(I).getAlignment())+1);
+    Vals.push_back(cast<LoadInst>(I).isVolatile());
+    if (cast<LoadInst>(I).isAtomic()) {
+      Vals.push_back(GetEncodedOrdering(cast<LoadInst>(I).getOrdering()));
+      Vals.push_back(GetEncodedSynchScope(cast<LoadInst>(I).getSyncScopeID()));
+    }
+    break;
+  case Instruction::Store:
+    if (cast<StoreInst>(I).isAtomic())
+      Code = bitc::FUNC_CODE_INST_STOREATOMIC_OLD;
+    else
+      Code = bitc::FUNC_CODE_INST_STORE_OLD;
+    PushValueAndType(I.getOperand(1), InstID, Vals, VE);  // ptrty + ptr
+    pushValue(I.getOperand(0), InstID, Vals, VE);         // val.
+    Vals.push_back(Log2_32(cast<StoreInst>(I).getAlignment())+1);
+    Vals.push_back(cast<StoreInst>(I).isVolatile());
+    if (cast<StoreInst>(I).isAtomic()) {
+      Vals.push_back(GetEncodedOrdering(cast<StoreInst>(I).getOrdering()));
+      Vals.push_back(GetEncodedSynchScope(cast<StoreInst>(I).getSyncScopeID()));
+    }
+    break;
+  case Instruction::AtomicCmpXchg:
+    Code = bitc::FUNC_CODE_INST_CMPXCHG_OLD;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);  // ptrty + ptr
+    pushValue(I.getOperand(1), InstID, Vals, VE);         // cmp.
+    pushValue(I.getOperand(2), InstID, Vals, VE);         // newval.
+    Vals.push_back(cast<AtomicCmpXchgInst>(I).isVolatile());
+    Vals.push_back(GetEncodedOrdering(
+                     cast<AtomicCmpXchgInst>(I).getSuccessOrdering()));
+    Vals.push_back(GetEncodedSynchScope(
+                     cast<AtomicCmpXchgInst>(I).getSyncScopeID()));
+    break;
+  case Instruction::AtomicRMW:
+    Code = bitc::FUNC_CODE_INST_ATOMICRMW_OLD;
+    PushValueAndType(I.getOperand(0), InstID, Vals, VE);  // ptrty + ptr
+    pushValue(I.getOperand(1), InstID, Vals, VE);         // val.
+    Vals.push_back(GetEncodedRMWOperation(
+                     cast<AtomicRMWInst>(I).getOperation()));
+    Vals.push_back(cast<AtomicRMWInst>(I).isVolatile());
+    Vals.push_back(GetEncodedOrdering(cast<AtomicRMWInst>(I).getOrdering()));
+    Vals.push_back(GetEncodedSynchScope(
+                     cast<AtomicRMWInst>(I).getSyncScopeID()));
+    break;
+  case Instruction::Fence:
+    Code = bitc::FUNC_CODE_INST_FENCE;
+    Vals.push_back(GetEncodedOrdering(cast<FenceInst>(I).getOrdering()));
+    Vals.push_back(GetEncodedSynchScope(cast<FenceInst>(I).getSyncScopeID()));
+    break;
+  case Instruction::Call: {
+    const CallInst &CI = cast<CallInst>(I);
+    FunctionType *FTy = CI.getFunctionType();
+
+    Code = bitc::FUNC_CODE_INST_CALL;
+
+    Vals.push_back(VE.getAttributeListID(CI.getAttributes()));
+    Vals.push_back(CI.getCallingConv() << bitc::CALL_CCONV |
+                   unsigned(CI.isTailCall()) << bitc::CALL_TAIL);
+
+    PushValueAndType(CI.getCalledOperand(), InstID, Vals, VE);  // Callee
+
+    // Emit value #'s for the fixed parameters.
+    for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i) {
+      // Check for labels (can happen with asm labels).
+      if (FTy->getParamType(i)->isLabelTy())
+        Vals.push_back(VE.getValueID(CI.getArgOperand(i)));
+      else
+        pushValue(CI.getArgOperand(i), InstID, Vals, VE);  // fixed param.
+    }
+
+    // Emit type/value pairs for varargs params.
+    if (FTy->isVarArg()) {
+      for (unsigned i = FTy->getNumParams(), e = CI.arg_size(); i != e; ++i)
+        PushValueAndType(CI.getArgOperand(i), InstID, Vals, VE);  // varargs
+    }
+    break;
+  }
+  case Instruction::VAArg:
+    Code = bitc::FUNC_CODE_INST_VAARG;
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));   // valistty
+    pushValue(I.getOperand(0), InstID, Vals, VE); // valist.
+    Vals.push_back(VE.getTypeID(I.getType())); // restype.
+    break;
+  case Instruction::Freeze: {
+    // freeze instruction is not supported by LLVM 3.2,
+    // but we can more or less emulate it as an identity function
+    // -> encode as a bitcast to the same type
+    auto Operand = I.getOperand(0);
+    Code = bitc::FUNC_CODE_INST_CAST;
+    if (!PushValueAndType(Operand, InstID, Vals, VE))
+      AbbrevToUse = FUNCTION_INST_CAST_ABBREV;
+    Vals.push_back(VE.getTypeID(Operand->getType()));
+    Vals.push_back(GetEncodedCastOpcode(Instruction::BitCast));
+    break;
+  }
+  case Instruction::CallBr:
+    report_fatal_error("can not encode CallBr instruction for LLVM 3.2");
+    break;
+  }
+
+  Stream.EmitRecord(Code, Vals, AbbrevToUse);
+  Vals.clear();
+}
+
+enum StringEncoding { SE_Char6, SE_Fixed7, SE_Fixed8 };
+
+/// Determine the encoding to use for the given string name and length.
+static StringEncoding getStringEncoding(const char *Str, unsigned StrLen) {
+  bool isChar6 = true;
+  for (const char *C = Str, *E = C + StrLen; C != E; ++C) {
+    if (isChar6)
+      isChar6 = BitCodeAbbrevOp::isChar6(*C);
+    if ((unsigned char)*C & 128)
+      // don't bother scanning the rest.
+      return SE_Fixed8;
+  }
+  if (isChar6)
+    return SE_Char6;
+  else
+    return SE_Fixed7;
+}
+
+/// Emit names for globals/functions etc. The VSTOffsetPlaceholder,
+/// BitcodeStartBit and FunctionIndex are only passed for the module-level
+/// VST, where we are including a function bitcode index and need to
+/// backpatch the VST forward declaration record.
+static void WriteValueSymbolTable(
+    const ValueSymbolTable &VST, const ValueEnumerator32 &VE,
+    BitstreamWriter &Stream, uint64_t VSTOffsetPlaceholder = 0,
+    uint64_t BitcodeStartBit = 0
+// TODO: is this necessary? handle it?
+// -> new one: DenseMap<const Function *, uint64_t> *FunctionToBitcodeIndex
+    /*DenseMap<const Function *, std::unique_ptr<FunctionInfo>> *FunctionIndex =
+        nullptr*/) {
+  if (VST.empty()) {
+    // WriteValueSymbolTableForwardDecl should have returned early as
+    // well. Ensure this handling remains in sync by asserting that
+    // the placeholder offset is not set.
+    assert(VSTOffsetPlaceholder == 0);
+    return;
+  }
+
+  Stream.EnterSubblock(bitc::VALUE_SYMTAB_BLOCK_ID, 4);
+
+  // FIXME: Set up the abbrev, we know how many values there are!
+  // FIXME: We know if the type names can use 7-bit ascii.
+  SmallVector<unsigned, 64> NameVals;
+
+  for (const ValueName &Name : VST) {
+    // Figure out the encoding to use for the name.
+    StringEncoding Bits =
+        getStringEncoding(Name.getKeyData(), Name.getKeyLength());
+
+    unsigned AbbrevToUse = VST_ENTRY_8_ABBREV;
+    NameVals.push_back(VE.getValueID(Name.getValue()));
+
+    // VST_ENTRY:   [valueid, namechar x N]
+    // VST_BBENTRY: [bbid, namechar x N]
+    unsigned Code;
+    if (isa<BasicBlock>(Name.getValue())) {
+      Code = bitc::VST_CODE_BBENTRY;
+      if (Bits == SE_Char6)
+        AbbrevToUse = VST_BBENTRY_6_ABBREV;
+    } else {
+      Code = bitc::VST_CODE_ENTRY;
+      if (Bits == SE_Char6)
+        AbbrevToUse = VST_ENTRY_6_ABBREV;
+      else if (Bits == SE_Fixed7)
+        AbbrevToUse = VST_ENTRY_7_ABBREV;
+    }
+
+    for (const auto P : Name.getKey())
+      NameVals.push_back((unsigned char)P);
+
+    // Emit the finished record.
+    Stream.EmitRecord(Code, NameVals, AbbrevToUse);
+    NameVals.clear();
+  }
+  Stream.ExitBlock();
+}
+
+/// Emit a function body to the module stream.
+static void WriteFunction(
+    const Function &F, ValueEnumerator32 &VE, BitstreamWriter &Stream) {
+  Stream.EnterSubblock(bitc::FUNCTION_BLOCK_ID, 4);
+  VE.incorporateFunction(F);
+
+  SmallVector<unsigned, 64> Vals;
+
+  // Emit the number of basic blocks, so the reader can create them ahead of
+  // time.
+  Vals.push_back(VE.getBasicBlocks().size());
+  Stream.EmitRecord(bitc::FUNC_CODE_DECLAREBLOCKS, Vals);
+  Vals.clear();
+
+  // If there are function-local constants, emit them now.
+  unsigned CstStart, CstEnd;
+  VE.getFunctionConstantRange(CstStart, CstEnd);
+  WriteConstants(CstStart, CstEnd, VE, Stream, false);
+
+  // If there is function-local metadata, emit it now.
+  WriteFunctionLocalMetadata(F, VE, Stream);
+
+  // Keep a running idea of what the instruction ID is.
+  unsigned InstID = CstEnd;
+
+  bool NeedsMetadataAttachment = false;
+
+  DILocation *LastDL = nullptr;
+
+  // Finally, emit all the instructions, in order.
+  for (Function::const_iterator BB = F.begin(), E = F.end(); BB != E; ++BB)
+    for (BasicBlock::const_iterator I = BB->begin(), E = BB->end();
+         I != E; ++I) {
+      WriteInstruction(*I, InstID, VE, Stream, Vals);
+
+      if (!I->getType()->isVoidTy())
+        ++InstID;
+
+      // If the instruction has metadata, write a metadata attachment later.
+      NeedsMetadataAttachment |= I->hasMetadataOtherThanDebugLoc();
+
+      // If the instruction has a debug location, emit it.
+      DILocation *DL = I->getDebugLoc();
+      if (!DL)
+        continue;
+
+      if (DL == LastDL) {
+        // Just repeat the same debug loc as last time.
+        Stream.EmitRecord(bitc::FUNC_CODE_DEBUG_LOC_AGAIN, Vals);
+        continue;
+      }
+
+      Vals.push_back(DL->getLine());
+      Vals.push_back(DL->getColumn());
+      Vals.push_back(VE.getMetadataOrNullID(DL->getScope()));
+      Vals.push_back(VE.getMetadataOrNullID(DL->getInlinedAt()));
+      Stream.EmitRecord(bitc::FUNC_CODE_DEBUG_LOC, Vals);
+      Vals.clear();
+
+      LastDL = DL;
+    }
+
+  // Emit names for all the instructions etc.
+  if (auto *Symtab = F.getValueSymbolTable())
+    WriteValueSymbolTable(*Symtab, VE, Stream);
+
+  if (NeedsMetadataAttachment)
+    WriteMetadataAttachment(F, VE, Stream);
+  VE.purgeFunction();
+  Stream.ExitBlock();
+}
+
+// Emit blockinfo, which defines the standard abbreviations etc.
+static void WriteBlockInfo(const ValueEnumerator32 &VE, BitstreamWriter &Stream) {
+  // We only want to emit block info records for blocks that have multiple
+  // instances: CONSTANTS_BLOCK, FUNCTION_BLOCK and VALUE_SYMTAB_BLOCK.
+  // Other blocks can define their abbrevs inline.
+  Stream.EnterBlockInfoBlock();
+
+  { // 8-bit fixed-width VST_ENTRY/VST_BBENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 3));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID,
+                                   Abbv) != VST_ENTRY_8_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // 7-bit fixed width VST_ENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_ENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID,
+                                   Abbv) != VST_ENTRY_7_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // 6-bit char6 VST_ENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_ENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID,
+                                   Abbv) != VST_ENTRY_6_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // 6-bit char6 VST_BBENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_BBENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID,
+                                   Abbv) != VST_BBENTRY_6_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+
+
+  { // SETTYPE abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_SETTYPE));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                              VE.computeBitsRequiredForTypeIndicies()));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID,
+                                   Abbv) != CONSTANTS_SETTYPE_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // INTEGER abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_INTEGER));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID,
+                                   Abbv) != CONSTANTS_INTEGER_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // CE_CAST abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CE_CAST));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4));  // cast opc
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,       // typeid
+                              VE.computeBitsRequiredForTypeIndicies()));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));    // value id
+
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID,
+                                   Abbv) != CONSTANTS_CE_CAST_Abbrev)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // NULL abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_NULL));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID,
+                                   Abbv) != CONSTANTS_NULL_Abbrev)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  // FIXME: This should only use space for first class types!
+
+  { // INST_LOAD abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_LOAD));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // Ptr
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4)); // Align
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1)); // volatile
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_LOAD_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_BINOP abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_BINOP));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // LHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // RHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4)); // opc
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_BINOP_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_BINOP_FLAGS abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_BINOP));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // LHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // RHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4)); // opc
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7)); // flags
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_BINOP_FLAGS_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_CAST abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_CAST));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));    // OpVal
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,       // dest ty
+                              VE.computeBitsRequiredForTypeIndicies()));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4));  // opc
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_CAST_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // INST_RET abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_RET));
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_RET_VOID_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_RET abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_RET));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // ValID
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_RET_VAL_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_UNREACHABLE abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_UNREACHABLE));
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID,
+                                   Abbv) != FUNCTION_INST_UNREACHABLE_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  Stream.ExitBlock();
+}
+
+/// WriteModule - Emit the specified module to the bitstream.
+static void WriteModule(const Module *M, BitstreamWriter &Stream) {
+  Stream.EnterSubblock(bitc::MODULE_BLOCK_ID, 3);
+
+  SmallVector<unsigned, 1> Vals;
+  unsigned CurVersion = 1;
+  Vals.push_back(CurVersion);
+  Stream.EmitRecord(bitc::MODULE_CODE_VERSION, Vals);
+
+  // Analyze the module, enumerating globals, functions, etc.
+  ValueEnumerator32 VE(*M);
+
+  // Emit blockinfo, which defines the standard abbreviations etc.
+  WriteBlockInfo(VE, Stream);
+
+  // Emit information about parameter attributes.
+  WriteAttributeTable(VE, Stream);
+
+  // Emit information describing all of the types in the module.
+  WriteTypeTable(VE, Stream);
+
+  // Emit top-level description of module, including target triple, inline asm,
+  // descriptors for global variables, and function prototype info.
+  WriteModuleInfo(M, VE, Stream);
+
+  // Emit constants.
+  WriteModuleConstants(VE, Stream);
+
+  // Emit metadata.
+  WriteModuleMetadata(M, VE, Stream);
+
+  // Emit metadata.
+  WriteModuleMetadataStore(M, Stream);
+
+  // Emit names for globals/functions etc.
+  WriteValueSymbolTable(M->getValueSymbolTable(), VE, Stream);
+
+  // Emit function bodies.
+  for (Module::const_iterator F = M->begin(), E = M->end(); F != E; ++F)
+    if (!F->isDeclaration())
+      WriteFunction(*F, VE, Stream);
+
+  Stream.ExitBlock();
+}
+
+/// EmitDarwinBCHeader - If generating a bc file on darwin, we have to emit a
+/// header and trailer to make it compatible with the system archiver.  To do
+/// this we emit the following header, and then emit a trailer that pads the
+/// file out to be a multiple of 16 bytes.
+///
+/// struct bc_header {
+///   uint32_t Magic;         // 0x0B17C0DE
+///   uint32_t Version;       // Version, currently always 0.
+///   uint32_t BitcodeOffset; // Offset to traditional bitcode file.
+///   uint32_t BitcodeSize;   // Size of traditional bitcode file.
+///   uint32_t CPUType;       // CPU specifier.
+///   ... potentially more later ...
+/// };
+enum {
+  DarwinBCSizeFieldOffset = 3*4, // Offset to bitcode_size.
+  DarwinBCHeaderSize = 5*4
+};
+
+static void WriteInt32ToBuffer(uint32_t Value, SmallVectorImpl<char> &Buffer,
+                               uint32_t &Position) {
+  support::endian::write32le(&Buffer[Position], Value);
+  Position += 4;
+}
+
+static void EmitDarwinBCHeaderAndTrailer(SmallVectorImpl<char> &Buffer,
+                                         const Triple &TT) {
+  unsigned CPUType = ~0U;
+
+  // Match x86_64-*, i[3-9]86-*, powerpc-*, powerpc64-*, arm-*, thumb-*,
+  // armv[0-9]-*, thumbv[0-9]-*, armv5te-*, or armv6t2-*. The CPUType is a magic
+  // number from /usr/include/mach/machine.h.  It is ok to reproduce the
+  // specific constants here because they are implicitly part of the Darwin ABI.
+  enum {
+    DARWIN_CPU_ARCH_ABI64      = 0x01000000,
+    DARWIN_CPU_TYPE_X86        = 7,
+    DARWIN_CPU_TYPE_ARM        = 12,
+    DARWIN_CPU_TYPE_POWERPC    = 18
+  };
+
+  Triple::ArchType Arch = TT.getArch();
+  if (Arch == Triple::x86_64)
+    CPUType = DARWIN_CPU_TYPE_X86 | DARWIN_CPU_ARCH_ABI64;
+  else if (Arch == Triple::x86)
+    CPUType = DARWIN_CPU_TYPE_X86;
+  else if (Arch == Triple::ppc)
+    CPUType = DARWIN_CPU_TYPE_POWERPC;
+  else if (Arch == Triple::ppc64)
+    CPUType = DARWIN_CPU_TYPE_POWERPC | DARWIN_CPU_ARCH_ABI64;
+  else if (Arch == Triple::arm || Arch == Triple::thumb)
+    CPUType = DARWIN_CPU_TYPE_ARM;
+
+  // Traditional Bitcode starts after header.
+  assert(Buffer.size() >= DarwinBCHeaderSize &&
+         "Expected header size to be reserved");
+  unsigned BCOffset = DarwinBCHeaderSize;
+  unsigned BCSize = Buffer.size()-DarwinBCHeaderSize;
+
+  // Write the magic and version.
+  unsigned Position = 0;
+  WriteInt32ToBuffer(0x0B17C0DE , Buffer, Position);
+  WriteInt32ToBuffer(0          , Buffer, Position); // Version.
+  WriteInt32ToBuffer(BCOffset   , Buffer, Position);
+  WriteInt32ToBuffer(BCSize     , Buffer, Position);
+  WriteInt32ToBuffer(CPUType    , Buffer, Position);
+
+  // If the file is not a multiple of 16 bytes, insert dummy padding.
+  while (Buffer.size() & 15)
+    Buffer.push_back(0);
+}
+
+/// Helper to write the header common to all bitcode files.
+static void WriteBitcodeHeader(BitstreamWriter &Stream) {
+  // Emit the file header.
+  Stream.Emit((unsigned)'B', 8);
+  Stream.Emit((unsigned)'C', 8);
+  Stream.Emit(0x0, 4);
+  Stream.Emit(0xC, 4);
+  Stream.Emit(0xE, 4);
+  Stream.Emit(0xD, 4);
+}
+
+/// WriteBitcodeToFile - Write the specified module to the specified output
+/// stream.
+void llvm::WriteBitcode32ToFile(const Module *M, raw_ostream &Out) {
+  SmallVector<char, 0> Buffer;
+  Buffer.reserve(256*1024);
+
+  // swap out DEBUG_METADATA_VERSION if it is present (yes, this is evil)
+  // NOTE: doesn't affect IOS_METAL_DEBUG_METADATA_VERSION, this is already correct
+  StringRef debug_info_version_str = "Debug Info Version";
+  if (auto debug_info_node = M->getModuleFlag(debug_info_version_str)) {
+    if (auto debug_info_version = dyn_cast<ConstantAsMetadata>(debug_info_node)) {
+      if(auto version_int = dyn_cast<ConstantInt>(debug_info_version->getValue())) {
+        if(version_int->getZExtValue() == DEBUG_METADATA_VERSION) {
+          // slightly awkward, the agony is real
+          NamedMDNode* module_flags = const_cast<Module*>(M)->getOrInsertModuleFlagsMetadata();
+          for(unsigned int i = 0, count = module_flags->getNumOperands(); i < count; ++i) {
+            auto Flag = module_flags->getOperand(i);
+            if (Flag->getNumOperands() >= 3 &&
+                dyn_cast_or_null<MDString>(Flag->getOperand(1)) &&
+                cast<MDString>(Flag->getOperand(1))->getString() == debug_info_version_str) {
+              Metadata *Ops[3] = {
+                ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M->getContext()), llvm::Module::Warning)),
+                MDString::get(M->getContext(), debug_info_version_str),
+                ConstantAsMetadata::get(ConstantInt::get(Type::getInt32Ty(M->getContext()), DEBUG_METADATA_VERSION_32))
+              };
+              module_flags->setOperand(i, MDNode::get(M->getContext(), Ops));
+              break;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  // If this is darwin or another generic macho target, reserve space for the
+  // header.
+  Triple TT(M->getTargetTriple());
+  if (TT.isOSDarwin())
+    Buffer.insert(Buffer.begin(), DarwinBCHeaderSize, 0);
+
+  // Emit the module into the buffer.
+  {
+    BitstreamWriter Stream(Buffer);
+
+    // Emit the file header.
+    WriteBitcodeHeader(Stream);
+
+    // Emit the module.
+    WriteModule(M, Stream);
+  }
+
+  if (TT.isOSDarwin())
+    EmitDarwinBCHeaderAndTrailer(Buffer, TT);
+
+  // Write the generated bitstream to "Out".
+  Out.write((char*)&Buffer.front(), Buffer.size());
+}
diff --git a/llvm/lib/Bitcode/Writer32/BitcodeWriterPass32.cpp b/llvm/lib/Bitcode/Writer32/BitcodeWriterPass32.cpp
new file mode 100644
index 000000000000..3f5835a53bc2
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/BitcodeWriterPass32.cpp
@@ -0,0 +1,49 @@
+//===- BitcodeWriterPass32.cpp - Bitcode 3.2 writing pass -----------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// BitcodeWriter32Pass implementation.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Bitcode/BitcodeWriterPass.h"
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/PassManager.h"
+#include "llvm/Pass.h"
+using namespace llvm;
+
+PreservedAnalyses Bitcode32WriterPass::run(Module &M, ModuleAnalysisManager &) {
+  WriteBitcode32ToFile(&M, OS);
+  return PreservedAnalyses::all();
+}
+
+namespace {
+  class WriteBitcode32Pass : public ModulePass {
+    raw_ostream &OS; // raw_ostream to print on
+
+  public:
+    static char ID; // Pass identification, replacement for typeid
+    explicit WriteBitcode32Pass(raw_ostream &o)
+        : ModulePass(ID), OS(o) {}
+
+    StringRef getPassName() const override { return "Bitcode 3.2 Writer"; }
+
+    bool runOnModule(Module &M) override {
+      WriteBitcode32ToFile(&M, OS);
+      return false;
+    }
+  };
+}
+
+char WriteBitcode32Pass::ID = 0;
+
+ModulePass *llvm::createBitcode32WriterPass(raw_ostream &Str) {
+  return new WriteBitcode32Pass(Str);
+}
diff --git a/llvm/lib/Bitcode/Writer32/CMakeLists.txt b/llvm/lib/Bitcode/Writer32/CMakeLists.txt
new file mode 100644
index 000000000000..0f2efb65b695
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/CMakeLists.txt
@@ -0,0 +1,16 @@
+add_llvm_component_library(LLVMBitWriter32
+  BitWriter32.cpp
+  BitcodeWriter32.cpp
+  BitcodeWriterPass32.cpp
+  ValueEnumerator32.cpp
+
+  DEPENDS
+  intrinsics_gen
+
+  LINK_COMPONENTS
+  Analysis
+  Core
+  MC
+  Object
+  Support
+  )
diff --git a/llvm/lib/Bitcode/Writer32/ValueEnumerator32.cpp b/llvm/lib/Bitcode/Writer32/ValueEnumerator32.cpp
new file mode 100644
index 000000000000..42d8264f7124
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/ValueEnumerator32.cpp
@@ -0,0 +1,684 @@
+//===- ValueEnumerator32.cpp - Number values and types for bitcode writer -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the ValueEnumerator32 class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "ValueEnumerator32.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DebugInfoMetadata.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/UseListOrder.h"
+#include "llvm/IR/ValueSymbolTable.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include <algorithm>
+using namespace llvm;
+
+static bool isIntOrIntVectorValue(const std::pair<const Value*, unsigned> &V) {
+  return V.first->getType()->isIntOrIntVectorTy();
+}
+
+ValueEnumerator32::ValueEnumerator32(const Module &M)
+    : HasMDString(false), HasDILocation(false), HasGenericDINode(false) {
+  // Enumerate the global variables.
+  for (const GlobalVariable &GV : M.globals())
+    EnumerateValue(&GV);
+
+  // Enumerate the functions.
+  for (const Function & F : M) {
+    EnumerateValue(&F);
+    EnumerateAttributes(F.getAttributes(), F.getContext());
+  }
+
+  // Enumerate the aliases.
+  for (const GlobalAlias &GA : M.aliases())
+    EnumerateValue(&GA);
+
+  // Remember what is the cutoff between globalvalue's and other constants.
+  unsigned FirstConstant = Values.size();
+
+  // Enumerate the global variable initializers.
+  for (const GlobalVariable &GV : M.globals())
+    if (GV.hasInitializer())
+      EnumerateValue(GV.getInitializer());
+
+  // Enumerate the aliasees.
+  for (const GlobalAlias &GA : M.aliases())
+    EnumerateValue(GA.getAliasee());
+
+  // Enumerate any optional Function data.
+  for (const Function &F : M)
+    for (const Use &U : F.operands())
+      EnumerateValue(U.get());
+
+  // Enumerate the metadata type.
+  //
+  // TODO: Move this to ValueEnumerator32::EnumerateOperandType() once bitcode
+  // only encodes the metadata type when it's used as a value.
+  EnumerateType(Type::getMetadataTy(M.getContext()));
+
+  // Insert constants and metadata that are named at module level into the slot
+  // pool so that the module symbol table can refer to them...
+  EnumerateValueSymbolTable(M.getValueSymbolTable());
+  EnumerateNamedMetadata(M);
+
+  SmallVector<std::pair<unsigned, MDNode *>, 8> MDs;
+
+  // Enumerate types used by function bodies and argument lists.
+  for (const Function &F : M) {
+    for (const Argument &A : F.args())
+      EnumerateType(A.getType());
+
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB) {
+        for (const Use &Op : I.operands()) {
+          auto *MD = dyn_cast<MetadataAsValue>(&Op);
+          if (!MD) {
+            EnumerateOperandType(Op);
+            continue;
+          }
+
+          // Local metadata is enumerated during function-incorporation.
+          if (isa<LocalAsMetadata>(MD->getMetadata()))
+            continue;
+
+          EnumerateMetadata(MD->getMetadata());
+        }
+        EnumerateType(I.getType());
+        if (const CallInst *CI = dyn_cast<CallInst>(&I))
+          EnumerateAttributes(CI->getAttributes(), M.getContext());
+        else if (const InvokeInst *II = dyn_cast<InvokeInst>(&I))
+          EnumerateAttributes(II->getAttributes(), M.getContext());
+        else if (const UnaryOperator *UnOp = dyn_cast<UnaryOperator>(&I);
+                 UnOp && UnOp->getOpcode() == Instruction::FNeg) {
+          // add -0.0 value that we'll use later
+          EnumerateValue(ConstantFP::get(UnOp->getOperand(0)->getType(), -0.0));
+        } else if (auto *SVI = dyn_cast<ShuffleVectorInst>(&I))
+          EnumerateType(SVI->getShuffleMaskForBitcode()->getType());
+
+        // Enumerate metadata attached with this instruction.
+        MDs.clear();
+        I.getAllMetadataOtherThanDebugLoc(MDs);
+        for (unsigned i = 0, e = MDs.size(); i != e; ++i)
+          EnumerateMetadata(MDs[i].second);
+
+        // Don't enumerate the location directly -- it has a special record
+        // type -- but enumerate its operands.
+        if (DILocation *L = I.getDebugLoc())
+          EnumerateMDNodeOperands(L);
+      }
+  }
+
+  // Optimize constant ordering.
+  OptimizeConstants(FirstConstant, Values.size());
+}
+
+unsigned ValueEnumerator32::getInstructionID(const Instruction *Inst) const {
+  InstructionMapType::const_iterator I = InstructionMap.find(Inst);
+  assert(I != InstructionMap.end() && "Instruction is not mapped!");
+  return I->second;
+}
+
+unsigned ValueEnumerator32::getComdatID(const Comdat *C) const {
+  unsigned ComdatID = Comdats.idFor(C);
+  assert(ComdatID && "Comdat not found!");
+  return ComdatID;
+}
+
+void ValueEnumerator32::setInstructionID(const Instruction *I) {
+  InstructionMap[I] = InstructionCount++;
+}
+
+unsigned ValueEnumerator32::getMetadataID(const Metadata *MD) const {
+  auto ID = getMetadataOrNullID(MD);
+#if 0
+  if(ID == 0) {
+    errs() << "invalid MD: ";
+    if(MD) errs() << MD;
+    else errs() << "nullptr";
+    errs() << "\n";
+  }
+#endif
+  assert(ID != 0 && "Metadata not in slotcalculator!");
+  return ID - 1;
+}
+
+unsigned ValueEnumerator32::getValueID(const Value *V) const {
+  if (auto *MD = dyn_cast<MetadataAsValue>(V))
+    return getMetadataID(MD->getMetadata());
+
+  ValueMapType::const_iterator I = ValueMap.find(V);
+#if 0
+  if(I == ValueMap.end()) {
+    errs() << "invalid value: " << *V << "\n";
+  }
+#endif
+  assert(I != ValueMap.end() && "Value not in slotcalculator!");
+  return I->second-1;
+}
+
+void ValueEnumerator32::dump() const {
+  print(dbgs(), ValueMap, "Default");
+  dbgs() << '\n';
+  print(dbgs(), MetadataMap, "MetaData");
+  dbgs() << '\n';
+}
+
+void ValueEnumerator32::print(raw_ostream &OS, const ValueMapType &Map,
+                              const char *Name) const {
+
+  OS << "Map Name: " << Name << "\n";
+  OS << "Size: " << Map.size() << "\n";
+  for (ValueMapType::const_iterator I = Map.begin(),
+         E = Map.end(); I != E; ++I) {
+
+    const Value *V = I->first;
+    if (V->hasName())
+      OS << "Value: " << V->getName();
+    else
+      OS << "Value: [null]\n";
+    V->print(errs());
+    errs() << '\n';
+
+    OS << " Uses(" << std::distance(V->use_begin(),V->use_end()) << "):";
+    for (const Use &U : V->uses()) {
+      if (&U != &*V->use_begin())
+        OS << ",";
+      if(U->hasName())
+        OS << " " << U->getName();
+      else
+        OS << " [null]";
+
+    }
+    OS <<  "\n\n";
+  }
+}
+
+void ValueEnumerator32::print(raw_ostream &OS, const MetadataMapType &Map,
+                              const char *Name) const {
+
+  OS << "Map Name: " << Name << "\n";
+  OS << "Size: " << Map.size() << "\n";
+  for (auto I = Map.begin(), E = Map.end(); I != E; ++I) {
+    const Metadata *MD = I->first;
+    OS << "Metadata: slot = " << I->second << "\n";
+    MD->print(OS);
+  }
+}
+
+/// OptimizeConstants - Reorder constant pool for denser encoding.
+void ValueEnumerator32::OptimizeConstants(unsigned CstStart, unsigned CstEnd) {
+  if (CstStart == CstEnd || CstStart+1 == CstEnd) return;
+
+  std::stable_sort(Values.begin() + CstStart, Values.begin() + CstEnd,
+                   [this](const std::pair<const Value *, unsigned> &LHS,
+                          const std::pair<const Value *, unsigned> &RHS) {
+    // Sort by plane.
+    if (LHS.first->getType() != RHS.first->getType())
+      return getTypeID(LHS.first->getType()) < getTypeID(RHS.first->getType());
+    // Then by frequency.
+    return LHS.second > RHS.second;
+  });
+
+  // Ensure that integer and vector of integer constants are at the start of the
+  // constant pool.  This is important so that GEP structure indices come before
+  // gep constant exprs.
+  std::partition(Values.begin()+CstStart, Values.begin()+CstEnd,
+                 isIntOrIntVectorValue);
+
+  // Rebuild the modified portion of ValueMap.
+  for (; CstStart != CstEnd; ++CstStart)
+    ValueMap[Values[CstStart].first] = CstStart+1;
+}
+
+
+/// EnumerateValueSymbolTable - Insert all of the values in the specified symbol
+/// table into the values table.
+void ValueEnumerator32::EnumerateValueSymbolTable(const ValueSymbolTable &VST) {
+  for (ValueSymbolTable::const_iterator VI = VST.begin(), VE = VST.end();
+       VI != VE; ++VI)
+    EnumerateValue(VI->getValue());
+}
+
+/// Insert all of the values referenced by named metadata in the specified
+/// module.
+void ValueEnumerator32::EnumerateNamedMetadata(const Module &M) {
+  for (const auto &I : M.named_metadata())
+    EnumerateNamedMDNode(&I);
+}
+
+void ValueEnumerator32::EnumerateNamedMDNode(const NamedMDNode *MD) {
+  for (unsigned i = 0, e = MD->getNumOperands(); i != e; ++i)
+    EnumerateMetadata(MD->getOperand(i));
+}
+
+/// EnumerateMDNodeOperands - Enumerate all non-function-local values
+/// and types referenced by the given MDNode.
+void ValueEnumerator32::EnumerateMDNodeOperands(const MDNode *N) {
+  for (unsigned i = 0, e = N->getNumOperands(); i != e; ++i) {
+    const Metadata* MD = N->getOperand(i);
+    if (!MD) {
+      EnumerateType(Type::getVoidTy(N->getContext()));
+      continue;
+    }
+    assert(!isa<LocalAsMetadata>(MD) && "MDNodes cannot be function-local");
+    if(isa<MDNode>(MD) || isa<MDString>(MD)) {
+      EnumerateMetadata(MD);
+    } else if(auto* V = dyn_cast<ValueAsMetadata>(MD)) {
+      EnumerateValue(V->getValue());
+    }
+  }
+}
+
+#define EnumerateI1(DI_obj, val) EnumerateValue(ConstantInt::get(Type::getInt1Ty(DI_obj->getContext()), val))
+#define EnumerateI32(DI_obj, val) EnumerateValue(ConstantInt::get(Type::getInt32Ty(DI_obj->getContext()), val))
+#define EnumerateI64(DI_obj, val) EnumerateValue(ConstantInt::get(Type::getInt64Ty(DI_obj->getContext()), val))
+#define DW_TAG(tag) (tag | (12 << 16))
+
+void ValueEnumerator32::EnumerateMetadata(const Metadata *MD) {
+  assert(
+      (isa<MDNode>(MD) || isa<MDString>(MD) || isa<ConstantAsMetadata>(MD)) &&
+      "Invalid metadata kind");
+
+  // Insert a dummy ID to block the co-recursive call to
+  // EnumerateMDNodeOperands() from re-visiting MD in a cyclic graph.
+  //
+  // Return early if there's already an ID.
+  if (!MetadataMap.insert(std::make_pair(MD, 0)).second)
+    return;
+
+  // Visit operands first to minimize RAUW.
+  // NOTE: debug info must be handled manually (this is different to 3.8 handling)
+  if (auto *DILoc = dyn_cast<DILocation>(MD)) {
+    EnumerateI32(DILoc, DILoc->getLine());
+    EnumerateI32(DILoc, DILoc->getColumn());
+    EnumerateMDNodeOperands(DILoc);
+  }
+  else if (auto *DIF = dyn_cast<DIFile>(MD)) {
+    EnumerateI32(DIF, DW_TAG(dwarf::DW_TAG_file_type));
+    EnumerateMDNodeOperands(DIF);
+    SmallVector<Metadata*, 2> file_node {{ DIF->getRawFilename(), DIF->getRawDirectory() }};
+    const_cast<DIFile*>(DIF)->contained_node = MDTuple::get(DIF->getContext(), file_node);
+    EnumerateMetadata(DIF->contained_node);
+  }
+  else if (auto *DICU = dyn_cast<DICompileUnit>(MD)) {
+    EnumerateI32(DICU, DW_TAG(dwarf::DW_TAG_compile_unit));
+    if(DICU->getFile()) {
+      // doesn't point to an actual DIFile node, but directly to { file, dir }
+      auto DIF = DICU->getFile();
+      EnumerateMDNodeOperands(DIF);
+      SmallVector<Metadata*, 2> file_node {{ DIF->getRawFilename(), DIF->getRawDirectory() }};
+      DIF->contained_node = MDTuple::get(DICU->getContext(), file_node);
+      EnumerateMetadata(DIF->contained_node);
+    }
+    EnumerateI32(DICU, DICU->getSourceLanguage());
+    if(DICU->getRawProducer()) EnumerateMetadata(DICU->getRawProducer());
+    EnumerateI1(DICU, DICU->isOptimized());
+    if(DICU->getRawFlags()) EnumerateMetadata(DICU->getRawFlags());
+    EnumerateI32(DICU, DICU->getRuntimeVersion());
+    if(DICU->getRawEnumTypes()) EnumerateMetadata(DICU->getRawEnumTypes());
+    if(DICU->getRawRetainedTypes()) EnumerateMetadata(DICU->getRawRetainedTypes());
+    //if(DICU->getRawSubprograms()) EnumerateMetadata(DICU->getRawSubprograms()); // TODO: fix subprograms
+    if(DICU->getRawGlobalVariables()) EnumerateMetadata(DICU->getRawGlobalVariables());
+    if(DICU->getRawImportedEntities()) EnumerateMetadata(DICU->getRawImportedEntities());
+    if(DICU->getRawSplitDebugFilename()) EnumerateMetadata(DICU->getRawSplitDebugFilename());
+    EnumerateI32(DICU, DICU->getEmissionKind());
+  }
+  else if (auto *DISP = dyn_cast<DISubprogram>(MD)) {
+    EnumerateI32(DISP, DW_TAG(dwarf::DW_TAG_subprogram));
+    if(DISP->getFile()) EnumerateMetadata(DISP->getFile());
+    if(DISP->getScope()) EnumerateMetadata(DISP->getScope());
+    if(DISP->getRawName()) EnumerateMetadata(DISP->getRawName());
+    if(DISP->getRawLinkageName()) EnumerateMetadata(DISP->getRawLinkageName());
+    EnumerateI32(DISP, DISP->getLine());
+    if(DISP->getType()) EnumerateMetadata(DISP->getType());
+    EnumerateI1(DISP, DISP->isLocalToUnit());
+    EnumerateI1(DISP, DISP->isDefinition());
+    EnumerateI32(DISP, DISP->getVirtuality());
+    EnumerateI32(DISP, DISP->getVirtualIndex());
+    if(DISP->getContainingType()) EnumerateMetadata(DISP->getContainingType());
+    EnumerateI32(DISP, DISP->getFlags());
+    EnumerateI1(DISP, DISP->isOptimized());
+    if(DISP->associated_function) {
+      EnumerateValue(DISP->associated_function);
+    }
+    if(DISP->getTemplateParams()) EnumerateMetadata(DISP->getTemplateParams().get());
+    if(DISP->getDeclaration()) EnumerateMetadata(DISP->getDeclaration());
+    
+    if(DISP->getRetainedNodes()) EnumerateMetadata(DISP->getRetainedNodes().get());
+    else {
+      auto empty_node = MDTuple::getTemporary(DISP->getContext(), {});
+      EnumerateMetadata(empty_node.get());
+    }
+    
+    EnumerateI32(DISP, DISP->getScopeLine());
+  }
+  else if(auto *DILB = dyn_cast<DILexicalBlock>(MD)) {
+    EnumerateI32(DILB, DW_TAG(dwarf::DW_TAG_lexical_block));
+    
+    static unsigned int unique_id = 0;
+    if(DILB->getFile()) EnumerateMetadata(DILB->getFile());
+    if(DILB->getScope()) EnumerateMetadata(DILB->getScope());
+    EnumerateI32(DILB, DILB->getLine());
+    EnumerateI32(DILB, DILB->getColumn());
+    EnumerateI32(DILB, 0);
+    EnumerateI32(DILB, unique_id++);
+  }
+  else if(auto *DIST = dyn_cast<DISubroutineType>(MD)) {
+    EnumerateI32(DIST, DW_TAG(dwarf::DW_TAG_subroutine_type));
+    
+    EnumerateI32(DIST, 0);
+    auto empty_str_node = MDString::get(DIST->getContext(), "");
+    EnumerateMetadata(empty_str_node);
+    EnumerateI64(DIST, 0);
+    EnumerateI32(DIST, DIST->getFlags());
+    if(DIST->getTypeArray()) EnumerateMetadata(DIST->getTypeArray().get());
+  }
+  else if (auto *N = dyn_cast<MDNode>(MD))
+    EnumerateMDNodeOperands(N);
+  else if (auto *C = dyn_cast<ConstantAsMetadata>(MD))
+    EnumerateValue(C->getValue());
+
+  HasMDString |= isa<MDString>(MD);
+  HasDILocation |= isa<DILocation>(MD);
+  HasGenericDINode |= isa<GenericDINode>(MD);
+
+  // Replace the dummy ID inserted above with the correct one.  MetadataMap may
+  // have changed by inserting operands, so we need a fresh lookup here.
+  MDs.push_back(MD);
+  MetadataMap[MD] = MDs.size();
+}
+
+/// EnumerateFunctionLocalMetadataa - Incorporate function-local metadata
+/// information reachable from the metadata.
+void ValueEnumerator32::EnumerateFunctionLocalMetadata(
+    const LocalAsMetadata *Local) {
+  // Check to see if it's already in!
+  unsigned &MetadataID = MetadataMap[Local];
+  if (MetadataID)
+    return;
+
+  MDs.push_back(Local);
+  MetadataID = MDs.size();
+
+  EnumerateValue(Local->getValue());
+
+  // Also, collect all function-local metadata for easy access.
+  FunctionLocalMDs.push_back(Local);
+}
+
+void ValueEnumerator32::EnumerateValue(const Value *V) {
+  assert(!V->getType()->isVoidTy() && "Can't insert void values!");
+  assert(!isa<MetadataAsValue>(V) && "EnumerateValue doesn't handle Metadata!");
+
+  // Check to see if it's already in!
+  unsigned &ValueID = ValueMap[V];
+  if (ValueID) {
+    // Increment use count.
+    Values[ValueID-1].second++;
+    return;
+  }
+
+  if (auto *GO = dyn_cast<GlobalObject>(V))
+    if (const Comdat *C = GO->getComdat())
+      Comdats.insert(C);
+
+  // Enumerate the type of this value.
+  EnumerateType(V->getType());
+
+  if (const Constant *C = dyn_cast<Constant>(V)) {
+    if (isa<GlobalValue>(C)) {
+      // Initializers for globals are handled explicitly elsewhere.
+    } else if (C->getNumOperands()) {
+      // If a constant has operands, enumerate them.  This makes sure that if a
+      // constant has uses (for example an array of const ints), that they are
+      // inserted also.
+
+      // We prefer to enumerate them with values before we enumerate the user
+      // itself.  This makes it more likely that we can avoid forward references
+      // in the reader.  We know that there can be no cycles in the constants
+      // graph that don't go through a global variable.
+      for (User::const_op_iterator I = C->op_begin(), E = C->op_end();
+           I != E; ++I)
+        if (!isa<BasicBlock>(*I)) // Don't enumerate BB operand to BlockAddress.
+          EnumerateValue(*I);
+      if (auto *CE = dyn_cast<ConstantExpr>(C))
+        if (CE->getOpcode() == Instruction::ShuffleVector)
+          EnumerateValue(CE->getShuffleMaskForBitcode());
+
+      // Finally, add the value.  Doing this could make the ValueID reference be
+      // dangling, don't reuse it.
+      Values.push_back(std::make_pair(V, 1U));
+      ValueMap[V] = Values.size();
+      return;
+    }
+  }
+
+  // Add the value.
+  Values.push_back(std::make_pair(V, 1U));
+  ValueID = Values.size();
+}
+
+
+void ValueEnumerator32::EnumerateType(Type *Ty) {
+  unsigned *TypeID = &TypeMap[Ty];
+
+  // We've already seen this type.
+  if (*TypeID)
+    return;
+
+  // If it is a non-anonymous struct, mark the type as being visited so that we
+  // don't recursively visit it.  This is safe because we allow forward
+  // references of these in the bitcode reader.
+  if (StructType *STy = dyn_cast<StructType>(Ty))
+    if (!STy->isLiteral())
+      *TypeID = ~0U;
+
+  // Enumerate all of the subtypes before we enumerate this type.  This ensures
+  // that the type will be enumerated in an order that can be directly built.
+  for (Type *SubTy : Ty->subtypes())
+    EnumerateType(SubTy);
+
+  // Refresh the TypeID pointer in case the table rehashed.
+  TypeID = &TypeMap[Ty];
+
+  // Check to see if we got the pointer another way.  This can happen when
+  // enumerating recursive types that hit the base case deeper than they start.
+  //
+  // If this is actually a struct that we are treating as forward ref'able,
+  // then emit the definition now that all of its contents are available.
+  if (*TypeID && *TypeID != ~0U)
+    return;
+
+  // Add this type now that its contents are all happily enumerated.
+  Types.push_back(Ty);
+
+  *TypeID = Types.size();
+}
+
+// Enumerate the types for the specified value.  If the value is a constant,
+// walk through it, enumerating the types of the constant.
+void ValueEnumerator32::EnumerateOperandType(const Value *V) {
+  EnumerateType(V->getType());
+
+  if (auto *MD = dyn_cast<MetadataAsValue>(V)) {
+    assert(!isa<LocalAsMetadata>(MD->getMetadata()) &&
+           "Function-local metadata should be left for later");
+
+    EnumerateMetadata(MD->getMetadata());
+    return;
+  }
+
+  const Constant *C = dyn_cast<Constant>(V);
+  if (!C)
+    return;
+
+  // If this constant is already enumerated, ignore it, we know its type must
+  // be enumerated.
+  if (ValueMap.count(C))
+    return;
+
+  // This constant may have operands, make sure to enumerate the types in
+  // them.
+  for (const Value *Op : C->operands()) {
+    // Don't enumerate basic blocks here, this happens as operands to
+    // blockaddress.
+    if (isa<BasicBlock>(Op))
+      continue;
+
+    EnumerateOperandType(Op);
+  }
+  if (auto *CE = dyn_cast<ConstantExpr>(C))
+    if (CE->getOpcode() == Instruction::ShuffleVector)
+      EnumerateOperandType(CE->getShuffleMaskForBitcode());
+}
+
+extern uint64_t getAttrKindEncodingBC32(Attribute::AttrKind Kind);
+void ValueEnumerator32::EnumerateAttributes(AttributeList PAL, LLVMContext& Context) {
+  if (PAL.isEmpty()) return;  // null is always 0.
+
+  // Do a lookup.
+  unsigned &Entry = AttributeListMap[PAL];
+  if (Entry == 0) {
+    // Never saw this before, add it.
+    AttributeLists.push_back(PAL);
+    Entry = AttributeLists.size();
+  }
+
+  // Do lookups for all attribute groups.
+  for (unsigned i : PAL.indexes()) {
+    AttributeSet AS = PAL.getAttributes(i);
+    if (!AS.hasAttributes())
+      continue;
+    // we need to skip attribute sets that don't have any valid LLVM BC 3.2 attribute
+    bool has_any_valid_attr = false;
+    for (Attribute Attr : AS) {
+      if (Attr.isEnumAttribute() || Attr.isIntAttribute()) {
+        if (getAttrKindEncodingBC32(Attr.getKindAsEnum()) > 0) {
+          has_any_valid_attr = true;
+          break;
+        }
+      } else if (Attr.isStringAttribute()) {
+        has_any_valid_attr = true;
+        break;
+      }
+      // else: ignore type attributes
+    }
+    auto AS_index = i;
+    if (!has_any_valid_attr) {
+      AS_index = ~0u;
+    }
+
+    IndexAndAttrSet Pair = {AS_index, AS};
+    unsigned &Entry = AttributeGroupMap[Pair];
+    if (Entry == 0) {
+      AttributeGroups.push_back(Pair);
+      Entry = AttributeGroups.size();
+    }
+  }
+}
+
+void ValueEnumerator32::incorporateFunction(const Function &F) {
+  InstructionCount = 0;
+  NumModuleValues = Values.size();
+  NumModuleMDs = MDs.size();
+
+  // Adding function arguments to the value table.
+  for (const auto &I : F.args())
+    EnumerateValue(&I);
+
+  FirstFuncConstantID = Values.size();
+
+  // Add all function-level constants to the value table.
+  for (const BasicBlock &BB : F) {
+    for (const Instruction &I : BB) {
+      for (const Use &OI : I.operands()) {
+        if ((isa<Constant>(OI) && !isa<GlobalValue>(OI)) || isa<InlineAsm>(OI))
+          EnumerateValue(OI);
+      }
+      if (auto *SVI = dyn_cast<ShuffleVectorInst>(&I))
+        EnumerateValue(SVI->getShuffleMaskForBitcode());
+    }
+    BasicBlocks.push_back(&BB);
+    ValueMap[&BB] = BasicBlocks.size();
+  }
+
+  // Optimize the constant layout.
+  OptimizeConstants(FirstFuncConstantID, Values.size());
+
+  // Add the function's parameter attributes so they are available for use in
+  // the function's instruction.
+  EnumerateAttributes(F.getAttributes(), F.getContext());
+
+  FirstInstID = Values.size();
+
+  SmallVector<LocalAsMetadata *, 8> FnLocalMDVector;
+  // Add all of the instructions.
+  for (const BasicBlock &BB : F) {
+    for (const Instruction &I : BB) {
+      for (const Use &OI : I.operands()) {
+        if (auto *MD = dyn_cast<MetadataAsValue>(&OI))
+          if (auto *Local = dyn_cast<LocalAsMetadata>(MD->getMetadata()))
+            // Enumerate metadata after the instructions they might refer to.
+            FnLocalMDVector.push_back(Local);
+      }
+
+      if (!I.getType()->isVoidTy())
+        EnumerateValue(&I);
+    }
+  }
+
+  // Add all of the function-local metadata.
+  for (unsigned i = 0, e = FnLocalMDVector.size(); i != e; ++i)
+    EnumerateFunctionLocalMetadata(FnLocalMDVector[i]);
+}
+
+void ValueEnumerator32::purgeFunction() {
+  /// Remove purged values from the ValueMap.
+  for (unsigned i = NumModuleValues, e = Values.size(); i != e; ++i)
+    ValueMap.erase(Values[i].first);
+  for (unsigned i = NumModuleMDs, e = MDs.size(); i != e; ++i)
+    MetadataMap.erase(MDs[i]);
+  for (unsigned i = 0, e = BasicBlocks.size(); i != e; ++i)
+    ValueMap.erase(BasicBlocks[i]);
+
+  Values.resize(NumModuleValues);
+  MDs.resize(NumModuleMDs);
+  BasicBlocks.clear();
+  FunctionLocalMDs.clear();
+}
+
+static void IncorporateFunctionInfoGlobalBBIDs(const Function *F,
+                                 DenseMap<const BasicBlock*, unsigned> &IDMap) {
+  unsigned Counter = 0;
+  for (const BasicBlock &BB : *F)
+    IDMap[&BB] = ++Counter;
+}
+
+/// getGlobalBasicBlockID - This returns the function-specific ID for the
+/// specified basic block.  This is relatively expensive information, so it
+/// should only be used by rare constructs such as address-of-label.
+unsigned ValueEnumerator32::getGlobalBasicBlockID(const BasicBlock *BB) const {
+  unsigned &Idx = GlobalBasicBlockIDs[BB];
+  if (Idx != 0)
+    return Idx-1;
+
+  IncorporateFunctionInfoGlobalBBIDs(BB->getParent(), GlobalBasicBlockIDs);
+  return getGlobalBasicBlockID(BB);
+}
+
+uint64_t ValueEnumerator32::computeBitsRequiredForTypeIndicies() const {
+  return Log2_32_Ceil(getTypes().size() + 1);
+}
diff --git a/llvm/lib/Bitcode/Writer32/ValueEnumerator32.h b/llvm/lib/Bitcode/Writer32/ValueEnumerator32.h
new file mode 100644
index 000000000000..e3671bb60a0a
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer32/ValueEnumerator32.h
@@ -0,0 +1,210 @@
+//===-- Bitcode/Writer32/ValueEnumerator32.h - Number values ----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This class gives values and types Unique ID's.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_BITCODE_32_WRITER_VALUEENUMERATOR_H
+#define LLVM_LIB_BITCODE_32_WRITER_VALUEENUMERATOR_H
+
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/UniqueVector.h"
+#include "llvm/IR/Attributes.h"
+#include "llvm/IR/UseListOrder.h"
+#include <vector>
+
+namespace llvm {
+
+class Type;
+class Value;
+class Instruction;
+class BasicBlock;
+class Comdat;
+class Function;
+class Module;
+class Metadata;
+class LocalAsMetadata;
+class MDNode;
+class NamedMDNode;
+class AttributeSet;
+class ValueSymbolTable;
+class MDSymbolTable;
+class raw_ostream;
+
+class ValueEnumerator32 {
+public:
+  typedef std::vector<Type*> TypeList;
+
+  // For each value, we remember its Value* and occurrence frequency.
+  typedef std::vector<std::pair<const Value*, unsigned> > ValueList;
+
+  /// Attribute groups as encoded in bitcode are almost AttributeSets, but they
+  /// include the AttributeList index, so we have to track that in our map.
+  using IndexAndAttrSet = std::pair<unsigned, AttributeSet>;
+
+  UseListOrderStack UseListOrders;
+
+private:
+  typedef DenseMap<Type*, unsigned> TypeMapType;
+  TypeMapType TypeMap;
+  TypeList Types;
+
+  typedef DenseMap<const Value*, unsigned> ValueMapType;
+  ValueMapType ValueMap;
+  ValueList Values;
+
+  typedef UniqueVector<const Comdat *> ComdatSetType;
+  ComdatSetType Comdats;
+
+  std::vector<const Metadata *> MDs;
+  SmallVector<const LocalAsMetadata *, 8> FunctionLocalMDs;
+  typedef DenseMap<const Metadata *, unsigned> MetadataMapType;
+  MetadataMapType MetadataMap;
+  bool HasMDString;
+  bool HasDILocation;
+  bool HasGenericDINode;
+
+  using AttributeGroupMapType = DenseMap<IndexAndAttrSet, unsigned>;
+  AttributeGroupMapType AttributeGroupMap;
+  std::vector<IndexAndAttrSet> AttributeGroups;
+
+  using AttributeListMapType = DenseMap<AttributeList, unsigned>;
+  AttributeListMapType AttributeListMap;
+  std::vector<AttributeList> AttributeLists;
+
+  /// GlobalBasicBlockIDs - This map memoizes the basic block ID's referenced by
+  /// the "getGlobalBasicBlockID" method.
+  mutable DenseMap<const BasicBlock*, unsigned> GlobalBasicBlockIDs;
+
+  typedef DenseMap<const Instruction*, unsigned> InstructionMapType;
+  InstructionMapType InstructionMap;
+  unsigned InstructionCount;
+
+  /// BasicBlocks - This contains all the basic blocks for the currently
+  /// incorporated function.  Their reverse mapping is stored in ValueMap.
+  std::vector<const BasicBlock*> BasicBlocks;
+
+  /// When a function is incorporated, this is the size of the Values list
+  /// before incorporation.
+  unsigned NumModuleValues;
+
+  /// When a function is incorporated, this is the size of the Metadatas list
+  /// before incorporation.
+  unsigned NumModuleMDs;
+
+  unsigned FirstFuncConstantID;
+  unsigned FirstInstID;
+
+  ValueEnumerator32(const ValueEnumerator32 &) = delete;
+  void operator=(const ValueEnumerator32 &) = delete;
+public:
+  ValueEnumerator32(const Module &M);
+
+  void dump() const;
+  void print(raw_ostream &OS, const ValueMapType &Map, const char *Name) const;
+  void print(raw_ostream &OS, const MetadataMapType &Map,
+             const char *Name) const;
+
+  unsigned getValueID(const Value *V) const;
+  unsigned getMetadataID(const Metadata *MD) const;
+  unsigned getMetadataOrNullID(const Metadata *MD) const {
+    return MetadataMap.lookup(MD);
+  }
+  unsigned numMDs() const { return MDs.size(); }
+
+  bool hasMDString() const { return HasMDString; }
+  bool hasDILocation() const { return HasDILocation; }
+  bool hasGenericDINode() const { return HasGenericDINode; }
+
+  unsigned getTypeID(Type *T) const {
+    TypeMapType::const_iterator I = TypeMap.find(T);
+    assert(I != TypeMap.end() && "Type not in ValueEnumerator32!");
+    return I->second-1;
+  }
+
+  unsigned getInstructionID(const Instruction *I) const;
+  void setInstructionID(const Instruction *I);
+
+  unsigned getAttributeListID(AttributeList PAL) const {
+    if (PAL.isEmpty()) return 0;  // Null maps to zero.
+    AttributeListMapType::const_iterator I = AttributeListMap.find(PAL);
+    assert(I != AttributeListMap.end() && "Attribute not in ValueEnumerator!");
+    return I->second;
+  }
+
+  unsigned getAttributeGroupID(IndexAndAttrSet Group) const {
+    if (!Group.second.hasAttributes())
+      return 0; // Null maps to zero.
+    AttributeGroupMapType::const_iterator I = AttributeGroupMap.find(Group);
+    assert(I != AttributeGroupMap.end() && "Attribute not in ValueEnumerator!");
+    if (I == AttributeGroupMap.end()) {
+      return ~0u;
+    }
+    return I->second;
+  }
+
+  /// getFunctionConstantRange - Return the range of values that corresponds to
+  /// function-local constants.
+  void getFunctionConstantRange(unsigned &Start, unsigned &End) const {
+    Start = FirstFuncConstantID;
+    End = FirstInstID;
+  }
+
+  const ValueList &getValues() const { return Values; }
+  const std::vector<const Metadata *> &getMDs() const { return MDs; }
+  const SmallVectorImpl<const LocalAsMetadata *> &getFunctionLocalMDs() const {
+    return FunctionLocalMDs;
+  }
+  const TypeList &getTypes() const { return Types; }
+  const std::vector<const BasicBlock*> &getBasicBlocks() const {
+    return BasicBlocks;
+  }
+
+  const std::vector<AttributeList> &getAttributeLists() const { return AttributeLists; }
+
+  const std::vector<IndexAndAttrSet> &getAttributeGroups() const {
+    return AttributeGroups;
+  }
+
+  const ComdatSetType &getComdats() const { return Comdats; }
+  unsigned getComdatID(const Comdat *C) const;
+
+  /// getGlobalBasicBlockID - This returns the function-specific ID for the
+  /// specified basic block.  This is relatively expensive information, so it
+  /// should only be used by rare constructs such as address-of-label.
+  unsigned getGlobalBasicBlockID(const BasicBlock *BB) const;
+
+  /// incorporateFunction/purgeFunction - If you'd like to deal with a function,
+  /// use these two methods to get its data into the ValueEnumerator32!
+  ///
+  void incorporateFunction(const Function &F);
+  void purgeFunction();
+  uint64_t computeBitsRequiredForTypeIndicies() const;
+
+private:
+  void OptimizeConstants(unsigned CstStart, unsigned CstEnd);
+
+  void EnumerateMDNodeOperands(const MDNode *N);
+  void EnumerateMetadata(const Metadata *MD);
+  void EnumerateFunctionLocalMetadata(const LocalAsMetadata *Local);
+  void EnumerateNamedMDNode(const NamedMDNode *NMD);
+  void EnumerateValue(const Value *V);
+  void EnumerateType(Type *T);
+  void EnumerateOperandType(const Value *V);
+  void EnumerateAttributes(AttributeList PAL, LLVMContext& Context);
+
+  void EnumerateValueSymbolTable(const ValueSymbolTable &ST);
+  void EnumerateNamedMetadata(const Module &M);
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/llvm/lib/Bitcode/Writer50/BitWriter50.cpp b/llvm/lib/Bitcode/Writer50/BitWriter50.cpp
new file mode 100644
index 000000000000..baf76070e164
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/BitWriter50.cpp
@@ -0,0 +1,50 @@
+//===-- BitWriter50.cpp ---------------------------------------------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm-c/BitWriter.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "llvm/IR/Module.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Support/MemoryBuffer.h"
+#include "llvm/Support/raw_ostream.h"
+using namespace llvm;
+
+
+/*===-- Operations on modules ---------------------------------------------===*/
+
+int LLVMWriteBitcode50ToFile(LLVMModuleRef M, const char *Path) {
+  std::error_code EC;
+  raw_fd_ostream OS(Path, EC, sys::fs::OF_None);
+
+  if (EC)
+    return -1;
+
+  WriteBitcode50ToFile(unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteBitcode50ToFD(LLVMModuleRef M, int FD, int ShouldClose,
+                         int Unbuffered) {
+  raw_fd_ostream OS(FD, ShouldClose, Unbuffered);
+
+  WriteBitcode50ToFile(unwrap(M), OS);
+  return 0;
+}
+
+int LLVMWriteBitcode50ToFileHandle(LLVMModuleRef M, int FileHandle) {
+  return LLVMWriteBitcode50ToFD(M, FileHandle, true, false);
+}
+
+LLVMMemoryBufferRef LLVMWriteBitcode50ToMemoryBuffer(LLVMModuleRef M) {
+  std::string Data;
+  raw_string_ostream OS(Data);
+
+  WriteBitcode50ToFile(unwrap(M), OS);
+  return wrap(MemoryBuffer::getMemBufferCopy(OS.str()).release());
+}
diff --git a/llvm/lib/Bitcode/Writer50/BitcodeWriter50.cpp b/llvm/lib/Bitcode/Writer50/BitcodeWriter50.cpp
new file mode 100644
index 000000000000..074289fe94e3
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/BitcodeWriter50.cpp
@@ -0,0 +1,4092 @@
+//===--- Bitcode/Writer50/BitcodeWriter50.cpp - Bitcode 5.0 Writer --------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Bitcode 5.0 writer implementation.
+//
+//===----------------------------------------------------------------------===//
+
+// enable errors when using > 5.0 bitcode enums from LLVMBitCodes.h
+#define LLVM_BITCODE_50 1
+
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "ValueEnumerator50.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Bitstream/BitstreamWriter.h"
+#include "llvm/Bitcode/LLVMBitCodes.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DebugInfoMetadata.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Operator.h"
+#include "llvm/IR/UseListOrder.h"
+#include "llvm/IR/ValueSymbolTable.h"
+#include "llvm/MC/StringTableBuilder.h"
+#include "llvm/MC/TargetRegistry.h"
+#include "llvm/Object/IRSymtab.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Support/Program.h"
+#include "llvm/Support/SHA1.h"
+#include "llvm/Support/raw_ostream.h"
+#include <cctype>
+#include <map>
+using namespace llvm;
+
+namespace {
+
+/// These are manifest constants used by the bitcode writer. They do not need to
+/// be kept in sync with the reader, but need to be consistent within this file.
+enum {
+  // VALUE_SYMTAB_BLOCK abbrev id's.
+  VST_ENTRY_8_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  VST_ENTRY_7_ABBREV,
+  VST_ENTRY_6_ABBREV,
+  VST_BBENTRY_6_ABBREV,
+
+  // CONSTANTS_BLOCK abbrev id's.
+  CONSTANTS_SETTYPE_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  CONSTANTS_INTEGER_ABBREV,
+  CONSTANTS_CE_CAST_Abbrev,
+  CONSTANTS_NULL_Abbrev,
+
+  // FUNCTION_BLOCK abbrev id's.
+  FUNCTION_INST_LOAD_ABBREV = bitc::FIRST_APPLICATION_ABBREV,
+  FUNCTION_INST_BINOP_ABBREV,
+  FUNCTION_INST_BINOP_FLAGS_ABBREV,
+  FUNCTION_INST_CAST_ABBREV,
+  FUNCTION_INST_RET_VOID_ABBREV,
+  FUNCTION_INST_RET_VAL_ABBREV,
+  FUNCTION_INST_UNREACHABLE_ABBREV,
+  FUNCTION_INST_GEP_ABBREV,
+};
+
+/// Abstract class to manage the bitcode writing, subclassed for each bitcode
+/// file type.
+class BitcodeWriterBase50 {
+protected:
+  /// The stream created and owned by the client.
+  BitstreamWriter &Stream;
+
+  StringTableBuilder &StrtabBuilder;
+
+public:
+  /// Constructs a BitcodeWriterBase50 object that writes to the provided
+  /// \p Stream.
+  BitcodeWriterBase50(BitstreamWriter &Stream, StringTableBuilder &StrtabBuilder)
+      : Stream(Stream), StrtabBuilder(StrtabBuilder) {}
+
+protected:
+  void writeBitcodeHeader();
+  void writeModuleVersion();
+};
+
+void BitcodeWriterBase50::writeModuleVersion() {
+  // VERSION: [version#]
+  Stream.EmitRecord(bitc::MODULE_CODE_VERSION, ArrayRef<uint64_t>{2});
+}
+
+/// Class to manage the bitcode writing for a module.
+class ModuleBitcodeWriter50 : public BitcodeWriterBase50 {
+  /// Pointer to the buffer allocated by caller for bitcode writing.
+  const SmallVectorImpl<char> &Buffer;
+
+  /// The Module to write to bitcode.
+  const Module &M;
+
+  /// Enumerates ids for all values in the module.
+  ValueEnumerator50 VE;
+
+  /// Optional per-module index to write for ThinLTO.
+  const ModuleSummaryIndex *Index;
+
+  /// True if a module hash record should be written.
+  bool GenerateHash;
+
+  SHA1 Hasher;
+
+  /// If non-null, when GenerateHash is true, the resulting hash is written
+  /// into ModHash. When GenerateHash is false, that specified value
+  /// is used as the hash instead of computing from the generated bitcode.
+  /// Can be used to produce the same module hash for a minimized bitcode
+  /// used just for the thin link as in the regular full bitcode that will
+  /// be used in the backend.
+  ModuleHash *ModHash;
+
+  /// The start bit of the identification block.
+  uint64_t BitcodeStartBit;
+
+  /// Map that holds the correspondence between GUIDs in the summary index,
+  /// that came from indirect call profiles, and a value id generated by this
+  /// class to use in the VST and summary block records.
+  std::map<GlobalValue::GUID, unsigned> GUIDToValueIdMap;
+
+  /// Tracks the last value id recorded in the GUIDToValueMap.
+  unsigned GlobalValueId;
+
+  /// Saves the offset of the VSTOffset record that must eventually be
+  /// backpatched with the offset of the actual VST.
+  uint64_t VSTOffsetPlaceholder = 0;
+
+public:
+  /// Constructs a ModuleBitcodeWriter50 object for the given Module,
+  /// writing to the provided \p Buffer.
+  ModuleBitcodeWriter50(const Module *M, SmallVectorImpl<char> &Buffer,
+                      StringTableBuilder &StrtabBuilder,
+                      BitstreamWriter &Stream, bool ShouldPreserveUseListOrder,
+                      const ModuleSummaryIndex *Index, bool GenerateHash,
+                      ModuleHash *ModHash = nullptr)
+      : BitcodeWriterBase50(Stream, StrtabBuilder), Buffer(Buffer), M(*M),
+        VE(*M, ShouldPreserveUseListOrder), Index(Index),
+        GenerateHash(GenerateHash), ModHash(ModHash),
+        BitcodeStartBit(Stream.GetCurrentBitNo()) {
+    // imitate Metal by having one llvm.dbg.cu entry per DISubprogram
+    if (auto dbg_cu = M->getNamedMetadata("llvm.dbg.cu"); dbg_cu) {
+      uint32_t subprogram_count = 0;
+      for (const auto &md : VE.getMetadataMap()) {
+        if (const DISubprogram *disubprog_node = dyn_cast_or_null<DISubprogram>(md.first); disubprog_node) {
+          ++subprogram_count;
+        }
+      }
+      if (subprogram_count > 1 && dbg_cu->getNumOperands() == 1) {
+        auto dup_op = dbg_cu->getOperand(0);
+        for (uint32_t i = 1; i < subprogram_count; ++i) {
+          dbg_cu->addOperand(dup_op);
+        }
+      }
+    }
+
+    // Assign ValueIds to any callee values in the index that came from
+    // indirect call profiles and were recorded as a GUID not a Value*
+    // (which would have been assigned an ID by the ValueEnumerator50).
+    // The starting ValueId is just after the number of values in the
+    // ValueEnumerator50, so that they can be emitted in the VST.
+    GlobalValueId = VE.getValues().size();
+    if (!Index)
+      return;
+    for (const auto &GUIDSummaryLists : *Index)
+      // Examine all summaries for this GUID.
+      for (auto &Summary : GUIDSummaryLists.second.SummaryList)
+        if (auto FS = dyn_cast<FunctionSummary>(Summary.get()))
+          // For each call in the function summary, see if the call
+          // is to a GUID (which means it is for an indirect call,
+          // otherwise we would have a Value for it). If so, synthesize
+          // a value id.
+          for (auto &CallEdge : FS->calls())
+            if (!CallEdge.first.getValue())
+              assignValueId(CallEdge.first.getGUID());
+  }
+
+  /// Emit the current module to the bitstream.
+  void write();
+
+private:
+  uint64_t bitcodeStartBit() { return BitcodeStartBit; }
+
+  size_t addToStrtab(StringRef Str);
+
+  void writeAttributeGroupTable();
+  void writeAttributeTable();
+  void writeTypeTable();
+  void writeComdats();
+  void writeValueSymbolTableForwardDecl();
+  void writeModuleInfo();
+  void writeValueAsMetadata(const ValueAsMetadata *MD,
+                            SmallVectorImpl<uint64_t> &Record);
+  void writeMDTuple(const MDTuple *N, SmallVectorImpl<uint64_t> &Record,
+                    unsigned Abbrev);
+  unsigned createDILocationAbbrev();
+  void writeDILocation(const DILocation *N, SmallVectorImpl<uint64_t> &Record,
+                       unsigned &Abbrev);
+  unsigned createGenericDINodeAbbrev();
+  void writeGenericDINode(const GenericDINode *N,
+                          SmallVectorImpl<uint64_t> &Record, unsigned &Abbrev);
+  void writeDISubrange(const DISubrange *N, SmallVectorImpl<uint64_t> &Record,
+                       unsigned Abbrev);
+  void writeDIEnumerator(const DIEnumerator *N,
+                         SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDIBasicType(const DIBasicType *N, SmallVectorImpl<uint64_t> &Record,
+                        unsigned Abbrev);
+  void writeDIDerivedType(const DIDerivedType *N,
+                          SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDICompositeType(const DICompositeType *N,
+                            SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDISubroutineType(const DISubroutineType *N,
+                             SmallVectorImpl<uint64_t> &Record,
+                             unsigned Abbrev);
+  void writeDIFile(const DIFile *N, SmallVectorImpl<uint64_t> &Record,
+                   unsigned Abbrev);
+  void writeDICompileUnit(const DICompileUnit *N,
+                          SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDISubprogram(const DISubprogram *N,
+                         SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDILexicalBlock(const DILexicalBlock *N,
+                           SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDILexicalBlockFile(const DILexicalBlockFile *N,
+                               SmallVectorImpl<uint64_t> &Record,
+                               unsigned Abbrev);
+  void writeDINamespace(const DINamespace *N, SmallVectorImpl<uint64_t> &Record,
+                        unsigned Abbrev);
+  void writeDIMacro(const DIMacro *N, SmallVectorImpl<uint64_t> &Record,
+                    unsigned Abbrev);
+  void writeDIMacroFile(const DIMacroFile *N, SmallVectorImpl<uint64_t> &Record,
+                        unsigned Abbrev);
+  void writeDIModule(const DIModule *N, SmallVectorImpl<uint64_t> &Record,
+                     unsigned Abbrev);
+  void writeDITemplateTypeParameter(const DITemplateTypeParameter *N,
+                                    SmallVectorImpl<uint64_t> &Record,
+                                    unsigned Abbrev);
+  void writeDITemplateValueParameter(const DITemplateValueParameter *N,
+                                     SmallVectorImpl<uint64_t> &Record,
+                                     unsigned Abbrev);
+  void writeDIGlobalVariable(const DIGlobalVariable *N,
+                             SmallVectorImpl<uint64_t> &Record,
+                             unsigned Abbrev);
+  void writeDILocalVariable(const DILocalVariable *N,
+                            SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDIExpression(const DIExpression *N,
+                         SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDIGlobalVariableExpression(const DIGlobalVariableExpression *N,
+                                       SmallVectorImpl<uint64_t> &Record,
+                                       unsigned Abbrev);
+  void writeDIObjCProperty(const DIObjCProperty *N,
+                           SmallVectorImpl<uint64_t> &Record, unsigned Abbrev);
+  void writeDIImportedEntity(const DIImportedEntity *N,
+                             SmallVectorImpl<uint64_t> &Record,
+                             unsigned Abbrev);
+  unsigned createNamedMetadataAbbrev();
+  void writeNamedMetadata(SmallVectorImpl<uint64_t> &Record);
+  unsigned createMetadataStringsAbbrev();
+  void writeMetadataStrings(ArrayRef<const Metadata *> Strings,
+                            SmallVectorImpl<uint64_t> &Record);
+  void writeMetadataRecords(ArrayRef<const Metadata *> MDs,
+                            SmallVectorImpl<uint64_t> &Record,
+                            std::vector<unsigned> *MDAbbrevs = nullptr,
+                            std::vector<uint64_t> *IndexPos = nullptr);
+  void writeModuleMetadata();
+  void writeFunctionMetadata(const Function &F);
+  void writeFunctionMetadataAttachment(const Function &F);
+  void writeGlobalVariableMetadataAttachment(const GlobalVariable &GV);
+  void pushGlobalMetadataAttachment(SmallVectorImpl<uint64_t> &Record,
+                                    const GlobalObject &GO);
+  void writeModuleMetadataKinds();
+  void writeOperandBundleTags();
+  void writeSyncScopeNames();
+  void writeConstants(unsigned FirstVal, unsigned LastVal, bool isGlobal);
+  void writeModuleConstants();
+  bool pushValueAndType(const Value *V, unsigned InstID,
+                        SmallVectorImpl<unsigned> &Vals);
+  void writeOperandBundles(const CallBase &CB, unsigned InstID);
+  void pushValue(const Value *V, unsigned InstID,
+                 SmallVectorImpl<unsigned> &Vals);
+  void pushValueSigned(const Value *V, unsigned InstID,
+                       SmallVectorImpl<uint64_t> &Vals);
+  void writeInstruction(const Instruction &I, unsigned InstID,
+                        SmallVectorImpl<unsigned> &Vals);
+  void writeFunctionLevelValueSymbolTable(const ValueSymbolTable &VST);
+  void writeGlobalValueSymbolTable(
+      DenseMap<const Function *, uint64_t> &FunctionToBitcodeIndex);
+  void writeUseList(UseListOrder &&Order);
+  void writeUseListBlock(const Function *F);
+  void
+  writeFunction(const Function &F,
+                DenseMap<const Function *, uint64_t> &FunctionToBitcodeIndex);
+  void writeBlockInfo();
+  void writePerModuleFunctionSummaryRecord(SmallVector<uint64_t, 64> &NameVals,
+                                           GlobalValueSummary *Summary,
+                                           unsigned ValueID,
+                                           unsigned FSCallsAbbrev,
+                                           unsigned FSCallsProfileAbbrev,
+                                           const Function &F);
+  void writeModuleLevelReferences(const GlobalVariable &V,
+                                  SmallVector<uint64_t, 64> &NameVals,
+                                  unsigned FSModRefsAbbrev);
+  void writePerModuleGlobalValueSummary();
+  void writeModuleHash(size_t BlockStartPos);
+
+  void assignValueId(GlobalValue::GUID ValGUID) {
+    GUIDToValueIdMap[ValGUID] = ++GlobalValueId;
+  }
+  unsigned getValueId(GlobalValue::GUID ValGUID) {
+    const auto &VMI = GUIDToValueIdMap.find(ValGUID);
+    // Expect that any GUID value had a value Id assigned by an
+    // earlier call to assignValueId.
+    assert(VMI != GUIDToValueIdMap.end() &&
+           "GUID does not have assigned value Id");
+    return VMI->second;
+  }
+  // Helper to get the valueId for the type of value recorded in VI.
+  unsigned getValueId(ValueInfo VI) {
+    if (!VI.getValue())
+      return getValueId(VI.getGUID());
+    return VE.getValueID(VI.getValue());
+  }
+  std::map<GlobalValue::GUID, unsigned> &valueIds() { return GUIDToValueIdMap; }
+
+  unsigned getEncodedSyncScopeID(SyncScope::ID SSID) {
+    return unsigned(SSID);
+  }
+};
+
+/// Class to manage the bitcode writing for a combined index.
+class IndexBitcodeWriter50 : public BitcodeWriterBase50 {
+  /// The combined index to write to bitcode.
+  const ModuleSummaryIndex &Index;
+
+  /// When writing a subset of the index for distributed backends, client
+  /// provides a map of modules to the corresponding GUIDs/summaries to write.
+  const std::map<std::string, GVSummaryMapTy> *ModuleToSummariesForIndex;
+
+  /// Map that holds the correspondence between the GUID used in the combined
+  /// index and a value id generated by this class to use in references.
+  std::map<GlobalValue::GUID, unsigned> GUIDToValueIdMap;
+
+  /// Tracks the last value id recorded in the GUIDToValueMap.
+  unsigned GlobalValueId = 0;
+
+public:
+  /// Constructs a IndexBitcodeWriter50 object for the given combined index,
+  /// writing to the provided \p Buffer. When writing a subset of the index
+  /// for a distributed backend, provide a \p ModuleToSummariesForIndex map.
+  IndexBitcodeWriter50(BitstreamWriter &Stream, StringTableBuilder &StrtabBuilder,
+                     const ModuleSummaryIndex &Index,
+                     const std::map<std::string, GVSummaryMapTy>
+                         *ModuleToSummariesForIndex = nullptr)
+      : BitcodeWriterBase50(Stream, StrtabBuilder), Index(Index),
+        ModuleToSummariesForIndex(ModuleToSummariesForIndex) {
+    // Assign unique value ids to all summaries to be written, for use
+    // in writing out the call graph edges. Save the mapping from GUID
+    // to the new global value id to use when writing those edges, which
+    // are currently saved in the index in terms of GUID.
+    forEachSummary([&](GVInfo I) {
+      GUIDToValueIdMap[I.first] = ++GlobalValueId;
+    });
+  }
+
+  /// The below iterator returns the GUID and associated summary.
+  typedef std::pair<GlobalValue::GUID, GlobalValueSummary *> GVInfo;
+
+  /// Calls the callback for each value GUID and summary to be written to
+  /// bitcode. This hides the details of whether they are being pulled from the
+  /// entire index or just those in a provided ModuleToSummariesForIndex map.
+  template<typename Functor>
+  void forEachSummary(Functor Callback) {
+    if (ModuleToSummariesForIndex) {
+      for (auto &M : *ModuleToSummariesForIndex)
+        for (auto &Summary : M.second)
+          Callback(Summary);
+    } else {
+      for (auto &Summaries : Index)
+        for (auto &Summary : Summaries.second.SummaryList)
+          Callback({Summaries.first, Summary.get()});
+    }
+  }
+
+  /// Calls the callback for each entry in the modulePaths StringMap that
+  /// should be written to the module path string table. This hides the details
+  /// of whether they are being pulled from the entire index or just those in a
+  /// provided ModuleToSummariesForIndex map.
+  template <typename Functor> void forEachModule(Functor Callback) {
+    if (ModuleToSummariesForIndex) {
+      for (const auto &M : *ModuleToSummariesForIndex) {
+        const auto &MPI = Index.modulePaths().find(M.first);
+        if (MPI == Index.modulePaths().end()) {
+          // This should only happen if the bitcode file was empty, in which
+          // case we shouldn't be importing (the ModuleToSummariesForIndex
+          // would only include the module we are writing and index for).
+          assert(ModuleToSummariesForIndex->size() == 1);
+          continue;
+        }
+        Callback(*MPI);
+      }
+    } else {
+      for (const auto &MPSE : Index.modulePaths())
+        Callback(MPSE);
+    }
+  }
+
+  /// Main entry point for writing a combined index to bitcode.
+  void write();
+
+private:
+  void writeModStrings();
+  void writeCombinedGlobalValueSummary();
+
+  Optional<unsigned> getValueId(GlobalValue::GUID ValGUID) {
+    auto VMI = GUIDToValueIdMap.find(ValGUID);
+    if (VMI == GUIDToValueIdMap.end())
+      return None;
+    return VMI->second;
+  }
+  std::map<GlobalValue::GUID, unsigned> &valueIds() { return GUIDToValueIdMap; }
+};
+} // end anonymous namespace
+
+static unsigned getEncodedCastOpcode(unsigned Opcode) {
+  switch (Opcode) {
+  default: llvm_unreachable("Unknown cast instruction!");
+  case Instruction::Trunc   : return bitc::CAST_TRUNC;
+  case Instruction::ZExt    : return bitc::CAST_ZEXT;
+  case Instruction::SExt    : return bitc::CAST_SEXT;
+  case Instruction::FPToUI  : return bitc::CAST_FPTOUI;
+  case Instruction::FPToSI  : return bitc::CAST_FPTOSI;
+  case Instruction::UIToFP  : return bitc::CAST_UITOFP;
+  case Instruction::SIToFP  : return bitc::CAST_SITOFP;
+  case Instruction::FPTrunc : return bitc::CAST_FPTRUNC;
+  case Instruction::FPExt   : return bitc::CAST_FPEXT;
+  case Instruction::PtrToInt: return bitc::CAST_PTRTOINT;
+  case Instruction::IntToPtr: return bitc::CAST_INTTOPTR;
+  case Instruction::BitCast : return bitc::CAST_BITCAST;
+  case Instruction::AddrSpaceCast: return bitc::CAST_ADDRSPACECAST;
+  }
+}
+
+static unsigned getEncodedBinaryOpcode(unsigned Opcode) {
+  switch (Opcode) {
+  default: llvm_unreachable("Unknown binary instruction!");
+  case Instruction::Add:
+  case Instruction::FAdd: return bitc::BINOP_ADD;
+  case Instruction::Sub:
+  case Instruction::FSub: return bitc::BINOP_SUB;
+  case Instruction::Mul:
+  case Instruction::FMul: return bitc::BINOP_MUL;
+  case Instruction::UDiv: return bitc::BINOP_UDIV;
+  case Instruction::FDiv:
+  case Instruction::SDiv: return bitc::BINOP_SDIV;
+  case Instruction::URem: return bitc::BINOP_UREM;
+  case Instruction::FRem:
+  case Instruction::SRem: return bitc::BINOP_SREM;
+  case Instruction::Shl:  return bitc::BINOP_SHL;
+  case Instruction::LShr: return bitc::BINOP_LSHR;
+  case Instruction::AShr: return bitc::BINOP_ASHR;
+  case Instruction::And:  return bitc::BINOP_AND;
+  case Instruction::Or:   return bitc::BINOP_OR;
+  case Instruction::Xor:  return bitc::BINOP_XOR;
+  }
+}
+
+static unsigned getEncodedRMWOperation(AtomicRMWInst::BinOp Op) {
+  switch (Op) {
+  default: llvm_unreachable("Unknown RMW operation!");
+  case AtomicRMWInst::Xchg: return bitc::RMW_XCHG;
+  case AtomicRMWInst::Add: return bitc::RMW_ADD;
+  case AtomicRMWInst::Sub: return bitc::RMW_SUB;
+  case AtomicRMWInst::And: return bitc::RMW_AND;
+  case AtomicRMWInst::Nand: return bitc::RMW_NAND;
+  case AtomicRMWInst::Or: return bitc::RMW_OR;
+  case AtomicRMWInst::Xor: return bitc::RMW_XOR;
+  case AtomicRMWInst::Max: return bitc::RMW_MAX;
+  case AtomicRMWInst::Min: return bitc::RMW_MIN;
+  case AtomicRMWInst::UMax: return bitc::RMW_UMAX;
+  case AtomicRMWInst::UMin: return bitc::RMW_UMIN;
+  }
+}
+
+static unsigned getEncodedOrdering(AtomicOrdering Ordering) {
+  switch (Ordering) {
+  case AtomicOrdering::NotAtomic: return bitc::ORDERING_NOTATOMIC;
+  case AtomicOrdering::Unordered: return bitc::ORDERING_UNORDERED;
+  case AtomicOrdering::Monotonic: return bitc::ORDERING_MONOTONIC;
+  case AtomicOrdering::Acquire: return bitc::ORDERING_ACQUIRE;
+  case AtomicOrdering::Release: return bitc::ORDERING_RELEASE;
+  case AtomicOrdering::AcquireRelease: return bitc::ORDERING_ACQREL;
+  case AtomicOrdering::SequentiallyConsistent: return bitc::ORDERING_SEQCST;
+  }
+  llvm_unreachable("Invalid ordering");
+}
+
+static void writeStringRecord(BitstreamWriter &Stream, unsigned Code,
+                              StringRef Str, unsigned AbbrevToUse) {
+  SmallVector<unsigned, 64> Vals;
+
+  // Code: [strchar x N]
+  for (unsigned i = 0, e = Str.size(); i != e; ++i) {
+    if (AbbrevToUse && !BitCodeAbbrevOp::isChar6(Str[i]))
+      AbbrevToUse = 0;
+    Vals.push_back(Str[i]);
+  }
+
+  // Emit the finished record.
+  Stream.EmitRecord(Code, Vals, AbbrevToUse);
+}
+
+uint64_t getAttrKindEncodingBC50(Attribute::AttrKind Kind); // for use in ValueEnumerator50 as well
+uint64_t getAttrKindEncodingBC50(Attribute::AttrKind Kind) {
+  switch (Kind) {
+  default:
+    return bitc::ATTR_KIND_INVALID;
+  case Attribute::Alignment:
+    return bitc::ATTR_KIND_ALIGNMENT;
+  case Attribute::AllocSize:
+    return bitc::ATTR_KIND_ALLOC_SIZE;
+  case Attribute::AlwaysInline:
+    return bitc::ATTR_KIND_ALWAYS_INLINE;
+  case Attribute::ArgMemOnly:
+    return bitc::ATTR_KIND_ARGMEMONLY;
+  case Attribute::Builtin:
+    return bitc::ATTR_KIND_BUILTIN;
+  case Attribute::ByVal:
+    return bitc::ATTR_KIND_BY_VAL;
+  case Attribute::Convergent:
+    return bitc::ATTR_KIND_CONVERGENT;
+  case Attribute::InAlloca:
+    return bitc::ATTR_KIND_IN_ALLOCA;
+  case Attribute::Cold:
+    return bitc::ATTR_KIND_COLD;
+  case Attribute::InaccessibleMemOnly:
+    return bitc::ATTR_KIND_INACCESSIBLEMEM_ONLY;
+  case Attribute::InaccessibleMemOrArgMemOnly:
+    return bitc::ATTR_KIND_INACCESSIBLEMEM_OR_ARGMEMONLY;
+  case Attribute::InlineHint:
+    return bitc::ATTR_KIND_INLINE_HINT;
+  case Attribute::InReg:
+    return bitc::ATTR_KIND_IN_REG;
+  case Attribute::JumpTable:
+    return bitc::ATTR_KIND_JUMP_TABLE;
+  case Attribute::MinSize:
+    return bitc::ATTR_KIND_MIN_SIZE;
+  case Attribute::Naked:
+    return bitc::ATTR_KIND_NAKED;
+  case Attribute::Nest:
+    return bitc::ATTR_KIND_NEST;
+  case Attribute::NoAlias:
+    return bitc::ATTR_KIND_NO_ALIAS;
+  case Attribute::NoBuiltin:
+    return bitc::ATTR_KIND_NO_BUILTIN;
+  case Attribute::NoCapture:
+    return bitc::ATTR_KIND_NO_CAPTURE;
+  case Attribute::NoDuplicate:
+    return bitc::ATTR_KIND_NO_DUPLICATE;
+  case Attribute::NoImplicitFloat:
+    return bitc::ATTR_KIND_NO_IMPLICIT_FLOAT;
+  case Attribute::NoInline:
+    return bitc::ATTR_KIND_NO_INLINE;
+  case Attribute::NoRecurse:
+    return bitc::ATTR_KIND_NO_RECURSE;
+  case Attribute::NonLazyBind:
+    return bitc::ATTR_KIND_NON_LAZY_BIND;
+  case Attribute::NonNull:
+    return bitc::ATTR_KIND_NON_NULL;
+  case Attribute::Dereferenceable:
+    return bitc::ATTR_KIND_DEREFERENCEABLE;
+  case Attribute::DereferenceableOrNull:
+    return bitc::ATTR_KIND_DEREFERENCEABLE_OR_NULL;
+  case Attribute::NoRedZone:
+    return bitc::ATTR_KIND_NO_RED_ZONE;
+  case Attribute::NoReturn:
+    return bitc::ATTR_KIND_NO_RETURN;
+  case Attribute::NoUnwind:
+    return bitc::ATTR_KIND_NO_UNWIND;
+  case Attribute::OptimizeForSize:
+    return bitc::ATTR_KIND_OPTIMIZE_FOR_SIZE;
+  case Attribute::OptimizeNone:
+    return bitc::ATTR_KIND_OPTIMIZE_NONE;
+  case Attribute::ReadNone:
+    return bitc::ATTR_KIND_READ_NONE;
+  case Attribute::ReadOnly:
+    return bitc::ATTR_KIND_READ_ONLY;
+  case Attribute::Returned:
+    return bitc::ATTR_KIND_RETURNED;
+  case Attribute::ReturnsTwice:
+    return bitc::ATTR_KIND_RETURNS_TWICE;
+  case Attribute::SExt:
+    return bitc::ATTR_KIND_S_EXT;
+  case Attribute::Speculatable:
+    return bitc::ATTR_KIND_SPECULATABLE;
+  case Attribute::StackAlignment:
+    return bitc::ATTR_KIND_STACK_ALIGNMENT;
+  case Attribute::StackProtect:
+    return bitc::ATTR_KIND_STACK_PROTECT;
+  case Attribute::StackProtectReq:
+    return bitc::ATTR_KIND_STACK_PROTECT_REQ;
+  case Attribute::StackProtectStrong:
+    return bitc::ATTR_KIND_STACK_PROTECT_STRONG;
+  case Attribute::SafeStack:
+    return bitc::ATTR_KIND_SAFESTACK;
+  case Attribute::StructRet:
+    return bitc::ATTR_KIND_STRUCT_RET;
+  case Attribute::SanitizeAddress:
+    return bitc::ATTR_KIND_SANITIZE_ADDRESS;
+  case Attribute::SanitizeThread:
+    return bitc::ATTR_KIND_SANITIZE_THREAD;
+  case Attribute::SanitizeMemory:
+    return bitc::ATTR_KIND_SANITIZE_MEMORY;
+  case Attribute::SwiftError:
+    return bitc::ATTR_KIND_SWIFT_ERROR;
+  case Attribute::SwiftSelf:
+    return bitc::ATTR_KIND_SWIFT_SELF;
+  case Attribute::UWTable:
+    return bitc::ATTR_KIND_UW_TABLE;
+  case Attribute::WriteOnly:
+    return bitc::ATTR_KIND_WRITEONLY;
+  case Attribute::ZExt:
+    return bitc::ATTR_KIND_Z_EXT;
+  case Attribute::EndAttrKinds:
+    llvm_unreachable("Can not encode end-attribute kinds marker.");
+  case Attribute::None:
+    llvm_unreachable("Can not encode none-attribute.");
+  }
+
+  llvm_unreachable("Trying to encode unknown attribute");
+}
+
+void ModuleBitcodeWriter50::writeAttributeGroupTable() {
+  const std::vector<ValueEnumerator50::IndexAndAttrSet> &AttrGrps =
+      VE.getAttributeGroups();
+  if (AttrGrps.empty()) return;
+
+  Stream.EnterSubblock(bitc::PARAMATTR_GROUP_BLOCK_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+  for (ValueEnumerator50::IndexAndAttrSet Pair : AttrGrps) {
+    if (Pair.first == ValueEnumerator50::invalid_attribute_group_id) {
+      // this complete set/group can't be encoded for 5.0
+      continue;
+    }
+    unsigned AttrListIndex = Pair.first;
+    AttributeSet AS = Pair.second;
+    Record.push_back(VE.getAttributeGroupID(Pair));
+    Record.push_back(AttrListIndex);
+
+    for (Attribute Attr : AS) {
+      if (Attr.isEnumAttribute() || Attr.isIntAttribute()) {
+        // only encode valid/compatible attributes
+        const auto enc_attr = getAttrKindEncodingBC50(Attr.getKindAsEnum());
+        if (enc_attr != llvm::bitc::ATTR_KIND_INVALID) {
+          if (Attr.isEnumAttribute()) {
+            Record.push_back(0);
+            Record.push_back(enc_attr);
+          } else {
+            Record.push_back(1);
+            Record.push_back(enc_attr);
+            Record.push_back(Attr.getValueAsInt());
+          }
+        }
+      } else if (Attr.isStringAttribute()) {
+        StringRef Kind = Attr.getKindAsString();
+        StringRef Val = Attr.getValueAsString();
+
+        Record.push_back(Val.empty() ? 3 : 4);
+        Record.append(Kind.begin(), Kind.end());
+        Record.push_back(0);
+        if (!Val.empty()) {
+          Record.append(Val.begin(), Val.end());
+          Record.push_back(0);
+        }
+      } else {
+        assert(Attr.isTypeAttribute());
+        // NOTE: we do want to encode the "byval" attribute (in the 5.0 format -> no type)
+        if (Attr.getKindAsEnum() == Attribute::ByVal) {
+          const auto enc_attr = getAttrKindEncodingBC50(Attr.getKindAsEnum());
+          Record.push_back(0);
+          Record.push_back(enc_attr);
+        }
+        // else: ignore this
+      }
+    }
+
+    Stream.EmitRecord(bitc::PARAMATTR_GRP_CODE_ENTRY, Record);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeAttributeTable() {
+  const std::vector<AttributeList> &Attrs = VE.getAttributeLists();
+  if (Attrs.empty()) return;
+
+  Stream.EnterSubblock(bitc::PARAMATTR_BLOCK_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+  for (unsigned i = 0, e = Attrs.size(); i != e; ++i) {
+    AttributeList AL = Attrs[i];
+    for (unsigned i : AL.indexes()) {
+      AttributeSet AS = AL.getAttributes(i);
+      if (AS.hasAttributes())
+        if (const auto group_id = VE.getAttributeGroupID({i, AS}); group_id != ~0u)
+          Record.push_back(group_id);
+    }
+
+    Stream.EmitRecord(bitc::PARAMATTR_CODE_ENTRY, Record);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+/// WriteTypeTable - Write out the type table for a module.
+void ModuleBitcodeWriter50::writeTypeTable() {
+  const ValueEnumerator50::TypeList &TypeList = VE.getTypes();
+
+  Stream.EnterSubblock(bitc::TYPE_BLOCK_ID_NEW, 4 /*count from # abbrevs */);
+  SmallVector<uint64_t, 64> TypeVals;
+
+  uint64_t NumBits = VE.computeBitsRequiredForTypeIndicies();
+
+  // Abbrev for TYPE_CODE_POINTER.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_POINTER));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+  Abbv->Add(BitCodeAbbrevOp(0));  // Addrspace = 0
+  unsigned PtrAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_FUNCTION.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_FUNCTION));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // isvararg
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned FunctionAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_ANON.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_ANON));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // ispacked
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned StructAnonAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_NAME.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_NAME));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+  unsigned StructNameAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_STRUCT_NAMED.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_STRUCT_NAMED));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));  // ispacked
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned StructNamedAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for TYPE_CODE_ARRAY.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::TYPE_CODE_ARRAY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // size
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, NumBits));
+
+  unsigned ArrayAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Emit an entry count so the reader can reserve space.
+  TypeVals.push_back(TypeList.size());
+  Stream.EmitRecord(bitc::TYPE_CODE_NUMENTRY, TypeVals);
+  TypeVals.clear();
+
+  // Loop over all of the types, emitting each in turn.
+  for (unsigned i = 0, e = TypeList.size(); i != e; ++i) {
+    Type *T = TypeList[i];
+    int AbbrevToUse = 0;
+    unsigned Code = 0;
+
+    switch (T->getTypeID()) {
+    case Type::VoidTyID:      Code = bitc::TYPE_CODE_VOID;      break;
+    case Type::HalfTyID:      Code = bitc::TYPE_CODE_HALF;      break;
+    case Type::FloatTyID:     Code = bitc::TYPE_CODE_FLOAT;     break;
+    case Type::DoubleTyID:    Code = bitc::TYPE_CODE_DOUBLE;    break;
+    case Type::X86_FP80TyID:  Code = bitc::TYPE_CODE_X86_FP80;  break;
+    case Type::FP128TyID:     Code = bitc::TYPE_CODE_FP128;     break;
+    case Type::PPC_FP128TyID: Code = bitc::TYPE_CODE_PPC_FP128; break;
+    case Type::LabelTyID:     Code = bitc::TYPE_CODE_LABEL;     break;
+    case Type::MetadataTyID:  Code = bitc::TYPE_CODE_METADATA;  break;
+    case Type::X86_MMXTyID:   Code = bitc::TYPE_CODE_X86_MMX;   break;
+    case Type::TokenTyID:     Code = bitc::TYPE_CODE_TOKEN;     break;
+    case Type::IntegerTyID:
+      // INTEGER: [width]
+      Code = bitc::TYPE_CODE_INTEGER;
+      TypeVals.push_back(cast<IntegerType>(T)->getBitWidth());
+      break;
+    case Type::PointerTyID: {
+      PointerType *PTy = cast<PointerType>(T);
+      // POINTER: [pointee type, address space]
+      Code = bitc::TYPE_CODE_POINTER;
+      TypeVals.push_back(VE.getTypeID(PTy->getElementType()));
+      unsigned AddressSpace = PTy->getAddressSpace();
+      TypeVals.push_back(AddressSpace);
+      if (AddressSpace == 0) AbbrevToUse = PtrAbbrev;
+      break;
+    }
+    case Type::FunctionTyID: {
+      FunctionType *FT = cast<FunctionType>(T);
+      // FUNCTION: [isvararg, retty, paramty x N]
+      Code = bitc::TYPE_CODE_FUNCTION;
+      TypeVals.push_back(FT->isVarArg());
+      TypeVals.push_back(VE.getTypeID(FT->getReturnType()));
+      for (unsigned i = 0, e = FT->getNumParams(); i != e; ++i)
+        TypeVals.push_back(VE.getTypeID(FT->getParamType(i)));
+      AbbrevToUse = FunctionAbbrev;
+      break;
+    }
+    case Type::StructTyID: {
+      StructType *ST = cast<StructType>(T);
+      // STRUCT: [ispacked, eltty x N]
+      TypeVals.push_back(ST->isPacked());
+      // Output all of the element types.
+      for (StructType::element_iterator I = ST->element_begin(),
+           E = ST->element_end(); I != E; ++I)
+        TypeVals.push_back(VE.getTypeID(*I));
+
+      if (ST->isLiteral()) {
+        Code = bitc::TYPE_CODE_STRUCT_ANON;
+        AbbrevToUse = StructAnonAbbrev;
+      } else {
+        if (ST->isOpaque()) {
+          Code = bitc::TYPE_CODE_OPAQUE;
+        } else {
+          Code = bitc::TYPE_CODE_STRUCT_NAMED;
+          AbbrevToUse = StructNamedAbbrev;
+        }
+
+        // Emit the name if it is present.
+        if (!ST->getName().empty())
+          writeStringRecord(Stream, bitc::TYPE_CODE_STRUCT_NAME, ST->getName(),
+                            StructNameAbbrev);
+      }
+      break;
+    }
+    case Type::ArrayTyID: {
+      ArrayType *AT = cast<ArrayType>(T);
+      // ARRAY: [numelts, eltty]
+      Code = bitc::TYPE_CODE_ARRAY;
+      TypeVals.push_back(AT->getNumElements());
+      TypeVals.push_back(VE.getTypeID(AT->getElementType()));
+      AbbrevToUse = ArrayAbbrev;
+      break;
+    }
+    case Type::FixedVectorTyID: {
+      FixedVectorType *VT = cast<FixedVectorType>(T);
+      // VECTOR [numelts, eltty]
+      Code = bitc::TYPE_CODE_VECTOR;
+      TypeVals.push_back(VT->getNumElements());
+      TypeVals.push_back(VE.getTypeID(VT->getElementType()));
+      break;
+    }
+    case Type::ScalableVectorTyID:
+      report_fatal_error("scalar vector types are not supported with LLVM 5.0");
+      break;
+    case Type::BFloatTyID:
+      report_fatal_error("bfloat16 type is not supported with LLVM 5.0");
+      break;
+    case Type::X86_AMXTyID:
+      report_fatal_error("AMX types are not supported with LLVM 5.0");
+      break;
+    }
+
+    // Emit the finished record.
+    Stream.EmitRecord(Code, TypeVals, AbbrevToUse);
+    TypeVals.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static unsigned getEncodedLinkage(const GlobalValue::LinkageTypes Linkage) {
+  switch (Linkage) {
+  case GlobalValue::ExternalLinkage:
+    return 0;
+  case GlobalValue::WeakAnyLinkage:
+    return 16;
+  case GlobalValue::AppendingLinkage:
+    return 2;
+  case GlobalValue::InternalLinkage:
+    return 3;
+  case GlobalValue::LinkOnceAnyLinkage:
+    return 18;
+  case GlobalValue::ExternalWeakLinkage:
+    return 7;
+  case GlobalValue::CommonLinkage:
+    return 8;
+  case GlobalValue::PrivateLinkage:
+    return 9;
+  case GlobalValue::WeakODRLinkage:
+    return 17;
+  case GlobalValue::LinkOnceODRLinkage:
+    return 19;
+  case GlobalValue::AvailableExternallyLinkage:
+    return 12;
+  }
+  llvm_unreachable("Invalid linkage");
+}
+
+static unsigned getEncodedLinkage(const GlobalValue &GV) {
+  return getEncodedLinkage(GV.getLinkage());
+}
+
+// Decode the flags for GlobalValue in the summary
+static uint64_t getEncodedGVSummaryFlags(GlobalValueSummary::GVFlags Flags) {
+  uint64_t RawFlags = 0;
+
+  RawFlags |= Flags.NotEligibleToImport; // bool
+  RawFlags |= (Flags.Live << 1);
+  // Linkage don't need to be remapped at that time for the summary. Any future
+  // change to the getEncodedLinkage() function will need to be taken into
+  // account here as well.
+  RawFlags = (RawFlags << 4) | Flags.Linkage; // 4 bits
+
+  return RawFlags;
+}
+
+static unsigned getEncodedVisibility(const GlobalValue &GV) {
+  switch (GV.getVisibility()) {
+  case GlobalValue::DefaultVisibility:   return 0;
+  case GlobalValue::HiddenVisibility:    return 1;
+  case GlobalValue::ProtectedVisibility: return 2;
+  }
+  llvm_unreachable("Invalid visibility");
+}
+
+static unsigned getEncodedDLLStorageClass(const GlobalValue &GV) {
+  switch (GV.getDLLStorageClass()) {
+  case GlobalValue::DefaultStorageClass:   return 0;
+  case GlobalValue::DLLImportStorageClass: return 1;
+  case GlobalValue::DLLExportStorageClass: return 2;
+  }
+  llvm_unreachable("Invalid DLL storage class");
+}
+
+static unsigned getEncodedThreadLocalMode(const GlobalValue &GV) {
+  switch (GV.getThreadLocalMode()) {
+    case GlobalVariable::NotThreadLocal:         return 0;
+    case GlobalVariable::GeneralDynamicTLSModel: return 1;
+    case GlobalVariable::LocalDynamicTLSModel:   return 2;
+    case GlobalVariable::InitialExecTLSModel:    return 3;
+    case GlobalVariable::LocalExecTLSModel:      return 4;
+  }
+  llvm_unreachable("Invalid TLS model");
+}
+
+static unsigned getEncodedComdatSelectionKind(const Comdat &C) {
+  switch (C.getSelectionKind()) {
+  case Comdat::Any:
+    return bitc::COMDAT_SELECTION_KIND_ANY;
+  case Comdat::ExactMatch:
+    return bitc::COMDAT_SELECTION_KIND_EXACT_MATCH;
+  case Comdat::Largest:
+    return bitc::COMDAT_SELECTION_KIND_LARGEST;
+  case Comdat::NoDeduplicate:
+    return bitc::COMDAT_SELECTION_KIND_NO_DUPLICATES;
+  case Comdat::SameSize:
+    return bitc::COMDAT_SELECTION_KIND_SAME_SIZE;
+  }
+  llvm_unreachable("Invalid selection kind");
+}
+
+static unsigned getEncodedUnnamedAddr(const GlobalValue &GV) {
+  switch (GV.getUnnamedAddr()) {
+  case GlobalValue::UnnamedAddr::None:   return 0;
+  case GlobalValue::UnnamedAddr::Local:  return 2;
+  case GlobalValue::UnnamedAddr::Global: return 1;
+  }
+  llvm_unreachable("Invalid unnamed_addr");
+}
+
+size_t ModuleBitcodeWriter50::addToStrtab(StringRef Str) {
+  if (GenerateHash)
+    Hasher.update(Str);
+  return StrtabBuilder.add(Str);
+}
+
+void ModuleBitcodeWriter50::writeComdats() {
+  SmallVector<unsigned, 64> Vals;
+  for (const Comdat *C : VE.getComdats()) {
+    // COMDAT: [strtab offset, strtab size, selection_kind]
+    Vals.push_back(addToStrtab(C->getName()));
+    Vals.push_back(C->getName().size());
+    Vals.push_back(getEncodedComdatSelectionKind(*C));
+    Stream.EmitRecord(bitc::MODULE_CODE_COMDAT, Vals, /*AbbrevToUse=*/0);
+    Vals.clear();
+  }
+}
+
+/// Write a record that will eventually hold the word offset of the
+/// module-level VST. For now the offset is 0, which will be backpatched
+/// after the real VST is written. Saves the bit offset to backpatch.
+void ModuleBitcodeWriter50::writeValueSymbolTableForwardDecl() {
+  // Write a placeholder value in for the offset of the real VST,
+  // which is written after the function blocks so that it can include
+  // the offset of each function. The placeholder offset will be
+  // updated when the real VST is written.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::MODULE_CODE_VSTOFFSET));
+  // Blocks are 32-bit aligned, so we can use a 32-bit word offset to
+  // hold the real VST offset. Must use fixed instead of VBR as we don't
+  // know how many VBR chunks to reserve ahead of time.
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  unsigned VSTOffsetAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Emit the placeholder
+  uint64_t Vals[] = {bitc::MODULE_CODE_VSTOFFSET, 0};
+  Stream.EmitRecordWithAbbrev(VSTOffsetAbbrev, Vals);
+
+  // Compute and save the bit offset to the placeholder, which will be
+  // patched when the real VST is written. We can simply subtract the 32-bit
+  // fixed size from the current bit number to get the location to backpatch.
+  VSTOffsetPlaceholder = Stream.GetCurrentBitNo() - 32;
+}
+
+enum StringEncoding { SE_Char6, SE_Fixed7, SE_Fixed8 };
+
+/// Determine the encoding to use for the given string name and length.
+static StringEncoding getStringEncoding(StringRef Str) {
+  bool isChar6 = true;
+  for (char C : Str) {
+    if (isChar6)
+      isChar6 = BitCodeAbbrevOp::isChar6(C);
+    if ((unsigned char)C & 128)
+      // don't bother scanning the rest.
+      return SE_Fixed8;
+  }
+  if (isChar6)
+    return SE_Char6;
+  return SE_Fixed7;
+}
+
+/// Emit top-level description of module, including target triple, inline asm,
+/// descriptors for global variables, and function prototype info.
+/// Returns the bit offset to backpatch with the location of the real VST.
+void ModuleBitcodeWriter50::writeModuleInfo() {
+  // Emit various pieces of data attached to a module.
+  if (!M.getTargetTriple().empty())
+    writeStringRecord(Stream, bitc::MODULE_CODE_TRIPLE, M.getTargetTriple(),
+                      0 /*TODO*/);
+  const std::string &DL = M.getDataLayoutStr();
+  if (!DL.empty())
+    writeStringRecord(Stream, bitc::MODULE_CODE_DATALAYOUT, DL, 0 /*TODO*/);
+  if (!M.getModuleInlineAsm().empty())
+    writeStringRecord(Stream, bitc::MODULE_CODE_ASM, M.getModuleInlineAsm(),
+                      0 /*TODO*/);
+
+  // Emit information about sections and GC, computing how many there are. Also
+  // compute the maximum alignment value.
+  std::map<std::string, unsigned> SectionMap;
+  std::map<std::string, unsigned> GCMap;
+  MaybeAlign MaxAlignment;
+  unsigned MaxGlobalType = 0;
+  const auto UpdateMaxAlignment = [&MaxAlignment](const MaybeAlign A) {
+    if (A)
+      MaxAlignment = !MaxAlignment ? *A : std::max(*MaxAlignment, *A);
+  };
+  for (const GlobalVariable &GV : M.globals()) {
+    UpdateMaxAlignment(GV.getAlign());
+    MaxGlobalType = std::max(MaxGlobalType, VE.getTypeID(GV.getValueType()));
+    if (GV.hasSection()) {
+      // Give section names unique ID's.
+      unsigned &Entry = SectionMap[std::string(GV.getSection())];
+      if (!Entry) {
+        writeStringRecord(Stream, bitc::MODULE_CODE_SECTIONNAME, GV.getSection(),
+                          0 /*TODO*/);
+        Entry = SectionMap.size();
+      }
+    }
+  }
+  for (const Function &F : M) {
+    UpdateMaxAlignment(F.getAlign());
+    if (F.hasSection()) {
+      // Give section names unique ID's.
+      unsigned &Entry = SectionMap[std::string(F.getSection())];
+      if (!Entry) {
+        writeStringRecord(Stream, bitc::MODULE_CODE_SECTIONNAME, F.getSection(),
+                          0 /*TODO*/);
+        Entry = SectionMap.size();
+      }
+    }
+    if (F.hasGC()) {
+      // Same for GC names.
+      unsigned &Entry = GCMap[F.getGC()];
+      if (!Entry) {
+        writeStringRecord(Stream, bitc::MODULE_CODE_GCNAME, F.getGC(),
+                          0 /*TODO*/);
+        Entry = GCMap.size();
+      }
+    }
+  }
+
+  // Emit abbrev for globals, now that we know # sections and max alignment.
+  unsigned SimpleGVarAbbrev = 0;
+  if (!M.global_empty()) {
+    // Add an abbrev for common globals with no visibility or thread localness.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::MODULE_CODE_GLOBALVAR));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                              Log2_32_Ceil(MaxGlobalType+1)));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // AddrSpace << 2
+                                                           //| explicitType << 1
+                                                           //| constant
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // Initializer.
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 5)); // Linkage.
+    if (!MaxAlignment)                                     // Alignment.
+      Abbv->Add(BitCodeAbbrevOp(0));
+    else {
+      unsigned MaxEncAlignment = Log2_32(MaxAlignment.getValue().value())+1;
+      Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                               Log2_32_Ceil(MaxEncAlignment+1)));
+    }
+    if (SectionMap.empty())                                    // Section.
+      Abbv->Add(BitCodeAbbrevOp(0));
+    else
+      Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                               Log2_32_Ceil(SectionMap.size()+1)));
+    // Don't bother emitting vis + thread local.
+    SimpleGVarAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  }
+
+  SmallVector<unsigned, 64> Vals;
+  // Emit the module's source file name.
+  {
+    StringEncoding Bits = getStringEncoding(M.getSourceFileName());
+    BitCodeAbbrevOp AbbrevOpToUse = BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8);
+    if (Bits == SE_Char6)
+      AbbrevOpToUse = BitCodeAbbrevOp(BitCodeAbbrevOp::Char6);
+    else if (Bits == SE_Fixed7)
+      AbbrevOpToUse = BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7);
+
+    // MODULE_CODE_SOURCE_FILENAME: [namechar x N]
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::MODULE_CODE_SOURCE_FILENAME));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(AbbrevOpToUse);
+    unsigned FilenameAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+    for (const auto P : M.getSourceFileName())
+      Vals.push_back((unsigned char)P);
+
+    // Emit the finished record.
+    Stream.EmitRecord(bitc::MODULE_CODE_SOURCE_FILENAME, Vals, FilenameAbbrev);
+    Vals.clear();
+  }
+
+  // Emit the global variable information.
+  for (const GlobalVariable &GV : M.globals()) {
+    unsigned AbbrevToUse = 0;
+
+    // GLOBALVAR: [strtab offset, strtab size, type, isconst, initid,
+    //             linkage, alignment, section, visibility, threadlocal,
+    //             unnamed_addr, externally_initialized, dllstorageclass,
+    //             comdat, attributes]
+    Vals.push_back(addToStrtab(GV.getName()));
+    Vals.push_back(GV.getName().size());
+    Vals.push_back(VE.getTypeID(GV.getValueType()));
+    Vals.push_back(GV.getType()->getAddressSpace() << 2 | 2 | GV.isConstant());
+    Vals.push_back(GV.isDeclaration() ? 0 :
+                   (VE.getValueID(GV.getInitializer()) + 1));
+    Vals.push_back(getEncodedLinkage(GV));
+    Vals.push_back(Log2_32(GV.getAlignment())+1);
+    Vals.push_back(GV.hasSection() ? SectionMap[std::string(GV.getSection())] : 0);
+    if (GV.isThreadLocal() ||
+        GV.getVisibility() != GlobalValue::DefaultVisibility ||
+        GV.getUnnamedAddr() != GlobalValue::UnnamedAddr::None ||
+        GV.isExternallyInitialized() ||
+        GV.getDLLStorageClass() != GlobalValue::DefaultStorageClass ||
+        GV.hasComdat() ||
+        GV.hasAttributes()) {
+      Vals.push_back(getEncodedVisibility(GV));
+      Vals.push_back(getEncodedThreadLocalMode(GV));
+      Vals.push_back(getEncodedUnnamedAddr(GV));
+      Vals.push_back(GV.isExternallyInitialized());
+      Vals.push_back(getEncodedDLLStorageClass(GV));
+      Vals.push_back(GV.hasComdat() ? VE.getComdatID(GV.getComdat()) : 0);
+
+      auto AL = GV.getAttributesAsList(AttributeList::FunctionIndex);
+      Vals.push_back(VE.getAttributeListID(AL));
+    } else {
+      AbbrevToUse = SimpleGVarAbbrev;
+    }
+
+    Stream.EmitRecord(bitc::MODULE_CODE_GLOBALVAR, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  // Emit the function proto information.
+  for (const Function &F : M) {
+    // FUNCTION:  [strtab offset, strtab size, type, callingconv, isproto,
+    //             linkage, paramattrs, alignment, section, visibility, gc,
+    //             unnamed_addr, prologuedata, dllstorageclass, comdat,
+    //             prefixdata, personalityfn]
+    Vals.push_back(addToStrtab(F.getName()));
+    Vals.push_back(F.getName().size());
+    Vals.push_back(VE.getTypeID(F.getFunctionType()));
+    Vals.push_back(F.getCallingConv());
+    Vals.push_back(F.isDeclaration());
+    Vals.push_back(getEncodedLinkage(F));
+    Vals.push_back(VE.getAttributeListID(F.getAttributes()));
+    Vals.push_back(Log2_32(F.getAlignment())+1);
+    Vals.push_back(F.hasSection() ? SectionMap[std::string(F.getSection())] : 0);
+    Vals.push_back(getEncodedVisibility(F));
+    Vals.push_back(F.hasGC() ? GCMap[F.getGC()] : 0);
+    Vals.push_back(getEncodedUnnamedAddr(F));
+    Vals.push_back(F.hasPrologueData() ? (VE.getValueID(F.getPrologueData()) + 1)
+                                       : 0);
+    Vals.push_back(getEncodedDLLStorageClass(F));
+    Vals.push_back(F.hasComdat() ? VE.getComdatID(F.getComdat()) : 0);
+    Vals.push_back(F.hasPrefixData() ? (VE.getValueID(F.getPrefixData()) + 1)
+                                     : 0);
+    Vals.push_back(
+        F.hasPersonalityFn() ? (VE.getValueID(F.getPersonalityFn()) + 1) : 0);
+
+    unsigned AbbrevToUse = 0;
+    Stream.EmitRecord(bitc::MODULE_CODE_FUNCTION, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  // Emit the alias information.
+  for (const GlobalAlias &A : M.aliases()) {
+    // ALIAS: [strtab offset, strtab size, alias type, aliasee val#, linkage,
+    //         visibility, dllstorageclass, threadlocal, unnamed_addr]
+    Vals.push_back(addToStrtab(A.getName()));
+    Vals.push_back(A.getName().size());
+    Vals.push_back(VE.getTypeID(A.getValueType()));
+    Vals.push_back(A.getType()->getAddressSpace());
+    Vals.push_back(VE.getValueID(A.getAliasee()));
+    Vals.push_back(getEncodedLinkage(A));
+    Vals.push_back(getEncodedVisibility(A));
+    Vals.push_back(getEncodedDLLStorageClass(A));
+    Vals.push_back(getEncodedThreadLocalMode(A));
+    Vals.push_back(getEncodedUnnamedAddr(A));
+    unsigned AbbrevToUse = 0;
+    Stream.EmitRecord(bitc::MODULE_CODE_ALIAS, Vals, AbbrevToUse);
+    Vals.clear();
+  }
+
+  // Emit the ifunc information.
+  for (const GlobalIFunc &I : M.ifuncs()) {
+    // IFUNC: [strtab offset, strtab size, ifunc type, address space, resolver
+    //         val#, linkage, visibility]
+    Vals.push_back(addToStrtab(I.getName()));
+    Vals.push_back(I.getName().size());
+    Vals.push_back(VE.getTypeID(I.getValueType()));
+    Vals.push_back(I.getType()->getAddressSpace());
+    Vals.push_back(VE.getValueID(I.getResolver()));
+    Vals.push_back(getEncodedLinkage(I));
+    Vals.push_back(getEncodedVisibility(I));
+    Stream.EmitRecord(bitc::MODULE_CODE_IFUNC, Vals);
+    Vals.clear();
+  }
+
+  writeValueSymbolTableForwardDecl();
+}
+
+static uint64_t getOptimizationFlags(const Value *V) {
+  uint64_t Flags = 0;
+
+  if (const auto *OBO = dyn_cast<OverflowingBinaryOperator>(V)) {
+    if (OBO->hasNoSignedWrap())
+      Flags |= 1 << bitc::OBO_NO_SIGNED_WRAP;
+    if (OBO->hasNoUnsignedWrap())
+      Flags |= 1 << bitc::OBO_NO_UNSIGNED_WRAP;
+  } else if (const auto *PEO = dyn_cast<PossiblyExactOperator>(V)) {
+    if (PEO->isExact())
+      Flags |= 1 << bitc::PEO_EXACT;
+  } else if (const auto *FPMO = dyn_cast<FPMathOperator>(V)) {
+    if (FPMO->isFast())
+      Flags |= bitc::UnsafeAlgebra;
+    if (FPMO->hasNoNaNs())
+      Flags |= bitc::NoNaNs;
+    if (FPMO->hasNoInfs())
+      Flags |= bitc::NoInfs;
+    if (FPMO->hasNoSignedZeros())
+      Flags |= bitc::NoSignedZeros;
+    if (FPMO->hasAllowReciprocal())
+      Flags |= bitc::AllowReciprocal;
+    if (FPMO->hasAllowContract())
+      Flags |= bitc::AllowContract;
+  }
+
+  return Flags;
+}
+
+void ModuleBitcodeWriter50::writeValueAsMetadata(
+    const ValueAsMetadata *MD, SmallVectorImpl<uint64_t> &Record) {
+  // Mimic an MDNode with a value as one operand.
+  Value *V = MD->getValue();
+  Record.push_back(VE.getTypeID(V->getType()));
+  Record.push_back(VE.getValueID(V));
+  Stream.EmitRecord(bitc::METADATA_VALUE, Record, 0);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeMDTuple(const MDTuple *N,
+                                       SmallVectorImpl<uint64_t> &Record,
+                                       unsigned Abbrev) {
+  for (unsigned i = 0, e = N->getNumOperands(); i != e; ++i) {
+    Metadata *MD = N->getOperand(i);
+    assert(!(MD && isa<LocalAsMetadata>(MD)) &&
+           "Unexpected function-local metadata");
+    Record.push_back(VE.getMetadataOrNullID(MD));
+  }
+  Stream.EmitRecord(N->isDistinct() ? bitc::METADATA_DISTINCT_NODE
+                                    : bitc::METADATA_NODE,
+                    Record, Abbrev);
+  Record.clear();
+}
+
+unsigned ModuleBitcodeWriter50::createDILocationAbbrev() {
+  // Assume the column is usually under 128, and always output the inlined-at
+  // location (it's never more expensive than building an array size 1).
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_LOCATION));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  return Stream.EmitAbbrev(std::move(Abbv));
+}
+
+void ModuleBitcodeWriter50::writeDILocation(const DILocation *N,
+                                          SmallVectorImpl<uint64_t> &Record,
+                                          unsigned &Abbrev) {
+  if (!Abbrev)
+    Abbrev = createDILocationAbbrev();
+
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getLine());
+  Record.push_back(N->getColumn());
+  Record.push_back(VE.getMetadataID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getInlinedAt()));
+
+  Stream.EmitRecord(bitc::METADATA_LOCATION, Record, Abbrev);
+  Record.clear();
+}
+
+unsigned ModuleBitcodeWriter50::createGenericDINodeAbbrev() {
+  // Assume the column is usually under 128, and always output the inlined-at
+  // location (it's never more expensive than building an array size 1).
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_GENERIC_DEBUG));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  return Stream.EmitAbbrev(std::move(Abbv));
+}
+
+void ModuleBitcodeWriter50::writeGenericDINode(const GenericDINode *N,
+                                             SmallVectorImpl<uint64_t> &Record,
+                                             unsigned &Abbrev) {
+  if (!Abbrev)
+    Abbrev = createGenericDINodeAbbrev();
+
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(0); // Per-tag version field; unused for now.
+
+  for (auto &I : N->operands())
+    Record.push_back(VE.getMetadataOrNullID(I));
+
+  Stream.EmitRecord(bitc::METADATA_GENERIC_DEBUG, Record, Abbrev);
+  Record.clear();
+}
+
+static uint64_t rotateSign(int64_t I) {
+  uint64_t U = I;
+  return I < 0 ? ~(U << 1) : U << 1;
+}
+
+void ModuleBitcodeWriter50::writeDISubrange(const DISubrange *N,
+                                            SmallVectorImpl<uint64_t> &Record,
+                                            unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  ConstantInt *CI = nullptr;
+  if (auto cnt = N->getCount(); cnt) {
+    CI = cnt.get<ConstantInt *>();
+  }
+  const auto LBMD = dyn_cast_or_null<ConstantAsMetadata>(N->getRawLowerBound());
+  if (CI && LBMD) {
+    Record.push_back(CI->getSExtValue());
+    const auto LB = cast<ConstantInt>(LBMD->getValue());
+    Record.push_back(rotateSign(LB->getZExtValue()));
+  } else {
+    Record.push_back(int64_t(0));
+    Record.push_back(uint64_t(0));
+  }
+
+  Stream.EmitRecord(bitc::METADATA_SUBRANGE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIEnumerator(const DIEnumerator *N,
+                                            SmallVectorImpl<uint64_t> &Record,
+                                            unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(rotateSign(N->getValue().getZExtValue()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+
+  Stream.EmitRecord(bitc::METADATA_ENUMERATOR, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIBasicType(const DIBasicType *N,
+                                           SmallVectorImpl<uint64_t> &Record,
+                                           unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getEncoding());
+
+  Stream.EmitRecord(bitc::METADATA_BASIC_TYPE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIDerivedType(const DIDerivedType *N,
+                                             SmallVectorImpl<uint64_t> &Record,
+                                             unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getBaseType()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getOffsetInBits());
+  Record.push_back(N->getFlags());
+  Record.push_back(VE.getMetadataOrNullID(N->getExtraData()));
+
+  // DWARF address space is encoded as N->getDWARFAddressSpace() + 1. 0 means
+  // that there is no DWARF address space associated with DIDerivedType.
+  if (const auto &DWARFAddressSpace = N->getDWARFAddressSpace())
+    Record.push_back(*DWARFAddressSpace + 1);
+  else
+    Record.push_back(0);
+
+  Stream.EmitRecord(bitc::METADATA_DERIVED_TYPE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDICompositeType(
+    const DICompositeType *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  const unsigned IsNotUsedInOldTypeRef = 0x2;
+  Record.push_back(IsNotUsedInOldTypeRef | (unsigned)N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getBaseType()));
+  Record.push_back(N->getSizeInBits());
+  Record.push_back(N->getAlignInBits());
+  Record.push_back(N->getOffsetInBits());
+  Record.push_back(N->getFlags());
+  Record.push_back(VE.getMetadataOrNullID(N->getElements().get()));
+  Record.push_back(N->getRuntimeLang());
+  Record.push_back(VE.getMetadataOrNullID(N->getVTableHolder()));
+  Record.push_back(VE.getMetadataOrNullID(N->getTemplateParams().get()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawIdentifier()));
+
+  Stream.EmitRecord(bitc::METADATA_COMPOSITE_TYPE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDISubroutineType(
+    const DISubroutineType *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  const unsigned HasNoOldTypeRefs = 0x2;
+  Record.push_back(HasNoOldTypeRefs | (unsigned)N->isDistinct());
+  Record.push_back(N->getFlags());
+  Record.push_back(VE.getMetadataOrNullID(N->getTypeArray().get()));
+  Record.push_back(N->getCC());
+
+  Stream.EmitRecord(bitc::METADATA_SUBROUTINE_TYPE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIFile(const DIFile *N,
+                                      SmallVectorImpl<uint64_t> &Record,
+                                      unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawFilename()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawDirectory()));
+  if (N->getRawChecksum()) {
+    Record.push_back(N->getRawChecksum()->Kind);
+    Record.push_back(VE.getMetadataOrNullID(N->getRawChecksum()->Value));
+  } else {
+    // Maintain backwards compatibility with the old internal representation of
+    // CSK_None in ChecksumKind by writing nulls here when Checksum is None.
+    Record.push_back(0);
+    Record.push_back(VE.getMetadataOrNullID(nullptr));
+  }
+
+  Stream.EmitRecord(bitc::METADATA_FILE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDICompileUnit(const DICompileUnit *N,
+                                             SmallVectorImpl<uint64_t> &Record,
+                                             unsigned Abbrev) {
+  assert(N->isDistinct() && "Expected distinct compile units");
+  Record.push_back(/* IsDistinct */ true);
+  Record.push_back(N->getSourceLanguage());
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawProducer()));
+  Record.push_back(N->isOptimized());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawFlags()));
+  Record.push_back(N->getRuntimeVersion());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawSplitDebugFilename()));
+  Record.push_back(N->getEmissionKind());
+  Record.push_back(VE.getMetadataOrNullID(N->getEnumTypes().get()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRetainedTypes().get()));
+  Record.push_back(/* subprograms */ 0);
+  Record.push_back(VE.getMetadataOrNullID(N->getGlobalVariables().get()));
+  Record.push_back(VE.getMetadataOrNullID(N->getImportedEntities().get()));
+  Record.push_back(N->getDWOId());
+  Record.push_back(VE.getMetadataOrNullID(N->getMacros().get()));
+  Record.push_back(N->getSplitDebugInlining());
+  Record.push_back(N->getDebugInfoForProfiling());
+
+  Stream.EmitRecord(bitc::METADATA_COMPILE_UNIT, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDISubprogram(const DISubprogram *N,
+                                            SmallVectorImpl<uint64_t> &Record,
+                                            unsigned Abbrev) {
+  uint64_t HasUnitFlag = 1 << 1;
+  Record.push_back(N->isDistinct() | HasUnitFlag);
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawLinkageName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(N->isLocalToUnit());
+  Record.push_back(N->isDefinition());
+  Record.push_back(N->getScopeLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getContainingType()));
+  Record.push_back(N->getVirtuality());
+  Record.push_back(N->getVirtualIndex());
+  Record.push_back(N->getFlags());
+  Record.push_back(N->isOptimized());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawUnit()));
+  Record.push_back(VE.getMetadataOrNullID(N->getTemplateParams().get()));
+  Record.push_back(VE.getMetadataOrNullID(N->getDeclaration()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRetainedNodes().get()));
+  Record.push_back(N->getThisAdjustment());
+  Record.push_back(VE.getMetadataOrNullID(N->getThrownTypes().get()));
+
+  Stream.EmitRecord(bitc::METADATA_SUBPROGRAM, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDILexicalBlock(const DILexicalBlock *N,
+                                              SmallVectorImpl<uint64_t> &Record,
+                                              unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(N->getColumn());
+
+  Stream.EmitRecord(bitc::METADATA_LEXICAL_BLOCK, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDILexicalBlockFile(
+    const DILexicalBlockFile *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getDiscriminator());
+
+  Stream.EmitRecord(bitc::METADATA_LEXICAL_BLOCK_FILE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDINamespace(const DINamespace *N,
+                                           SmallVectorImpl<uint64_t> &Record,
+                                           unsigned Abbrev) {
+  Record.push_back(N->isDistinct() | N->getExportSymbols() << 1);
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+
+  Stream.EmitRecord(bitc::METADATA_NAMESPACE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIMacro(const DIMacro *N,
+                                       SmallVectorImpl<uint64_t> &Record,
+                                       unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getMacinfoType());
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawValue()));
+
+  Stream.EmitRecord(bitc::METADATA_MACRO, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIMacroFile(const DIMacroFile *N,
+                                           SmallVectorImpl<uint64_t> &Record,
+                                           unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getMacinfoType());
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(VE.getMetadataOrNullID(N->getElements().get()));
+
+  Stream.EmitRecord(bitc::METADATA_MACRO_FILE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIModule(const DIModule *N,
+                                        SmallVectorImpl<uint64_t> &Record,
+                                        unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  for (auto &I : N->operands())
+    Record.push_back(VE.getMetadataOrNullID(I));
+
+  Stream.EmitRecord(bitc::METADATA_MODULE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDITemplateTypeParameter(
+    const DITemplateTypeParameter *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+
+  Stream.EmitRecord(bitc::METADATA_TEMPLATE_TYPE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDITemplateValueParameter(
+    const DITemplateValueParameter *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(VE.getMetadataOrNullID(N->getValue()));
+
+  Stream.EmitRecord(bitc::METADATA_TEMPLATE_VALUE, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIGlobalVariable(
+    const DIGlobalVariable *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  const uint64_t Version = 1 << 1;
+  Record.push_back((uint64_t)N->isDistinct() | Version);
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawLinkageName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(N->isLocalToUnit());
+  Record.push_back(N->isDefinition());
+  Record.push_back(/* expr */ 0);
+  Record.push_back(VE.getMetadataOrNullID(N->getStaticDataMemberDeclaration()));
+  Record.push_back(N->getAlignInBits());
+
+  Stream.EmitRecord(bitc::METADATA_GLOBAL_VAR, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDILocalVariable(
+    const DILocalVariable *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  // In order to support all possible bitcode formats in BitcodeReader we need
+  // to distinguish the following cases:
+  // 1) Record has no artificial tag (Record[1]),
+  //   has no obsolete inlinedAt field (Record[9]).
+  //   In this case Record size will be 8, HasAlignment flag is false.
+  // 2) Record has artificial tag (Record[1]),
+  //   has no obsolete inlignedAt field (Record[9]).
+  //   In this case Record size will be 9, HasAlignment flag is false.
+  // 3) Record has both artificial tag (Record[1]) and
+  //   obsolete inlignedAt field (Record[9]).
+  //   In this case Record size will be 10, HasAlignment flag is false.
+  // 4) Record has neither artificial tag, nor inlignedAt field, but
+  //   HasAlignment flag is true and Record[8] contains alignment value.
+  const uint64_t HasAlignmentFlag = 1 << 1;
+  Record.push_back((uint64_t)N->isDistinct() | HasAlignmentFlag);
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+  Record.push_back(N->getArg());
+  Record.push_back(N->getFlags());
+  Record.push_back(N->getAlignInBits());
+
+  Stream.EmitRecord(bitc::METADATA_LOCAL_VAR, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIExpression(const DIExpression *N,
+                                            SmallVectorImpl<uint64_t> &Record,
+                                            unsigned Abbrev) {
+  Record.reserve(N->getElements().size() + 1);
+  const uint64_t Version = 3 << 1;
+  Record.push_back((uint64_t)N->isDistinct() | Version);
+  Record.append(N->elements_begin(), N->elements_end());
+
+  Stream.EmitRecord(bitc::METADATA_EXPRESSION, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIGlobalVariableExpression(
+    const DIGlobalVariableExpression *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getVariable()));
+  Record.push_back(VE.getMetadataOrNullID(N->getExpression()));
+  
+  Stream.EmitRecord(bitc::METADATA_GLOBAL_VAR_EXPR, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIObjCProperty(const DIObjCProperty *N,
+                                              SmallVectorImpl<uint64_t> &Record,
+                                              unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getFile()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawSetterName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawGetterName()));
+  Record.push_back(N->getAttributes());
+  Record.push_back(VE.getMetadataOrNullID(N->getType()));
+
+  Stream.EmitRecord(bitc::METADATA_OBJC_PROPERTY, Record, Abbrev);
+  Record.clear();
+}
+
+void ModuleBitcodeWriter50::writeDIImportedEntity(
+    const DIImportedEntity *N, SmallVectorImpl<uint64_t> &Record,
+    unsigned Abbrev) {
+  Record.push_back(N->isDistinct());
+  Record.push_back(N->getTag());
+  Record.push_back(VE.getMetadataOrNullID(N->getScope()));
+  Record.push_back(VE.getMetadataOrNullID(N->getEntity()));
+  Record.push_back(N->getLine());
+  Record.push_back(VE.getMetadataOrNullID(N->getRawName()));
+  Record.push_back(VE.getMetadataOrNullID(N->getRawFile()));
+
+  Stream.EmitRecord(bitc::METADATA_IMPORTED_ENTITY, Record, Abbrev);
+  Record.clear();
+}
+
+unsigned ModuleBitcodeWriter50::createNamedMetadataAbbrev() {
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_NAME));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+  return Stream.EmitAbbrev(std::move(Abbv));
+}
+
+void ModuleBitcodeWriter50::writeNamedMetadata(
+    SmallVectorImpl<uint64_t> &Record) {
+  if (M.named_metadata_empty())
+    return;
+
+  unsigned Abbrev = createNamedMetadataAbbrev();
+  for (const NamedMDNode &NMD : M.named_metadata()) {
+    // Write name.
+    StringRef Str = NMD.getName();
+    Record.append(Str.bytes_begin(), Str.bytes_end());
+    Stream.EmitRecord(bitc::METADATA_NAME, Record, Abbrev);
+    Record.clear();
+
+    // Write named metadata operands.
+    for (const MDNode *N : NMD.operands())
+      Record.push_back(VE.getMetadataID(N));
+    Stream.EmitRecord(bitc::METADATA_NAMED_NODE, Record, 0);
+    Record.clear();
+  }
+}
+
+unsigned ModuleBitcodeWriter50::createMetadataStringsAbbrev() {
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_STRINGS));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // # of strings
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // offset to chars
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Blob));
+  return Stream.EmitAbbrev(std::move(Abbv));
+}
+
+/// Write out a record for MDString.
+///
+/// All the metadata strings in a metadata block are emitted in a single
+/// record.  The sizes and strings themselves are shoved into a blob.
+void ModuleBitcodeWriter50::writeMetadataStrings(
+    ArrayRef<const Metadata *> Strings, SmallVectorImpl<uint64_t> &Record) {
+  if (Strings.empty())
+    return;
+
+  // Start the record with the number of strings.
+  Record.push_back(bitc::METADATA_STRINGS);
+  Record.push_back(Strings.size());
+
+  // Emit the sizes of the strings in the blob.
+  SmallString<256> Blob;
+  {
+    BitstreamWriter W(Blob);
+    for (const Metadata *MD : Strings)
+      W.EmitVBR(cast<MDString>(MD)->getLength(), 6);
+    W.FlushToWord();
+  }
+
+  // Add the offset to the strings to the record.
+  Record.push_back(Blob.size());
+
+  // Add the strings to the blob.
+  for (const Metadata *MD : Strings)
+    Blob.append(cast<MDString>(MD)->getString());
+
+  // Emit the final record.
+  Stream.EmitRecordWithBlob(createMetadataStringsAbbrev(), Record, Blob);
+  Record.clear();
+}
+
+// Generates an enum to use as an index in the Abbrev array of Metadata record.
+enum MetadataAbbrev : unsigned {
+#define HANDLE_MDNODE_LEAF(CLASS) CLASS##AbbrevID,
+#include "llvm/IR/Metadata50.def"
+  LastPlusOne
+};
+
+void ModuleBitcodeWriter50::writeMetadataRecords(
+    ArrayRef<const Metadata *> MDs, SmallVectorImpl<uint64_t> &Record,
+    std::vector<unsigned> *MDAbbrevs, std::vector<uint64_t> *IndexPos) {
+  if (MDs.empty())
+    return;
+
+  // Initialize MDNode abbreviations.
+#define HANDLE_MDNODE_LEAF(CLASS) unsigned CLASS##Abbrev = 0;
+#include "llvm/IR/Metadata50.def"
+
+  for (const Metadata *MD : MDs) {
+    if (IndexPos)
+      IndexPos->push_back(Stream.GetCurrentBitNo());
+    if (const MDNode *N = dyn_cast<MDNode>(MD)) {
+      assert(N->isResolved() && "Expected forward references to be resolved");
+
+      switch (N->getMetadataID()) {
+      default:
+        llvm_unreachable("Invalid MDNode subclass");
+#define HANDLE_MDNODE_LEAF(CLASS)                                              \
+  case Metadata::CLASS##Kind:                                                  \
+    if (MDAbbrevs)                                                             \
+      write##CLASS(cast<CLASS>(N), Record,                                     \
+                   (*MDAbbrevs)[MetadataAbbrev::CLASS##AbbrevID]);             \
+    else                                                                       \
+      write##CLASS(cast<CLASS>(N), Record, CLASS##Abbrev);                     \
+    continue;
+#include "llvm/IR/Metadata50.def"
+      }
+    }
+    writeValueAsMetadata(cast<ValueAsMetadata>(MD), Record);
+  }
+}
+
+void ModuleBitcodeWriter50::writeModuleMetadata() {
+  if (!VE.hasMDs() && M.named_metadata_empty())
+    return;
+
+  Stream.EnterSubblock(bitc::METADATA_BLOCK_ID, 4);
+  SmallVector<uint64_t, 64> Record;
+
+  // Emit all abbrevs upfront, so that the reader can jump in the middle of the
+  // block and load any metadata.
+  std::vector<unsigned> MDAbbrevs;
+
+  MDAbbrevs.resize(MetadataAbbrev::LastPlusOne);
+  MDAbbrevs[MetadataAbbrev::DILocationAbbrevID] = createDILocationAbbrev();
+  MDAbbrevs[MetadataAbbrev::GenericDINodeAbbrevID] =
+      createGenericDINodeAbbrev();
+
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_INDEX_OFFSET));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  unsigned OffsetAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  (void)OffsetAbbrev; // unused
+
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::METADATA_INDEX));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  unsigned IndexAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  (void)IndexAbbrev; // unused
+
+  // Emit MDStrings together upfront.
+  writeMetadataStrings(VE.getMDStrings(), Record);
+
+#if 0 // don't want this
+  // We only emit an index for the metadata record if we have more than a given
+  // (naive) threshold of metadatas, otherwise it is not worth it.
+  if (VE.getNonMDStrings().size() > IndexThreshold) {
+    // Write a placeholder value in for the offset of the metadata index,
+    // which is written after the records, so that it can include
+    // the offset of each entry. The placeholder offset will be
+    // updated after all records are emitted.
+    uint64_t Vals[] = {0, 0};
+    Stream.EmitRecord(bitc::METADATA_INDEX_OFFSET, Vals, OffsetAbbrev);
+  }
+#endif
+
+  // Compute and save the bit offset to the current position, which will be
+  // patched when we emit the index later. We can simply subtract the 64-bit
+  // fixed size from the current bit number to get the location to backpatch.
+  uint64_t IndexOffsetRecordBitPos = Stream.GetCurrentBitNo();
+  (void)IndexOffsetRecordBitPos; // unused
+
+  // This index will contain the bitpos for each individual record.
+  std::vector<uint64_t> IndexPos;
+  IndexPos.reserve(VE.getNonMDStrings().size());
+
+  // Write all the records
+  writeMetadataRecords(VE.getNonMDStrings(), Record, &MDAbbrevs, &IndexPos);
+
+#if 0 // don't want this
+  if (VE.getNonMDStrings().size() > IndexThreshold) {
+    // Now that we have emitted all the records we will emit the index. But
+    // first
+    // backpatch the forward reference so that the reader can skip the records
+    // efficiently.
+    Stream.BackpatchWord64(IndexOffsetRecordBitPos - 64,
+                           Stream.GetCurrentBitNo() - IndexOffsetRecordBitPos);
+
+    // Delta encode the index.
+    uint64_t PreviousValue = IndexOffsetRecordBitPos;
+    for (auto &Elt : IndexPos) {
+      auto EltDelta = Elt - PreviousValue;
+      PreviousValue = Elt;
+      Elt = EltDelta;
+    }
+    // Emit the index record.
+    Stream.EmitRecord(bitc::METADATA_INDEX, IndexPos, IndexAbbrev);
+    IndexPos.clear();
+  }
+#endif
+
+  // Write the named metadata now.
+  writeNamedMetadata(Record);
+
+  auto AddDeclAttachedMetadata = [&](const GlobalObject &GO) {
+    SmallVector<uint64_t, 4> Record;
+    Record.push_back(VE.getValueID(&GO));
+    pushGlobalMetadataAttachment(Record, GO);
+    Stream.EmitRecord(bitc::METADATA_GLOBAL_DECL_ATTACHMENT, Record);
+  };
+  for (const Function &F : M)
+    if (F.isDeclaration() && F.hasMetadata())
+      AddDeclAttachedMetadata(F);
+  // FIXME: Only store metadata for declarations here, and move data for global
+  // variable definitions to a separate block (PR28134).
+  for (const GlobalVariable &GV : M.globals())
+    if (GV.hasMetadata())
+      AddDeclAttachedMetadata(GV);
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeFunctionMetadata(const Function &F) {
+  if (!VE.hasMDs())
+    return;
+
+  Stream.EnterSubblock(bitc::METADATA_BLOCK_ID, 3);
+  SmallVector<uint64_t, 64> Record;
+  writeMetadataStrings(VE.getMDStrings(), Record);
+  writeMetadataRecords(VE.getNonMDStrings(), Record);
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::pushGlobalMetadataAttachment(
+    SmallVectorImpl<uint64_t> &Record, const GlobalObject &GO) {
+  // [n x [id, mdnode]]
+  SmallVector<std::pair<unsigned, MDNode *>, 4> MDs;
+  GO.getAllMetadata(MDs);
+  for (const auto &I : MDs) {
+    Record.push_back(I.first);
+    Record.push_back(VE.getMetadataID(I.second));
+  }
+}
+
+void ModuleBitcodeWriter50::writeFunctionMetadataAttachment(const Function &F) {
+  Stream.EnterSubblock(bitc::METADATA_ATTACHMENT_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+
+  if (F.hasMetadata()) {
+    pushGlobalMetadataAttachment(Record, F);
+    Stream.EmitRecord(bitc::METADATA_ATTACHMENT, Record, 0);
+    Record.clear();
+  }
+
+  // Write metadata attachments
+  // METADATA_ATTACHMENT - [m x [value, [n x [id, mdnode]]]
+  SmallVector<std::pair<unsigned, MDNode *>, 4> MDs;
+  for (const BasicBlock &BB : F)
+    for (const Instruction &I : BB) {
+      MDs.clear();
+      I.getAllMetadataOtherThanDebugLoc(MDs);
+
+      // If no metadata, ignore instruction.
+      if (MDs.empty()) continue;
+
+      Record.push_back(VE.getInstructionID(&I));
+
+      for (unsigned i = 0, e = MDs.size(); i != e; ++i) {
+        Record.push_back(MDs[i].first);
+        Record.push_back(VE.getMetadataID(MDs[i].second));
+      }
+      Stream.EmitRecord(bitc::METADATA_ATTACHMENT, Record, 0);
+      Record.clear();
+    }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeModuleMetadataKinds() {
+  SmallVector<uint64_t, 64> Record;
+
+  // Write metadata kinds
+  // METADATA_KIND - [n x [id, name]]
+  SmallVector<StringRef, 8> Names;
+  M.getMDKindNames(Names);
+
+  if (Names.empty()) return;
+
+  Stream.EnterSubblock(bitc::METADATA_KIND_BLOCK_ID, 3);
+
+  for (unsigned MDKindID = 0, e = Names.size(); MDKindID != e; ++MDKindID) {
+    Record.push_back(MDKindID);
+    StringRef KName = Names[MDKindID];
+    Record.append(KName.begin(), KName.end());
+
+    Stream.EmitRecord(bitc::METADATA_KIND, Record, 0);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeOperandBundleTags() {
+  // Write metadata kinds
+  //
+  // OPERAND_BUNDLE_TAGS_BLOCK_ID : N x OPERAND_BUNDLE_TAG
+  //
+  // OPERAND_BUNDLE_TAG - [strchr x N]
+
+  SmallVector<StringRef, 8> Tags;
+  M.getOperandBundleTags(Tags);
+
+  if (Tags.empty())
+    return;
+
+  Stream.EnterSubblock(bitc::OPERAND_BUNDLE_TAGS_BLOCK_ID, 3);
+
+  SmallVector<uint64_t, 64> Record;
+
+  for (auto Tag : Tags) {
+    Record.append(Tag.begin(), Tag.end());
+
+    Stream.EmitRecord(bitc::OPERAND_BUNDLE_TAG, Record, 0);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeSyncScopeNames() {
+  SmallVector<StringRef, 8> SSNs;
+  M.getContext().getSyncScopeNames(SSNs);
+  if (SSNs.empty())
+    return;
+
+  Stream.EnterSubblock(bitc::SYNC_SCOPE_NAMES_BLOCK_ID, 2);
+
+  SmallVector<uint64_t, 64> Record;
+  for (auto SSN : SSNs) {
+    Record.append(SSN.begin(), SSN.end());
+    Stream.EmitRecord(bitc::SYNC_SCOPE_NAME, Record, 0);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+static void emitSignedInt64(SmallVectorImpl<uint64_t> &Vals, uint64_t V) {
+  if ((int64_t)V >= 0)
+    Vals.push_back(V << 1);
+  else
+    Vals.push_back((-V << 1) | 1);
+}
+
+void ModuleBitcodeWriter50::writeConstants(unsigned FirstVal, unsigned LastVal,
+                                         bool isGlobal) {
+  if (FirstVal == LastVal) return;
+
+  Stream.EnterSubblock(bitc::CONSTANTS_BLOCK_ID, 4);
+
+  unsigned AggregateAbbrev = 0;
+  unsigned String8Abbrev = 0;
+  unsigned CString7Abbrev = 0;
+  unsigned CString6Abbrev = 0;
+  // If this is a constant pool for the module, emit module-specific abbrevs.
+  if (isGlobal) {
+    // Abbrev for CST_CODE_AGGREGATE.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_AGGREGATE));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, Log2_32_Ceil(LastVal+1)));
+    AggregateAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+    // Abbrev for CST_CODE_STRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_STRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+    String8Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+    // Abbrev for CST_CODE_CSTRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CSTRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7));
+    CString7Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+    // Abbrev for CST_CODE_CSTRING.
+    Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CSTRING));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    CString6Abbrev = Stream.EmitAbbrev(std::move(Abbv));
+  }
+
+  SmallVector<uint64_t, 64> Record;
+
+  const ValueEnumerator50::ValueList &Vals = VE.getValues();
+  Type *LastTy = nullptr;
+  for (unsigned i = FirstVal; i != LastVal; ++i) {
+    const Value *V = Vals[i].first;
+    // If we need to switch types, do so now.
+    if (V->getType() != LastTy) {
+      LastTy = V->getType();
+      Record.push_back(VE.getTypeID(LastTy));
+      Stream.EmitRecord(bitc::CST_CODE_SETTYPE, Record,
+                        CONSTANTS_SETTYPE_ABBREV);
+      Record.clear();
+    }
+
+    if (const InlineAsm *IA = dyn_cast<InlineAsm>(V)) {
+      Record.push_back(unsigned(IA->hasSideEffects()) |
+                       unsigned(IA->isAlignStack()) << 1 |
+                       unsigned(IA->getDialect()&1) << 2);
+
+      // Add the asm string.
+      const std::string &AsmStr = IA->getAsmString();
+      Record.push_back(AsmStr.size());
+      Record.append(AsmStr.begin(), AsmStr.end());
+
+      // Add the constraint string.
+      const std::string &ConstraintStr = IA->getConstraintString();
+      Record.push_back(ConstraintStr.size());
+      Record.append(ConstraintStr.begin(), ConstraintStr.end());
+      Stream.EmitRecord(bitc::CST_CODE_INLINEASM_OLD2, Record);
+      Record.clear();
+      continue;
+    }
+    const Constant *C = cast<Constant>(V);
+    unsigned Code = -1U;
+    unsigned AbbrevToUse = 0;
+    if (C->isNullValue()) {
+      Code = bitc::CST_CODE_NULL;
+    } else if (isa<UndefValue>(C)) {
+      Code = bitc::CST_CODE_UNDEF;
+    } else if (const ConstantInt *IV = dyn_cast<ConstantInt>(C)) {
+      if (IV->getBitWidth() <= 64) {
+        uint64_t V = IV->getSExtValue();
+        emitSignedInt64(Record, V);
+        Code = bitc::CST_CODE_INTEGER;
+        AbbrevToUse = CONSTANTS_INTEGER_ABBREV;
+      } else {                             // Wide integers, > 64 bits in size.
+        // We have an arbitrary precision integer value to write whose
+        // bit width is > 64. However, in canonical unsigned integer
+        // format it is likely that the high bits are going to be zero.
+        // So, we only write the number of active words.
+        unsigned NWords = IV->getValue().getActiveWords();
+        const uint64_t *RawWords = IV->getValue().getRawData();
+        for (unsigned i = 0; i != NWords; ++i) {
+          emitSignedInt64(Record, RawWords[i]);
+        }
+        Code = bitc::CST_CODE_WIDE_INTEGER;
+      }
+    } else if (const ConstantFP *CFP = dyn_cast<ConstantFP>(C)) {
+      Code = bitc::CST_CODE_FLOAT;
+      Type *Ty = CFP->getType();
+      if (Ty->isHalfTy() || Ty->isFloatTy() || Ty->isDoubleTy()) {
+        Record.push_back(CFP->getValueAPF().bitcastToAPInt().getZExtValue());
+      } else if (Ty->isX86_FP80Ty()) {
+        // api needed to prevent premature destruction
+        // bits are not in the same order as a normal i80 APInt, compensate.
+        APInt api = CFP->getValueAPF().bitcastToAPInt();
+        const uint64_t *p = api.getRawData();
+        Record.push_back((p[1] << 48) | (p[0] >> 16));
+        Record.push_back(p[0] & 0xffffLL);
+      } else if (Ty->isFP128Ty() || Ty->isPPC_FP128Ty()) {
+        APInt api = CFP->getValueAPF().bitcastToAPInt();
+        const uint64_t *p = api.getRawData();
+        Record.push_back(p[0]);
+        Record.push_back(p[1]);
+      } else {
+        assert (0 && "Unknown FP type!");
+      }
+    } else if (isa<ConstantDataSequential>(C) &&
+               cast<ConstantDataSequential>(C)->isString()) {
+      const ConstantDataSequential *Str = cast<ConstantDataSequential>(C);
+      // Emit constant strings specially.
+      unsigned NumElts = Str->getNumElements();
+      // If this is a null-terminated string, use the denser CSTRING encoding.
+      if (Str->isCString()) {
+        Code = bitc::CST_CODE_CSTRING;
+        --NumElts;  // Don't encode the null, which isn't allowed by char6.
+      } else {
+        Code = bitc::CST_CODE_STRING;
+        AbbrevToUse = String8Abbrev;
+      }
+      bool isCStr7 = Code == bitc::CST_CODE_CSTRING;
+      bool isCStrChar6 = Code == bitc::CST_CODE_CSTRING;
+      for (unsigned i = 0; i != NumElts; ++i) {
+        unsigned char V = Str->getElementAsInteger(i);
+        Record.push_back(V);
+        isCStr7 &= (V & 128) == 0;
+        if (isCStrChar6)
+          isCStrChar6 = BitCodeAbbrevOp::isChar6(V);
+      }
+
+      if (isCStrChar6)
+        AbbrevToUse = CString6Abbrev;
+      else if (isCStr7)
+        AbbrevToUse = CString7Abbrev;
+    } else if (const ConstantDataSequential *CDS =
+                  dyn_cast<ConstantDataSequential>(C)) {
+      Code = bitc::CST_CODE_DATA;
+      Type *EltTy = CDS->getElementType();
+      if (isa<IntegerType>(EltTy)) {
+        for (unsigned i = 0, e = CDS->getNumElements(); i != e; ++i)
+          Record.push_back(CDS->getElementAsInteger(i));
+      } else {
+        for (unsigned i = 0, e = CDS->getNumElements(); i != e; ++i)
+          Record.push_back(
+              CDS->getElementAsAPFloat(i).bitcastToAPInt().getLimitedValue());
+      }
+    } else if (isa<ConstantAggregate>(C)) {
+      Code = bitc::CST_CODE_AGGREGATE;
+      for (const Value *Op : C->operands())
+        Record.push_back(VE.getValueID(Op));
+      AbbrevToUse = AggregateAbbrev;
+    } else if (const ConstantExpr *CE = dyn_cast<ConstantExpr>(C)) {
+      switch (CE->getOpcode()) {
+      default:
+        if (Instruction::isCast(CE->getOpcode())) {
+          Code = bitc::CST_CODE_CE_CAST;
+          Record.push_back(getEncodedCastOpcode(CE->getOpcode()));
+          Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+          Record.push_back(VE.getValueID(C->getOperand(0)));
+          AbbrevToUse = CONSTANTS_CE_CAST_Abbrev;
+        } else {
+          assert(CE->getNumOperands() == 2 && "Unknown constant expr!");
+          Code = bitc::CST_CODE_CE_BINOP;
+          Record.push_back(getEncodedBinaryOpcode(CE->getOpcode()));
+          Record.push_back(VE.getValueID(C->getOperand(0)));
+          Record.push_back(VE.getValueID(C->getOperand(1)));
+          uint64_t Flags = getOptimizationFlags(CE);
+          if (Flags != 0)
+            Record.push_back(Flags);
+        }
+        break;
+      case Instruction::GetElementPtr: {
+        Code = bitc::CST_CODE_CE_GEP;
+        const auto *GO = cast<GEPOperator>(C);
+        Record.push_back(VE.getTypeID(GO->getSourceElementType()));
+        if (Optional<unsigned> Idx = GO->getInRangeIndex()) {
+          Code = bitc::CST_CODE_CE_GEP_WITH_INRANGE_INDEX;
+          Record.push_back((*Idx << 1) | GO->isInBounds());
+        } else if (GO->isInBounds())
+          Code = bitc::CST_CODE_CE_INBOUNDS_GEP;
+        for (unsigned i = 0, e = CE->getNumOperands(); i != e; ++i) {
+          Record.push_back(VE.getTypeID(C->getOperand(i)->getType()));
+          Record.push_back(VE.getValueID(C->getOperand(i)));
+        }
+        break;
+      }
+      case Instruction::Select:
+        Code = bitc::CST_CODE_CE_SELECT;
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ExtractElement:
+        Code = bitc::CST_CODE_CE_EXTRACTELT;
+        Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getTypeID(C->getOperand(1)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        break;
+      case Instruction::InsertElement:
+        Code = bitc::CST_CODE_CE_INSERTELT;
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getTypeID(C->getOperand(2)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ShuffleVector:
+        // If the return type and argument types are the same, this is a
+        // standard shufflevector instruction.  If the types are different,
+        // then the shuffle is widening or truncating the input vectors, and
+        // the argument type must also be encoded.
+        if (C->getType() == C->getOperand(0)->getType()) {
+          Code = bitc::CST_CODE_CE_SHUFFLEVEC;
+        } else {
+          Code = bitc::CST_CODE_CE_SHUFVEC_EX;
+          Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        }
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(VE.getValueID(C->getOperand(2)));
+        break;
+      case Instruction::ICmp:
+      case Instruction::FCmp:
+        Code = bitc::CST_CODE_CE_CMP;
+        Record.push_back(VE.getTypeID(C->getOperand(0)->getType()));
+        Record.push_back(VE.getValueID(C->getOperand(0)));
+        Record.push_back(VE.getValueID(C->getOperand(1)));
+        Record.push_back(CE->getPredicate());
+        break;
+      }
+    } else if (const BlockAddress *BA = dyn_cast<BlockAddress>(C)) {
+      Code = bitc::CST_CODE_BLOCKADDRESS;
+      Record.push_back(VE.getTypeID(BA->getFunction()->getType()));
+      Record.push_back(VE.getValueID(BA->getFunction()));
+      Record.push_back(VE.getGlobalBasicBlockID(BA->getBasicBlock()));
+    } else {
+#ifndef NDEBUG
+      C->dump();
+#endif
+      llvm_unreachable("Unknown constant!");
+    }
+    Stream.EmitRecord(Code, Record, AbbrevToUse);
+    Record.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeModuleConstants() {
+  const ValueEnumerator50::ValueList &Vals = VE.getValues();
+
+  // Find the first constant to emit, which is the first non-globalvalue value.
+  // We know globalvalues have been emitted by WriteModuleInfo.
+  for (unsigned i = 0, e = Vals.size(); i != e; ++i) {
+    if (!isa<GlobalValue>(Vals[i].first)) {
+      writeConstants(i, Vals.size(), true);
+      return;
+    }
+  }
+}
+
+/// pushValueAndType - The file has to encode both the value and type id for
+/// many values, because we need to know what type to create for forward
+/// references.  However, most operands are not forward references, so this type
+/// field is not needed.
+///
+/// This function adds V's value ID to Vals.  If the value ID is higher than the
+/// instruction ID, then it is a forward reference, and it also includes the
+/// type ID.  The value ID that is written is encoded relative to the InstID.
+bool ModuleBitcodeWriter50::pushValueAndType(const Value *V, unsigned InstID,
+                                           SmallVectorImpl<unsigned> &Vals) {
+  unsigned ValID = VE.getValueID(V);
+  // Make encoding relative to the InstID.
+  Vals.push_back(InstID - ValID);
+  if (ValID >= InstID) {
+    Vals.push_back(VE.getTypeID(V->getType()));
+    return true;
+  }
+  return false;
+}
+
+void ModuleBitcodeWriter50::writeOperandBundles(const CallBase &CS,
+                                                unsigned InstID) {
+  SmallVector<unsigned, 64> Record;
+  LLVMContext &C = CS.getContext();
+
+  for (unsigned i = 0, e = CS.getNumOperandBundles(); i != e; ++i) {
+    const auto &Bundle = CS.getOperandBundleAt(i);
+    Record.push_back(C.getOperandBundleTagID(Bundle.getTagName()));
+
+    for (auto &Input : Bundle.Inputs)
+      pushValueAndType(Input, InstID, Record);
+
+    Stream.EmitRecord(bitc::FUNC_CODE_OPERAND_BUNDLE, Record);
+    Record.clear();
+  }
+}
+
+/// pushValue - Like pushValueAndType, but where the type of the value is
+/// omitted (perhaps it was already encoded in an earlier operand).
+void ModuleBitcodeWriter50::pushValue(const Value *V, unsigned InstID,
+                                    SmallVectorImpl<unsigned> &Vals) {
+  unsigned ValID = VE.getValueID(V);
+  Vals.push_back(InstID - ValID);
+}
+
+void ModuleBitcodeWriter50::pushValueSigned(const Value *V, unsigned InstID,
+                                          SmallVectorImpl<uint64_t> &Vals) {
+  unsigned ValID = VE.getValueID(V);
+  int64_t diff = ((int32_t)InstID - (int32_t)ValID);
+  emitSignedInt64(Vals, diff);
+}
+
+/// WriteInstruction - Emit an instruction to the specified stream.
+void ModuleBitcodeWriter50::writeInstruction(const Instruction &I,
+                                           unsigned InstID,
+                                           SmallVectorImpl<unsigned> &Vals) {
+  unsigned Code = 0;
+  unsigned AbbrevToUse = 0;
+  VE.setInstructionID(&I);
+  switch (I.getOpcode()) {
+  default:
+    if (Instruction::isCast(I.getOpcode())) {
+      Code = bitc::FUNC_CODE_INST_CAST;
+      if (!pushValueAndType(I.getOperand(0), InstID, Vals))
+        AbbrevToUse = FUNCTION_INST_CAST_ABBREV;
+      Vals.push_back(VE.getTypeID(I.getType()));
+      Vals.push_back(getEncodedCastOpcode(I.getOpcode()));
+    } else {
+      assert(isa<BinaryOperator>(I) && "Unknown instruction!");
+      Code = bitc::FUNC_CODE_INST_BINOP;
+      if (!pushValueAndType(I.getOperand(0), InstID, Vals))
+        AbbrevToUse = FUNCTION_INST_BINOP_ABBREV;
+      pushValue(I.getOperand(1), InstID, Vals);
+      Vals.push_back(getEncodedBinaryOpcode(I.getOpcode()));
+      uint64_t Flags = getOptimizationFlags(&I);
+      if (Flags != 0) {
+        if (AbbrevToUse == FUNCTION_INST_BINOP_ABBREV)
+          AbbrevToUse = FUNCTION_INST_BINOP_FLAGS_ABBREV;
+        Vals.push_back(Flags);
+      }
+    }
+    break;
+
+  case Instruction::FNeg: {
+    // emit as "fsub -0, value"
+    Code = bitc::FUNC_CODE_INST_BINOP;
+    pushValue(ConstantFP::get(I.getOperand(0)->getType(), -0.0), InstID, Vals);
+    if (!pushValueAndType(I.getOperand(0), InstID, Vals))
+      AbbrevToUse = FUNCTION_INST_BINOP_ABBREV;
+    Vals.push_back(getEncodedBinaryOpcode(Instruction::FSub));
+    uint64_t Flags = getOptimizationFlags(&I);
+    if (Flags != 0) {
+      if (AbbrevToUse == FUNCTION_INST_BINOP_ABBREV)
+        AbbrevToUse = FUNCTION_INST_BINOP_FLAGS_ABBREV;
+      Vals.push_back(Flags);
+    }
+    break;
+  }
+  case Instruction::GetElementPtr: {
+    Code = bitc::FUNC_CODE_INST_GEP;
+    AbbrevToUse = FUNCTION_INST_GEP_ABBREV;
+    auto &GEPInst = cast<GetElementPtrInst>(I);
+    Vals.push_back(GEPInst.isInBounds());
+    Vals.push_back(VE.getTypeID(GEPInst.getSourceElementType()));
+    for (unsigned i = 0, e = I.getNumOperands(); i != e; ++i)
+      pushValueAndType(I.getOperand(i), InstID, Vals);
+    break;
+  }
+  case Instruction::ExtractValue: {
+    Code = bitc::FUNC_CODE_INST_EXTRACTVAL;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    const ExtractValueInst *EVI = cast<ExtractValueInst>(&I);
+    Vals.append(EVI->idx_begin(), EVI->idx_end());
+    break;
+  }
+  case Instruction::InsertValue: {
+    Code = bitc::FUNC_CODE_INST_INSERTVAL;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    pushValueAndType(I.getOperand(1), InstID, Vals);
+    const InsertValueInst *IVI = cast<InsertValueInst>(&I);
+    Vals.append(IVI->idx_begin(), IVI->idx_end());
+    break;
+  }
+  case Instruction::Select:
+    Code = bitc::FUNC_CODE_INST_VSELECT;
+    pushValueAndType(I.getOperand(1), InstID, Vals);
+    pushValue(I.getOperand(2), InstID, Vals);
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    break;
+  case Instruction::ExtractElement:
+    Code = bitc::FUNC_CODE_INST_EXTRACTELT;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    pushValueAndType(I.getOperand(1), InstID, Vals);
+    break;
+  case Instruction::InsertElement:
+    Code = bitc::FUNC_CODE_INST_INSERTELT;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    pushValue(I.getOperand(1), InstID, Vals);
+    pushValueAndType(I.getOperand(2), InstID, Vals);
+    break;
+  case Instruction::ShuffleVector:
+    Code = bitc::FUNC_CODE_INST_SHUFFLEVEC;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    pushValue(I.getOperand(1), InstID, Vals);
+    //pushValue(I.getOperand(2), InstID, Vals); // TODO: ???
+    pushValue(cast<ShuffleVectorInst>(I).getShuffleMaskForBitcode(), InstID,
+              Vals);
+    break;
+  case Instruction::ICmp:
+  case Instruction::FCmp: {
+    // compare returning Int1Ty or vector of Int1Ty
+    Code = bitc::FUNC_CODE_INST_CMP2;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    pushValue(I.getOperand(1), InstID, Vals);
+    Vals.push_back(cast<CmpInst>(I).getPredicate());
+    uint64_t Flags = getOptimizationFlags(&I);
+    if (Flags != 0)
+      Vals.push_back(Flags);
+    break;
+  }
+
+  case Instruction::Ret:
+    {
+      Code = bitc::FUNC_CODE_INST_RET;
+      unsigned NumOperands = I.getNumOperands();
+      if (NumOperands == 0)
+        AbbrevToUse = FUNCTION_INST_RET_VOID_ABBREV;
+      else if (NumOperands == 1) {
+        if (!pushValueAndType(I.getOperand(0), InstID, Vals))
+          AbbrevToUse = FUNCTION_INST_RET_VAL_ABBREV;
+      } else {
+        for (unsigned i = 0, e = NumOperands; i != e; ++i)
+          pushValueAndType(I.getOperand(i), InstID, Vals);
+      }
+    }
+    break;
+  case Instruction::Br:
+    {
+      Code = bitc::FUNC_CODE_INST_BR;
+      const BranchInst &II = cast<BranchInst>(I);
+      Vals.push_back(VE.getValueID(II.getSuccessor(0)));
+      if (II.isConditional()) {
+        Vals.push_back(VE.getValueID(II.getSuccessor(1)));
+        pushValue(II.getCondition(), InstID, Vals);
+      }
+    }
+    break;
+  case Instruction::Switch:
+    {
+      Code = bitc::FUNC_CODE_INST_SWITCH;
+      const SwitchInst &SI = cast<SwitchInst>(I);
+      Vals.push_back(VE.getTypeID(SI.getCondition()->getType()));
+      pushValue(SI.getCondition(), InstID, Vals);
+      Vals.push_back(VE.getValueID(SI.getDefaultDest()));
+      for (auto Case : SI.cases()) {
+        Vals.push_back(VE.getValueID(Case.getCaseValue()));
+        Vals.push_back(VE.getValueID(Case.getCaseSuccessor()));
+      }
+    }
+    break;
+  case Instruction::IndirectBr:
+    Code = bitc::FUNC_CODE_INST_INDIRECTBR;
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));
+    // Encode the address operand as relative, but not the basic blocks.
+    pushValue(I.getOperand(0), InstID, Vals);
+    for (unsigned i = 1, e = I.getNumOperands(); i != e; ++i)
+      Vals.push_back(VE.getValueID(I.getOperand(i)));
+    break;
+
+  case Instruction::Invoke: {
+    const InvokeInst *II = cast<InvokeInst>(&I);
+    const Value *Callee = II->getCalledOperand();
+    FunctionType *FTy = II->getFunctionType();
+
+    if (II->hasOperandBundles())
+      writeOperandBundles(*II, InstID);
+
+    Code = bitc::FUNC_CODE_INST_INVOKE;
+
+    Vals.push_back(VE.getAttributeListID(II->getAttributes()));
+    Vals.push_back(II->getCallingConv() | 1 << 13);
+    Vals.push_back(VE.getValueID(II->getNormalDest()));
+    Vals.push_back(VE.getValueID(II->getUnwindDest()));
+    Vals.push_back(VE.getTypeID(FTy));
+    pushValueAndType(Callee, InstID, Vals);
+
+    // Emit value #'s for the fixed parameters.
+    for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i)
+      pushValue(I.getOperand(i), InstID, Vals); // fixed param.
+
+    // Emit type/value pairs for varargs params.
+    if (FTy->isVarArg()) {
+      for (unsigned i = FTy->getNumParams(), e = II->arg_size(); i != e; ++i)
+        pushValueAndType(I.getOperand(i), InstID, Vals); // vararg
+    }
+    break;
+  }
+  case Instruction::Resume:
+    Code = bitc::FUNC_CODE_INST_RESUME;
+    pushValueAndType(I.getOperand(0), InstID, Vals);
+    break;
+  case Instruction::CleanupRet: {
+    Code = bitc::FUNC_CODE_INST_CLEANUPRET;
+    const auto &CRI = cast<CleanupReturnInst>(I);
+    pushValue(CRI.getCleanupPad(), InstID, Vals);
+    if (CRI.hasUnwindDest())
+      Vals.push_back(VE.getValueID(CRI.getUnwindDest()));
+    break;
+  }
+  case Instruction::CatchRet: {
+    Code = bitc::FUNC_CODE_INST_CATCHRET;
+    const auto &CRI = cast<CatchReturnInst>(I);
+    pushValue(CRI.getCatchPad(), InstID, Vals);
+    Vals.push_back(VE.getValueID(CRI.getSuccessor()));
+    break;
+  }
+  case Instruction::CleanupPad:
+  case Instruction::CatchPad: {
+    const auto &FuncletPad = cast<FuncletPadInst>(I);
+    Code = isa<CatchPadInst>(FuncletPad) ? bitc::FUNC_CODE_INST_CATCHPAD
+                                         : bitc::FUNC_CODE_INST_CLEANUPPAD;
+    pushValue(FuncletPad.getParentPad(), InstID, Vals);
+
+    unsigned NumArgOperands = FuncletPad.getNumArgOperands();
+    Vals.push_back(NumArgOperands);
+    for (unsigned Op = 0; Op != NumArgOperands; ++Op)
+      pushValueAndType(FuncletPad.getArgOperand(Op), InstID, Vals);
+    break;
+  }
+  case Instruction::CatchSwitch: {
+    Code = bitc::FUNC_CODE_INST_CATCHSWITCH;
+    const auto &CatchSwitch = cast<CatchSwitchInst>(I);
+
+    pushValue(CatchSwitch.getParentPad(), InstID, Vals);
+
+    unsigned NumHandlers = CatchSwitch.getNumHandlers();
+    Vals.push_back(NumHandlers);
+    for (const BasicBlock *CatchPadBB : CatchSwitch.handlers())
+      Vals.push_back(VE.getValueID(CatchPadBB));
+
+    if (CatchSwitch.hasUnwindDest())
+      Vals.push_back(VE.getValueID(CatchSwitch.getUnwindDest()));
+    break;
+  }
+  case Instruction::Unreachable:
+    Code = bitc::FUNC_CODE_INST_UNREACHABLE;
+    AbbrevToUse = FUNCTION_INST_UNREACHABLE_ABBREV;
+    break;
+
+  case Instruction::PHI: {
+    const PHINode &PN = cast<PHINode>(I);
+    Code = bitc::FUNC_CODE_INST_PHI;
+    // With the newer instruction encoding, forward references could give
+    // negative valued IDs.  This is most common for PHIs, so we use
+    // signed VBRs.
+    SmallVector<uint64_t, 128> Vals64;
+    Vals64.push_back(VE.getTypeID(PN.getType()));
+    for (unsigned i = 0, e = PN.getNumIncomingValues(); i != e; ++i) {
+      pushValueSigned(PN.getIncomingValue(i), InstID, Vals64);
+      Vals64.push_back(VE.getValueID(PN.getIncomingBlock(i)));
+    }
+    // Emit a Vals64 vector and exit.
+    Stream.EmitRecord(Code, Vals64, AbbrevToUse);
+    Vals64.clear();
+    return;
+  }
+
+  case Instruction::LandingPad: {
+    const LandingPadInst &LP = cast<LandingPadInst>(I);
+    Code = bitc::FUNC_CODE_INST_LANDINGPAD;
+    Vals.push_back(VE.getTypeID(LP.getType()));
+    Vals.push_back(LP.isCleanup());
+    Vals.push_back(LP.getNumClauses());
+    for (unsigned I = 0, E = LP.getNumClauses(); I != E; ++I) {
+      if (LP.isCatch(I))
+        Vals.push_back(LandingPadInst::Catch);
+      else
+        Vals.push_back(LandingPadInst::Filter);
+      pushValueAndType(LP.getClause(I), InstID, Vals);
+    }
+    break;
+  }
+
+  case Instruction::Alloca: {
+    Code = bitc::FUNC_CODE_INST_ALLOCA;
+    const AllocaInst &AI = cast<AllocaInst>(I);
+    Vals.push_back(VE.getTypeID(AI.getAllocatedType()));
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));
+    Vals.push_back(VE.getValueID(I.getOperand(0))); // size.
+    unsigned AlignRecord = Log2_32(AI.getAlignment()) + 1;
+    assert(Log2_32(AI.getAlignment()) + 1 < 1 << 5 &&
+           "not enough bits for maximum alignment");
+    assert(AlignRecord < 1 << 5 && "alignment greater than 1 << 64");
+    AlignRecord |= AI.isUsedWithInAlloca() << 5;
+    AlignRecord |= 1 << 6;
+    AlignRecord |= AI.isSwiftError() << 7;
+    Vals.push_back(AlignRecord);
+    break;
+  }
+
+  case Instruction::Load:
+    if (cast<LoadInst>(I).isAtomic()) {
+      Code = bitc::FUNC_CODE_INST_LOADATOMIC;
+      pushValueAndType(I.getOperand(0), InstID, Vals);
+    } else {
+      Code = bitc::FUNC_CODE_INST_LOAD;
+      if (!pushValueAndType(I.getOperand(0), InstID, Vals)) // ptr
+        AbbrevToUse = FUNCTION_INST_LOAD_ABBREV;
+    }
+    Vals.push_back(VE.getTypeID(I.getType()));
+    Vals.push_back(Log2_32(cast<LoadInst>(I).getAlignment())+1);
+    Vals.push_back(cast<LoadInst>(I).isVolatile());
+    if (cast<LoadInst>(I).isAtomic()) {
+      Vals.push_back(getEncodedOrdering(cast<LoadInst>(I).getOrdering()));
+      Vals.push_back(getEncodedSyncScopeID(cast<LoadInst>(I).getSyncScopeID()));
+    }
+    break;
+  case Instruction::Store:
+    if (cast<StoreInst>(I).isAtomic())
+      Code = bitc::FUNC_CODE_INST_STOREATOMIC;
+    else
+      Code = bitc::FUNC_CODE_INST_STORE;
+    pushValueAndType(I.getOperand(1), InstID, Vals); // ptrty + ptr
+    pushValueAndType(I.getOperand(0), InstID, Vals); // valty + val
+    Vals.push_back(Log2_32(cast<StoreInst>(I).getAlignment())+1);
+    Vals.push_back(cast<StoreInst>(I).isVolatile());
+    if (cast<StoreInst>(I).isAtomic()) {
+      Vals.push_back(getEncodedOrdering(cast<StoreInst>(I).getOrdering()));
+      Vals.push_back(
+          getEncodedSyncScopeID(cast<StoreInst>(I).getSyncScopeID()));
+    }
+    break;
+  case Instruction::AtomicCmpXchg:
+    Code = bitc::FUNC_CODE_INST_CMPXCHG;
+    pushValueAndType(I.getOperand(0), InstID, Vals); // ptrty + ptr
+    pushValueAndType(I.getOperand(1), InstID, Vals); // cmp.
+    pushValue(I.getOperand(2), InstID, Vals);        // newval.
+    Vals.push_back(cast<AtomicCmpXchgInst>(I).isVolatile());
+    Vals.push_back(
+        getEncodedOrdering(cast<AtomicCmpXchgInst>(I).getSuccessOrdering()));
+    Vals.push_back(
+        getEncodedSyncScopeID(cast<AtomicCmpXchgInst>(I).getSyncScopeID()));
+    Vals.push_back(
+        getEncodedOrdering(cast<AtomicCmpXchgInst>(I).getFailureOrdering()));
+    Vals.push_back(cast<AtomicCmpXchgInst>(I).isWeak());
+    break;
+  case Instruction::AtomicRMW:
+    Code = bitc::FUNC_CODE_INST_ATOMICRMW_OLD;
+    pushValueAndType(I.getOperand(0), InstID, Vals); // ptrty + ptr
+    pushValue(I.getOperand(1), InstID, Vals);        // val.
+    Vals.push_back(
+        getEncodedRMWOperation(cast<AtomicRMWInst>(I).getOperation()));
+    Vals.push_back(cast<AtomicRMWInst>(I).isVolatile());
+    Vals.push_back(getEncodedOrdering(cast<AtomicRMWInst>(I).getOrdering()));
+    Vals.push_back(
+        getEncodedSyncScopeID(cast<AtomicRMWInst>(I).getSyncScopeID()));
+    break;
+  case Instruction::Fence:
+    Code = bitc::FUNC_CODE_INST_FENCE;
+    Vals.push_back(getEncodedOrdering(cast<FenceInst>(I).getOrdering()));
+    Vals.push_back(getEncodedSyncScopeID(cast<FenceInst>(I).getSyncScopeID()));
+    break;
+  case Instruction::Call: {
+    const CallInst &CI = cast<CallInst>(I);
+    FunctionType *FTy = CI.getFunctionType();
+
+    if (CI.hasOperandBundles())
+      writeOperandBundles(CI, InstID);
+
+    Code = bitc::FUNC_CODE_INST_CALL;
+
+    Vals.push_back(VE.getAttributeListID(CI.getAttributes()));
+
+    unsigned Flags = getOptimizationFlags(&I);
+    Vals.push_back(CI.getCallingConv() << bitc::CALL_CCONV |
+                   unsigned(CI.isTailCall()) << bitc::CALL_TAIL |
+                   unsigned(CI.isMustTailCall()) << bitc::CALL_MUSTTAIL |
+                   1 << bitc::CALL_EXPLICIT_TYPE |
+                   unsigned(CI.isNoTailCall()) << bitc::CALL_NOTAIL |
+                   unsigned(Flags != 0) << bitc::CALL_FMF);
+    if (Flags != 0)
+      Vals.push_back(Flags);
+
+    Vals.push_back(VE.getTypeID(FTy));
+    pushValueAndType(CI.getCalledOperand(), InstID, Vals); // Callee
+
+    // Emit value #'s for the fixed parameters.
+    for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i) {
+      // Check for labels (can happen with asm labels).
+      if (FTy->getParamType(i)->isLabelTy())
+        Vals.push_back(VE.getValueID(CI.getArgOperand(i)));
+      else
+        pushValue(CI.getArgOperand(i), InstID, Vals); // fixed param.
+    }
+
+    // Emit type/value pairs for varargs params.
+    if (FTy->isVarArg()) {
+      for (unsigned i = FTy->getNumParams(), e = CI.arg_size(); i != e; ++i)
+        pushValueAndType(CI.getArgOperand(i), InstID, Vals); // varargs
+    }
+    break;
+  }
+  case Instruction::VAArg:
+    Code = bitc::FUNC_CODE_INST_VAARG;
+    Vals.push_back(VE.getTypeID(I.getOperand(0)->getType()));   // valistty
+    pushValue(I.getOperand(0), InstID, Vals);                   // valist.
+    Vals.push_back(VE.getTypeID(I.getType())); // restype.
+    break;
+  case Instruction::Freeze: {
+    // freeze instruction is not supported by LLVM 5.0,
+    // but we can more or less emulate it as an identity function
+    // -> encode as a bitcast to the same type
+    auto Operand = I.getOperand(0);
+    Code = bitc::FUNC_CODE_INST_CAST;
+    if (!pushValueAndType(Operand, InstID, Vals))
+      AbbrevToUse = FUNCTION_INST_CAST_ABBREV;
+    Vals.push_back(VE.getTypeID(Operand->getType()));
+    Vals.push_back(getEncodedCastOpcode(Instruction::BitCast));
+    break;
+  }
+  case Instruction::CallBr:
+    report_fatal_error("can not encode CallBr instruction for LLVM 5.0");
+    break;
+  }
+
+  Stream.EmitRecord(Code, Vals, AbbrevToUse);
+  Vals.clear();
+}
+
+/// Write a GlobalValue VST to the module. The purpose of this data structure is
+/// to allow clients to efficiently find the function body.
+void ModuleBitcodeWriter50::writeGlobalValueSymbolTable(
+  DenseMap<const Function *, uint64_t> &FunctionToBitcodeIndex) {
+  // Get the offset of the VST we are writing, and backpatch it into
+  // the VST forward declaration record.
+  uint64_t VSTOffset = Stream.GetCurrentBitNo();
+  // The BitcodeStartBit was the stream offset of the identification block.
+  VSTOffset -= bitcodeStartBit();
+  assert((VSTOffset & 31) == 0 && "VST block not 32-bit aligned");
+  // Note that we add 1 here because the offset is relative to one word
+  // before the start of the identification block, which was historically
+  // always the start of the regular bitcode header.
+  Stream.BackpatchWord(VSTOffsetPlaceholder, VSTOffset / 32 + 1);
+
+  Stream.EnterSubblock(bitc::VALUE_SYMTAB_BLOCK_ID, 4);
+
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_FNENTRY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8)); // value id
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8)); // funcoffset
+  unsigned FnEntryAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  for (const Function &F : M) {
+    uint64_t Record[2];
+
+    if (F.isDeclaration())
+      continue;
+
+    Record[0] = VE.getValueID(&F);
+
+    // Save the word offset of the function (from the start of the
+    // actual bitcode written to the stream).
+    uint64_t BitcodeIndex = FunctionToBitcodeIndex[&F] - bitcodeStartBit();
+    assert((BitcodeIndex & 31) == 0 && "function block not 32-bit aligned");
+    // Note that we add 1 here because the offset is relative to one word
+    // before the start of the identification block, which was historically
+    // always the start of the regular bitcode header.
+    Record[1] = BitcodeIndex / 32 + 1;
+
+    Stream.EmitRecord(bitc::VST_CODE_FNENTRY, Record, FnEntryAbbrev);
+  }
+
+  Stream.ExitBlock();
+}
+
+/// Emit names for arguments, instructions and basic blocks in a function.
+void ModuleBitcodeWriter50::writeFunctionLevelValueSymbolTable(
+    const ValueSymbolTable &VST) {
+  if (VST.empty())
+    return;
+
+  Stream.EnterSubblock(bitc::VALUE_SYMTAB_BLOCK_ID, 4);
+
+  // FIXME: Set up the abbrev, we know how many values there are!
+  // FIXME: We know if the type names can use 7-bit ascii.
+  SmallVector<uint64_t, 64> NameVals;
+
+  for (const ValueName &Name : VST) {
+    // Figure out the encoding to use for the name.
+    StringEncoding Bits = getStringEncoding(Name.getKey());
+
+    unsigned AbbrevToUse = VST_ENTRY_8_ABBREV;
+    NameVals.push_back(VE.getValueID(Name.getValue()));
+
+    // VST_CODE_ENTRY:   [valueid, namechar x N]
+    // VST_CODE_BBENTRY: [bbid, namechar x N]
+    unsigned Code;
+    if (isa<BasicBlock>(Name.getValue())) {
+      Code = bitc::VST_CODE_BBENTRY;
+      if (Bits == SE_Char6)
+        AbbrevToUse = VST_BBENTRY_6_ABBREV;
+    } else {
+      Code = bitc::VST_CODE_ENTRY;
+      if (Bits == SE_Char6)
+        AbbrevToUse = VST_ENTRY_6_ABBREV;
+      else if (Bits == SE_Fixed7)
+        AbbrevToUse = VST_ENTRY_7_ABBREV;
+    }
+
+    for (const auto P : Name.getKey())
+      NameVals.push_back((unsigned char)P);
+
+    // Emit the finished record.
+    Stream.EmitRecord(Code, NameVals, AbbrevToUse);
+    NameVals.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeUseList(UseListOrder &&Order) {
+  assert(Order.Shuffle.size() >= 2 && "Shuffle too small");
+  unsigned Code;
+  if (isa<BasicBlock>(Order.V))
+    Code = bitc::USELIST_CODE_BB;
+  else
+    Code = bitc::USELIST_CODE_DEFAULT;
+
+  SmallVector<uint64_t, 64> Record(Order.Shuffle.begin(), Order.Shuffle.end());
+  Record.push_back(VE.getValueID(Order.V));
+  Stream.EmitRecord(Code, Record);
+}
+
+void ModuleBitcodeWriter50::writeUseListBlock(const Function *F) {
+  assert(VE.shouldPreserveUseListOrder() &&
+         "Expected to be preserving use-list order");
+
+  auto hasMore = [&]() {
+    return !VE.UseListOrders.empty() && VE.UseListOrders.back().F == F;
+  };
+  if (!hasMore())
+    // Nothing to do.
+    return;
+
+  Stream.EnterSubblock(bitc::USELIST_BLOCK_ID, 3);
+  while (hasMore()) {
+    writeUseList(std::move(VE.UseListOrders.back()));
+    VE.UseListOrders.pop_back();
+  }
+  Stream.ExitBlock();
+}
+
+/// Emit a function body to the module stream.
+void ModuleBitcodeWriter50::writeFunction(
+    const Function &F,
+    DenseMap<const Function *, uint64_t> &FunctionToBitcodeIndex) {
+  // Save the bitcode index of the start of this function block for recording
+  // in the VST.
+  FunctionToBitcodeIndex[&F] = Stream.GetCurrentBitNo();
+
+  Stream.EnterSubblock(bitc::FUNCTION_BLOCK_ID, 4);
+  VE.incorporateFunction(F);
+
+  SmallVector<unsigned, 64> Vals;
+
+  // Emit the number of basic blocks, so the reader can create them ahead of
+  // time.
+  Vals.push_back(VE.getBasicBlocks().size());
+  Stream.EmitRecord(bitc::FUNC_CODE_DECLAREBLOCKS, Vals);
+  Vals.clear();
+
+  // If there are function-local constants, emit them now.
+  unsigned CstStart, CstEnd;
+  VE.getFunctionConstantRange(CstStart, CstEnd);
+  writeConstants(CstStart, CstEnd, false);
+
+  // If there is function-local metadata, emit it now.
+  writeFunctionMetadata(F);
+
+  // Keep a running idea of what the instruction ID is.
+  unsigned InstID = CstEnd;
+
+  bool NeedsMetadataAttachment = F.hasMetadata();
+
+  DILocation *LastDL = nullptr;
+  // Finally, emit all the instructions, in order.
+  for (Function::const_iterator BB = F.begin(), E = F.end(); BB != E; ++BB)
+    for (BasicBlock::const_iterator I = BB->begin(), E = BB->end();
+         I != E; ++I) {
+      writeInstruction(*I, InstID, Vals);
+
+      if (!I->getType()->isVoidTy())
+        ++InstID;
+
+      // If the instruction has metadata, write a metadata attachment later.
+      NeedsMetadataAttachment |= I->hasMetadataOtherThanDebugLoc();
+
+      // If the instruction has a debug location, emit it.
+      DILocation *DL = I->getDebugLoc();
+      if (!DL)
+        continue;
+
+      if (DL == LastDL) {
+        // Just repeat the same debug loc as last time.
+        Stream.EmitRecord(bitc::FUNC_CODE_DEBUG_LOC_AGAIN, Vals);
+        continue;
+      }
+
+      Vals.push_back(DL->getLine());
+      Vals.push_back(DL->getColumn());
+      Vals.push_back(VE.getMetadataOrNullID(DL->getScope()));
+      Vals.push_back(VE.getMetadataOrNullID(DL->getInlinedAt()));
+      Stream.EmitRecord(bitc::FUNC_CODE_DEBUG_LOC, Vals);
+      Vals.clear();
+
+      LastDL = DL;
+    }
+
+  // Emit names for all the instructions etc.
+  if (auto *Symtab = F.getValueSymbolTable())
+    writeFunctionLevelValueSymbolTable(*Symtab);
+
+  if (NeedsMetadataAttachment)
+    writeFunctionMetadataAttachment(F);
+  if (VE.shouldPreserveUseListOrder())
+    writeUseListBlock(&F);
+  VE.purgeFunction();
+  Stream.ExitBlock();
+}
+
+// Emit blockinfo, which defines the standard abbreviations etc.
+void ModuleBitcodeWriter50::writeBlockInfo() {
+  // We only want to emit block info records for blocks that have multiple
+  // instances: CONSTANTS_BLOCK, FUNCTION_BLOCK and VALUE_SYMTAB_BLOCK.
+  // Other blocks can define their abbrevs inline.
+  Stream.EnterBlockInfoBlock();
+
+  { // 8-bit fixed-width VST_CODE_ENTRY/VST_CODE_BBENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 3));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID, Abbv) !=
+        VST_ENTRY_8_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // 7-bit fixed width VST_CODE_ENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_ENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID, Abbv) !=
+        VST_ENTRY_7_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // 6-bit char6 VST_CODE_ENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_ENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID, Abbv) !=
+        VST_ENTRY_6_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // 6-bit char6 VST_CODE_BBENTRY strings.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::VST_CODE_BBENTRY));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+    if (Stream.EmitBlockInfoAbbrev(bitc::VALUE_SYMTAB_BLOCK_ID, Abbv) !=
+        VST_BBENTRY_6_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+
+
+  { // SETTYPE abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_SETTYPE));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,
+                              VE.computeBitsRequiredForTypeIndicies()));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID, Abbv) !=
+        CONSTANTS_SETTYPE_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // INTEGER abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_INTEGER));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID, Abbv) !=
+        CONSTANTS_INTEGER_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // CE_CAST abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_CE_CAST));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4));  // cast opc
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,       // typeid
+                              VE.computeBitsRequiredForTypeIndicies()));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));    // value id
+
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID, Abbv) !=
+        CONSTANTS_CE_CAST_Abbrev)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // NULL abbrev for CONSTANTS_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::CST_CODE_NULL));
+    if (Stream.EmitBlockInfoAbbrev(bitc::CONSTANTS_BLOCK_ID, Abbv) !=
+        CONSTANTS_NULL_Abbrev)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  // FIXME: This should only use space for first class types!
+
+  { // INST_LOAD abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_LOAD));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // Ptr
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,    // dest ty
+                              VE.computeBitsRequiredForTypeIndicies()));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4)); // Align
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1)); // volatile
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_LOAD_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_BINOP abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_BINOP));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // LHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // RHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4)); // opc
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_BINOP_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_BINOP_FLAGS abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_BINOP));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // LHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // RHS
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4)); // opc
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7)); // flags
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_BINOP_FLAGS_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_CAST abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_CAST));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));    // OpVal
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed,       // dest ty
+                              VE.computeBitsRequiredForTypeIndicies()));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 4));  // opc
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_CAST_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  { // INST_RET abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_RET));
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_RET_VOID_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_RET abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_RET));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // ValID
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_RET_VAL_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  { // INST_UNREACHABLE abbrev for FUNCTION_BLOCK.
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_UNREACHABLE));
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_UNREACHABLE_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+  {
+    auto Abbv = std::make_shared<BitCodeAbbrev>();
+    Abbv->Add(BitCodeAbbrevOp(bitc::FUNC_CODE_INST_GEP));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 1));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, // dest ty
+                              Log2_32_Ceil(VE.getTypes().size() + 1)));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+    Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+    if (Stream.EmitBlockInfoAbbrev(bitc::FUNCTION_BLOCK_ID, Abbv) !=
+        FUNCTION_INST_GEP_ABBREV)
+      llvm_unreachable("Unexpected abbrev ordering!");
+  }
+
+  Stream.ExitBlock();
+}
+
+/// Write the module path strings, currently only used when generating
+/// a combined index file.
+void IndexBitcodeWriter50::writeModStrings() {
+  Stream.EnterSubblock(bitc::MODULE_STRTAB_BLOCK_ID, 3);
+
+  // TODO: See which abbrev sizes we actually need to emit
+
+  // 8-bit fixed-width MST_ENTRY strings.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::MST_CODE_ENTRY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 8));
+  unsigned Abbrev8Bit = Stream.EmitAbbrev(std::move(Abbv));
+
+  // 7-bit fixed width MST_ENTRY strings.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::MST_CODE_ENTRY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 7));
+  unsigned Abbrev7Bit = Stream.EmitAbbrev(std::move(Abbv));
+
+  // 6-bit char6 MST_ENTRY strings.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::MST_CODE_ENTRY));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+  unsigned Abbrev6Bit = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Module Hash, 160 bits SHA1. Optionally, emitted after each MST_CODE_ENTRY.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::MST_CODE_HASH));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Fixed, 32));
+  unsigned AbbrevHash = Stream.EmitAbbrev(std::move(Abbv));
+
+  SmallVector<unsigned, 64> Vals;
+  forEachModule(
+      [&](const StringMapEntry<std::pair<uint64_t, ModuleHash>> &MPSE) {
+        StringRef Key = MPSE.getKey();
+        const auto &Value = MPSE.getValue();
+        StringEncoding Bits = getStringEncoding(Key);
+        unsigned AbbrevToUse = Abbrev8Bit;
+        if (Bits == SE_Char6)
+          AbbrevToUse = Abbrev6Bit;
+        else if (Bits == SE_Fixed7)
+          AbbrevToUse = Abbrev7Bit;
+
+        Vals.push_back(Value.first);
+        Vals.append(Key.begin(), Key.end());
+
+        // Emit the finished record.
+        Stream.EmitRecord(bitc::MST_CODE_ENTRY, Vals, AbbrevToUse);
+
+        // Emit an optional hash for the module now
+        const auto &Hash = Value.second;
+        if (llvm::any_of(Hash, [](uint32_t H) { return H; })) {
+          Vals.assign(Hash.begin(), Hash.end());
+          // Emit the hash record.
+          Stream.EmitRecord(bitc::MST_CODE_HASH, Vals, AbbrevHash);
+        }
+
+        Vals.clear();
+      });
+  Stream.ExitBlock();
+}
+
+/// Write the function type metadata related records that need to appear before
+/// a function summary entry (whether per-module or combined).
+static void writeFunctionTypeMetadataRecords(BitstreamWriter &Stream,
+                                             FunctionSummary *FS) {
+  if (!FS->type_tests().empty())
+    Stream.EmitRecord(bitc::FS_TYPE_TESTS, FS->type_tests());
+
+  SmallVector<uint64_t, 64> Record;
+
+  auto WriteVFuncIdVec = [&](uint64_t Ty,
+                             ArrayRef<FunctionSummary::VFuncId> VFs) {
+    if (VFs.empty())
+      return;
+    Record.clear();
+    for (auto &VF : VFs) {
+      Record.push_back(VF.GUID);
+      Record.push_back(VF.Offset);
+    }
+    Stream.EmitRecord(Ty, Record);
+  };
+
+  WriteVFuncIdVec(bitc::FS_TYPE_TEST_ASSUME_VCALLS,
+                  FS->type_test_assume_vcalls());
+  WriteVFuncIdVec(bitc::FS_TYPE_CHECKED_LOAD_VCALLS,
+                  FS->type_checked_load_vcalls());
+
+  auto WriteConstVCallVec = [&](uint64_t Ty,
+                                ArrayRef<FunctionSummary::ConstVCall> VCs) {
+    for (auto &VC : VCs) {
+      Record.clear();
+      Record.push_back(VC.VFunc.GUID);
+      Record.push_back(VC.VFunc.Offset);
+      Record.insert(Record.end(), VC.Args.begin(), VC.Args.end());
+      Stream.EmitRecord(Ty, Record);
+    }
+  };
+
+  WriteConstVCallVec(bitc::FS_TYPE_TEST_ASSUME_CONST_VCALL,
+                     FS->type_test_assume_const_vcalls());
+  WriteConstVCallVec(bitc::FS_TYPE_CHECKED_LOAD_CONST_VCALL,
+                     FS->type_checked_load_const_vcalls());
+}
+
+// Helper to emit a single function summary record.
+void ModuleBitcodeWriter50::writePerModuleFunctionSummaryRecord(
+    SmallVector<uint64_t, 64> &NameVals, GlobalValueSummary *Summary,
+    unsigned ValueID, unsigned FSCallsAbbrev, unsigned FSCallsProfileAbbrev,
+    const Function &F) {
+  NameVals.push_back(ValueID);
+
+  FunctionSummary *FS = cast<FunctionSummary>(Summary);
+  writeFunctionTypeMetadataRecords(Stream, FS);
+
+  NameVals.push_back(getEncodedGVSummaryFlags(FS->flags()));
+  NameVals.push_back(FS->instCount());
+  NameVals.push_back(FS->refs().size());
+
+  for (auto &RI : FS->refs())
+    NameVals.push_back(VE.getValueID(RI.getValue()));
+
+  bool HasProfileData = F.getEntryCount().hasValue();
+  for (auto &ECI : FS->calls()) {
+    NameVals.push_back(getValueId(ECI.first));
+    if (HasProfileData)
+      NameVals.push_back(static_cast<uint8_t>(ECI.second.Hotness));
+  }
+
+  unsigned FSAbbrev = (HasProfileData ? FSCallsProfileAbbrev : FSCallsAbbrev);
+  unsigned Code =
+      (HasProfileData ? bitc::FS_PERMODULE_PROFILE : bitc::FS_PERMODULE);
+
+  // Emit the finished record.
+  Stream.EmitRecord(Code, NameVals, FSAbbrev);
+  NameVals.clear();
+}
+
+// Collect the global value references in the given variable's initializer,
+// and emit them in a summary record.
+void ModuleBitcodeWriter50::writeModuleLevelReferences(
+    const GlobalVariable &V, SmallVector<uint64_t, 64> &NameVals,
+    unsigned FSModRefsAbbrev) {
+  auto VI = Index->getValueInfo(GlobalValue::getGUID(V.getName()));
+  if (!VI || VI.getSummaryList().empty()) {
+    // Only declarations should not have a summary (a declaration might however
+    // have a summary if the def was in module level asm).
+    assert(V.isDeclaration());
+    return;
+  }
+  auto *Summary = VI.getSummaryList()[0].get();
+  NameVals.push_back(VE.getValueID(&V));
+  GlobalVarSummary *VS = cast<GlobalVarSummary>(Summary);
+  NameVals.push_back(getEncodedGVSummaryFlags(VS->flags()));
+
+  unsigned SizeBeforeRefs = NameVals.size();
+  for (auto &RI : VS->refs())
+    NameVals.push_back(VE.getValueID(RI.getValue()));
+  // Sort the refs for determinism output, the vector returned by FS->refs() has
+  // been initialized from a DenseSet.
+  std::sort(NameVals.begin() + SizeBeforeRefs, NameVals.end());
+
+  Stream.EmitRecord(bitc::FS_PERMODULE_GLOBALVAR_INIT_REFS, NameVals,
+                    FSModRefsAbbrev);
+  NameVals.clear();
+}
+
+// Current version for the summary.
+// This is bumped whenever we introduce changes in the way some record are
+// interpreted, like flags for instance.
+static const uint64_t INDEX_VERSION = 3;
+
+/// Emit the per-module summary section alongside the rest of
+/// the module's bitcode.
+void ModuleBitcodeWriter50::writePerModuleGlobalValueSummary() {
+  // By default we compile with ThinLTO if the module has a summary, but the
+  // client can request full LTO with a module flag.
+  bool IsThinLTO = true;
+  if (auto *MD =
+          mdconst::extract_or_null<ConstantInt>(M.getModuleFlag("ThinLTO")))
+    IsThinLTO = MD->getZExtValue();
+  Stream.EnterSubblock(IsThinLTO ? bitc::GLOBALVAL_SUMMARY_BLOCK_ID
+                                 : bitc::FULL_LTO_GLOBALVAL_SUMMARY_BLOCK_ID,
+                       4);
+
+  Stream.EmitRecord(bitc::FS_VERSION, ArrayRef<uint64_t>{INDEX_VERSION});
+
+  if (Index->begin() == Index->end()) {
+    Stream.ExitBlock();
+    return;
+  }
+
+  for (const auto &GVI : valueIds()) {
+    Stream.EmitRecord(bitc::FS_VALUE_GUID,
+                      ArrayRef<uint64_t>{GVI.second, GVI.first});
+  }
+
+  // Abbrev for FS_PERMODULE.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_PERMODULE));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // instcount
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4));   // numrefs
+  // numrefs x valueid, n x (valueid)
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSCallsAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_PERMODULE_PROFILE.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_PERMODULE_PROFILE));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // instcount
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4));   // numrefs
+  // numrefs x valueid, n x (valueid, hotness)
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSCallsProfileAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_PERMODULE_GLOBALVAR_INIT_REFS.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_PERMODULE_GLOBALVAR_INIT_REFS));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8)); // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6)); // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));  // valueids
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSModRefsAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_ALIAS.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_ALIAS));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  unsigned FSAliasAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  SmallVector<uint64_t, 64> NameVals;
+  // Iterate over the list of functions instead of the Index to
+  // ensure the ordering is stable.
+  for (const Function &F : M) {
+    // Summary emission does not support anonymous functions, they have to
+    // renamed using the anonymous function renaming pass.
+    if (!F.hasName())
+      report_fatal_error("Unexpected anonymous function when writing summary");
+
+    ValueInfo VI = Index->getValueInfo(GlobalValue::getGUID(F.getName()));
+    if (!VI || VI.getSummaryList().empty()) {
+      // Only declarations should not have a summary (a declaration might
+      // however have a summary if the def was in module level asm).
+      assert(F.isDeclaration());
+      continue;
+    }
+    auto *Summary = VI.getSummaryList()[0].get();
+    writePerModuleFunctionSummaryRecord(NameVals, Summary, VE.getValueID(&F),
+                                        FSCallsAbbrev, FSCallsProfileAbbrev, F);
+  }
+
+  // Capture references from GlobalVariable initializers, which are outside
+  // of a function scope.
+  for (const GlobalVariable &G : M.globals())
+    writeModuleLevelReferences(G, NameVals, FSModRefsAbbrev);
+
+  for (const GlobalAlias &A : M.aliases()) {
+    auto *Aliasee = A.getAliaseeObject();
+    if (!Aliasee->hasName())
+      // Nameless function don't have an entry in the summary, skip it.
+      continue;
+    auto AliasId = VE.getValueID(&A);
+    auto AliaseeId = VE.getValueID(Aliasee);
+    NameVals.push_back(AliasId);
+    auto *Summary = Index->getGlobalValueSummary(A);
+    AliasSummary *AS = cast<AliasSummary>(Summary);
+    NameVals.push_back(getEncodedGVSummaryFlags(AS->flags()));
+    NameVals.push_back(AliaseeId);
+    Stream.EmitRecord(bitc::FS_ALIAS, NameVals, FSAliasAbbrev);
+    NameVals.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+/// Emit the combined summary section into the combined index file.
+void IndexBitcodeWriter50::writeCombinedGlobalValueSummary() {
+  Stream.EnterSubblock(bitc::GLOBALVAL_SUMMARY_BLOCK_ID, 3);
+  Stream.EmitRecord(bitc::FS_VERSION, ArrayRef<uint64_t>{INDEX_VERSION});
+
+  for (const auto &GVI : valueIds()) {
+    Stream.EmitRecord(bitc::FS_VALUE_GUID,
+                      ArrayRef<uint64_t>{GVI.second, GVI.first});
+  }
+
+  // Abbrev for FS_COMBINED.
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_COMBINED));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // modid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // instcount
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4));   // numrefs
+  // numrefs x valueid, n x (valueid)
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSCallsAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_COMBINED_PROFILE.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_COMBINED_PROFILE));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // modid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // instcount
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 4));   // numrefs
+  // numrefs x valueid, n x (valueid, hotness)
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSCallsProfileAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_COMBINED_GLOBALVAR_INIT_REFS.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_COMBINED_GLOBALVAR_INIT_REFS));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // modid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));    // valueids
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));
+  unsigned FSModRefsAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // Abbrev for FS_COMBINED_ALIAS.
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::FS_COMBINED_ALIAS));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // modid
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));   // flags
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 8));   // valueid
+  unsigned FSAliasAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+
+  // The aliases are emitted as a post-pass, and will point to the value
+  // id of the aliasee. Save them in a vector for post-processing.
+  SmallVector<AliasSummary *, 64> Aliases;
+
+  // Save the value id for each summary for alias emission.
+  DenseMap<const GlobalValueSummary *, unsigned> SummaryToValueIdMap;
+
+  SmallVector<uint64_t, 64> NameVals;
+
+  // For local linkage, we also emit the original name separately
+  // immediately after the record.
+  auto MaybeEmitOriginalName = [&](GlobalValueSummary &S) {
+    if (!GlobalValue::isLocalLinkage(S.linkage()))
+      return;
+    NameVals.push_back(S.getOriginalName());
+    Stream.EmitRecord(bitc::FS_COMBINED_ORIGINAL_NAME, NameVals);
+    NameVals.clear();
+  };
+
+  forEachSummary([&](GVInfo I) {
+    GlobalValueSummary *S = I.second;
+    assert(S);
+
+    auto ValueId = getValueId(I.first);
+    assert(ValueId);
+    SummaryToValueIdMap[S] = *ValueId;
+
+    if (auto *AS = dyn_cast<AliasSummary>(S)) {
+      // Will process aliases as a post-pass because the reader wants all
+      // global to be loaded first.
+      Aliases.push_back(AS);
+      return;
+    }
+
+    if (auto *VS = dyn_cast<GlobalVarSummary>(S)) {
+      NameVals.push_back(*ValueId);
+      NameVals.push_back(Index.getModuleId(VS->modulePath()));
+      NameVals.push_back(getEncodedGVSummaryFlags(VS->flags()));
+      for (auto &RI : VS->refs()) {
+        auto RefValueId = getValueId(RI.getGUID());
+        if (!RefValueId)
+          continue;
+        NameVals.push_back(*RefValueId);
+      }
+
+      // Emit the finished record.
+      Stream.EmitRecord(bitc::FS_COMBINED_GLOBALVAR_INIT_REFS, NameVals,
+                        FSModRefsAbbrev);
+      NameVals.clear();
+      MaybeEmitOriginalName(*S);
+      return;
+    }
+
+    auto *FS = cast<FunctionSummary>(S);
+    writeFunctionTypeMetadataRecords(Stream, FS);
+
+    NameVals.push_back(*ValueId);
+    NameVals.push_back(Index.getModuleId(FS->modulePath()));
+    NameVals.push_back(getEncodedGVSummaryFlags(FS->flags()));
+    NameVals.push_back(FS->instCount());
+    // Fill in below
+    NameVals.push_back(0);
+
+    unsigned Count = 0;
+    for (auto &RI : FS->refs()) {
+      auto RefValueId = getValueId(RI.getGUID());
+      if (!RefValueId)
+        continue;
+      NameVals.push_back(*RefValueId);
+      Count++;
+    }
+    NameVals[4] = Count;
+
+    bool HasProfileData = false;
+    for (auto &EI : FS->calls()) {
+      HasProfileData |= EI.second.getHotness() != CalleeInfo::HotnessType::Unknown;
+      if (HasProfileData)
+        break;
+    }
+
+    for (auto &EI : FS->calls()) {
+      // If this GUID doesn't have a value id, it doesn't have a function
+      // summary and we don't need to record any calls to it.
+      GlobalValue::GUID GUID = EI.first.getGUID();
+      auto CallValueId = getValueId(GUID);
+      if (!CallValueId) {
+        // For SamplePGO, the indirect call targets for local functions will
+        // have its original name annotated in profile. We try to find the
+        // corresponding PGOFuncName as the GUID.
+        GUID = Index.getGUIDFromOriginalID(GUID);
+        if (GUID == 0)
+          continue;
+        CallValueId = getValueId(GUID);
+        if (!CallValueId)
+          continue;
+      }
+      NameVals.push_back(*CallValueId);
+      if (HasProfileData)
+        NameVals.push_back(static_cast<uint8_t>(EI.second.Hotness));
+    }
+
+    unsigned FSAbbrev = (HasProfileData ? FSCallsProfileAbbrev : FSCallsAbbrev);
+    unsigned Code =
+        (HasProfileData ? bitc::FS_COMBINED_PROFILE : bitc::FS_COMBINED);
+
+    // Emit the finished record.
+    Stream.EmitRecord(Code, NameVals, FSAbbrev);
+    NameVals.clear();
+    MaybeEmitOriginalName(*S);
+  });
+
+  for (auto *AS : Aliases) {
+    auto AliasValueId = SummaryToValueIdMap[AS];
+    assert(AliasValueId);
+    NameVals.push_back(AliasValueId);
+    NameVals.push_back(Index.getModuleId(AS->modulePath()));
+    NameVals.push_back(getEncodedGVSummaryFlags(AS->flags()));
+    auto AliaseeValueId = SummaryToValueIdMap[&AS->getAliasee()];
+    assert(AliaseeValueId);
+    NameVals.push_back(AliaseeValueId);
+
+    // Emit the finished record.
+    Stream.EmitRecord(bitc::FS_COMBINED_ALIAS, NameVals, FSAliasAbbrev);
+    NameVals.clear();
+    MaybeEmitOriginalName(*AS);
+  }
+
+  if (!Index.cfiFunctionDefs().empty()) {
+    for (auto &S : Index.cfiFunctionDefs()) {
+      NameVals.push_back(StrtabBuilder.add(S));
+      NameVals.push_back(S.size());
+    }
+    Stream.EmitRecord(bitc::FS_CFI_FUNCTION_DEFS, NameVals);
+    NameVals.clear();
+  }
+
+  if (!Index.cfiFunctionDecls().empty()) {
+    for (auto &S : Index.cfiFunctionDecls()) {
+      NameVals.push_back(StrtabBuilder.add(S));
+      NameVals.push_back(S.size());
+    }
+    Stream.EmitRecord(bitc::FS_CFI_FUNCTION_DECLS, NameVals);
+    NameVals.clear();
+  }
+
+  Stream.ExitBlock();
+}
+
+/// Create the "IDENTIFICATION_BLOCK_ID" containing a single string with the
+/// current llvm version, and a record for the epoch number.
+static void writeIdentificationBlock(BitstreamWriter &Stream) {
+  Stream.EnterSubblock(bitc::IDENTIFICATION_BLOCK_ID, 5);
+
+  // Write the "user readable" string identifying the bitcode producer
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::IDENTIFICATION_CODE_STRING));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Array));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Char6));
+  auto StringAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  writeStringRecord(Stream, bitc::IDENTIFICATION_CODE_STRING,
+                    "LLVM" LLVM_VERSION_STRING, StringAbbrev);
+
+  // Write the epoch version
+  Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(bitc::IDENTIFICATION_CODE_EPOCH));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::VBR, 6));
+  auto EpochAbbrev = Stream.EmitAbbrev(std::move(Abbv));
+  SmallVector<unsigned, 1> Vals = {bitc::BITCODE_CURRENT_EPOCH};
+  Stream.EmitRecord(bitc::IDENTIFICATION_CODE_EPOCH, Vals, EpochAbbrev);
+  Stream.ExitBlock();
+}
+
+void ModuleBitcodeWriter50::writeModuleHash(size_t BlockStartPos) {
+  // Emit the module's hash.
+  // MODULE_CODE_HASH: [5*i32]
+  if (GenerateHash) {
+    uint32_t Vals[5];
+    Hasher.update(ArrayRef<uint8_t>((const uint8_t *)&(Buffer)[BlockStartPos],
+                                    Buffer.size() - BlockStartPos));
+    StringRef Hash = Hasher.result();
+    for (int Pos = 0; Pos < 20; Pos += 4) {
+      Vals[Pos / 4] = support::endian::read32be(Hash.data() + Pos);
+    }
+
+    // Emit the finished record.
+    Stream.EmitRecord(bitc::MODULE_CODE_HASH, Vals);
+
+    if (ModHash)
+      // Save the written hash value.
+      std::copy(std::begin(Vals), std::end(Vals), std::begin(*ModHash));
+  } else if (ModHash)
+    Stream.EmitRecord(bitc::MODULE_CODE_HASH, ArrayRef<uint32_t>(*ModHash));
+}
+
+void ModuleBitcodeWriter50::write() {
+  writeIdentificationBlock(Stream);
+
+  Stream.EnterSubblock(bitc::MODULE_BLOCK_ID, 3);
+  size_t BlockStartPos = Buffer.size();
+
+  writeModuleVersion();
+
+  // Emit blockinfo, which defines the standard abbreviations etc.
+  writeBlockInfo();
+
+  // Emit information about attribute groups.
+  writeAttributeGroupTable();
+
+  // Emit information about parameter attributes.
+  writeAttributeTable();
+
+  // Emit information describing all of the types in the module.
+  writeTypeTable();
+
+  writeComdats();
+
+  // Emit top-level description of module, including target triple, inline asm,
+  // descriptors for global variables, and function prototype info.
+  writeModuleInfo();
+
+  // Emit constants.
+  writeModuleConstants();
+
+  // Emit metadata kind names.
+  writeModuleMetadataKinds();
+
+  // Emit metadata.
+  writeModuleMetadata();
+
+  // Emit module-level use-lists.
+  if (VE.shouldPreserveUseListOrder())
+    writeUseListBlock(nullptr);
+
+  writeOperandBundleTags();
+  writeSyncScopeNames();
+
+  // Emit function bodies.
+  DenseMap<const Function *, uint64_t> FunctionToBitcodeIndex;
+  for (Module::const_iterator F = M.begin(), E = M.end(); F != E; ++F)
+    if (!F->isDeclaration())
+      writeFunction(*F, FunctionToBitcodeIndex);
+
+  // Need to write after the above call to WriteFunction which populates
+  // the summary information in the index.
+  if (Index)
+    writePerModuleGlobalValueSummary();
+
+  writeGlobalValueSymbolTable(FunctionToBitcodeIndex);
+
+  writeModuleHash(BlockStartPos);
+
+  Stream.ExitBlock();
+}
+
+static void writeInt32ToBuffer(uint32_t Value, SmallVectorImpl<char> &Buffer,
+                               uint32_t &Position) {
+  support::endian::write32le(&Buffer[Position], Value);
+  Position += 4;
+}
+
+/// If generating a bc file on darwin, we have to emit a
+/// header and trailer to make it compatible with the system archiver.  To do
+/// this we emit the following header, and then emit a trailer that pads the
+/// file out to be a multiple of 16 bytes.
+///
+/// struct bc_header {
+///   uint32_t Magic;         // 0x0B17C0DE
+///   uint32_t Version;       // Version, currently always 0.
+///   uint32_t BitcodeOffset; // Offset to traditional bitcode file.
+///   uint32_t BitcodeSize;   // Size of traditional bitcode file.
+///   uint32_t CPUType;       // CPU specifier.
+///   ... potentially more later ...
+/// };
+static void emitDarwinBCHeaderAndTrailer(SmallVectorImpl<char> &Buffer,
+                                         const Triple &TT) {
+  unsigned CPUType = ~0U;
+
+  // Match x86_64-*, i[3-9]86-*, powerpc-*, powerpc64-*, arm-*, thumb-*,
+  // armv[0-9]-*, thumbv[0-9]-*, armv5te-*, or armv6t2-*. The CPUType is a magic
+  // number from /usr/include/mach/machine.h.  It is ok to reproduce the
+  // specific constants here because they are implicitly part of the Darwin ABI.
+  enum {
+    DARWIN_CPU_ARCH_ABI64      = 0x01000000,
+    DARWIN_CPU_TYPE_X86        = 7,
+    DARWIN_CPU_TYPE_ARM        = 12,
+    DARWIN_CPU_TYPE_POWERPC    = 18
+  };
+
+  Triple::ArchType Arch = TT.getArch();
+  if (Arch == Triple::x86_64)
+    CPUType = DARWIN_CPU_TYPE_X86 | DARWIN_CPU_ARCH_ABI64;
+  else if (Arch == Triple::x86)
+    CPUType = DARWIN_CPU_TYPE_X86;
+  else if (Arch == Triple::ppc)
+    CPUType = DARWIN_CPU_TYPE_POWERPC;
+  else if (Arch == Triple::ppc64)
+    CPUType = DARWIN_CPU_TYPE_POWERPC | DARWIN_CPU_ARCH_ABI64;
+  else if (Arch == Triple::arm || Arch == Triple::thumb)
+    CPUType = DARWIN_CPU_TYPE_ARM;
+
+  // Traditional Bitcode starts after header.
+  assert(Buffer.size() >= BWH_HeaderSize &&
+         "Expected header size to be reserved");
+  unsigned BCOffset = BWH_HeaderSize;
+  unsigned BCSize = Buffer.size() - BWH_HeaderSize;
+
+  // Write the magic and version.
+  unsigned Position = 0;
+  writeInt32ToBuffer(0x0B17C0DE, Buffer, Position);
+  writeInt32ToBuffer(0, Buffer, Position); // Version.
+  writeInt32ToBuffer(BCOffset, Buffer, Position);
+  writeInt32ToBuffer(BCSize, Buffer, Position);
+  writeInt32ToBuffer(CPUType, Buffer, Position);
+
+  // If the file is not a multiple of 16 bytes, insert dummy padding.
+  while (Buffer.size() & 15)
+    Buffer.push_back(0);
+}
+
+/// Helper to write the header common to all bitcode files.
+static void writeBitcodeHeader(BitstreamWriter &Stream) {
+  // Emit the file header.
+  Stream.Emit((unsigned)'B', 8);
+  Stream.Emit((unsigned)'C', 8);
+  Stream.Emit(0x0, 4);
+  Stream.Emit(0xC, 4);
+  Stream.Emit(0xE, 4);
+  Stream.Emit(0xD, 4);
+}
+
+BitcodeWriter50::BitcodeWriter50(SmallVectorImpl<char> &Buffer)
+    : Buffer(Buffer), Stream(new BitstreamWriter(Buffer)) {
+  writeBitcodeHeader(*Stream);
+}
+
+BitcodeWriter50::~BitcodeWriter50() { assert(WroteStrtab); }
+
+void BitcodeWriter50::writeBlob(unsigned Block, unsigned Record, StringRef Blob) {
+  Stream->EnterSubblock(Block, 3);
+
+  auto Abbv = std::make_shared<BitCodeAbbrev>();
+  Abbv->Add(BitCodeAbbrevOp(Record));
+  Abbv->Add(BitCodeAbbrevOp(BitCodeAbbrevOp::Blob));
+  auto AbbrevNo = Stream->EmitAbbrev(std::move(Abbv));
+
+  Stream->EmitRecordWithBlob(AbbrevNo, ArrayRef<uint64_t>{Record}, Blob);
+
+  Stream->ExitBlock();
+}
+
+void BitcodeWriter50::writeSymtab() {
+  assert(!WroteStrtab && !WroteSymtab);
+
+  // If any module has module-level inline asm, we will require a registered asm
+  // parser for the target so that we can create an accurate symbol table for
+  // the module.
+  for (Module *M : Mods) {
+    if (M->getModuleInlineAsm().empty())
+      continue;
+
+    std::string Err;
+    const Triple TT(M->getTargetTriple());
+    const Target *T = TargetRegistry::lookupTarget(TT.str(), Err);
+    if (!T || !T->hasMCAsmParser())
+      return;
+  }
+
+  WroteSymtab = true;
+  SmallVector<char, 0> Symtab;
+  // The irsymtab::build function may be unable to create a symbol table if the
+  // module is malformed (e.g. it contains an invalid alias). Writing a symbol
+  // table is not required for correctness, but we still want to be able to
+  // write malformed modules to bitcode files, so swallow the error.
+  if (Error E = irsymtab::build(Mods, Symtab, StrtabBuilder, Alloc)) {
+    consumeError(std::move(E));
+    return;
+  }
+
+  writeBlob(bitc::SYMTAB_BLOCK_ID, bitc::SYMTAB_BLOB,
+            {Symtab.data(), Symtab.size()});
+}
+
+void BitcodeWriter50::writeStrtab() {
+  assert(!WroteStrtab);
+
+  std::vector<char> Strtab;
+  StrtabBuilder.finalizeInOrder();
+  Strtab.resize(StrtabBuilder.getSize());
+  StrtabBuilder.write((uint8_t *)Strtab.data());
+
+  writeBlob(bitc::STRTAB_BLOCK_ID, bitc::STRTAB_BLOB,
+            {Strtab.data(), Strtab.size()});
+
+  WroteStrtab = true;
+}
+
+void BitcodeWriter50::copyStrtab(StringRef Strtab) {
+  writeBlob(bitc::STRTAB_BLOCK_ID, bitc::STRTAB_BLOB, Strtab);
+  WroteStrtab = true;
+}
+
+void BitcodeWriter50::writeModule(const Module *M,
+                                bool ShouldPreserveUseListOrder,
+                                const ModuleSummaryIndex *Index,
+                                bool GenerateHash, ModuleHash *ModHash) {
+  assert(!WroteStrtab);
+
+  // The Mods vector is used by irsymtab::build, which requires non-const
+  // Modules in case it needs to materialize metadata. But the bitcode writer
+  // requires that the module is materialized, so we can cast to non-const here,
+  // after checking that it is in fact materialized.
+  assert(M->isMaterialized());
+  Mods.push_back(const_cast<Module *>(M));
+
+  ModuleBitcodeWriter50 ModuleWriter(M, Buffer, StrtabBuilder, *Stream,
+                                   ShouldPreserveUseListOrder, Index,
+                                   GenerateHash, ModHash);
+  ModuleWriter.write();
+}
+
+void BitcodeWriter50::writeIndex(
+    const ModuleSummaryIndex *Index,
+    const std::map<std::string, GVSummaryMapTy> *ModuleToSummariesForIndex) {
+  IndexBitcodeWriter50 IndexWriter(*Stream, StrtabBuilder, *Index,
+                                 ModuleToSummariesForIndex);
+  IndexWriter.write();
+}
+
+/// WriteBitcode50ToFile - Write the specified module to the specified output
+/// stream.
+void llvm::WriteBitcode50ToFile(const Module *M, raw_ostream &Out,
+                              bool ShouldPreserveUseListOrder,
+                              const ModuleSummaryIndex *Index,
+                              bool GenerateHash, ModuleHash *ModHash) {
+  SmallVector<char, 0> Buffer;
+  Buffer.reserve(256*1024);
+
+  // If this is darwin or another generic macho target, reserve space for the
+  // header.
+  Triple TT(M->getTargetTriple());
+  if (TT.isOSDarwin() || TT.isOSBinFormatMachO())
+    Buffer.insert(Buffer.begin(), BWH_HeaderSize, 0);
+
+  BitcodeWriter50 Writer(Buffer);
+  Writer.writeModule(M, ShouldPreserveUseListOrder, Index, GenerateHash,
+                     ModHash);
+  Writer.writeSymtab();
+  Writer.writeStrtab();
+
+  if (TT.isOSDarwin() || TT.isOSBinFormatMachO())
+    emitDarwinBCHeaderAndTrailer(Buffer, TT);
+
+  // Write the generated bitstream to "Out".
+  Out.write((char*)&Buffer.front(), Buffer.size());
+}
+
+void IndexBitcodeWriter50::write() {
+  Stream.EnterSubblock(bitc::MODULE_BLOCK_ID, 3);
+
+  writeModuleVersion();
+
+  // Write the module paths in the combined index.
+  writeModStrings();
+
+  // Write the summary combined index records.
+  writeCombinedGlobalValueSummary();
+
+  Stream.ExitBlock();
+}
+
+// Write the specified module summary index to the given raw output stream,
+// where it will be written in a new bitcode block. This is used when
+// writing the combined index file for ThinLTO. When writing a subset of the
+// index for a distributed backend, provide a \p ModuleToSummariesForIndex map.
+void llvm::WriteIndex50ToFile(
+    const ModuleSummaryIndex &Index, raw_ostream &Out,
+    const std::map<std::string, GVSummaryMapTy> *ModuleToSummariesForIndex) {
+  SmallVector<char, 0> Buffer;
+  Buffer.reserve(256 * 1024);
+
+  BitcodeWriter50 Writer(Buffer);
+  Writer.writeIndex(&Index, ModuleToSummariesForIndex);
+  Writer.writeStrtab();
+
+  Out.write((char *)&Buffer.front(), Buffer.size());
+}
diff --git a/llvm/lib/Bitcode/Writer50/BitcodeWriterPass50.cpp b/llvm/lib/Bitcode/Writer50/BitcodeWriterPass50.cpp
new file mode 100644
index 000000000000..4a20dd4f3ccc
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/BitcodeWriterPass50.cpp
@@ -0,0 +1,83 @@
+//===- BitcodeWriterPass50.cpp - Bitcode 5.0 writing pass -----------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// BitcodeWriterPass50 implementation.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Bitcode/BitcodeWriterPass.h"
+#include "llvm/Analysis/ModuleSummaryAnalysis.h"
+#include "llvm/Bitcode/BitcodeWriter.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/PassManager.h"
+#include "llvm/Pass.h"
+using namespace llvm;
+
+PreservedAnalyses BitcodeWriterPass50::run(Module &M, ModuleAnalysisManager &AM) {
+  const ModuleSummaryIndex *Index =
+      EmitSummaryIndex ? &(AM.getResult<ModuleSummaryIndexAnalysis>(M))
+                       : nullptr;
+  WriteBitcode50ToFile(&M, OS, ShouldPreserveUseListOrder, Index, EmitModuleHash);
+  return PreservedAnalyses::all();
+}
+
+namespace {
+  class WriteBitcodePass50 : public ModulePass {
+    raw_ostream &OS; // raw_ostream to print on
+    bool ShouldPreserveUseListOrder;
+    bool EmitSummaryIndex;
+    bool EmitModuleHash;
+
+  public:
+    static char ID; // Pass identification, replacement for typeid
+    WriteBitcodePass50() : ModulePass(ID), OS(dbgs()) {
+      initializeWriteBitcodePass50Pass(*PassRegistry::getPassRegistry());
+    }
+
+    explicit WriteBitcodePass50(raw_ostream &o, bool ShouldPreserveUseListOrder,
+                              bool EmitSummaryIndex, bool EmitModuleHash)
+        : ModulePass(ID), OS(o),
+          ShouldPreserveUseListOrder(ShouldPreserveUseListOrder),
+          EmitSummaryIndex(EmitSummaryIndex), EmitModuleHash(EmitModuleHash) {
+      initializeWriteBitcodePass50Pass(*PassRegistry::getPassRegistry());
+    }
+
+    StringRef getPassName() const override { return "Bitcode 5.0 Writer"; }
+
+    bool runOnModule(Module &M) override {
+      const ModuleSummaryIndex *Index =
+          EmitSummaryIndex
+              ? &(getAnalysis<ModuleSummaryIndexWrapperPass>().getIndex())
+              : nullptr;
+      WriteBitcode50ToFile(&M, OS, ShouldPreserveUseListOrder, Index,
+                         EmitModuleHash);
+      return false;
+    }
+    void getAnalysisUsage(AnalysisUsage &AU) const override {
+      AU.setPreservesAll();
+      if (EmitSummaryIndex)
+        AU.addRequired<ModuleSummaryIndexWrapperPass>();
+    }
+  };
+}
+
+char WriteBitcodePass50::ID = 0;
+INITIALIZE_PASS_BEGIN(WriteBitcodePass50, "write-bitcode50", "Write Bitcode50", false,
+                      true)
+INITIALIZE_PASS_DEPENDENCY(ModuleSummaryIndexWrapperPass)
+INITIALIZE_PASS_END(WriteBitcodePass50, "write-bitcode50", "Write Bitcode50", false,
+                    true)
+
+ModulePass *llvm::createBitcode50WriterPass(raw_ostream &Str,
+                                            bool ShouldPreserveUseListOrder,
+                                            bool EmitSummaryIndex, bool EmitModuleHash) {
+  return new WriteBitcodePass50(Str, ShouldPreserveUseListOrder,
+                              EmitSummaryIndex, EmitModuleHash);
+}
diff --git a/llvm/lib/Bitcode/Writer50/CMakeLists.txt b/llvm/lib/Bitcode/Writer50/CMakeLists.txt
new file mode 100644
index 000000000000..d5de180f3cce
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/CMakeLists.txt
@@ -0,0 +1,16 @@
+add_llvm_component_library(LLVMBitWriter50
+  BitWriter50.cpp
+  BitcodeWriter50.cpp
+  BitcodeWriterPass50.cpp
+  ValueEnumerator50.cpp
+
+  DEPENDS
+  intrinsics_gen
+
+  LINK_COMPONENTS
+  Analysis
+  Core
+  MC
+  Object
+  Support
+  )
diff --git a/llvm/lib/Bitcode/Writer50/ValueEnumerator50.cpp b/llvm/lib/Bitcode/Writer50/ValueEnumerator50.cpp
new file mode 100644
index 000000000000..cc917b72741c
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/ValueEnumerator50.cpp
@@ -0,0 +1,1055 @@
+//===- ValueEnumerator50.cpp - Number values and types for bitcode writer -===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the ValueEnumerator50 class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "ValueEnumerator50.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DebugInfoMetadata.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/UseListOrder.h"
+#include "llvm/IR/ValueSymbolTable.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include <algorithm>
+using namespace llvm;
+
+namespace {
+struct OrderMap {
+  DenseMap<const Value *, std::pair<unsigned, bool>> IDs;
+  unsigned LastGlobalConstantID;
+  unsigned LastGlobalValueID;
+
+  OrderMap() : LastGlobalConstantID(0), LastGlobalValueID(0) {}
+
+  bool isGlobalConstant(unsigned ID) const {
+    return ID <= LastGlobalConstantID;
+  }
+  bool isGlobalValue(unsigned ID) const {
+    return ID <= LastGlobalValueID && !isGlobalConstant(ID);
+  }
+
+  unsigned size() const { return IDs.size(); }
+  std::pair<unsigned, bool> &operator[](const Value *V) { return IDs[V]; }
+  std::pair<unsigned, bool> lookup(const Value *V) const {
+    return IDs.lookup(V);
+  }
+  void index(const Value *V) {
+    // Explicitly sequence get-size and insert-value operations to avoid UB.
+    unsigned ID = IDs.size() + 1;
+    IDs[V].first = ID;
+  }
+};
+}
+
+static void orderValue(const Value *V, OrderMap &OM) {
+  if (OM.lookup(V).first)
+    return;
+
+  if (const Constant *C = dyn_cast<Constant>(V))
+    if (C->getNumOperands() && !isa<GlobalValue>(C))
+      for (const Value *Op : C->operands())
+        if (!isa<BasicBlock>(Op) && !isa<GlobalValue>(Op))
+          orderValue(Op, OM);
+
+  // Note: we cannot cache this lookup above, since inserting into the map
+  // changes the map's size, and thus affects the other IDs.
+  OM.index(V);
+}
+
+static OrderMap orderModule(const Module &M) {
+  // This needs to match the order used by ValueEnumerator50::ValueEnumerator50()
+  // and ValueEnumerator50::incorporateFunction().
+  OrderMap OM;
+
+  // In the reader, initializers of GlobalValues are set *after* all the
+  // globals have been read.  Rather than awkwardly modeling this behaviour
+  // directly in predictValueUseListOrderImpl(), just assign IDs to
+  // initializers of GlobalValues before GlobalValues themselves to model this
+  // implicitly.
+  for (const GlobalVariable &G : M.globals())
+    if (G.hasInitializer())
+      if (!isa<GlobalValue>(G.getInitializer()))
+        orderValue(G.getInitializer(), OM);
+  for (const GlobalAlias &A : M.aliases())
+    if (!isa<GlobalValue>(A.getAliasee()))
+      orderValue(A.getAliasee(), OM);
+  for (const GlobalIFunc &I : M.ifuncs())
+    if (!isa<GlobalValue>(I.getResolver()))
+      orderValue(I.getResolver(), OM);
+  for (const Function &F : M) {
+    for (const Use &U : F.operands())
+      if (!isa<GlobalValue>(U.get()))
+        orderValue(U.get(), OM);
+  }
+  OM.LastGlobalConstantID = OM.size();
+
+  // Initializers of GlobalValues are processed in
+  // BitcodeReader::ResolveGlobalAndAliasInits().  Match the order there rather
+  // than ValueEnumerator50, and match the code in predictValueUseListOrderImpl()
+  // by giving IDs in reverse order.
+  //
+  // Since GlobalValues never reference each other directly (just through
+  // initializers), their relative IDs only matter for determining order of
+  // uses in their initializers.
+  for (const Function &F : M)
+    orderValue(&F, OM);
+  for (const GlobalAlias &A : M.aliases())
+    orderValue(&A, OM);
+  for (const GlobalIFunc &I : M.ifuncs())
+    orderValue(&I, OM);
+  for (const GlobalVariable &G : M.globals())
+    orderValue(&G, OM);
+  OM.LastGlobalValueID = OM.size();
+
+  for (const Function &F : M) {
+    if (F.isDeclaration())
+      continue;
+    // Here we need to match the union of ValueEnumerator50::incorporateFunction()
+    // and WriteFunction().  Basic blocks are implicitly declared before
+    // anything else (by declaring their size).
+    for (const BasicBlock &BB : F)
+      orderValue(&BB, OM);
+    for (const Argument &A : F.args())
+      orderValue(&A, OM);
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB)
+        for (const Value *Op : I.operands())
+          if ((isa<Constant>(*Op) && !isa<GlobalValue>(*Op)) ||
+              isa<InlineAsm>(*Op))
+            orderValue(Op, OM);
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB)
+        orderValue(&I, OM);
+  }
+  return OM;
+}
+
+static void predictValueUseListOrderImpl(const Value *V, const Function *F,
+                                         unsigned ID, const OrderMap &OM,
+                                         UseListOrderStack &Stack) {
+  // Predict use-list order for this one.
+  typedef std::pair<const Use *, unsigned> Entry;
+  SmallVector<Entry, 64> List;
+  for (const Use &U : V->uses())
+    // Check if this user will be serialized.
+    if (OM.lookup(U.getUser()).first)
+      List.push_back(std::make_pair(&U, List.size()));
+
+  if (List.size() < 2)
+    // We may have lost some users.
+    return;
+
+  bool IsGlobalValue = OM.isGlobalValue(ID);
+  std::sort(List.begin(), List.end(), [&](const Entry &L, const Entry &R) {
+    const Use *LU = L.first;
+    const Use *RU = R.first;
+    if (LU == RU)
+      return false;
+
+    auto LID = OM.lookup(LU->getUser()).first;
+    auto RID = OM.lookup(RU->getUser()).first;
+
+    // Global values are processed in reverse order.
+    //
+    // Moreover, initializers of GlobalValues are set *after* all the globals
+    // have been read (despite having earlier IDs).  Rather than awkwardly
+    // modeling this behaviour here, orderModule() has assigned IDs to
+    // initializers of GlobalValues before GlobalValues themselves.
+    if (OM.isGlobalValue(LID) && OM.isGlobalValue(RID))
+      return LID < RID;
+
+    // If ID is 4, then expect: 7 6 5 1 2 3.
+    if (LID < RID) {
+      if (RID <= ID)
+        if (!IsGlobalValue) // GlobalValue uses don't get reversed.
+          return true;
+      return false;
+    }
+    if (RID < LID) {
+      if (LID <= ID)
+        if (!IsGlobalValue) // GlobalValue uses don't get reversed.
+          return false;
+      return true;
+    }
+
+    // LID and RID are equal, so we have different operands of the same user.
+    // Assume operands are added in order for all instructions.
+    if (LID <= ID)
+      if (!IsGlobalValue) // GlobalValue uses don't get reversed.
+        return LU->getOperandNo() < RU->getOperandNo();
+    return LU->getOperandNo() > RU->getOperandNo();
+  });
+
+  if (std::is_sorted(
+          List.begin(), List.end(),
+          [](const Entry &L, const Entry &R) { return L.second < R.second; }))
+    // Order is already correct.
+    return;
+
+  // Store the shuffle.
+  Stack.emplace_back(V, F, List.size());
+  assert(List.size() == Stack.back().Shuffle.size() && "Wrong size");
+  for (size_t I = 0, E = List.size(); I != E; ++I)
+    Stack.back().Shuffle[I] = List[I].second;
+}
+
+static void predictValueUseListOrder(const Value *V, const Function *F,
+                                     OrderMap &OM, UseListOrderStack &Stack) {
+  auto &IDPair = OM[V];
+  assert(IDPair.first && "Unmapped value");
+  if (IDPair.second)
+    // Already predicted.
+    return;
+
+  // Do the actual prediction.
+  IDPair.second = true;
+  if (!V->use_empty() && std::next(V->use_begin()) != V->use_end())
+    predictValueUseListOrderImpl(V, F, IDPair.first, OM, Stack);
+
+  // Recursive descent into constants.
+  if (const Constant *C = dyn_cast<Constant>(V))
+    if (C->getNumOperands()) // Visit GlobalValues.
+      for (const Value *Op : C->operands())
+        if (isa<Constant>(Op)) // Visit GlobalValues.
+          predictValueUseListOrder(Op, F, OM, Stack);
+}
+
+static UseListOrderStack predictUseListOrder(const Module &M) {
+  OrderMap OM = orderModule(M);
+
+  // Use-list orders need to be serialized after all the users have been added
+  // to a value, or else the shuffles will be incomplete.  Store them per
+  // function in a stack.
+  //
+  // Aside from function order, the order of values doesn't matter much here.
+  UseListOrderStack Stack;
+
+  // We want to visit the functions backward now so we can list function-local
+  // constants in the last Function they're used in.  Module-level constants
+  // have already been visited above.
+  for (auto I = M.rbegin(), E = M.rend(); I != E; ++I) {
+    const Function &F = *I;
+    if (F.isDeclaration())
+      continue;
+    for (const BasicBlock &BB : F)
+      predictValueUseListOrder(&BB, &F, OM, Stack);
+    for (const Argument &A : F.args())
+      predictValueUseListOrder(&A, &F, OM, Stack);
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB)
+        for (const Value *Op : I.operands())
+          if (isa<Constant>(*Op) || isa<InlineAsm>(*Op)) // Visit GlobalValues.
+            predictValueUseListOrder(Op, &F, OM, Stack);
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB)
+        predictValueUseListOrder(&I, &F, OM, Stack);
+  }
+
+  // Visit globals last, since the module-level use-list block will be seen
+  // before the function bodies are processed.
+  for (const GlobalVariable &G : M.globals())
+    predictValueUseListOrder(&G, nullptr, OM, Stack);
+  for (const Function &F : M)
+    predictValueUseListOrder(&F, nullptr, OM, Stack);
+  for (const GlobalAlias &A : M.aliases())
+    predictValueUseListOrder(&A, nullptr, OM, Stack);
+  for (const GlobalIFunc &I : M.ifuncs())
+    predictValueUseListOrder(&I, nullptr, OM, Stack);
+  for (const GlobalVariable &G : M.globals())
+    if (G.hasInitializer())
+      predictValueUseListOrder(G.getInitializer(), nullptr, OM, Stack);
+  for (const GlobalAlias &A : M.aliases())
+    predictValueUseListOrder(A.getAliasee(), nullptr, OM, Stack);
+  for (const GlobalIFunc &I : M.ifuncs())
+    predictValueUseListOrder(I.getResolver(), nullptr, OM, Stack);
+  for (const Function &F : M) {
+    for (const Use &U : F.operands())
+      predictValueUseListOrder(U.get(), nullptr, OM, Stack);
+  }
+
+  return Stack;
+}
+
+static bool isIntOrIntVectorValue(const std::pair<const Value*, unsigned> &V) {
+  return V.first->getType()->isIntOrIntVectorTy();
+}
+
+ValueEnumerator50::ValueEnumerator50(const Module &M,
+                                 bool ShouldPreserveUseListOrder)
+    : ShouldPreserveUseListOrder(ShouldPreserveUseListOrder) {
+  if (ShouldPreserveUseListOrder)
+    UseListOrders = predictUseListOrder(M);
+
+  // Enumerate the global variables.
+  for (const GlobalVariable &GV : M.globals())
+    EnumerateValue(&GV);
+
+  // Enumerate the functions.
+  for (const Function & F : M) {
+    EnumerateValue(&F);
+    EnumerateAttributes(F.getAttributes(), F.getContext());
+  }
+
+  // Enumerate the aliases.
+  for (const GlobalAlias &GA : M.aliases())
+    EnumerateValue(&GA);
+
+  // Enumerate the ifuncs.
+  for (const GlobalIFunc &GIF : M.ifuncs())
+    EnumerateValue(&GIF);
+
+  // Remember what is the cutoff between globalvalue's and other constants.
+  unsigned FirstConstant = Values.size();
+
+  // Enumerate the global variable initializers and attributes.
+  for (const GlobalVariable &GV : M.globals()) {
+    if (GV.hasInitializer())
+      EnumerateValue(GV.getInitializer());
+    if (GV.hasAttributes())
+      EnumerateAttributes(GV.getAttributesAsList(AttributeList::FunctionIndex), GV.getContext());
+  }
+
+  // Enumerate the aliasees.
+  for (const GlobalAlias &GA : M.aliases())
+    EnumerateValue(GA.getAliasee());
+
+  // Enumerate the ifunc resolvers.
+  for (const GlobalIFunc &GIF : M.ifuncs())
+    EnumerateValue(GIF.getResolver());
+
+  // Enumerate any optional Function data.
+  for (const Function &F : M)
+    for (const Use &U : F.operands())
+      EnumerateValue(U.get());
+
+  // Enumerate the metadata type.
+  //
+  // TODO: Move this to ValueEnumerator50::EnumerateOperandType() once bitcode
+  // only encodes the metadata type when it's used as a value.
+  EnumerateType(Type::getMetadataTy(M.getContext()));
+
+  // Insert constants and metadata that are named at module level into the slot
+  // pool so that the module symbol table can refer to them...
+  EnumerateValueSymbolTable(M.getValueSymbolTable());
+  EnumerateNamedMetadata(M);
+
+  SmallVector<std::pair<unsigned, MDNode *>, 8> MDs;
+  for (const GlobalVariable &GV : M.globals()) {
+    MDs.clear();
+    GV.getAllMetadata(MDs);
+    for (const auto &I : MDs)
+      // FIXME: Pass GV to EnumerateMetadata and arrange for the bitcode writer
+      // to write metadata to the global variable's own metadata block
+      // (PR28134).
+      EnumerateMetadata(nullptr, I.second);
+  }
+
+  // Enumerate types used by function bodies and argument lists.
+  for (const Function &F : M) {
+    for (const Argument &A : F.args())
+      EnumerateType(A.getType());
+
+    // Enumerate metadata attached to this function.
+    MDs.clear();
+    F.getAllMetadata(MDs);
+    for (const auto &I : MDs)
+      EnumerateMetadata(F.isDeclaration() ? nullptr : &F, I.second);
+
+    for (const BasicBlock &BB : F)
+      for (const Instruction &I : BB) {
+        for (const Use &Op : I.operands()) {
+          auto *MD = dyn_cast<MetadataAsValue>(&Op);
+          if (!MD) {
+            EnumerateOperandType(Op);
+            continue;
+          }
+
+          // Local metadata is enumerated during function-incorporation.
+          if (isa<LocalAsMetadata>(MD->getMetadata()))
+            continue;
+
+          EnumerateMetadata(&F, MD->getMetadata());
+        }
+        EnumerateType(I.getType());
+        if (const CallInst *CI = dyn_cast<CallInst>(&I))
+          EnumerateAttributes(CI->getAttributes(), M.getContext());
+        else if (const InvokeInst *II = dyn_cast<InvokeInst>(&I))
+          EnumerateAttributes(II->getAttributes(), M.getContext());
+        else if (const UnaryOperator *UnOp = dyn_cast<UnaryOperator>(&I);
+                 UnOp && UnOp->getOpcode() == Instruction::FNeg) {
+          // add -0.0 value that we'll use later
+          EnumerateValue(ConstantFP::get(UnOp->getOperand(0)->getType(), -0.0));
+        } else if (auto *SVI = dyn_cast<ShuffleVectorInst>(&I))
+          EnumerateType(SVI->getShuffleMaskForBitcode()->getType());
+
+        // Enumerate metadata attached with this instruction.
+        MDs.clear();
+        I.getAllMetadataOtherThanDebugLoc(MDs);
+        for (unsigned i = 0, e = MDs.size(); i != e; ++i)
+          EnumerateMetadata(&F, MDs[i].second);
+
+        // Don't enumerate the location directly -- it has a special record
+        // type -- but enumerate its operands.
+        if (DILocation *L = I.getDebugLoc())
+          for (const Metadata *Op : L->operands())
+            EnumerateMetadata(&F, Op);
+      }
+  }
+
+  // Optimize constant ordering.
+  OptimizeConstants(FirstConstant, Values.size());
+
+  // Organize metadata ordering.
+  organizeMetadata();
+}
+
+unsigned ValueEnumerator50::getInstructionID(const Instruction *Inst) const {
+  InstructionMapType::const_iterator I = InstructionMap.find(Inst);
+  assert(I != InstructionMap.end() && "Instruction is not mapped!");
+  return I->second;
+}
+
+unsigned ValueEnumerator50::getComdatID(const Comdat *C) const {
+  unsigned ComdatID = Comdats.idFor(C);
+  assert(ComdatID && "Comdat not found!");
+  return ComdatID;
+}
+
+void ValueEnumerator50::setInstructionID(const Instruction *I) {
+  InstructionMap[I] = InstructionCount++;
+}
+
+unsigned ValueEnumerator50::getValueID(const Value *V) const {
+  if (auto *MD = dyn_cast<MetadataAsValue>(V))
+    return getMetadataID(MD->getMetadata());
+
+  ValueMapType::const_iterator I = ValueMap.find(V);
+  assert(I != ValueMap.end() && "Value not in slotcalculator!");
+  return I->second-1;
+}
+
+#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
+LLVM_DUMP_METHOD void ValueEnumerator50::dump() const {
+  print(dbgs(), ValueMap, "Default");
+  dbgs() << '\n';
+  print(dbgs(), MetadataMap, "MetaData");
+  dbgs() << '\n';
+}
+#endif
+
+void ValueEnumerator50::print(raw_ostream &OS, const ValueMapType &Map,
+                            const char *Name) const {
+
+  OS << "Map Name: " << Name << "\n";
+  OS << "Size: " << Map.size() << "\n";
+  for (ValueMapType::const_iterator I = Map.begin(),
+         E = Map.end(); I != E; ++I) {
+
+    const Value *V = I->first;
+    if (V->hasName())
+      OS << "Value: " << V->getName();
+    else
+      OS << "Value: [null]\n";
+    V->print(errs());
+    errs() << '\n';
+
+    OS << " Uses(" << std::distance(V->use_begin(),V->use_end()) << "):";
+    for (const Use &U : V->uses()) {
+      if (&U != &*V->use_begin())
+        OS << ",";
+      if(U->hasName())
+        OS << " " << U->getName();
+      else
+        OS << " [null]";
+
+    }
+    OS <<  "\n\n";
+  }
+}
+
+void ValueEnumerator50::print(raw_ostream &OS, const MetadataMapType &Map,
+                            const char *Name) const {
+
+  OS << "Map Name: " << Name << "\n";
+  OS << "Size: " << Map.size() << "\n";
+  for (auto I = Map.begin(), E = Map.end(); I != E; ++I) {
+    const Metadata *MD = I->first;
+    OS << "Metadata: slot = " << I->second.ID << "\n";
+    OS << "Metadata: function = " << I->second.F << "\n";
+    MD->print(OS);
+    OS << "\n";
+  }
+}
+
+/// OptimizeConstants - Reorder constant pool for denser encoding.
+void ValueEnumerator50::OptimizeConstants(unsigned CstStart, unsigned CstEnd) {
+  if (CstStart == CstEnd || CstStart+1 == CstEnd) return;
+
+  if (ShouldPreserveUseListOrder)
+    // Optimizing constants makes the use-list order difficult to predict.
+    // Disable it for now when trying to preserve the order.
+    return;
+
+  std::stable_sort(Values.begin() + CstStart, Values.begin() + CstEnd,
+                   [this](const std::pair<const Value *, unsigned> &LHS,
+                          const std::pair<const Value *, unsigned> &RHS) {
+    // Sort by plane.
+    if (LHS.first->getType() != RHS.first->getType())
+      return getTypeID(LHS.first->getType()) < getTypeID(RHS.first->getType());
+    // Then by frequency.
+    return LHS.second > RHS.second;
+  });
+
+  // Ensure that integer and vector of integer constants are at the start of the
+  // constant pool.  This is important so that GEP structure indices come before
+  // gep constant exprs.
+  std::stable_partition(Values.begin() + CstStart, Values.begin() + CstEnd,
+                        isIntOrIntVectorValue);
+
+  // Rebuild the modified portion of ValueMap.
+  for (; CstStart != CstEnd; ++CstStart)
+    ValueMap[Values[CstStart].first] = CstStart+1;
+}
+
+
+/// EnumerateValueSymbolTable - Insert all of the values in the specified symbol
+/// table into the values table.
+void ValueEnumerator50::EnumerateValueSymbolTable(const ValueSymbolTable &VST) {
+  for (ValueSymbolTable::const_iterator VI = VST.begin(), VE = VST.end();
+       VI != VE; ++VI)
+    EnumerateValue(VI->getValue());
+}
+
+/// Insert all of the values referenced by named metadata in the specified
+/// module.
+void ValueEnumerator50::EnumerateNamedMetadata(const Module &M) {
+  for (const auto &I : M.named_metadata())
+    EnumerateNamedMDNode(&I);
+}
+
+void ValueEnumerator50::EnumerateNamedMDNode(const NamedMDNode *MD) {
+  for (unsigned i = 0, e = MD->getNumOperands(); i != e; ++i)
+    EnumerateMetadata(nullptr, MD->getOperand(i));
+}
+
+unsigned ValueEnumerator50::getMetadataFunctionID(const Function *F) const {
+  return F ? getValueID(F) + 1 : 0;
+}
+
+void ValueEnumerator50::EnumerateMetadata(const Function *F, const Metadata *MD) {
+  EnumerateMetadata(getMetadataFunctionID(F), MD);
+}
+
+void ValueEnumerator50::EnumerateFunctionLocalMetadata(
+    const Function &F, const LocalAsMetadata *Local) {
+  EnumerateFunctionLocalMetadata(getMetadataFunctionID(&F), Local);
+}
+
+void ValueEnumerator50::dropFunctionFromMetadata(
+    MetadataMapType::value_type &FirstMD) {
+  SmallVector<const MDNode *, 64> Worklist;
+  auto push = [&Worklist](MetadataMapType::value_type &MD) {
+    auto &Entry = MD.second;
+
+    // Nothing to do if this metadata isn't tagged.
+    if (!Entry.F)
+      return;
+
+    // Drop the function tag.
+    Entry.F = 0;
+
+    // If this is has an ID and is an MDNode, then its operands have entries as
+    // well.  We need to drop the function from them too.
+    if (Entry.ID)
+      if (auto *N = dyn_cast<MDNode>(MD.first))
+        Worklist.push_back(N);
+  };
+  push(FirstMD);
+  while (!Worklist.empty())
+    for (const Metadata *Op : Worklist.pop_back_val()->operands()) {
+      if (!Op)
+        continue;
+      auto MD = MetadataMap.find(Op);
+      if (MD != MetadataMap.end())
+        push(*MD);
+    }
+}
+
+void ValueEnumerator50::EnumerateMetadata(unsigned F, const Metadata *MD) {
+  // It's vital for reader efficiency that uniqued subgraphs are done in
+  // post-order; it's expensive when their operands have forward references.
+  // If a distinct node is referenced from a uniqued node, it'll be delayed
+  // until the uniqued subgraph has been completely traversed.
+  SmallVector<const MDNode *, 32> DelayedDistinctNodes;
+
+  // Start by enumerating MD, and then work through its transitive operands in
+  // post-order.  This requires a depth-first search.
+  SmallVector<std::pair<const MDNode *, MDNode::op_iterator>, 32> Worklist;
+  if (const MDNode *N = enumerateMetadataImpl(F, MD))
+    Worklist.push_back(std::make_pair(N, N->op_begin()));
+
+  while (!Worklist.empty()) {
+    const MDNode *N = Worklist.back().first;
+
+    // Enumerate operands until we hit a new node.  We need to traverse these
+    // nodes' operands before visiting the rest of N's operands.
+    MDNode::op_iterator I = std::find_if(
+        Worklist.back().second, N->op_end(),
+        [&](const Metadata *MD) { return enumerateMetadataImpl(F, MD); });
+    if (I != N->op_end()) {
+      auto *Op = cast<MDNode>(*I);
+      Worklist.back().second = ++I;
+
+      // Delay traversing Op if it's a distinct node and N is uniqued.
+      if (Op->isDistinct() && !N->isDistinct())
+        DelayedDistinctNodes.push_back(Op);
+      else
+        Worklist.push_back(std::make_pair(Op, Op->op_begin()));
+      continue;
+    }
+
+    // All the operands have been visited.  Now assign an ID.
+    Worklist.pop_back();
+    MDs.push_back(N);
+    MetadataMap[N].ID = MDs.size();
+
+    // Flush out any delayed distinct nodes; these are all the distinct nodes
+    // that are leaves in last uniqued subgraph.
+    if (Worklist.empty() || Worklist.back().first->isDistinct()) {
+      for (const MDNode *N : DelayedDistinctNodes)
+        Worklist.push_back(std::make_pair(N, N->op_begin()));
+      DelayedDistinctNodes.clear();
+    }
+  }
+}
+
+const MDNode *ValueEnumerator50::enumerateMetadataImpl(unsigned F, const Metadata *MD) {
+  if (!MD)
+    return nullptr;
+
+  assert(
+      (isa<MDNode>(MD) || isa<MDString>(MD) || isa<ConstantAsMetadata>(MD)) &&
+      "Invalid metadata kind");
+
+  auto Insertion = MetadataMap.insert(std::make_pair(MD, MDIndex(F)));
+  MDIndex &Entry = Insertion.first->second;
+  if (!Insertion.second) {
+    // Already mapped.  If F doesn't match the function tag, drop it.
+    if (Entry.hasDifferentFunction(F))
+      dropFunctionFromMetadata(*Insertion.first);
+    return nullptr;
+  }
+
+  // Don't assign IDs to metadata nodes.
+  if (auto *N = dyn_cast<MDNode>(MD))
+    return N;
+
+  // Save the metadata.
+  MDs.push_back(MD);
+  Entry.ID = MDs.size();
+
+  // Enumerate the constant, if any.
+  if (auto *C = dyn_cast<ConstantAsMetadata>(MD))
+    EnumerateValue(C->getValue());
+
+  return nullptr;
+}
+
+/// EnumerateFunctionLocalMetadataa - Incorporate function-local metadata
+/// information reachable from the metadata.
+void ValueEnumerator50::EnumerateFunctionLocalMetadata(
+    unsigned F, const LocalAsMetadata *Local) {
+  assert(F && "Expected a function");
+
+  // Check to see if it's already in!
+  MDIndex &Index = MetadataMap[Local];
+  if (Index.ID) {
+    assert(Index.F == F && "Expected the same function");
+    return;
+  }
+
+  MDs.push_back(Local);
+  Index.F = F;
+  Index.ID = MDs.size();
+
+  EnumerateValue(Local->getValue());
+}
+
+static unsigned getMetadataTypeOrder(const Metadata *MD) {
+  // Strings are emitted in bulk and must come first.
+  if (isa<MDString>(MD))
+    return 0;
+
+  // ConstantAsMetadata doesn't reference anything.  We may as well shuffle it
+  // to the front since we can detect it.
+  auto *N = dyn_cast<MDNode>(MD);
+  if (!N)
+    return 1;
+
+  // The reader is fast forward references for distinct node operands, but slow
+  // when uniqued operands are unresolved.
+  return N->isDistinct() ? 2 : 3;
+}
+
+void ValueEnumerator50::organizeMetadata() {
+  assert(MetadataMap.size() == MDs.size() &&
+         "Metadata map and vector out of sync");
+
+  if (MDs.empty())
+    return;
+
+  // Copy out the index information from MetadataMap in order to choose a new
+  // order.
+  SmallVector<MDIndex, 64> Order;
+  Order.reserve(MetadataMap.size());
+  for (const Metadata *MD : MDs)
+    Order.push_back(MetadataMap.lookup(MD));
+
+  // Partition:
+  //   - by function, then
+  //   - by isa<MDString>
+  // and then sort by the original/current ID.  Since the IDs are guaranteed to
+  // be unique, the result of std::sort will be deterministic.  There's no need
+  // for std::stable_sort.
+  std::sort(Order.begin(), Order.end(), [this](MDIndex LHS, MDIndex RHS) {
+    return std::make_tuple(LHS.F, getMetadataTypeOrder(LHS.get(MDs)), LHS.ID) <
+           std::make_tuple(RHS.F, getMetadataTypeOrder(RHS.get(MDs)), RHS.ID);
+  });
+
+  // Rebuild MDs, index the metadata ranges for each function in FunctionMDs,
+  // and fix up MetadataMap.
+  std::vector<const Metadata *> OldMDs = std::move(MDs);
+  MDs.reserve(OldMDs.size());
+  for (unsigned I = 0, E = Order.size(); I != E && !Order[I].F; ++I) {
+    auto *MD = Order[I].get(OldMDs);
+    MDs.push_back(MD);
+    MetadataMap[MD].ID = I + 1;
+    if (isa<MDString>(MD))
+      ++NumMDStrings;
+  }
+
+  // Return early if there's nothing for the functions.
+  if (MDs.size() == Order.size())
+    return;
+
+  // Build the function metadata ranges.
+  MDRange R;
+  FunctionMDs.reserve(OldMDs.size());
+  unsigned PrevF = 0;
+  for (unsigned I = MDs.size(), E = Order.size(), ID = MDs.size(); I != E;
+       ++I) {
+    unsigned F = Order[I].F;
+    if (!PrevF) {
+      PrevF = F;
+    } else if (PrevF != F) {
+      R.Last = FunctionMDs.size();
+      std::swap(R, FunctionMDInfo[PrevF]);
+      R.First = FunctionMDs.size();
+
+      ID = MDs.size();
+      PrevF = F;
+    }
+
+    auto *MD = Order[I].get(OldMDs);
+    FunctionMDs.push_back(MD);
+    MetadataMap[MD].ID = ++ID;
+    if (isa<MDString>(MD))
+      ++R.NumStrings;
+  }
+  R.Last = FunctionMDs.size();
+  FunctionMDInfo[PrevF] = R;
+}
+
+void ValueEnumerator50::incorporateFunctionMetadata(const Function &F) {
+  NumModuleMDs = MDs.size();
+
+  auto R = FunctionMDInfo.lookup(getValueID(&F) + 1);
+  NumMDStrings = R.NumStrings;
+  MDs.insert(MDs.end(), FunctionMDs.begin() + R.First,
+             FunctionMDs.begin() + R.Last);
+}
+
+void ValueEnumerator50::EnumerateValue(const Value *V) {
+  assert(!V->getType()->isVoidTy() && "Can't insert void values!");
+  assert(!isa<MetadataAsValue>(V) && "EnumerateValue doesn't handle Metadata!");
+
+  // Check to see if it's already in!
+  unsigned &ValueID = ValueMap[V];
+  if (ValueID) {
+    // Increment use count.
+    Values[ValueID-1].second++;
+    return;
+  }
+
+  if (auto *GO = dyn_cast<GlobalObject>(V))
+    if (const Comdat *C = GO->getComdat())
+      Comdats.insert(C);
+
+  // Enumerate the type of this value.
+  EnumerateType(V->getType());
+
+  if (const Constant *C = dyn_cast<Constant>(V)) {
+    if (isa<GlobalValue>(C)) {
+      // Initializers for globals are handled explicitly elsewhere.
+    } else if (C->getNumOperands()) {
+      // If a constant has operands, enumerate them.  This makes sure that if a
+      // constant has uses (for example an array of const ints), that they are
+      // inserted also.
+
+      // We prefer to enumerate them with values before we enumerate the user
+      // itself.  This makes it more likely that we can avoid forward references
+      // in the reader.  We know that there can be no cycles in the constants
+      // graph that don't go through a global variable.
+      for (User::const_op_iterator I = C->op_begin(), E = C->op_end();
+           I != E; ++I)
+        if (!isa<BasicBlock>(*I)) // Don't enumerate BB operand to BlockAddress.
+          EnumerateValue(*I);
+      if (auto *CE = dyn_cast<ConstantExpr>(C))
+        if (CE->getOpcode() == Instruction::ShuffleVector)
+          EnumerateValue(CE->getShuffleMaskForBitcode());
+
+      // Finally, add the value.  Doing this could make the ValueID reference be
+      // dangling, don't reuse it.
+      Values.push_back(std::make_pair(V, 1U));
+      ValueMap[V] = Values.size();
+      return;
+    }
+  }
+
+  // Add the value.
+  Values.push_back(std::make_pair(V, 1U));
+  ValueID = Values.size();
+}
+
+
+void ValueEnumerator50::EnumerateType(Type *Ty) {
+  unsigned *TypeID = &TypeMap[Ty];
+
+  // We've already seen this type.
+  if (*TypeID)
+    return;
+
+  // If it is a non-anonymous struct, mark the type as being visited so that we
+  // don't recursively visit it.  This is safe because we allow forward
+  // references of these in the bitcode reader.
+  if (StructType *STy = dyn_cast<StructType>(Ty))
+    if (!STy->isLiteral())
+      *TypeID = ~0U;
+
+  // Enumerate all of the subtypes before we enumerate this type.  This ensures
+  // that the type will be enumerated in an order that can be directly built.
+  for (Type *SubTy : Ty->subtypes())
+    EnumerateType(SubTy);
+
+  // Refresh the TypeID pointer in case the table rehashed.
+  TypeID = &TypeMap[Ty];
+
+  // Check to see if we got the pointer another way.  This can happen when
+  // enumerating recursive types that hit the base case deeper than they start.
+  //
+  // If this is actually a struct that we are treating as forward ref'able,
+  // then emit the definition now that all of its contents are available.
+  if (*TypeID && *TypeID != ~0U)
+    return;
+
+  // Add this type now that its contents are all happily enumerated.
+  Types.push_back(Ty);
+
+  *TypeID = Types.size();
+}
+
+// Enumerate the types for the specified value.  If the value is a constant,
+// walk through it, enumerating the types of the constant.
+void ValueEnumerator50::EnumerateOperandType(const Value *V) {
+  EnumerateType(V->getType());
+
+  assert(!isa<MetadataAsValue>(V) && "Unexpected metadata operand");
+
+  const Constant *C = dyn_cast<Constant>(V);
+  if (!C)
+    return;
+
+  // If this constant is already enumerated, ignore it, we know its type must
+  // be enumerated.
+  if (ValueMap.count(C))
+    return;
+
+  // This constant may have operands, make sure to enumerate the types in
+  // them.
+  for (const Value *Op : C->operands()) {
+    // Don't enumerate basic blocks here, this happens as operands to
+    // blockaddress.
+    if (isa<BasicBlock>(Op))
+      continue;
+
+    EnumerateOperandType(Op);
+  }
+  if (auto *CE = dyn_cast<ConstantExpr>(C))
+    if (CE->getOpcode() == Instruction::ShuffleVector)
+      EnumerateOperandType(CE->getShuffleMaskForBitcode());
+}
+
+extern uint64_t getAttrKindEncodingBC50(Attribute::AttrKind Kind);
+void ValueEnumerator50::EnumerateAttributes(AttributeList PAL, LLVMContext& Context) {
+  if (PAL.isEmpty()) return;  // null is always 0.
+
+  // Do a lookup.
+  unsigned &Entry = AttributeListMap[PAL];
+  if (Entry == 0) {
+    // Never saw this before, add it.
+    AttributeLists.push_back(PAL);
+    Entry = AttributeLists.size();
+  }
+
+  // Do lookups for all attribute groups.
+  for (unsigned i : PAL.indexes()) {
+    AttributeSet AS = PAL.getAttributes(i);
+    if (!AS.hasAttributes())
+      continue;
+    // we need to skip attribute sets that don't have any valid LLVM BC 5.0 attribute
+    bool has_any_valid_attr = false;
+    for (Attribute Attr : AS) {
+      if (Attr.isEnumAttribute() || Attr.isIntAttribute()) {
+        if (getAttrKindEncodingBC50(Attr.getKindAsEnum()) > 0) {
+          has_any_valid_attr = true;
+          break;
+        }
+      } else if (Attr.isStringAttribute()) {
+        has_any_valid_attr = true;
+        break;
+      } else if (Attr.isTypeAttribute()) {
+        if (Attr.getKindAsEnum() == Attribute::ByVal) {
+          has_any_valid_attr = true;
+          break;
+        }
+        // else: ignore all other type attributes
+      }
+      // else: ignore
+    }
+    auto AS_index = i;
+    if (!has_any_valid_attr) {
+      AS_index = invalid_attribute_group_id;
+    }
+
+    IndexAndAttrSet Pair = {AS_index, AS};
+    unsigned &Entry = AttributeGroupMap[Pair];
+    if (Entry == 0) {
+      AttributeGroups.push_back(Pair);
+      Entry = AttributeGroups.size();
+    }
+  }
+}
+
+void ValueEnumerator50::incorporateFunction(const Function &F) {
+  InstructionCount = 0;
+  NumModuleValues = Values.size();
+
+  // Add global metadata to the function block.  This doesn't include
+  // LocalAsMetadata.
+  incorporateFunctionMetadata(F);
+
+  // Adding function arguments to the value table.
+  for (const auto &I : F.args())
+    EnumerateValue(&I);
+
+  FirstFuncConstantID = Values.size();
+
+  // Add all function-level constants to the value table.
+  for (const BasicBlock &BB : F) {
+    for (const Instruction &I : BB) {
+      for (const Use &OI : I.operands()) {
+        if ((isa<Constant>(OI) && !isa<GlobalValue>(OI)) || isa<InlineAsm>(OI))
+          EnumerateValue(OI);
+      }
+      if (auto *SVI = dyn_cast<ShuffleVectorInst>(&I))
+        EnumerateValue(SVI->getShuffleMaskForBitcode());
+    }
+    BasicBlocks.push_back(&BB);
+    ValueMap[&BB] = BasicBlocks.size();
+  }
+
+  // Optimize the constant layout.
+  OptimizeConstants(FirstFuncConstantID, Values.size());
+
+  // Add the function's parameter attributes so they are available for use in
+  // the function's instruction.
+  EnumerateAttributes(F.getAttributes(), F.getContext());
+
+  FirstInstID = Values.size();
+
+  SmallVector<LocalAsMetadata *, 8> FnLocalMDVector;
+  // Add all of the instructions.
+  for (const BasicBlock &BB : F) {
+    for (const Instruction &I : BB) {
+      for (const Use &OI : I.operands()) {
+        if (auto *MD = dyn_cast<MetadataAsValue>(&OI))
+          if (auto *Local = dyn_cast<LocalAsMetadata>(MD->getMetadata()))
+            // Enumerate metadata after the instructions they might refer to.
+            FnLocalMDVector.push_back(Local);
+      }
+
+      if (!I.getType()->isVoidTy())
+        EnumerateValue(&I);
+    }
+  }
+
+  // Add all of the function-local metadata.
+  for (unsigned i = 0, e = FnLocalMDVector.size(); i != e; ++i) {
+    // At this point, every local values have been incorporated, we shouldn't
+    // have a metadata operand that references a value that hasn't been seen.
+    assert(ValueMap.count(FnLocalMDVector[i]->getValue()) &&
+           "Missing value for metadata operand");
+    EnumerateFunctionLocalMetadata(F, FnLocalMDVector[i]);
+  }
+}
+
+void ValueEnumerator50::purgeFunction() {
+  /// Remove purged values from the ValueMap.
+  for (unsigned i = NumModuleValues, e = Values.size(); i != e; ++i)
+    ValueMap.erase(Values[i].first);
+  for (unsigned i = NumModuleMDs, e = MDs.size(); i != e; ++i)
+    MetadataMap.erase(MDs[i]);
+  for (unsigned i = 0, e = BasicBlocks.size(); i != e; ++i)
+    ValueMap.erase(BasicBlocks[i]);
+
+  Values.resize(NumModuleValues);
+  MDs.resize(NumModuleMDs);
+  BasicBlocks.clear();
+  NumMDStrings = 0;
+}
+
+static void IncorporateFunctionInfoGlobalBBIDs(const Function *F,
+                                 DenseMap<const BasicBlock*, unsigned> &IDMap) {
+  unsigned Counter = 0;
+  for (const BasicBlock &BB : *F)
+    IDMap[&BB] = ++Counter;
+}
+
+/// getGlobalBasicBlockID - This returns the function-specific ID for the
+/// specified basic block.  This is relatively expensive information, so it
+/// should only be used by rare constructs such as address-of-label.
+unsigned ValueEnumerator50::getGlobalBasicBlockID(const BasicBlock *BB) const {
+  unsigned &Idx = GlobalBasicBlockIDs[BB];
+  if (Idx != 0)
+    return Idx-1;
+
+  IncorporateFunctionInfoGlobalBBIDs(BB->getParent(), GlobalBasicBlockIDs);
+  return getGlobalBasicBlockID(BB);
+}
+
+uint64_t ValueEnumerator50::computeBitsRequiredForTypeIndicies() const {
+  return Log2_32_Ceil(getTypes().size() + 1);
+}
diff --git a/llvm/lib/Bitcode/Writer50/ValueEnumerator50.h b/llvm/lib/Bitcode/Writer50/ValueEnumerator50.h
new file mode 100644
index 000000000000..8495039cc9e0
--- /dev/null
+++ b/llvm/lib/Bitcode/Writer50/ValueEnumerator50.h
@@ -0,0 +1,310 @@
+//===-- Bitcode/Writer50/ValueEnumerator50.h - Number values ----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This class gives values and types Unique ID's.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_BITCODE_WRITER50_VALUEENUMERATOR50_H
+#define LLVM_LIB_BITCODE_WRITER50_VALUEENUMERATOR50_H
+
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/ADT/UniqueVector.h"
+#include "llvm/IR/Attributes.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Type.h"
+#include "llvm/IR/UseListOrder.h"
+#include <vector>
+
+namespace llvm {
+
+class Type;
+class Value;
+class Instruction;
+class BasicBlock;
+class Comdat;
+class Function;
+class Module;
+class Metadata;
+class LocalAsMetadata;
+class MDNode;
+class MDOperand;
+class NamedMDNode;
+class AttributeList;
+class ValueSymbolTable;
+class MDSymbolTable;
+class raw_ostream;
+
+class ValueEnumerator50 {
+public:
+  typedef std::vector<Type*> TypeList;
+
+  // For each value, we remember its Value* and occurrence frequency.
+  typedef std::vector<std::pair<const Value*, unsigned> > ValueList;
+
+  /// Attribute groups as encoded in bitcode are almost AttributeSets, but they
+  /// include the AttributeList index, so we have to track that in our map.
+  typedef std::pair<unsigned, AttributeSet> IndexAndAttrSet;
+
+  UseListOrderStack UseListOrders;
+
+private:
+  typedef DenseMap<Type*, unsigned> TypeMapType;
+  TypeMapType TypeMap;
+  TypeList Types;
+
+  typedef DenseMap<const Value*, unsigned> ValueMapType;
+  ValueMapType ValueMap;
+  ValueList Values;
+
+  typedef UniqueVector<const Comdat *> ComdatSetType;
+  ComdatSetType Comdats;
+
+  std::vector<const Metadata *> MDs;
+  std::vector<const Metadata *> FunctionMDs;
+
+  /// Index of information about a piece of metadata.
+  struct MDIndex {
+    unsigned F = 0;  ///< The ID of the function for this metadata, if any.
+    unsigned ID = 0; ///< The implicit ID of this metadata in bitcode.
+
+    MDIndex() = default;
+    explicit MDIndex(unsigned F) : F(F) {}
+
+    /// Check if this has a function tag, and it's different from NewF.
+    bool hasDifferentFunction(unsigned NewF) const { return F && F != NewF; }
+
+    /// Fetch the MD this references out of the given metadata array.
+    const Metadata *get(ArrayRef<const Metadata *> MDs) const {
+      assert(ID && "Expected non-zero ID");
+      assert(ID <= MDs.size() && "Expected valid ID");
+      return MDs[ID - 1];
+    }
+  };
+
+  typedef DenseMap<const Metadata *, MDIndex> MetadataMapType;
+  MetadataMapType MetadataMap;
+
+  /// Range of metadata IDs, as a half-open range.
+  struct MDRange {
+    unsigned First = 0;
+    unsigned Last = 0;
+
+    /// Number of strings in the prefix of the metadata range.
+    unsigned NumStrings = 0;
+
+    MDRange() {}
+    explicit MDRange(unsigned First) : First(First) {}
+  };
+  SmallDenseMap<unsigned, MDRange, 1> FunctionMDInfo;
+
+  bool ShouldPreserveUseListOrder;
+
+  typedef DenseMap<IndexAndAttrSet, unsigned> AttributeGroupMapType;
+  AttributeGroupMapType AttributeGroupMap;
+  std::vector<IndexAndAttrSet> AttributeGroups;
+
+  typedef DenseMap<AttributeList, unsigned> AttributeListMapType;
+  AttributeListMapType AttributeListMap;
+  std::vector<AttributeList> AttributeLists;
+
+  /// GlobalBasicBlockIDs - This map memoizes the basic block ID's referenced by
+  /// the "getGlobalBasicBlockID" method.
+  mutable DenseMap<const BasicBlock*, unsigned> GlobalBasicBlockIDs;
+
+  typedef DenseMap<const Instruction*, unsigned> InstructionMapType;
+  InstructionMapType InstructionMap;
+  unsigned InstructionCount;
+
+  /// BasicBlocks - This contains all the basic blocks for the currently
+  /// incorporated function.  Their reverse mapping is stored in ValueMap.
+  std::vector<const BasicBlock*> BasicBlocks;
+
+  /// When a function is incorporated, this is the size of the Values list
+  /// before incorporation.
+  unsigned NumModuleValues;
+
+  /// When a function is incorporated, this is the size of the Metadatas list
+  /// before incorporation.
+  unsigned NumModuleMDs = 0;
+  unsigned NumMDStrings = 0;
+
+  unsigned FirstFuncConstantID;
+  unsigned FirstInstID;
+
+  ValueEnumerator50(const ValueEnumerator50 &) = delete;
+  void operator=(const ValueEnumerator50 &) = delete;
+public:
+  ValueEnumerator50(const Module &M, bool ShouldPreserveUseListOrder);
+
+  //! signals that an attribute group id is invalid / should not be used
+  static constexpr const uint32_t invalid_attribute_group_id = 0x7FFF'FFFFu;
+
+  void dump() const;
+  void print(raw_ostream &OS, const ValueMapType &Map, const char *Name) const;
+  void print(raw_ostream &OS, const MetadataMapType &Map,
+             const char *Name) const;
+
+  unsigned getValueID(const Value *V) const;
+  unsigned getMetadataID(const Metadata *MD) const {
+    auto ID = getMetadataOrNullID(MD);
+    assert(ID != 0 && "Metadata not in slotcalculator!");
+    return ID - 1;
+  }
+  unsigned getMetadataOrNullID(const Metadata *MD) const {
+    return MetadataMap.lookup(MD).ID;
+  }
+  unsigned numMDs() const { return MDs.size(); }
+
+  bool shouldPreserveUseListOrder() const { return ShouldPreserveUseListOrder; }
+
+  unsigned getTypeID(Type *T) const {
+    TypeMapType::const_iterator I = TypeMap.find(T);
+    assert(I != TypeMap.end() && "Type not in ValueEnumerator50!");
+    return I->second-1;
+  }
+
+  unsigned getInstructionID(const Instruction *I) const;
+  void setInstructionID(const Instruction *I);
+
+  unsigned getAttributeListID(AttributeList PAL) const {
+    if (PAL.isEmpty()) return 0;  // Null maps to zero.
+    AttributeListMapType::const_iterator I = AttributeListMap.find(PAL);
+    assert(I != AttributeListMap.end() && "Attribute not in ValueEnumerator50!");
+    return I->second;
+  }
+
+  unsigned getAttributeGroupID(IndexAndAttrSet Group) const {
+    if (!Group.second.hasAttributes())
+      return 0; // Null maps to zero.
+    AttributeGroupMapType::const_iterator I = AttributeGroupMap.find(Group);
+    //assert(I != AttributeGroupMap.end() && "Attribute not in ValueEnumerator50!");
+    if (I == AttributeGroupMap.end()) {
+      return invalid_attribute_group_id;
+    }
+    return I->second;
+  }
+
+  /// getFunctionConstantRange - Return the range of values that corresponds to
+  /// function-local constants.
+  void getFunctionConstantRange(unsigned &Start, unsigned &End) const {
+    Start = FirstFuncConstantID;
+    End = FirstInstID;
+  }
+
+  const ValueList &getValues() const { return Values; }
+
+  /// Check whether the current block has any metadata to emit.
+  bool hasMDs() const { return NumModuleMDs < MDs.size(); }
+
+  /// Get the MDString metadata for this block.
+  ArrayRef<const Metadata *> getMDStrings() const {
+    return makeArrayRef(MDs).slice(NumModuleMDs, NumMDStrings);
+  }
+
+  /// Get the non-MDString metadata for this block.
+  ArrayRef<const Metadata *> getNonMDStrings() const {
+    return makeArrayRef(MDs).slice(NumModuleMDs).slice(NumMDStrings);
+  }
+
+  ArrayRef<const Metadata *> getMDs() const {
+    return makeArrayRef(MDs);
+  }
+  const MetadataMapType& getMetadataMap() const {
+    return MetadataMap;
+  }
+
+  const TypeList &getTypes() const { return Types; }
+  const std::vector<const BasicBlock*> &getBasicBlocks() const {
+    return BasicBlocks;
+  }
+  const std::vector<AttributeList> &getAttributeLists() const { return AttributeLists; }
+  const std::vector<IndexAndAttrSet> &getAttributeGroups() const {
+    return AttributeGroups;
+  }
+
+  const ComdatSetType &getComdats() const { return Comdats; }
+  unsigned getComdatID(const Comdat *C) const;
+
+  /// getGlobalBasicBlockID - This returns the function-specific ID for the
+  /// specified basic block.  This is relatively expensive information, so it
+  /// should only be used by rare constructs such as address-of-label.
+  unsigned getGlobalBasicBlockID(const BasicBlock *BB) const;
+
+  /// incorporateFunction/purgeFunction - If you'd like to deal with a function,
+  /// use these two methods to get its data into the ValueEnumerator50!
+  ///
+  void incorporateFunction(const Function &F);
+  void purgeFunction();
+  uint64_t computeBitsRequiredForTypeIndicies() const;
+
+private:
+  void OptimizeConstants(unsigned CstStart, unsigned CstEnd);
+
+  /// Reorder the reachable metadata.
+  ///
+  /// This is not just an optimization, but is mandatory for emitting MDString
+  /// correctly.
+  void organizeMetadata();
+
+  /// Drop the function tag from the transitive operands of the given node.
+  void dropFunctionFromMetadata(MetadataMapType::value_type &FirstMD);
+
+  /// Incorporate the function metadata.
+  ///
+  /// This should be called before enumerating LocalAsMetadata for the
+  /// function.
+  void incorporateFunctionMetadata(const Function &F);
+
+  /// Enumerate a single instance of metadata with the given function tag.
+  ///
+  /// If \c MD has already been enumerated, check that \c F matches its
+  /// function tag.  If not, call \a dropFunctionFromMetadata().
+  ///
+  /// Otherwise, mark \c MD as visited.  Assign it an ID, or just return it if
+  /// it's an \a MDNode.
+  const MDNode *enumerateMetadataImpl(unsigned F, const Metadata *MD);
+
+  unsigned getMetadataFunctionID(const Function *F) const;
+
+  /// Enumerate reachable metadata in (almost) post-order.
+  ///
+  /// Enumerate all the metadata reachable from MD.  We want to minimize the
+  /// cost of reading bitcode records, and so the primary consideration is that
+  /// operands of uniqued nodes are resolved before the nodes are read.  This
+  /// avoids re-uniquing them on the context and factors away RAUW support.
+  ///
+  /// This algorithm guarantees that subgraphs of uniqued nodes are in
+  /// post-order.  Distinct subgraphs reachable only from a single uniqued node
+  /// will be in post-order.
+  ///
+  /// \note The relative order of a distinct and uniqued node is irrelevant.
+  /// \a organizeMetadata() will later partition distinct nodes ahead of
+  /// uniqued ones.
+  ///{
+  void EnumerateMetadata(const Function *F, const Metadata *MD);
+  void EnumerateMetadata(unsigned F, const Metadata *MD);
+  ///}
+
+  void EnumerateFunctionLocalMetadata(const Function &F,
+                                      const LocalAsMetadata *Local);
+  void EnumerateFunctionLocalMetadata(unsigned F, const LocalAsMetadata *Local);
+  void EnumerateNamedMDNode(const NamedMDNode *NMD);
+  void EnumerateValue(const Value *V);
+  void EnumerateType(Type *T);
+  void EnumerateOperandType(const Value *V);
+  void EnumerateAttributes(AttributeList PAL, LLVMContext& Context);
+
+  void EnumerateValueSymbolTable(const ValueSymbolTable &ST);
+  void EnumerateNamedMetadata(const Module &M);
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/llvm/lib/CMakeLists.txt b/llvm/lib/CMakeLists.txt
index d88bb157a03b..919e85bc9751 100644
--- a/llvm/lib/CMakeLists.txt
+++ b/llvm/lib/CMakeLists.txt
@@ -33,6 +33,7 @@ add_subdirectory(AsmParser)
 add_subdirectory(LineEditor)
 add_subdirectory(ProfileData)
 add_subdirectory(Passes)
+add_subdirectory(SPIRVerifier)
 add_subdirectory(TextAPI)
 add_subdirectory(ToolDrivers)
 add_subdirectory(XRay)
diff --git a/llvm/lib/CodeGen/CMakeLists.txt b/llvm/lib/CodeGen/CMakeLists.txt
index 1f629f9e0885..64d6c94510e6 100644
--- a/llvm/lib/CodeGen/CMakeLists.txt
+++ b/llvm/lib/CodeGen/CMakeLists.txt
@@ -212,6 +212,7 @@ add_llvm_component_library(LLVMCodeGen
   BitReader
   BitWriter
   Core
+  LibFloor
   MC
   ProfileData
   Scalar
diff --git a/llvm/lib/CodeGen/MachineOutliner.cpp b/llvm/lib/CodeGen/MachineOutliner.cpp
index 1d55bd00e033..47c96f0e69ee 100644
--- a/llvm/lib/CodeGen/MachineOutliner.cpp
+++ b/llvm/lib/CodeGen/MachineOutliner.cpp
@@ -612,6 +612,12 @@ MachineFunction *MachineOutliner::createOutlinedFunction(
   F->setLinkage(GlobalValue::InternalLinkage);
   F->setUnnamedAddr(GlobalValue::UnnamedAddr::Global);
 
+  // Set nounwind, so we don't generate eh_frame.
+  if (llvm::all_of(OF.Candidates, [](const outliner::Candidate &C) {
+        return C.getMF()->getFunction().hasFnAttribute(Attribute::NoUnwind);
+      }))
+    F->addFnAttr(Attribute::NoUnwind);
+
   // Set optsize/minsize, so we don't insert padding between outlined
   // functions.
   F->addFnAttr(Attribute::OptimizeForSize);
diff --git a/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp b/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
index 1d3bb286c882..eb9fc02028d6 100644
--- a/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
+++ b/llvm/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
@@ -307,7 +307,9 @@ void TargetLoweringObjectFileELF::emitModuleMetadata(MCStreamer &Streamer,
                                                      Module &M) const {
   auto &C = getContext();
 
-  if (NamedMDNode *LinkerOptions = M.getNamedMetadata("llvm.linker.options")) {
+  // we do not want to emit this for host-compute
+  if (NamedMDNode *LinkerOptions = M.getNamedMetadata("llvm.linker.options");
+      LinkerOptions && llvm::Triple(M.getTargetTriple()).getEnvironment() != llvm::Triple::EnvironmentType::FloorHostCompute) {
     auto *S = C.getELFSection(".linker-options", ELF::SHT_LLVM_LINKER_OPTIONS,
                               ELF::SHF_EXCLUDE);
 
diff --git a/llvm/lib/IR/AsmWriter.cpp b/llvm/lib/IR/AsmWriter.cpp
index c7a46bd5fe25..6adf9fbb2784 100644
--- a/llvm/lib/IR/AsmWriter.cpp
+++ b/llvm/lib/IR/AsmWriter.cpp
@@ -28,6 +28,7 @@
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/ADT/StringRef.h"
+#include "llvm/ADT/Triple.h"
 #include "llvm/ADT/iterator_range.h"
 #include "llvm/BinaryFormat/Dwarf.h"
 #include "llvm/Config/llvm-config.h"
@@ -320,8 +321,10 @@ static void PrintCallingConv(unsigned cc, raw_ostream &Out) {
   case CallingConv::PTX_Device:    Out << "ptx_device"; break;
   case CallingConv::X86_64_SysV:   Out << "x86_64_sysvcc"; break;
   case CallingConv::Win64:         Out << "win64cc"; break;
-  case CallingConv::SPIR_FUNC:     Out << "spir_func"; break;
-  case CallingConv::SPIR_KERNEL:   Out << "spir_kernel"; break;
+  case CallingConv::FLOOR_FUNC:    Out << "floor_func"; break;
+  case CallingConv::FLOOR_VERTEX:  Out << "floor_vertex"; break;
+  case CallingConv::FLOOR_FRAGMENT:Out << "floor_fragment"; break;
+  case CallingConv::FLOOR_KERNEL:  Out << "floor_kernel"; break;
   case CallingConv::Swift:         Out << "swiftcc"; break;
   case CallingConv::SwiftTail:     Out << "swifttailcc"; break;
   case CallingConv::X86_INTR:      Out << "x86_intrcc"; break;
@@ -3525,13 +3528,21 @@ void AssemblyWriter::printGlobal(const GlobalVariable *GV) {
   PrintVisibility(GV->getVisibility(), Out);
   PrintDLLStorageClass(GV->getDLLStorageClass(), Out);
   PrintThreadLocalModel(GV->getThreadLocalMode(), Out);
+  // Metal/AIR requires unnamed_addr to come after addrspace
+  // -> only add it here for anyone else
+  const bool is_air64 = (llvm::Triple(TheModule->getTargetTriple()).getArch() == Triple::air64);
   StringRef UA = getUnnamedAddrEncoding(GV->getUnnamedAddr());
-  if (!UA.empty())
+  if (!UA.empty() && !is_air64)
       Out << UA << ' ';
 
   if (unsigned AddressSpace = GV->getType()->getAddressSpace())
     Out << "addrspace(" << AddressSpace << ") ";
   if (GV->isExternallyInitialized()) Out << "externally_initialized ";
+
+  // insert after the addrspace for Metal/AIR
+  if (!UA.empty() && is_air64)
+      Out << UA << ' ';
+
   Out << (GV->isConstant() ? "constant " : "global ");
   TypePrinter.print(GV->getValueType(), Out);
 
diff --git a/llvm/lib/IR/AutoUpgrade.cpp b/llvm/lib/IR/AutoUpgrade.cpp
index d73d1e9c20b3..0b1d1aa1624b 100644
--- a/llvm/lib/IR/AutoUpgrade.cpp
+++ b/llvm/lib/IR/AutoUpgrade.cpp
@@ -4026,7 +4026,8 @@ Value *llvm::UpgradeBitCastExpr(unsigned Opc, Constant *C, Type *DestTy) {
 /// info. Return true if module is modified.
 bool llvm::UpgradeDebugInfo(Module &M) {
   unsigned Version = getDebugMetadataVersionFromModule(M);
-  if (Version == DEBUG_METADATA_VERSION) {
+  if (Version == DEBUG_METADATA_VERSION ||
+      Version == IOS_METAL_DEBUG_METADATA_VERSION) {
     bool BrokenDebugInfo = false;
     if (verifyModule(M, &llvm::errs(), &BrokenDebugInfo))
       report_fatal_error("Broken module found, compilation aborted!");
diff --git a/llvm/lib/IR/Constants.cpp b/llvm/lib/IR/Constants.cpp
index c66cfb6e9ac1..4face84fc12a 100644
--- a/llvm/lib/IR/Constants.cpp
+++ b/llvm/lib/IR/Constants.cpp
@@ -1990,7 +1990,8 @@ Constant *ConstantExpr::getCast(unsigned oc, Constant *C, Type *Ty,
   Instruction::CastOps opc = Instruction::CastOps(oc);
   assert(Instruction::isCast(opc) && "opcode out of range");
   assert(C && Ty && "Null arguments to getCast");
-  assert(CastInst::castIsValid(opc, C, Ty) && "Invalid constantexpr cast!");
+  // TODO: fix this!
+  //assert(CastInst::castIsValid(opc, C, Ty) && "Invalid constantexpr cast!");
 
   switch (opc) {
   default:
@@ -2232,8 +2233,10 @@ Constant *ConstantExpr::getIntToPtr(Constant *C, Type *DstTy,
 
 Constant *ConstantExpr::getBitCast(Constant *C, Type *DstTy,
                                    bool OnlyIfReduced) {
+#if 0 // TODO/NOTE: disabled for now
   assert(CastInst::castIsValid(Instruction::BitCast, C, DstTy) &&
          "Invalid constantexpr bitcast!");
+#endif
 
   // It is common to ask for a bitcast of a value to its own type, handle this
   // speedily.
diff --git a/llvm/lib/IR/DataLayout.cpp b/llvm/lib/IR/DataLayout.cpp
index 5edff7a74136..920bdc96a9fa 100644
--- a/llvm/lib/IR/DataLayout.cpp
+++ b/llvm/lib/IR/DataLayout.cpp
@@ -75,6 +75,19 @@ StructLayout::StructLayout(StructType *ST, const DataLayout &DL) {
     IsPadded = true;
     StructSize = alignTo(StructSize, StructAlignment);
   }
+
+  // fix up graphics I/O types (densely pack vector types)
+  if (ST->isGraphicsIOType()) {
+    uint64_t offset_fix = 0;
+    for (uint32_t i = 0; i < NumElements; ++i) {
+      getMemberOffsets()[i] -= offset_fix;
+
+      const auto type = ST->getElementType(i);
+      if (type->isVectorTy()) {
+        offset_fix += DL.getABITypeAlignment(type) - DL.getTypeStoreSize(type);
+      }
+    }
+  }
 }
 
 /// getElementContainingOffset - Given a valid offset into the structure,
diff --git a/llvm/lib/IR/DiagnosticInfo.cpp b/llvm/lib/IR/DiagnosticInfo.cpp
index 0a872a81f911..7f9b41633b25 100644
--- a/llvm/lib/IR/DiagnosticInfo.cpp
+++ b/llvm/lib/IR/DiagnosticInfo.cpp
@@ -67,6 +67,8 @@ void DiagnosticInfoInlineAsm::print(DiagnosticPrinter &DP) const {
   DP << getMsgStr();
   if (getLocCookie())
     DP << " at line " << getLocCookie();
+  if (Instr)
+    DP << *Instr;
 }
 
 void DiagnosticInfoResourceLimit::print(DiagnosticPrinter &DP) const {
diff --git a/llvm/lib/IR/Function.cpp b/llvm/lib/IR/Function.cpp
index 2049dc16a3c1..786db6705282 100644
--- a/llvm/lib/IR/Function.cpp
+++ b/llvm/lib/IR/Function.cpp
@@ -389,8 +389,13 @@ Function::Function(FunctionType *Ty, LinkageTypes Linkage, unsigned AddrSpace,
   setGlobalObjectSubClassData(0);
 
   // We only need a symbol table for a function if the context keeps value names
-  if (!getContext().shouldDiscardValueNames())
+  if (!getContext().shouldDiscardValueNames()) {
+#if 0
     SymTab = std::make_unique<ValueSymbolTable>(NonGlobalValueMaxNameSize);
+#else // always allow unlimited names
+    SymTab = std::make_unique<ValueSymbolTable>(-1);
+#endif
+  }
 
   // If the function has arguments, mark them as lazily built.
   if (Ty->getNumParams())
diff --git a/llvm/lib/IR/Instructions.cpp b/llvm/lib/IR/Instructions.cpp
index 1bc4ebc7ac16..dc17d2755aec 100644
--- a/llvm/lib/IR/Instructions.cpp
+++ b/llvm/lib/IR/Instructions.cpp
@@ -502,10 +502,12 @@ void CallInst::init(FunctionType *FTy, Value *Func, ArrayRef<Value *> Args,
           (FTy->isVarArg() && Args.size() > FTy->getNumParams())) &&
          "Calling a function with bad signature!");
 
+#if 0 // TODO: disabled for now, need to ignore address space mismatches
   for (unsigned i = 0; i != Args.size(); ++i)
     assert((i >= FTy->getNumParams() ||
             FTy->getParamType(i) == Args[i]->getType()) &&
            "Calling a function with a bad signature!");
+#endif
 #endif
 
   // Set operands in order of their index to match use-list-order
diff --git a/llvm/lib/IR/Metadata.cpp b/llvm/lib/IR/Metadata.cpp
index 01bfff84f0c7..ce38adb4f6d0 100644
--- a/llvm/lib/IR/Metadata.cpp
+++ b/llvm/lib/IR/Metadata.cpp
@@ -408,11 +408,20 @@ void ValueAsMetadata::handleDeletion(Value *V) {
   delete MD;
 }
 
-void ValueAsMetadata::handleRAUW(Value *From, Value *To) {
+void ValueAsMetadata::handleRAUW(Value *From, Value *To, const bool AllowASChange) {
   assert(From && "Expected valid value");
   assert(To && "Expected valid value");
   assert(From != To && "Expected changed value");
-  assert(From->getType() == To->getType() && "Unexpected type change");
+  if (AllowASChange &&
+      From->getType()->isPointerTy() &&
+      To->getType()->isPointerTy()) {
+    assert(From->getType()->getPointerElementType() ==
+           To->getType()->getPointerElementType() &&
+           "Unexpected type change");
+  } else {
+    assert(From->getType() == To->getType() &&
+           "Unexpected type change");
+  }
 
   LLVMContext &Context = From->getType()->getContext();
   auto &Store = Context.pImpl->ValuesAsMetadata;
@@ -1291,6 +1300,11 @@ bool Value::eraseMetadata(unsigned KindID) {
 void Value::clearMetadata() {
   if (!HasMetadata)
     return;
+  if (auto F = dyn_cast<Function>(this)) {
+    if (F->getSubprogram()) {
+      F->getSubprogram()->associated_function = nullptr;
+    }
+  }
   assert(getContext().pImpl->ValueMetadata.count(this) &&
          "bit out of sync with hash table");
   getContext().pImpl->ValueMetadata.erase(this);
@@ -1536,6 +1550,7 @@ GlobalObject::VCallVisibility GlobalObject::getVCallVisibility() const {
 
 void Function::setSubprogram(DISubprogram *SP) {
   setMetadata(LLVMContext::MD_dbg, SP);
+  if(SP) SP->associated_function = this;
 }
 
 DISubprogram *Function::getSubprogram() const {
diff --git a/llvm/lib/IR/Type.cpp b/llvm/lib/IR/Type.cpp
index 0a28a001ef0d..6b3efd820886 100644
--- a/llvm/lib/IR/Type.cpp
+++ b/llvm/lib/IR/Type.cpp
@@ -445,6 +445,15 @@ bool StructType::containsScalableVectorType() const {
 }
 
 void StructType::setBody(ArrayRef<Type*> Elements, bool isPacked) {
+  // ignore already layed out (cached) image types
+  if (!isOpaque() && !Elements.empty()) {
+    if (Elements[0]->isPointerTy() &&
+        Elements[0]->getPointerAddressSpace() != 0 &&
+        !Elements[0]->getPointerElementType()->isSized()) {
+      return;
+    }
+  }
+
   assert(isOpaque() && "Struct body already set!");
 
   setSubclassData(getSubclassData() | SCDB_HasBody);
diff --git a/llvm/lib/IR/Value.cpp b/llvm/lib/IR/Value.cpp
index 4136a9afc9cf..6a71cd491f3a 100644
--- a/llvm/lib/IR/Value.cpp
+++ b/llvm/lib/IR/Value.cpp
@@ -498,18 +498,26 @@ static bool contains(Value *Expr, Value *V) {
 }
 #endif // NDEBUG
 
-void Value::doRAUW(Value *New, ReplaceMetadataUses ReplaceMetaUses) {
+void Value::doRAUW(Value *New, ReplaceMetadataUses ReplaceMetaUses, const bool AllowASChange) {
   assert(New && "Value::replaceAllUsesWith(<null>) is invalid!");
   assert(!contains(New, this) &&
          "this->replaceAllUsesWith(expr(this)) is NOT valid!");
-  assert(New->getType() == getType() &&
-         "replaceAllUses of value with new value of different type!");
+  if (AllowASChange &&
+      New->getType()->isPointerTy() &&
+      getType()->isPointerTy()) {
+    assert(New->getType()->getPointerElementType() ==
+           getType()->getPointerElementType() &&
+           "replaceAllUses of value with new value of different type!");
+  } else {
+    assert(New->getType() == getType() &&
+           "replaceAllUses of value with new value of different type!");
+  }
 
   // Notify all ValueHandles (if present) that this value is going away.
   if (HasValueHandle)
-    ValueHandleBase::ValueIsRAUWd(this, New);
+    ValueHandleBase::ValueIsRAUWd(this, New, AllowASChange);
   if (ReplaceMetaUses == ReplaceMetadataUses::Yes && isUsedByMetadata())
-    ValueAsMetadata::handleRAUW(this, New);
+    ValueAsMetadata::handleRAUW(this, New, AllowASChange);
 
   while (!materialized_use_empty()) {
     Use &U = *UseList;
@@ -529,8 +537,8 @@ void Value::doRAUW(Value *New, ReplaceMetadataUses ReplaceMetaUses) {
     BB->replaceSuccessorsPhiUsesWith(cast<BasicBlock>(New));
 }
 
-void Value::replaceAllUsesWith(Value *New) {
-  doRAUW(New, ReplaceMetadataUses::Yes);
+void Value::replaceAllUsesWith(Value *New, const bool AllowASChange) {
+  doRAUW(New, ReplaceMetadataUses::Yes, AllowASChange);
 }
 
 void Value::replaceNonMetadataUsesWith(Value *New) {
@@ -1190,11 +1198,19 @@ void ValueHandleBase::ValueIsDeleted(Value *V) {
   }
 }
 
-void ValueHandleBase::ValueIsRAUWd(Value *Old, Value *New) {
+void ValueHandleBase::ValueIsRAUWd(Value *Old, Value *New, const bool AllowASChange) {
   assert(Old->HasValueHandle &&"Should only be called if ValueHandles present");
   assert(Old != New && "Changing value into itself!");
-  assert(Old->getType() == New->getType() &&
-         "replaceAllUses of value with new value of different type!");
+  if (AllowASChange &&
+      Old->getType()->isPointerTy() &&
+      New->getType()->isPointerTy()) {
+    assert(Old->getType()->getPointerElementType() ==
+           New->getType()->getPointerElementType() &&
+           "replaceAllUses of value with new value of different type!");
+  } else {
+    assert(Old->getType() == New->getType() &&
+           "replaceAllUses of value with new value of different type!");
+  }
 
   // Get the linked list base, which is guaranteed to exist since the
   // HasValueHandle flag is set.
diff --git a/llvm/lib/IR/Verifier.cpp b/llvm/lib/IR/Verifier.cpp
index de092ec632f4..8f2e5cb55d74 100644
--- a/llvm/lib/IR/Verifier.cpp
+++ b/llvm/lib/IR/Verifier.cpp
@@ -2378,7 +2378,7 @@ void Verifier::visitFunction(const Function &F) {
     break;
   }
   case CallingConv::AMDGPU_KERNEL:
-  case CallingConv::SPIR_KERNEL:
+  case CallingConv::FLOOR_KERNEL:
     Assert(F.getReturnType()->isVoidTy(),
            "Calling convention requires void return type", &F);
     LLVM_FALLTHROUGH;
@@ -2389,7 +2389,7 @@ void Verifier::visitFunction(const Function &F) {
   case CallingConv::AMDGPU_CS:
     Assert(!F.hasStructRetAttr(),
            "Calling convention does not allow sret", &F);
-    if (F.getCallingConv() != CallingConv::SPIR_KERNEL) {
+    if (F.getCallingConv() != CallingConv::FLOOR_KERNEL) {
       const unsigned StackAS = DL.getAllocaAddrSpace();
       unsigned i = 0;
       for (const Argument &Arg : F.args()) {
@@ -3066,8 +3066,12 @@ void Verifier::visitCallBase(CallBase &Call) {
          "Called function must be a pointer!", Call);
   PointerType *FPTy = cast<PointerType>(Call.getCalledOperand()->getType());
 
+#if 0 // TODO/NOTE: disabled, since parameter address spaces might not match (see below)
   Assert(FPTy->isOpaqueOrPointeeTypeMatches(Call.getFunctionType()),
          "Called function is not the same type as the call!", Call);
+#else
+  (void)FPTy;
+#endif
 
   FunctionType *FTy = Call.getFunctionType();
 
@@ -3081,10 +3085,17 @@ void Verifier::visitCallBase(CallBase &Call) {
            "Incorrect number of arguments passed to called function!", Call);
 
   // Verify that all arguments to the call match the function type.
-  for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i)
-    Assert(Call.getArgOperand(i)->getType() == FTy->getParamType(i),
+  // Note that address space mismatches will be fixed later.
+  for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i) {
+    Assert(Call.getArgOperand(i)->getType() == FTy->getParamType(i) ||
+           (Call.getArgOperand(i)->getType()->isPointerTy() &&
+            FTy->getParamType(i)->isPointerTy() &&
+            PointerType::get(cast<PointerType>(Call.getArgOperand(i)->getType())->getElementType(),
+                             FTy->getParamType(i)->getPointerAddressSpace()) ==
+            FTy->getParamType(i)),
            "Call parameter type does not match function signature!",
-           Call.getArgOperand(i), FTy->getParamType(i), Call);
+		   Call.getArgOperand(i), FTy->getParamType(i), Call);
+  }
 
   AttributeList Attrs = Call.getAttributes();
 
@@ -4530,8 +4541,10 @@ void Verifier::visitInstruction(Instruction &I) {
   }
 
   if (MDNode *Range = I.getMetadata(LLVMContext::MD_range)) {
+#if 0 // NOTE: we want range info to be propagated and used everywhere, so ignore this
     Assert(isa<LoadInst>(I) || isa<CallInst>(I) || isa<InvokeInst>(I),
            "Ranges are only for loads, calls and invokes!", &I);
+#endif
     visitRangeMetadata(I, Range, I.getType());
   }
 
diff --git a/llvm/lib/LTO/CMakeLists.txt b/llvm/lib/LTO/CMakeLists.txt
index 3abeceab055c..510cf588f45d 100644
--- a/llvm/lib/LTO/CMakeLists.txt
+++ b/llvm/lib/LTO/CMakeLists.txt
@@ -25,6 +25,7 @@ add_llvm_component_library(LLVMLTO
   IPO
   InstCombine
   Instrumentation
+  LibFloor
   Linker
   MC
   ObjCARC
diff --git a/llvm/lib/MC/ELFObjectWriter.cpp b/llvm/lib/MC/ELFObjectWriter.cpp
index 410d9964922d..6ed2f53216ea 100644
--- a/llvm/lib/MC/ELFObjectWriter.cpp
+++ b/llvm/lib/MC/ELFObjectWriter.cpp
@@ -620,10 +620,12 @@ void ELFWriter::computeSymbolTable(
 
   std::vector<ELFSymbolData> LocalSymbolData;
   std::vector<ELFSymbolData> ExternalSymbolData;
+#if 0 // don't emit file names, we don't need them or want to expose them
   MutableArrayRef<std::pair<std::string, size_t>> FileNames =
       Asm.getFileNames();
   for (const std::pair<std::string, size_t> &F : FileNames)
     StrTabBuilder.add(F.first);
+#endif
 
   // Add the data for the symbols.
   bool HasLargeSectionIndex = false;
@@ -719,11 +721,14 @@ void ELFWriter::computeSymbolTable(
 
   // Make the first STT_FILE precede previous local symbols.
   unsigned Index = 1;
+#if 0 // see above, there are no filenames
   auto FileNameIt = FileNames.begin();
   if (!FileNames.empty())
     FileNames[0].second = 0;
+#endif
 
   for (ELFSymbolData &MSD : LocalSymbolData) {
+#if 0 // see above, there are no filenames
     // Emit STT_FILE symbols before their associated local symbols.
     for (; FileNameIt != FileNames.end() && FileNameIt->second <= MSD.Order;
          ++FileNameIt) {
@@ -732,6 +737,7 @@ void ELFWriter::computeSymbolTable(
                          ELF::SHN_ABS, true);
       ++Index;
     }
+#endif
 
     unsigned StringIndex = MSD.Symbol->getType() == ELF::STT_SECTION
                                ? 0
@@ -739,12 +745,14 @@ void ELFWriter::computeSymbolTable(
     MSD.Symbol->setIndex(Index++);
     writeSymbol(Writer, StringIndex, MSD, Layout);
   }
+#if 0 // see above, there are no filenames
   for (; FileNameIt != FileNames.end(); ++FileNameIt) {
     Writer.writeSymbol(StrTabBuilder.getOffset(FileNameIt->first),
                        ELF::STT_FILE | ELF::STB_LOCAL, 0, 0, ELF::STV_DEFAULT,
                        ELF::SHN_ABS, true);
     ++Index;
   }
+#endif
 
   // Write the symbol table entries.
   LastLocalSymbolIndex = Index;
diff --git a/llvm/lib/MC/MCAsmInfoELF.cpp b/llvm/lib/MC/MCAsmInfoELF.cpp
index 9b8b8db794f0..db1bd02f3d18 100644
--- a/llvm/lib/MC/MCAsmInfoELF.cpp
+++ b/llvm/lib/MC/MCAsmInfoELF.cpp
@@ -21,6 +21,10 @@ using namespace llvm;
 void MCAsmInfoELF::anchor() {}
 
 MCSection *MCAsmInfoELF::getNonexecutableStackSection(MCContext &Ctx) const {
+  if (Ctx.getTargetTriple().getEnvironment() == Triple::EnvironmentType::FloorHostCompute) {
+    // we don't want to emit this
+    return nullptr;
+  }
   return Ctx.getELFSection(".note.GNU-stack", ELF::SHT_PROGBITS, 0);
 }
 
diff --git a/llvm/lib/MC/MCELFStreamer.cpp b/llvm/lib/MC/MCELFStreamer.cpp
index 1ba999a63113..d1a3e2b4e2af 100644
--- a/llvm/lib/MC/MCELFStreamer.cpp
+++ b/llvm/lib/MC/MCELFStreamer.cpp
@@ -94,7 +94,8 @@ void MCELFStreamer::initSections(bool NoExecStack, const MCSubtargetInfo &STI) {
   emitCodeAlignment(Ctx.getObjectFileInfo()->getTextSectionAlignment(), &STI);
 
   if (NoExecStack)
-    SwitchSection(Ctx.getAsmInfo()->getNonexecutableStackSection(Ctx));
+    if (auto sec = Ctx.getAsmInfo()->getNonexecutableStackSection(Ctx); sec)
+      SwitchSection(sec);
 }
 
 void MCELFStreamer::emitLabel(MCSymbol *S, SMLoc Loc) {
diff --git a/llvm/lib/MC/MCObjectFileInfo.cpp b/llvm/lib/MC/MCObjectFileInfo.cpp
index d7f85f793c55..3233b1b8189d 100644
--- a/llvm/lib/MC/MCObjectFileInfo.cpp
+++ b/llvm/lib/MC/MCObjectFileInfo.cpp
@@ -495,8 +495,12 @@ void MCObjectFileInfo::initELFMCObjectFileInfo(const Triple &T, bool Large) {
   FaultMapSection =
       Ctx->getELFSection(".llvm_faultmaps", ELF::SHT_PROGBITS, ELF::SHF_ALLOC);
 
-  EHFrameSection =
-      Ctx->getELFSection(".eh_frame", EHSectionType, EHSectionFlags);
+  // don't emit eh_frame for host-compute
+  if (T.getEnvironment() == Triple::FloorHostCompute)
+    SupportsCompactUnwindWithoutEHFrame = true;
+  else
+    EHFrameSection =
+        Ctx->getELFSection(".eh_frame", EHSectionType, EHSectionFlags);
 
   StackSizesSection = Ctx->getELFSection(".stack_sizes", ELF::SHT_PROGBITS, 0);
 
diff --git a/llvm/lib/Passes/CMakeLists.txt b/llvm/lib/Passes/CMakeLists.txt
index 703969f8b5f4..1cfb0e117cdd 100644
--- a/llvm/lib/Passes/CMakeLists.txt
+++ b/llvm/lib/Passes/CMakeLists.txt
@@ -20,6 +20,7 @@ add_llvm_component_library(LLVMPasses
   Coroutines
   IPO
   InstCombine
+  LibFloor
   ObjCARC
   Scalar
   Support
diff --git a/llvm/lib/SPIRVerifier/CMakeLists.txt b/llvm/lib/SPIRVerifier/CMakeLists.txt
new file mode 100644
index 000000000000..72c5d5c92adf
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/CMakeLists.txt
@@ -0,0 +1,9 @@
+add_llvm_component_library(LLVMSPIRVerifier
+  SpirIterators.cpp
+  SpirTables.cpp
+  SpirErrors.cpp
+  SpirValidation.cpp
+  
+  DEPENDS
+  intrinsics_gen
+  )
diff --git a/llvm/lib/SPIRVerifier/README.md b/llvm/lib/SPIRVerifier/README.md
new file mode 100644
index 000000000000..d845066b6f91
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/README.md
@@ -0,0 +1,8 @@
+SPIR Verifier
+=============
+
+The SPIR verifier tool checks if a given file is valid according to SPIR 1.2 Specification for OpenCL document.
+The verifier is a work in progress, the current implementation is partial and does not check
+all restrictions in the Specification document.
+
+SPIR 1.2 Specification can be found under: http://www.khronos.org/files/opencl-spir-12-provisional.pdf
diff --git a/llvm/lib/SPIRVerifier/SpirErrors.cpp b/llvm/lib/SPIRVerifier/SpirErrors.cpp
new file mode 100644
index 000000000000..1256e5dddcea
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/SpirErrors.cpp
@@ -0,0 +1,376 @@
+//===-------------------------- SpirErrors.cpp ---------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#include "llvm/SPIRVerifier/SpirErrors.h"
+#include "llvm/SPIRVerifier/SpirTables.h"
+#include "llvm/IR/Type.h"
+#include "llvm/IR/Value.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/Support/raw_ostream.h"
+
+#include <string>
+#include <sstream>
+#include <map>
+
+using namespace llvm;
+
+namespace SPIR {
+
+ typedef enum {
+  INFO_NONE = 0,
+  // Information types
+  INFO_OCL_TYPE,
+  INFO_LLVM_TYPE,
+  INFO_KERNEL_RETURN_TYPE,
+  INFO_KERNEL_ARG_ADDRESS_SPACE,
+  INFO_GLOBAL_AS3_VAR,
+  INFO_GLOBAL_VAR_ADDRES_SPACES,
+  INFO_OCL_TO_LLVM_TYPE,
+  INFO_CORE_FEATURE_METADATA,
+  INFO_KHR_EXT_METADATA,
+  INFO_COMPILER_OPTION_METADATA,
+  INFO_INTRINSIC,
+  INFO_ADDRESS_SPACE,
+  INFO_CALLING_CONVENTION,
+  INFO_LINKAGE_TYPE,
+  INFO_INDIRECT_CALL,
+  INFO_NAMED_METADATA,
+  INFO_METADATA_KERNEL_ARG_INFO,
+  INFO_METADATA_VERSION,
+  INFO_MEM_FENCE,
+
+  SPIR_INFO_NUM
+} SPIR_INFO_TYPE;
+
+
+/// @brief Base error class, all error types inherit from it.
+struct ValidationError {
+public:
+  /// @brief Constructor.
+  /// @param T error type
+  /// @param S error message
+  ValidationError(SPIR_ERROR_TYPE T, llvm::StringRef S) :
+    ErrType(T), ErrMSG(S) {
+  }
+
+  /// @brief Get error type.
+  /// @returns error type
+  virtual SPIR_ERROR_TYPE getErrorType() const {
+    return ErrType;
+  }
+
+  /// @brief Get error message.
+  /// @returns error message
+  virtual llvm::StringRef toString() const {
+    return StringRef(ErrMSG);
+  }
+
+  /// @brief Destructor.
+  virtual ~ValidationError() {}
+
+protected:
+  SPIR_ERROR_TYPE ErrType;
+  std::string ErrMSG;
+};
+
+struct ErrorComperator {
+  const ValidationError * LHS;
+
+  ErrorComperator(const ValidationError *Err) : LHS(Err) {}
+
+  bool operator() (const ValidationError *RHS) {
+    if (LHS == RHS)
+      return true;
+
+    return
+      (LHS->getErrorType() == RHS->getErrorType()) &&
+      (LHS->toString() == RHS->toString());
+  }
+};
+
+#define MAX_ERROR_INFO_PER_ERROR (4)
+struct SPIR_ERROR_DATA {
+  SPIR_ERROR_TYPE T;
+  std::string MSG;
+  SPIR_INFO_TYPE InfoList[MAX_ERROR_INFO_PER_ERROR];
+  std::string ErrTypeStr;
+};
+
+typedef std::string (GetInfoMsgFunc)();
+struct SPIR_INFO_DATA {
+  SPIR_INFO_TYPE T;
+  GetInfoMsgFunc *GetMsg;
+};
+
+typedef std::map<SPIR_INFO_TYPE, unsigned> SPIRInfoTypeNumMap;
+
+const SPIR_ERROR_DATA g_ErrorData[SPIR_ERROR_NUM] = {
+  // Module (general) errors
+  // Type errors
+  {ERR_INVALID_OCL_TYPE, "Invalid OpenCL C type",
+      {INFO_OCL_TYPE, INFO_CORE_FEATURE_METADATA, INFO_KHR_EXT_METADATA}, "ERR_INVALID_OCL_TYPE"},
+  {ERR_INVALID_LLVM_TYPE, "Invalid LLVM type",
+      {INFO_LLVM_TYPE, INFO_CORE_FEATURE_METADATA, INFO_KHR_EXT_METADATA}, "ERR_INVALID_LLVM_TYPE"},
+  {ERR_INVALID_KERNEL_RETURN_TYPE, "Invalid SPIR kernel return type",
+      {INFO_KERNEL_RETURN_TYPE}, "ERR_INVALID_KERNEL_RETURN_TYPE"},
+  {ERR_KERNEL_ARG_PTRPTR, "SPIR kernel argument is a pointer to pointer",
+      {}, "ERR_KERNEL_ARG_PTRPTR"},
+  {ERR_KERNEL_ARG_AS0, "SPIR kernel argument is a pointer to private address space",
+      {INFO_KERNEL_ARG_ADDRESS_SPACE}, "ERR_KERNEL_ARG_AS0"},
+  {ERR_MISMATCH_OCL_AND_LLVM_TYPES, "Mismatch between OpenCL C and LLVM types",
+      {INFO_OCL_TO_LLVM_TYPE}, "ERR_MISMATCH_OCL_AND_LLVM_TYPES"},
+  {ERR_INVALID_GLOBAL_AS3_VAR, "Invalid program scope __local variable",
+    {INFO_GLOBAL_AS3_VAR}, "ERR_INVALID_GLOBAL_AS3_VAR"},
+  {ERR_INVALID_GLOBAL_VAR_ADDRESS_SPACE, "program scope variable in a wrong address space",
+    {INFO_GLOBAL_VAR_ADDRES_SPACES}, "ERR_INVALID_GLOBAL_VAR_ADDRESS_SPACE"},
+  // Instruction errors
+  {ERR_INVALID_INTRINSIC, "Invalid intrinsic",
+      {INFO_INTRINSIC},"ERR_INVALID_INTRINSIC"},
+  {ERR_INVALID_ADDR_SPACE, "Invalid address space",
+      {INFO_ADDRESS_SPACE}, "ERR_INVALID_ADDR_SPACE"},
+  {ERR_INVALID_ADDR_SPACE_CAST, "Invalid address space cast",
+      {INFO_ADDRESS_SPACE}, "ERR_INVALID_ADDR_SPACE_CAST"},
+  {ERR_INVALID_INDIRECT_CALL, "Invalid indirect call",
+      {INFO_INDIRECT_CALL}, "ERR_INVALID_INDIRECT_CALL"},
+  {ERR_INVALID_MEM_FENCE, "Invalid cl_mem_fence value",
+      {INFO_MEM_FENCE}, "ERR_INVALID_MEM_FENCE"},
+  // Function errors
+  {ERR_INVALID_CALLING_CONVENTION, "Invalid calling convention",
+      {INFO_CALLING_CONVENTION}, "ERR_INVALID_CALLING_CONVENTION"},
+  {ERR_INVALID_LINKAGE_TYPE, "Invalid linkage type",
+      {INFO_LINKAGE_TYPE}, "ERR_INVALID_LINKAGE_TYPE"},
+  // Metadata errors
+  {ERR_INVALID_CORE_FEATURE, "Invalid core features",
+      {INFO_CORE_FEATURE_METADATA}, "ERR_INVALID_CORE_FEATURE"},
+  {ERR_INVALID_KHR_EXT, "Invalid KHR extensions",
+      {INFO_KHR_EXT_METADATA}, "ERR_INVALID_KHR_EXT"},
+  {ERR_INVALID_COMPILER_OPTION, "Invalid compiler options",
+      {INFO_COMPILER_OPTION_METADATA},"ERR_INVALID_COMPILER_OPTION"},
+  {ERR_MISSING_NAMED_METADATA, "Named Metadata is missing",
+      {INFO_NAMED_METADATA}, "ERR_MISSING_NAMED_METADATA"},
+  {ERR_INVALID_METADATA_KERNEL, "Invalid kernel metatdata",
+      {}, "ERR_INVALID_METADATA_KERNEL"},
+  {ERR_INVALID_METADATA_KERNEL_INFO, "Invalid kernel metadata ARG Info",
+      {INFO_METADATA_KERNEL_ARG_INFO}, "ERR_INVALID_METADATA_KERNEL_INFO"},
+  {ERR_MISSING_METADATA_KERNEL_INFO, "Kernel metadata is missing ARG Info",
+      {INFO_METADATA_KERNEL_ARG_INFO}, "ERR_MISSING_METADATA_KERNEL_INFO"},
+  {ERR_INVALID_METADATA_VERSION, "Invalid OpenCL (OCL/SPIR) version",
+      {INFO_METADATA_VERSION}, "ERR_INVALID_METADATA_VERSION"},
+  {ERR_MISMATCH_METADATA_ADDR_SPACE, "Address space mismatch between kernel prototype and metadata",
+      {}, "ERR_MISMATCH_METADATA_ADDR_SPACE"}
+};
+
+const SPIR_INFO_DATA g_InfoData[SPIR_INFO_NUM] = {
+  {INFO_NONE, nullptr},
+  {INFO_OCL_TYPE, getValidOpenCLTypeMsg},
+  {INFO_LLVM_TYPE, getValidLLVMTypeMsg},
+  {INFO_KERNEL_RETURN_TYPE, getValidKernelReturnTypeMsg},
+  {INFO_KERNEL_ARG_ADDRESS_SPACE, getValidKernelArgAddressSpaceMsg},
+  {INFO_GLOBAL_AS3_VAR, getValidGlobalAS3VariableMsg},
+  {INFO_GLOBAL_VAR_ADDRES_SPACES, getValidGlobalVarAddressSpacesMsg},
+  {INFO_OCL_TO_LLVM_TYPE, getMapOpenCLToLLVMMsg},
+  {INFO_CORE_FEATURE_METADATA, getValidCoreFeaturesMsg},
+  {INFO_KHR_EXT_METADATA, getValidKHRExtensionsMsg},
+  {INFO_COMPILER_OPTION_METADATA, getValidCompilerOptionsMsg},
+  {INFO_INTRINSIC, getValidIntrinsicMsg},
+  {INFO_ADDRESS_SPACE, getValidAddressSpaceMsg},
+  {INFO_CALLING_CONVENTION, getValidCallingConventionMsg},
+  {INFO_LINKAGE_TYPE, getValidLinkageTypeMsg},
+  {INFO_INDIRECT_CALL, getValidIndirectCallMsg},
+  {INFO_NAMED_METADATA, getValidNamedMetadataMsg},
+  {INFO_METADATA_KERNEL_ARG_INFO, getValidKernelArgInfoMsg},
+  {INFO_METADATA_VERSION, getValidVersionMsg},
+  {INFO_MEM_FENCE, getValidMemFenceMsg}
+};
+
+static bool isValidTables() {
+  for (unsigned i=0; i<SPIR_ERROR_NUM; i++) {
+    if (g_ErrorData[i].T != (SPIR_ERROR_TYPE)i)
+      return false;
+  }
+
+  for (unsigned i=0; i<SPIR_INFO_NUM; i++) {
+    if (g_InfoData[i].T != (SPIR_INFO_TYPE)i)
+      return false;
+  }
+
+  return true;
+}
+
+//
+// Validation Errors
+//
+
+/// @brief Returns type's name.
+/// @param Ty type
+/// @returns type's name as std::string.
+static std::string getObjectAsString(const Type *Ty) {
+  std::string type_str;
+  llvm::raw_string_ostream rso(type_str);
+  Ty->print(rso);
+  return rso.str();
+}
+
+/// @brief Returns Value as string.
+/// @param V value
+/// @returns value as std::string.
+static std::string getObjectAsString(const llvm::Value *V) {
+  std::string type_str;
+  llvm::raw_string_ostream rso(type_str);
+  V->print(rso);
+  return rso.str();
+}
+
+/// @brief Returns Metadata as string.
+/// @param V value
+/// @returns value as std::string.
+static std::string getObjectAsString(const llvm::Metadata *MD) {
+  std::string type_str;
+  llvm::raw_string_ostream rso(type_str);
+  MD->print(rso);
+  return rso.str();
+}
+
+/// @brief Returns NamedMDNode as string.
+/// @param NMD named metadata node
+/// @returns named metadata node as std::string.
+static std::string getObjectAsString(const llvm::NamedMDNode *NMD) {
+  std::string type_str;
+  llvm::raw_string_ostream rso(type_str);
+  NMD->print(rso);
+  return rso.str();
+}
+
+ErrorHolder::ErrorHolder() {
+  assert(isValidTables() && "SPIR Error/Info data tables are invalid!");
+}
+
+ErrorHolder::~ErrorHolder() {
+  for (ErrorList::iterator ei=EL.begin(), ee=EL.end(); ei!=ee; ei++) {
+    const ValidationError *Err = *ei;
+    delete Err;
+  }
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::StringRef S) {
+  std::string ErrMsg;
+  ErrMsg += S.str() + "\n";
+  ValidationError *VE = new ValidationError(Err, ErrMsg);
+  EL.push_back(VE);
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::Value *V) {
+  ValidationError *VE = new ValidationError(Err, getObjectAsString(V));
+  EL.push_back(VE);
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::Metadata *MD) {
+  ValidationError *VE = new ValidationError(Err, getObjectAsString(MD));
+  EL.push_back(VE);
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::NamedMDNode *NMD) {
+  ValidationError *VE = new ValidationError(Err, getObjectAsString(NMD));
+  EL.push_back(VE);
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::Type *T,
+                                                const llvm::StringRef S) {
+  std::string ErrMsg;
+  ErrMsg += "Type: " + getObjectAsString(T) + "\n";
+  ErrMsg += "Found in prototype of Function: " + S.str() + "\n";
+  ValidationError *VE = new ValidationError(Err, ErrMsg);
+  EL.push_back(VE);
+}
+
+void ErrorHolder::addError(SPIR_ERROR_TYPE Err, const llvm::Type *T,
+                                                const llvm::Value *V) {
+  std::string ErrMsg;
+  ErrMsg += "Type: " + getObjectAsString(T) + "\n";
+  ErrMsg += "Found in: " + getObjectAsString(V) + "\n";
+  ValidationError *VE = new ValidationError(Err, ErrMsg);
+  EL.push_back(VE);
+}
+
+void ErrorHolder::print(llvm::raw_ostream &S, bool LITMode) const {
+  ErrorList UEL;
+  SPIRInfoTypeNumMap ITmap;
+  // Calculate unique error list
+  // Collect relevant info types
+  for (ErrorList::const_iterator ei=EL.begin(), ee=EL.end(); ei!=ee; ei++) {
+    const ValidationError *Err = *ei;
+    ErrorComperator Dup(Err);
+    if (std::find_if(UEL.begin(), UEL.end(), Dup) == UEL.end()) {
+      // Add to unique error list
+      UEL.push_back(Err);
+      // Add to info type map, initialize number to zero
+      SPIR_ERROR_TYPE ErrType = Err->getErrorType();
+      for (unsigned i=0; i<MAX_ERROR_INFO_PER_ERROR; i++) {
+        SPIR_INFO_TYPE InfoType = g_ErrorData[ErrType].InfoList[i];
+        if (InfoType != INFO_NONE) {
+          ITmap[InfoType] = 0;
+        }
+      }
+    }
+  }
+
+  // Assign error info number for each relevant info type
+  // Create SPIR Info message
+  std::string InfoMsg;
+  if (!LITMode) {
+    unsigned ErrInfoNum = 0;
+    InfoMsg += "---------------------------------------------";
+    InfoMsg += "---------------------------------------------\n";
+    for (unsigned i=0; i<SPIR_INFO_NUM; i++) {
+      SPIR_INFO_TYPE InfoType = g_InfoData[i].T;
+      if (ITmap.count(InfoType) != 0) {
+        // Set error info number
+        ITmap[InfoType] = ++ErrInfoNum;
+        // Append error info message
+        std::stringstream SS;
+        SS << "[" << ErrInfoNum << "] " << g_InfoData[i].GetMsg() << "\n";
+        InfoMsg += SS.str();
+      }
+    }
+  }
+
+  // Create error message
+  std::string ErrMsg;
+  unsigned ErrNum = 0;
+
+  for (ErrorList::const_iterator ei=UEL.begin(), ee=UEL.end(); ei!=ee; ei++) {
+    const ValidationError *Err = *ei;
+    std::stringstream SS;
+    SPIR_ERROR_TYPE ErrType = Err->getErrorType();
+    SS << "(" << ++ErrNum << ") Error";
+    if(!LITMode) {
+      for (unsigned i=0; i<MAX_ERROR_INFO_PER_ERROR; i++) {
+        SPIR_INFO_TYPE InfoType = g_ErrorData[ErrType].InfoList[i];
+        if (InfoType != INFO_NONE) {
+          SS << "[" << ITmap[InfoType] << "]";
+        }
+      }
+      SS << " " << g_ErrorData[ErrType].MSG.c_str() << ":\n";
+    } else {
+      SS << " " << g_ErrorData[ErrType].ErrTypeStr << ":\n";
+    }
+    SS << Err->toString().str().c_str() << "\n";
+    ErrMsg += SS.str();
+  }
+
+  // Print error message and SPIR info message to output stream
+  S << ErrMsg;
+  S << InfoMsg;
+}
+
+bool ErrorHolder::hasErrors() const {
+  return !EL.empty();
+}
+
+} // End SPIR namespace
diff --git a/llvm/lib/SPIRVerifier/SpirIterators.cpp b/llvm/lib/SPIRVerifier/SpirIterators.cpp
new file mode 100644
index 000000000000..f399dee13833
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/SpirIterators.cpp
@@ -0,0 +1,1029 @@
+//===------------------------ SpirValidation.h ---------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#include "llvm/SPIRVerifier/SpirIterators.h"
+#include "llvm/SPIRVerifier/SpirErrors.h"
+#include "llvm/SPIRVerifier/SpirTables.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/Instruction.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Value.h"
+#include <sstream>
+#include <algorithm>
+
+namespace SPIR {
+
+//
+// Iterator classes (impl).
+//
+
+void BasicBlockIterator::execute(const llvm::BasicBlock& BB) {
+  // Run over all instructions in basic block.
+  BasicBlock::const_iterator ii = BB.begin(), ie = BB.end();
+  for (; ii != ie; ii++) {
+    // For each instruction apply all executors from the list.
+    const Instruction *I = &*ii;
+    InstructionExecutorList::iterator iei = m_iel.begin(), iee = m_iel.end();
+    for (; iei != iee; iei++) {
+      (*iei)->execute(I);
+    }
+  }
+}
+
+void FunctionIterator::execute(const llvm::Function& F) {
+  // Apply all executors from the list on the given function.
+  FunctionExecutorList::iterator fei = m_fel.begin(), fee = m_fel.end();
+  for (; fei != fee; fei++) {
+    (*fei)->execute(&F);
+  }
+  // If basic block iterator available
+  // Apply it for each basic block in the given function.
+  if (m_bbi) {
+    Function::const_iterator bi = F.begin(), be = F.end();
+    for (; bi != be; bi++) {
+      const BasicBlock *BB = &*bi;
+      m_bbi->execute(*BB);
+    }
+  }
+}
+
+void GlobalVariableIterator::execute(const llvm::GlobalVariable& GV) {
+  // Apply all executors from the list on the given global variable.
+  GlobalVariableExecutorList::iterator gei = m_gvel.begin(),
+                                       gee = m_gvel.end();
+  for (; gei != gee; gei++) {
+    (*gei)->execute(&GV);
+  }
+}
+
+void ModuleIterator::execute(const llvm::Module& M) {
+  // Apply all executors from the list on the given module.
+  ModuleExecutorList::iterator mei = m_mel.begin(), mee = m_mel.end();
+  for (; mei != mee; mei++) {
+    (*mei)->execute(&M);
+  }
+  // If function iterator available
+  // Apply it for each function in the given module.
+  if (m_fi) {
+    Module::const_iterator fi = M.begin(), fe = M.end();
+    for (; fi != fe; fi++) {
+      const Function *F = &*fi;
+      m_fi->execute(*F);
+    }
+  }
+  // If global variable iterator available
+  // Apply it for each global variable in the given module.
+  if (m_gi) {
+    Module::const_global_iterator gi = M.global_begin(), ge = M.global_end();
+    for (; gi != ge; gi++) {
+      const GlobalVariable *GV = &*gi;
+      m_gi->execute(*GV);
+    }
+  }
+}
+
+void MetaDataIterator::execute(const llvm::MDNode& Node) {
+  // Apply all executors from the list on the given Metadata node.
+  for (unsigned int i=0; i<Node.getNumOperands(); ++i) {
+    const llvm::MDNode *Op = dyn_cast<llvm::MDNode>(Node.getOperand(i));
+    if (Op) {
+      MDNodeExecutorList::iterator nei = m_nel.begin(), nee = m_nel.end();
+      for (; nei != nee; nei++) {
+        (*nei)->execute(Op);
+      }
+    }
+  }
+}
+
+//
+// Utility functions.
+//
+
+/// @brief Check if given name is valid according to given valid list.
+/// @param Name given name to validate.
+/// @param ValidList given valid list to validate against.
+/// @returns true if name is valid, false otherwise.
+static bool isValidNameOf(StringRef Name, const char *ValidList[], unsigned len) {
+  for (unsigned i=0; i<len; i++) {
+    StringRef candidate(ValidList[i]);
+    if (Name == candidate)
+      return true;
+  }
+  return false;
+}
+
+/// @brief Check if given name start with valid prefix according to given valid list.
+/// @param Name given name to validate.
+/// @param ValidList given valid list to validate against.
+/// @returns size of valid prefix, 0 if no valid prefix.
+static int hasPrefixValidNameOf(StringRef Name, const char *ValidList[], unsigned len) {
+  for (unsigned i=0; i<len; i++) {
+    StringRef candidate(ValidList[i]);
+    if (Name.startswith(candidate))
+      return candidate.size();
+  }
+  return 0;
+}
+
+// Returns true if the string is a legal name.
+static bool isValidTypeName(StringRef TyName) {
+  // Check if type start with a prefix of ignored type
+  if (hasPrefixValidNameOf(TyName, g_ignored_ocl_types,
+                                   g_ignored_ocl_types_len)) {
+    return true;
+  }
+  // Check if type is a valid OCL type.
+  if( isValidNameOf(TyName, g_valid_ocl_opaque_types,
+                            g_valid_ocl_opaque_types_len) ) {
+    return true;
+  }
+  // Check if type is a valid vector element type.
+  int prefixLen = hasPrefixValidNameOf(TyName,
+    g_valid_ocl_vector_element_types, g_valid_ocl_vector_element_types_len);
+  if (prefixLen) {
+    TyName = TyName.substr(prefixLen);
+    // Check for vector length suffix.
+    prefixLen = hasPrefixValidNameOf(TyName,
+      g_valid_vector_type_lengths, g_valid_vector_type_lengths_len);
+    TyName = TyName.substr(prefixLen);
+  } else {
+    // Check if type is a valid scalar primitive type.
+    prefixLen = hasPrefixValidNameOf(TyName,
+      g_valid_ocl_primitives, g_valid_ocl_primitives_len);
+    TyName = TyName.substr(prefixLen);
+  }
+  // '*' is the only possible suffix now (spaces are ignored).
+    for (unsigned int pos = 0; pos < TyName.size(); ++pos) {
+      if (TyName[pos] == ' ')
+        continue;
+      if (TyName[pos] == '[') {
+        // Array types are ignored. TODO: are they allowed?
+        return true;
+      }
+      if (TyName[pos] != '*')
+        return false;
+    }
+    return true;
+}
+
+/// @brief Check if given Value is an MDNode of given Type name.
+/// @param V given value to validate.
+/// @param type given type name validate against.
+/// @returns true if match, false otherwise.
+static bool isMDNodeTypeOf(const Metadata *V, StringRef type) {
+  if (!isa<MDNode>(V))
+    return false;
+
+  const MDNode *N = cast<MDNode>(V);
+  const MDString *StringVal = dyn_cast<MDString>(N->getOperand(0));
+  return StringVal && StringVal->getString() == type;
+}
+
+static bool isAllowedIntrinsic(StringRef FName) {
+  bool IsValidIntrinsic = hasPrefixValidNameOf(FName,
+    g_valid_instrinsic, g_valid_instrinsic_len) != 0;
+  bool IsIgnoredIntrinsic = hasPrefixValidNameOf(FName,
+    g_ignored_instrinsic, g_ignored_instrinsic_len) != 0;
+  return IsValidIntrinsic || IsIgnoredIntrinsic;
+}
+
+//
+// LLVM types validaiton
+//
+static bool isValidPrimitiveType(Type *Ty, DataHolder *D, bool isPointer) {
+  return
+    (Ty->isDoubleTy() && D->HasDoubleFeature) ||
+    (Ty->isHalfTy() && (D->HASFp16Extension || isPointer)) ||
+    Ty->isFloatTy() ||
+    Ty->isIntegerTy(1) || Ty->isIntegerTy(8) || Ty->isIntegerTy(16) ||
+    Ty->isIntegerTy(32) || Ty->isIntegerTy(64) || Ty->isVoidTy();
+}
+
+static bool isIgnoredPrimitiveType(Type *Ty) {
+  return Ty->isMetadataTy();
+}
+
+static bool isValidVectorElementType(Type *Ty, DataHolder *D, bool isPointer) {
+  return
+    (Ty->isDoubleTy() && D->HasDoubleFeature) ||
+    (Ty->isHalfTy() && (D->HASFp16Extension || isPointer)) ||
+    Ty->isFloatTy() ||
+    Ty->isIntegerTy(8) || Ty->isIntegerTy(16) || Ty->isIntegerTy(32) ||
+    Ty->isIntegerTy(64);
+}
+
+static bool isValidVectorElementsNum(unsigned ElementsNum) {
+  return ElementsNum == 2 || ElementsNum == 3 || ElementsNum == 4 ||
+    ElementsNum == 8 || ElementsNum == 16;
+}
+
+static bool isValidOCLOpaqueType(const StructType *Ty, DataHolder *D) {
+  return
+    isValidNameOf(Ty->getName(), g_valid_llvm_opaque_types,
+                                 g_valid_llvm_opaque_types_len) ||
+    (isValidNameOf(Ty->getName(), g_valid_llvm_image_types,
+                                  g_valid_llvm_image_types_len) &&
+     (!D || D->HasImageFeature));
+}
+
+static bool isValidType(Type *Ty, DataHolder *D,
+                        bool isBoolAllowed, bool isOpaqueAllowed,
+                        bool isBoolVecAllowed, bool isPointer) {
+  // Check if it is a pointer
+  if (Ty->isPointerTy()) {
+    return isValidType(Ty->getContainedType(0), D,
+      true, true, isBoolVecAllowed, true);
+  }
+
+  // Check if it is an Array
+  if (Ty->isArrayTy()) {
+    return isValidType(Ty->getContainedType(0), D,
+      false, false, isBoolVecAllowed, isPointer);
+  }
+
+  // Check if it is a Structure
+  if (const StructType *STy = dyn_cast<StructType>(Ty)) {
+    if (STy->isOpaque()) {
+      if (!isOpaqueAllowed) {
+        return false;
+      }
+      // Check of it is allowed OpenCL opaque types
+      if (isValidOCLOpaqueType(STy, D)) {
+        return true;
+      }
+      // Ignor other opaque type
+      // TODO: check if it is supported by SPIR
+      return true;
+    }
+    for (unsigned i=0; i<STy->getStructNumElements(); i++) {
+      if (!isValidType(STy->getStructElementType(i), D,
+          false, false, isBoolVecAllowed, isPointer)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  // Check if it is a vector
+  if (const FixedVectorType *VTy = dyn_cast<FixedVectorType>(Ty)) {
+    if (!isValidVectorElementsNum(VTy->getNumElements())) {
+      return false;
+    }
+    if (isBoolVecAllowed) {
+      return isValidType(VTy->getElementType(), D,
+        isBoolAllowed, false, isBoolVecAllowed, isPointer);
+    }
+    return isValidType(VTy->getElementType(), D,
+      false, false, isBoolVecAllowed, isPointer);
+  }
+
+  // Check if it is a valid primitive type
+  if (!isBoolAllowed) {
+    return isValidVectorElementType(Ty, D, isPointer);
+  }
+  return isValidPrimitiveType(Ty, D, isPointer) || isIgnoredPrimitiveType(Ty);
+
+}
+
+static std::string MapLLVMToOCL(Type *Ty, bool &Ignore) {
+  // Check if it is a pointer
+  if (Ty->isPointerTy()) {
+    return MapLLVMToOCL(Ty->getContainedType(0), Ignore) + "*";
+  }
+
+  // Check if it is an Array
+  if (Ty->isArrayTy()) {
+    // Do not know how to handle Array, mark this check as ignored
+    Ignore = true;
+    return "";
+  }
+
+  // Check if it is a Structure
+  if (const StructType *STy = dyn_cast<StructType>(Ty)) {
+    if (STy->isOpaque() && isValidOCLOpaqueType(STy, nullptr)) {
+      std::string TypeName = STy->getStructName().str();
+      TypeName = TypeName.substr(StringRef(g_llvm_opaque_prefix).size());
+      return TypeName;
+    }
+    std::string TypeName = STy->getStructName().str();
+    TypeName = "struct " + TypeName.substr(StringRef("struct.").size());
+    // TODO: maybe it is better to just Ignore this check
+    Ignore = true;
+    return TypeName;
+  }
+
+  // Check if it is a vector
+  if (const FixedVectorType *VTy = dyn_cast<FixedVectorType>(Ty)) {
+    std::stringstream SS;
+    SS << MapLLVMToOCL(VTy->getElementType(), Ignore) << VTy->getNumElements();
+    return SS.str();
+  }
+
+  // Check if it is a valid primitive type
+  if (Ty->isFloatTy()) {
+    return "float";
+  }
+  if (Ty->isDoubleTy()) {
+    return "double";
+  }
+  if (Ty->isHalfTy()) {
+    return "half";
+  }
+  if (Ty->isIntegerTy(1)) {
+    return "bool";
+  }
+  if (Ty->isIntegerTy(8)) {
+    return "char";
+  }
+  if (Ty->isIntegerTy(16)) {
+    return "short";
+  }
+  if (Ty->isIntegerTy(32)) {
+    return "int";
+  }
+  if (Ty->isIntegerTy(64)) {
+    return "long";
+  }
+  // Any other type is not allowed, return empty string
+  return "";
+}
+
+static bool isValidMapOCLToLLVM(StringRef TyName, Type *Ty, DataHolder *D) {
+  // Check if type start with a prefix of ignored type
+  if (hasPrefixValidNameOf(TyName, g_ignored_ocl_types,
+                                   g_ignored_ocl_types_len)) {
+    return true;
+  }
+
+  bool Ignore = false;
+  std::string ConvertedName = MapLLVMToOCL(Ty, Ignore);
+
+  if (Ignore) {
+    // Do not know how to convert, ignore this check.
+    return true;
+  }
+
+  if (TyName.find("void") != std::string::npos) {
+    // TODO: Can 'void' be verified?
+    return true;
+  }
+
+  std::string StrName = TyName.str();
+  // Handle special type conversions
+  if( isValidNameOf(TyName, g_valid_ocl_opaque_types,
+                            g_valid_ocl_opaque_types_len) ) {
+    if (TyName == "sampler_t") {
+      StrName = "int"; //"i32"
+    }
+    else if (TyName == "size_t" || TyName == "ptrdiff_t" ||
+        TyName == "uintptr_t" || TyName == "intptr_t") {
+      if (D->Is32Bit) {
+        StrName = "int"; //"i32"
+      }
+      else {
+        StrName = "long";
+      }
+    }
+    else {
+      StrName += "*";
+    }
+  }
+  else if (TyName.startswith("unsigned")) {
+    StrName = TyName.substr(StringRef("unsigned").size());
+  }
+  else if (TyName.startswith("u")) {
+    StrName = TyName.substr(StringRef("u").size());
+  }
+
+  return (StrName == ConvertedName);
+}
+
+static bool isValidAddrSpaceCast(const BitCastInst *BI) {
+  const PointerType *LHS = dyn_cast<PointerType>(BI->getDestTy()),
+                    *RHS = dyn_cast<PointerType>(BI->getSrcTy());
+  if (!LHS || !RHS)
+    return true;
+
+  const unsigned int DstAddress = LHS->getAddressSpace();
+  const unsigned int SrcAddress = RHS->getAddressSpace();
+  return (SrcAddress == DstAddress);
+}
+
+static bool isValidAddrSpaceCast(const User *V) {
+
+  for (unsigned i = 0; i < V->getNumOperands(); i++) {
+    // If the operand is not a constant expression, we will (or already did),
+    // visit it as a command from the main block iteration.
+    if (const ConstantExpr *CE = dyn_cast<ConstantExpr>(V->getOperand(i)))
+      isValidAddrSpaceCast(CE);
+  }
+
+  const ConstantExpr *CE = dyn_cast<ConstantExpr>(V);
+  if (!CE)
+    return true;
+
+  const PointerType *PTy  = dyn_cast<PointerType>(CE->getType());
+  if (!PTy)
+    return true;
+
+  const unsigned int DstAddress = PTy->getAddressSpace();
+  if (Instruction::BitCast == CE->getOpcode()) {
+    const PointerType *STy = dyn_cast<PointerType>(CE->getOperand(0)->getType());
+    if (STy) {
+      const unsigned int SrcAddress = STy->getAddressSpace();
+      return (SrcAddress == DstAddress);
+    }
+  }
+
+  return true;
+}
+
+static bool isValidAddrSpace(unsigned AddSpace) {
+  assert(g_valid_address_space_len == 4 &&
+    "In SPIR 1.2 we have only 4 address spaces");
+  return AddSpace < g_valid_address_space_len;
+}
+
+static bool isValidOCLVersion(unsigned Major, unsigned Minor) {
+  return (Major == 1 && Minor == 2);
+}
+
+static bool isValidSPIRVersion(unsigned Major, unsigned Minor) {
+  return (Major == 1 && Minor == 2);
+}
+
+static bool isValidMemfence(unsigned Val) {
+  return (Val == 1 || Val == 2 || Val == 3);
+}
+
+static bool isValidLinkageType(llvm::GlobalValue::LinkageTypes LT) {
+  return LT == llvm::GlobalValue::ExternalLinkage
+      || LT == llvm::GlobalValue::PrivateLinkage
+      || LT == llvm::GlobalValue::InternalLinkage
+      || LT == llvm::GlobalValue::AvailableExternallyLinkage;
+}
+
+//
+// Verify Executor classes (impl).
+//
+void VerifyCall::execute(const Instruction *I) {
+  const CallInst *CI = dyn_cast<CallInst>(I);
+  if (!CI)
+    return;
+
+  // Verify that this call is not indirect.
+  const Function *F = CI->getCalledFunction();
+  if (!F) {
+    ErrCreator->addError(ERR_INVALID_INDIRECT_CALL, I);
+    return;
+  }
+
+  if (!F->isDeclaration()) {
+    // Verify that this call has valid calling convention.
+    if (CI->getCallingConv() != CallingConv::FLOOR_KERNEL &&
+        CI->getCallingConv() != CallingConv::FLOOR_FUNC) {
+        ErrCreator->addError(ERR_INVALID_CALLING_CONVENTION, I);
+    }
+  }
+
+  // Verify valid memfence for synchronize functions
+  if (hasPrefixValidNameOf(F->getName(), g_valid_sync_bi, g_valid_sync_bi_len)) {
+    if (CI->arg_size() != 1) {
+      ErrCreator->addError(ERR_INVALID_MEM_FENCE, I);
+    }
+    ConstantInt *MemfenceVal = dyn_cast<ConstantInt>(CI->getOperand(0));
+    if (!MemfenceVal ||
+      !isValidMemfence((unsigned)MemfenceVal->getZExtValue())) {
+      ErrCreator->addError(ERR_INVALID_MEM_FENCE, I);
+    }
+  }
+
+  // Verify that this call is valid intrinsic.
+  if (F->isIntrinsic() && !isAllowedIntrinsic(F->getName())) {
+    ErrCreator->addError(ERR_INVALID_INTRINSIC, I);
+  }
+}
+
+void VerifyBitcast::execute(const Instruction *I) {
+  if (const BitCastInst *BI = dyn_cast<BitCastInst>(I)) {
+    // Verify that this bitcast is not adress space cast.
+    if (!isValidAddrSpaceCast(BI))
+      ErrCreator->addError(ERR_INVALID_ADDR_SPACE_CAST, I);
+  }
+
+  for (unsigned i = 0; i < I->getNumOperands(); i++) {
+    // Verify that each opernad is not const expression adress space cast.
+    if (const User *Usr = dyn_cast<User>(I->getOperand(i)))
+      if (!isValidAddrSpaceCast(Usr) )
+        ErrCreator->addError(ERR_INVALID_ADDR_SPACE_CAST, I);
+  }
+}
+
+void VerifyInstructionType::execute(const Instruction *I) {
+  Type *Ty = I->getType();
+  bool isValid = true;
+  switch(I->getOpcode()) {
+  case Instruction::ICmp:
+  case Instruction::FCmp:
+    isValid = isValidType(Ty, Data, true, true, true, false);
+    break;
+  default:
+    isValid = isValidType(Ty, Data, true, true, false, false);
+    break;
+  }
+  if (!isValid)
+    ErrCreator->addError(ERR_INVALID_LLVM_TYPE, Ty, I);
+}
+
+void VerifyFunctionPrototype::execute(const Function *F) {
+  if (!F->isDeclaration()) {
+    // Verify calling convention for user defined functions
+    if (F->getCallingConv() != CallingConv::FLOOR_KERNEL &&
+        F->getCallingConv() != CallingConv::FLOOR_FUNC)
+          ErrCreator->addError(ERR_INVALID_CALLING_CONVENTION, F->getName());
+  }
+  Function::const_arg_iterator ai = F->arg_begin(), ae = F->arg_end();
+  for (; ai != ae; ai++) {
+    Type *Ty = ai->getType();
+    if (!isValidType(Ty, Data, true, true, false, false)) {
+      ErrCreator->addError(ERR_INVALID_LLVM_TYPE, Ty, F->getName());
+    }
+  }
+  // Verify function linkage
+  if (!isValidLinkageType(F->getLinkage())) {
+    ErrCreator->addError(ERR_INVALID_LINKAGE_TYPE, F->getName());
+  }
+  // Verify function return type.
+  if (!isValidType(F->getReturnType(), Data, true, true, false, false)) {
+    ErrCreator->addError(
+      ERR_INVALID_LLVM_TYPE, F->getReturnType(), F->getName());
+  }
+}
+
+void VerifyKernelPrototype::execute(const Function *F) {
+  // detect kernel by looking at the calling convention
+  if (F->getCallingConv() != CallingConv::FLOOR_KERNEL)
+    return;
+
+  // check arguments
+  Function::const_arg_iterator ai = F->arg_begin(), ae = F->arg_end();
+  for (; ai != ae; ai++) {
+    Type *Ty = ai->getType();
+
+    // kernel arguments shall not be pointers to pointers
+    if (Ty->isPointerTy() && Ty->getPointerElementType()->isPointerTy()) {
+      ErrCreator->addError(ERR_KERNEL_ARG_PTRPTR, Ty, F->getName());
+    }
+
+    // kernel arguments shall not be pointers to a private addrspace
+	// except for aggregate types which are always pointer types
+    if (Ty->isPointerTy() &&
+        !Ty->getPointerElementType()->isStructTy() &&
+        Ty->getPointerAddressSpace() == PRIVATE_ADDR_SPACE) {
+      ErrCreator->addError(ERR_KERNEL_ARG_AS0, Ty, F->getName());
+    }
+  }
+
+  // the return type shall be void
+  if (!F->getReturnType()->isVoidTy()) {
+    ErrCreator->addError(
+      ERR_INVALID_KERNEL_RETURN_TYPE, F->getReturnType(), F->getName());
+  }
+}
+
+void VerifyGlobalVariable::execute(const GlobalVariable *GV) {
+  // Verify variable linkage
+  if (!isValidLinkageType(GV->getLinkage())) {
+    ErrCreator->addError(ERR_INVALID_LINKAGE_TYPE, GV->getName());
+  }
+
+  // check the global variable address space
+  switch (GV->getType()->getPointerAddressSpace()) {
+  case CONSTANT_ADDR_SPACE:
+    // constant address space: everything OK
+    break;
+  case LOCAL_ADDR_SPACE: {
+    // local address space:
+    // it is a function-scope variable,
+    // must contain a prefix that is equal to the name of a function
+    // and should be used only in it
+    for (Value::const_use_iterator ib = GV->use_begin(), ie = GV->use_end(); ib != ie; ++ib) {
+      if (const Instruction *Inst = dyn_cast<Instruction>(*ib)) {
+        const Function * func = Inst->getParent()->getParent();
+        if (!(GV->getName().startswith(func->getName().str() + "."))) {
+           ErrCreator->addError(ERR_INVALID_GLOBAL_AS3_VAR, GV);
+           break;
+        }
+      }
+    }
+    break;
+  }
+  default:
+    ErrCreator->addError(ERR_INVALID_GLOBAL_VAR_ADDRESS_SPACE, GV);
+    break;
+  }
+}
+
+void VerifyMetadataArgAddrSpace::execute(const llvm::MDNode *Node) {
+  if (!isMDNodeTypeOf(Node, KERNEL_ARG_ADDR_SPACE))
+    return;
+
+  WasFound = true;
+  // Verify that kernel arg base type metadata list is valid.
+  for (unsigned i=1; i<Node->getNumOperands(); i++) {
+    auto *AddrSpaceMD = dyn_cast<ConstantAsMetadata>(Node->getOperand(i));
+    if (!AddrSpaceMD) {
+      ErrCreator->addError(ERR_INVALID_METADATA_KERNEL_INFO, Node);
+      continue;
+    }
+    ConstantInt *AddrSpace = dyn_cast<ConstantInt>(AddrSpaceMD->getValue());
+    if (!AddrSpace) {
+      ErrCreator->addError(ERR_INVALID_METADATA_KERNEL_INFO, Node);
+      continue;
+    }
+    unsigned AddrSpaceVal = (unsigned)AddrSpace->getZExtValue();
+    if (!isValidAddrSpace(AddrSpaceVal)) {
+      ErrCreator->addError(ERR_INVALID_ADDR_SPACE, Node);
+    }
+    // Verify that LLVM Type of relevant function prototype argument
+    // has same address space.
+    const unsigned ArgIndex = i-1;
+    Type *Ty = Func->getFunctionType()->getParamType(ArgIndex);
+    unsigned ArgAddrSpace = 0;
+    if (PointerType *PTy = dyn_cast<PointerType>(Ty)) {
+      ArgAddrSpace = PTy->getAddressSpace();
+    }
+    if (ArgAddrSpace != AddrSpaceVal) {
+      ErrCreator->addError(ERR_INVALID_METADATA_KERNEL_INFO, Node);
+      ErrCreator->addError(ERR_MISMATCH_METADATA_ADDR_SPACE, Ty,
+                                                            Func->getName());
+    }
+  }
+}
+
+void VerifyMetadataArgType::execute(const llvm::MDNode *Node) {
+  if (isMDNodeTypeOf(Node, KERNEL_ARG_TY)) {
+    WasFound = true;
+  }
+}
+
+void VerifyMetadataArgBaseType::execute(const llvm::MDNode *Node) {
+  if (!isMDNodeTypeOf(Node, KERNEL_ARG_BASE_TY))
+    return;
+
+  WasFound = true;
+  // Verify that kernel arg base type metadata list is valid.
+  for (unsigned i=1; i<Node->getNumOperands(); i++) {
+    MDString *StringValue = dyn_cast<MDString>(Node->getOperand(i));
+    if (!StringValue) {
+      ErrCreator->addError(ERR_INVALID_METADATA_KERNEL_INFO, Node);
+      continue;
+    }
+    StringRef S = StringValue->getString();
+    if (!isValidTypeName(S)) {
+      ErrCreator->addError(ERR_INVALID_OCL_TYPE, Node);
+      continue;
+    }
+    // Verify that LLVM Type of relevant function prototype argument
+    // has same address space.
+    const unsigned ArgIndex = i-1;
+    Type *Ty = Func->getFunctionType()->getParamType(ArgIndex);
+    if (!isValidMapOCLToLLVM(S, Ty, Data)) {
+      ErrCreator->addError(ERR_MISMATCH_OCL_AND_LLVM_TYPES, Node);
+      ErrCreator->addError(ERR_MISMATCH_OCL_AND_LLVM_TYPES, Ty,
+                                                            Func->getName());
+    }
+  }
+}
+
+void VerifyMetadataKernel::execute(const llvm::MDNode *Node) {
+  // Verify that first operand is a valid function type.
+  if (Node->getNumOperands() < 1) {
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Node);
+    return;
+  }
+  ValueAsMetadata* FMD = dyn_cast<ValueAsMetadata>(Node->getOperand(0));
+  if (!FMD) {
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Node);
+    return;
+  }
+  Function *F = dyn_cast<Function>(FMD->getValue());
+  if (!F) {
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Node);
+    return;
+  }
+  if (F->getCallingConv() != CallingConv::FLOOR_KERNEL) {
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Node);
+  }
+  if (FoundMap.count(F)) {
+    // Function has two kernel metadata nodes
+    // Mark both of them as invalid metadata kernel
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, FoundMap[F]);
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Node);
+  }
+  // Insert <F, Node> pair to the found map.
+  FoundMap[F] = Node;
+
+  // Initialize second level executors.
+  MDNodeExecutorList nel;
+  // kernel arg address space metadata verifier.
+  VerifyMetadataArgAddrSpace vmdaas(ErrCreator, F);
+  nel.push_back(&vmdaas);
+  // kernel arg type metadata verifier.
+  VerifyMetadataArgType vmdat(ErrCreator);
+  nel.push_back(&vmdat);
+  // kernel arg base type metadata verifier.
+  VerifyMetadataArgBaseType vmdabt(ErrCreator, F, Data);
+  nel.push_back(&vmdabt);
+
+  MetaDataIterator mdi(nel);
+  mdi.execute(*Node);
+
+  // Varify that metadata arg address space exists.
+  if (!vmdaas.found()) {
+    ErrCreator->addError(ERR_MISSING_METADATA_KERNEL_INFO, Node);
+  }
+
+  // Varify that metadata arg type exists.
+  if (!vmdat.found()) {
+    ErrCreator->addError(ERR_MISSING_METADATA_KERNEL_INFO, Node);
+  }
+
+  // Varify that metadata arg base type exists.
+  if (!vmdabt.found()) {
+    ErrCreator->addError(ERR_MISSING_METADATA_KERNEL_INFO, Node);
+  }
+}
+
+void VerifyMetadataKernels::execute(const llvm::Module *M) {
+  // Counting the number of kernels in the module.
+  unsigned int NumKernels = 0;
+  Module::const_iterator fi = M->begin(), fe = M->end();
+  for (; fi != fe; fi++) {
+    const Function *F = &*fi;
+    if (F->getCallingConv() == CallingConv::FLOOR_KERNEL) {
+      NumKernels++;
+    }
+  }
+
+  // Acquiring kernels node.
+  NamedMDNode *MDKernels = M->getNamedMetadata(OPENCL_KERNELS);
+  if (!MDKernels) {
+    ErrCreator->addError(ERR_MISSING_NAMED_METADATA, OPENCL_KERNELS);
+    return;
+  }
+
+  // Verify that number of function kernels mach number of metadata kernels.
+  const unsigned int NumMDKernels = MDKernels->getNumOperands();
+
+  if (NumKernels != NumMDKernels) {
+    std::stringstream Msg;
+    Msg << "inconsistency in kernels nodes. ";
+    Msg << "The module has " << NumMDKernels << " metadata nodes, but ";
+    Msg << NumKernels << " kernels";
+    ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, Msg.str());
+  }
+
+  // If there are no kernels, we have no more tests to do.
+  if (!NumKernels)
+    return;
+
+  //Kernel MetaData structure:
+  // !opencl.kernels = {!0, !1, ...}
+  // !0 = {llvm::Function*, !10, !11, ...}
+  // ...
+  // !10 = {metadata !"kernel_arg_base_type", metadata !"<TY1>", ...}
+  // !11 = {metadata !"kernel_arg_type", metadata !"<TY1>", ...}
+
+  FunctionToMDNodeMap FoundMap;
+  VerifyMetadataKernel vmk(ErrCreator, Data, FoundMap);
+  for (unsigned i = 0; i < NumMDKernels; i++) {
+    MDNode *N = dyn_cast<MDNode>(MDKernels->getOperand(i));
+    if (!N) {
+      // Is this possible for LLVM valid IR?
+      ErrCreator->addError(ERR_INVALID_METADATA_KERNEL, MDKernels);
+    }
+    // Apply Metadata kernel executor.
+    vmk.execute(N);
+  }
+}
+
+void VerifyMetadataVersions::execute(const llvm::Module *M) {
+  const char *VersionName = nullptr;
+  switch (VType) {
+  case VERSION_OCL:
+    VersionName = OPENCL_OCL_VERSION;
+    break;
+  case VERSION_SPIR:
+    VersionName = OPENCL_SPIR_VERSION;
+    break;
+  default:
+    assert(false && "Unknown OpenCL version type");
+  }
+
+  // Verify version exists.
+  NamedMDNode *NMDVersion = M->getNamedMetadata(VersionName);
+  if (!NMDVersion) {
+    ErrCreator->addError(ERR_MISSING_NAMED_METADATA, VersionName);
+    return;
+  }
+
+  // version MetaData structure:
+  // !opencl.ocl.version  = {!6}
+  // !opencl.spir.version = {!7}
+  // ...
+  // !6 = metadata !{i32 1, i32 2}
+  // !7 = metadata !{i32 1, i32 2}
+
+  // Verify that there is exactly one version.
+  if (NMDVersion->getNumOperands() != 1) {
+    ErrCreator->addError(ERR_INVALID_METADATA_VERSION, NMDVersion);
+    return;
+  }
+
+  MDNode *VersionNode = NMDVersion->getOperand(0);
+
+  // Verify valid version node
+  if (VersionNode->getNumOperands() != 2) {
+    ErrCreator->addError(ERR_INVALID_METADATA_VERSION, VersionNode);
+    return;
+  }
+
+  ConstantAsMetadata *VersionMajorMD = dyn_cast<ConstantAsMetadata>(VersionNode->getOperand(0));
+  ConstantAsMetadata *VersionMinorMD = dyn_cast<ConstantAsMetadata>(VersionNode->getOperand(1));
+
+  if (!VersionMajorMD || !VersionMinorMD) {
+    ErrCreator->addError(ERR_INVALID_METADATA_VERSION, VersionNode);
+    return;
+  }
+
+  ConstantInt *VersionMajor = dyn_cast<ConstantInt>(VersionMajorMD->getValue());
+  ConstantInt *VersionMinor = dyn_cast<ConstantInt>(VersionMinorMD->getValue());
+
+  if (!VersionMajor || !VersionMinor) {
+    ErrCreator->addError(ERR_INVALID_METADATA_VERSION, VersionNode);
+    return;
+  }
+
+  unsigned Major = (unsigned)VersionMajor->getZExtValue();
+  unsigned Minor = (unsigned)VersionMinor->getZExtValue();
+
+  bool IsValidVersion = false;
+  switch (VType) {
+  case VERSION_OCL:
+    IsValidVersion = isValidOCLVersion(Major, Minor);
+    break;
+  case VERSION_SPIR:
+    IsValidVersion = isValidSPIRVersion(Major, Minor);
+    break;
+  default:
+    assert(false && "Unknown OpenCL version type");
+  }
+
+  if (!IsValidVersion) {
+    ErrCreator->addError(ERR_INVALID_METADATA_VERSION, VersionNode);
+    return;
+  }
+}
+
+void VerifyMetadataCoreFeatures::execute(const llvm::Module *M) {
+  // Verify OpenCL optional core features metadata exists.
+  NamedMDNode *NMDCoreFeatures = M->getNamedMetadata(OPENCL_CORE_FEATURES);
+  if (!NMDCoreFeatures) {
+    ErrCreator->addError(ERR_MISSING_NAMED_METADATA, OPENCL_CORE_FEATURES);
+    return;
+  }
+
+  // Optional core features MetaData structure:
+  // !opencl.used.optional.core.features  = {!8}
+  // ...
+  // !8 = metadata !{metadata !"cl_doubles", metadata !"cl_images"}
+
+  // Verify that there is exactly one entry.
+  if (NMDCoreFeatures->getNumOperands() != 1) {
+    ErrCreator->addError(ERR_INVALID_CORE_FEATURE, NMDCoreFeatures);
+    return;
+  }
+
+  MDNode *Node = NMDCoreFeatures->getOperand(0);
+
+  // Verify valid optional core feature nodes
+  for (unsigned i=0; i<Node->getNumOperands(); i++) {
+    MDString *StringValue = dyn_cast<MDString>(Node->getOperand(i));
+    if (!StringValue || !isValidNameOf(StringValue->getString(),
+                                       g_valid_core_feature,
+                                       g_valid_core_feature_len)) {
+      ErrCreator->addError(ERR_INVALID_CORE_FEATURE, Node);
+      continue;
+    }
+    if (StringValue->getString() == CORE_FEATURE_CL_DOUBLES) {
+      if (Data->HasDoubleFeature) {
+        // Core feature appears twice
+        ErrCreator->addError(ERR_INVALID_CORE_FEATURE, Node);
+        continue;
+      }
+      Data->HasDoubleFeature = true;
+    }
+    else if(StringValue->getString() == CORE_FEATURE_CL_IMAGES) {
+      if (Data->HasImageFeature) {
+        // Core feature appears twice
+        ErrCreator->addError(ERR_INVALID_CORE_FEATURE, Node);
+        continue;
+      }
+      Data->HasImageFeature = true;
+    }
+    else {
+      assert(false && "Unhandled core feature");
+    }
+  }
+}
+
+
+void VerifyMetadataKHRExtensions::execute(const llvm::Module *M) {
+  // Verify OpenCL optional KHR extensions metadata exists.
+  NamedMDNode *NMDExts = M->getNamedMetadata(OPENCL_KHR_EXTENSIONS);
+  if (!NMDExts) {
+    ErrCreator->addError(ERR_MISSING_NAMED_METADATA, OPENCL_KHR_EXTENSIONS);
+    return;
+  }
+
+  // KHR extensions MetaData structure:
+  // !opencl.used.extensions = !{!6}
+  // ...
+  // !6 = metadata !{metadata !"cl_khr_fp16", metadata !"cl_khr_int64_base_atomics"}
+
+  // Verify that there is exactly one entry.
+  if (NMDExts->getNumOperands() != 1) {
+    ErrCreator->addError(ERR_INVALID_CORE_FEATURE, NMDExts);
+    return;
+  }
+
+  MDNode *Node = NMDExts->getOperand(0);
+
+  // Verify valid optional core feature nodes
+  for (unsigned i=0; i<Node->getNumOperands(); i++) {
+    MDString *StringValue = dyn_cast<MDString>(Node->getOperand(i));
+    if (!StringValue || !isValidNameOf(StringValue->getString(),
+                                       g_valid_khr_ext,
+                                       g_valid_khr_ext_len)) {
+      ErrCreator->addError(ERR_INVALID_KHR_EXT, Node);
+      continue;
+    }
+    if (StringValue->getString() == EXTENSION_CL_KHR_FP16) {
+      if (Data->HASFp16Extension) {
+        // KHR extension appears twice
+        ErrCreator->addError(ERR_INVALID_KHR_EXT, Node);
+        continue;
+      }
+      Data->HasDoubleFeature = true;
+    }
+    else {
+      // TODO enable the following assertion once all extensions are handled.
+      //assert(false && "Unhandled KHR extension");
+    }
+  }
+}
+
+void VerifyMetadataCompilerOptions::execute(const llvm::Module *M) {
+  // Verify OpenCL compiler options metadata exists.
+  NamedMDNode *NMDOptions = M->getNamedMetadata(OPENCL_COMPILER_OPTIONS);
+  if (!NMDOptions) {
+    ErrCreator->addError(ERR_MISSING_NAMED_METADATA, OPENCL_COMPILER_OPTIONS);
+    return;
+  }
+
+  // Compiler options MetaData structure:
+  // !opencl.compiler.options = !{!9}
+  // ...
+  // !9 = metadata !{metadata !"-cl-mad-enable", metadata !"-cl-denorms-are-zero"}
+
+  // Verify that there is exactly one entry.
+  if (NMDOptions->getNumOperands() != 1) {
+    ErrCreator->addError(ERR_INVALID_COMPILER_OPTION, NMDOptions);
+    return;
+  }
+
+  MDNode *Node = NMDOptions->getOperand(0);
+
+  // Verify valid optional core feature nodes
+  for (unsigned i=0; i<Node->getNumOperands(); i++) {
+    MDString *StringValue = dyn_cast<MDString>(Node->getOperand(i));
+    if (!StringValue || !isValidNameOf(StringValue->getString(),
+                                       g_valid_compiler_options,
+                                       g_valid_compiler_options_len)) {
+      ErrCreator->addError(ERR_INVALID_COMPILER_OPTION, Node);
+      continue;
+    }
+  }
+}
+
+} // End SPIR namespace
+
diff --git a/llvm/lib/SPIRVerifier/SpirTables.cpp b/llvm/lib/SPIRVerifier/SpirTables.cpp
new file mode 100644
index 000000000000..1df07e70f26c
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/SpirTables.cpp
@@ -0,0 +1,567 @@
+//===-------------------------- SpirTables.cpp ---------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#include "llvm/SPIRVerifier/SpirTables.h"
+#include <string>
+#include <sstream>
+
+namespace SPIR {
+
+//
+// Constant definitions.
+//
+
+#define DCL_ARRAY_LENGTH(arr) \
+  const unsigned arr##_len = (sizeof(arr)/sizeof(char*))
+
+#define STR_IND1 std::string("  ")
+#define STR_IND2 std::string("    ")
+#define STR_SPIR std::string("SPIR")
+#define STR_NOTE std::string("Note: ")
+
+
+const char *CORE_FEATURE_CL_DOUBLES = "cl_doubles";
+const char *CORE_FEATURE_CL_IMAGES = "cl_images";
+const char *g_valid_core_feature[] = {
+  CORE_FEATURE_CL_DOUBLES,
+  CORE_FEATURE_CL_IMAGES
+};
+DCL_ARRAY_LENGTH(g_valid_core_feature);
+const char *g_valid_core_feature_prefix = "cl_";
+
+
+const char *EXTENSION_CL_KHR_FP16 = "cl_khr_fp16";
+const char *g_valid_khr_ext[] = {
+  "cl_khr_int64_base_atomics",
+  "cl_khr_int64_extended_atomics",
+  EXTENSION_CL_KHR_FP16,
+  "cl_khr_fp64",
+  "cl_khr_gl_sharing",
+  "cl_khr_gl_event",
+  "cl_khr_d3d10_sharing",
+  "cl_khr_media_sharing",
+  "cl_khr_d3d11_sharing",
+  "cl_khr_global_int32_base_atomics",
+  "cl_khr_global_int32_extended_atomics",
+  "cl_khr_local_int32_base_atomics",
+  "cl_khr_local_int32_extended_atomics",
+  "cl_khr_byte_addressable_store",
+  "cl_khr_3d_image_writes",
+  "cl_khr_gl_msaa_sharing",
+  "cl_khr_depth_images",
+  "cl_khr_gl_depth_images",
+  "cl_khr_mipmap_image",
+  "cl_khr_mipmap_image_writes",
+};
+DCL_ARRAY_LENGTH(g_valid_khr_ext);
+const char *g_valid_khr_ext_prefix = "cl_khr_";
+
+
+const char *g_valid_compiler_options[] = {
+  "-cl-single-precision-constant",
+  "-cl-denorms-are-zero",
+  "-cl-fp32-correctly-rounded-divide-sqrt",
+  "-cl-opt-disable",
+  "-cl-mad-enable",
+  "-cl-no-signed-zeros",
+  "-cl-unsafe-math-optimizations",
+  "-cl-finite-math-only",
+  "-cl-fast-relaxed-math",
+  "-w",
+  "-Werror",
+  "-cl-kernel-arg-info"
+};
+DCL_ARRAY_LENGTH(g_valid_compiler_options);
+
+///
+/// OpenCL C Type tables
+///
+const char *g_valid_ocl_primitives[] = {
+  "bool"  ,
+  "char"  , "uchar" , "unsigned char" ,
+  "short" , "ushort", "unsigned short",
+  "int"   , "uint"  , "unsigned int"  ,
+  "long"  , "ulong" , "unsigned long" ,
+  "half"  ,
+  "float" ,
+  "double",
+  "void"
+};
+DCL_ARRAY_LENGTH(g_valid_ocl_primitives);
+
+const char *g_valid_ocl_vector_element_types[] = {
+  "char"  , "uchar" , "unsigned char" ,
+  "short" , "ushort", "unsigned short",
+  "int"   , "uint"  , "unsigned int"  ,
+  "long"  , "ulong" , "unsigned long" ,
+  "half"  ,
+  "float" ,
+  "double"
+};
+DCL_ARRAY_LENGTH(g_valid_ocl_vector_element_types);
+
+const char *g_valid_ocl_opaque_types[] = {
+  "image1d_t",
+  "image1d_array_t",
+  "image1d_buffer_t",
+  "image2d_t",
+  "image2d_depth_t",
+  "image2d_msaa_t",
+  "image2d_msaa_depth_t",
+  "image2d_array_t",
+  "image2d_array_depth_t",
+  "image2d_array_msaa_t",
+  "image2d_array_msaa_depth_t",
+  "image3d_t",
+  "event_t",
+  "sampler_t",
+  // TODO: The following are not part of 'kernel_arg_base_type' metadata -
+  // according to SPIR generator. Should they still be allowed (ignored)?
+  "size_t",
+  "ptrdiff_t",
+  "uintptr_t",
+  "intptr_t"
+};
+DCL_ARRAY_LENGTH(g_valid_ocl_opaque_types);
+
+const char *g_opencl_opaque_sufix = "_t";
+
+const char *g_ignored_ocl_types[] = {
+  "struct ", "union ", "enum "
+};
+DCL_ARRAY_LENGTH(g_ignored_ocl_types);
+
+///
+/// OpenCL C Type tables
+///
+const char *g_valid_llvm_primitives[] = {
+  "i1", "i8", "i16", "i32", "i64", "half", "float", "double", "void"
+};
+DCL_ARRAY_LENGTH(g_valid_llvm_primitives);
+
+const char *g_valid_llvm_vector_element_types[] = {
+  "i8", "i16", "i32", "i64", "half", "float", "double"
+};
+DCL_ARRAY_LENGTH(g_valid_llvm_vector_element_types);
+
+const char *g_valid_llvm_image_types[] = {
+  "opencl.image1d_t",
+  "opencl.image1d_array_t",
+  "opencl.image1d_buffer_t",
+  "opencl.image2d_t",
+  "opencl.image2d_depth_t",
+  "opencl.image2d_msaa_t",
+  "opencl.image2d_msaa_depth_t",
+  "opencl.image2d_array_t",
+  "opencl.image2d_array_msaa_t",
+  "opencl.image2d_array_msaa_depth_t",
+  "opencl.image2d_array_depth_t",
+  "opencl.image3d_t",
+};
+DCL_ARRAY_LENGTH(g_valid_llvm_image_types);
+
+const char *g_valid_llvm_opaque_types[] = {
+  "opencl.event_t"
+};
+DCL_ARRAY_LENGTH(g_valid_llvm_opaque_types);
+
+const char *g_llvm_opaque_prefix = "opencl.";
+
+const char *g_valid_vector_type_lengths[] = {
+  "2" , "3" , "4" , "8" , "16"
+};
+DCL_ARRAY_LENGTH(g_valid_vector_type_lengths);
+
+
+const char *g_valid_instrinsic[] = {
+  "llvm.memcpy."
+};
+DCL_ARRAY_LENGTH(g_valid_instrinsic);
+
+const char *g_ignored_instrinsic[] = {
+  "llvm.dbg."
+};
+DCL_ARRAY_LENGTH(g_ignored_instrinsic);
+
+
+const char *g_valid_sync_bi[] = {
+  "_Z7barrier"
+};
+DCL_ARRAY_LENGTH(g_valid_sync_bi);
+
+const char *g_valid_address_space[] = {
+  "private",
+  "global",
+  "constant",
+  "local"
+};
+DCL_ARRAY_LENGTH(g_valid_address_space);
+
+const char *g_valid_calling_convention[] = {
+  "SPIR_FUNC",
+  "SPIR_KERNEL"
+};
+DCL_ARRAY_LENGTH(g_valid_calling_convention);
+
+const char *g_valid_linkage_type[] = {
+  "private",
+  "internal",
+  "available_externally",
+  "external"
+};
+DCL_ARRAY_LENGTH(g_valid_linkage_type);
+
+
+const char *OPENCL_KERNELS = "opencl.kernels";
+const char *OPENCL_SPIR_VERSION = "opencl.spir.version";
+const char *OPENCL_OCL_VERSION = "opencl.ocl.version";
+const char *OPENCL_KHR_EXTENSIONS = "opencl.used.extensions";
+const char *OPENCL_CORE_FEATURES = "opencl.used.optional.core.features";
+const char *OPENCL_COMPILER_OPTIONS = "opencl.compiler.options";
+const char *g_valid_named_metadata[] = {
+  "opencl.kernels",
+  "opencl.enable.FP_CONTRACT",
+  OPENCL_SPIR_VERSION,
+  OPENCL_OCL_VERSION,
+  OPENCL_KHR_EXTENSIONS,
+  OPENCL_CORE_FEATURES,
+  OPENCL_COMPILER_OPTIONS
+};
+DCL_ARRAY_LENGTH(g_valid_named_metadata);
+
+const char *KERNEL_ARG_ADDR_SPACE = "kernel_arg_addr_space";
+const char *KERNEL_ARG_TY = "kernel_arg_type";
+const char *KERNEL_ARG_BASE_TY = "kernel_arg_base_type";
+const char *g_valid_kernel_arg_info[] = {
+  KERNEL_ARG_ADDR_SPACE,
+  "kernel_arg_access_qual",
+  KERNEL_ARG_TY,
+  KERNEL_ARG_BASE_TY,
+  "kernel_arg_type_qual",
+  "kernel_arg_name",
+};
+DCL_ARRAY_LENGTH(g_valid_kernel_arg_info);
+
+const char *g_valid_version_names[] = {
+  "opencl.ocl.version",
+  "opencl.spir.version"
+};
+DCL_ARRAY_LENGTH(g_valid_version_names);
+
+const char *g_valid_spir_versions[][2] = {
+  {"1", "2"}
+};
+DCL_ARRAY_LENGTH(g_valid_spir_versions)/2;
+
+const char *g_valid_ocl_versions[][2] = {
+  {"1", "2"}
+};
+DCL_ARRAY_LENGTH(g_valid_ocl_versions)/2;
+
+
+///
+/// get error info message functions
+///
+
+std::string getValidOpenCLTypeMsg() {
+  std::string Msg;
+  Msg += "Valid OpenCL C Types in " + STR_SPIR + "\n";
+  Msg += STR_IND1 + "Primitive types: ";
+  for (unsigned i=0; i<g_valid_ocl_primitives_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_ocl_primitives[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Opaque types:";
+  for (unsigned i=0; i<g_valid_ocl_opaque_types_len; i++) {
+    Msg += "\n" + STR_IND2;
+    Msg += g_valid_ocl_opaque_types[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Vector element types: ";
+  for (unsigned i=0; i<g_valid_ocl_vector_element_types_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_ocl_vector_element_types[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Vector type lengthes: ";
+  for (unsigned i=0; i<g_valid_vector_type_lengths_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_vector_type_lengths[i];
+  }
+  Msg += "\n";
+  return Msg;
+}
+
+std::string getValidLLVMTypeMsg() {
+  std::string Msg;
+  Msg += "Valid LLVM Types in " + STR_SPIR + "\n";
+  Msg += STR_IND1 + "Primitive types: ";
+  for (unsigned i=0; i<g_valid_llvm_primitives_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_llvm_primitives[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Image opaque types:";
+  for (unsigned i=0; i<g_valid_llvm_image_types_len; i++) {
+    Msg += "\n" + STR_IND2;
+    Msg += g_valid_llvm_image_types[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Other opaque types:";
+  for (unsigned i=0; i<g_valid_llvm_opaque_types_len; i++) {
+    Msg += "\n" + STR_IND2;
+    Msg += g_valid_llvm_opaque_types[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Vector element types: ";
+  for (unsigned i=0; i<g_valid_llvm_vector_element_types_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_llvm_vector_element_types[i];
+  }
+  Msg += "\n\n";
+  Msg += STR_IND1 + "Vector type lengthes: ";
+  for (unsigned i=0; i<g_valid_vector_type_lengths_len; i++) {
+    Msg += ((i==0) ? "" : ", ");
+    Msg += g_valid_vector_type_lengths[i];
+  }
+  Msg += "\n";
+  Msg += "\n" + STR_IND1 + STR_NOTE +
+    "In addition, arrays and structures of the above types are allowed.\n";
+  return Msg;
+}
+
+std::string getValidKernelReturnTypeMsg() {
+  return "SPIR kernel has to return void";
+}
+
+std::string getValidIntrinsicMsg() {
+  std::string Msg;
+  Msg += "Valid intrinsic in " + STR_SPIR + " are:\n";
+  for (unsigned i=0; i<g_valid_instrinsic_len; i++) {
+    std::stringstream SS;
+    SS << STR_IND1 << g_valid_instrinsic[i] << "\n";
+    Msg += SS.str();
+  }
+  return Msg;
+}
+
+std::string getValidAddressSpaceMsg() {
+  std::string Msg;
+  Msg += "Valid address space in " + STR_SPIR + " are:\n";
+  for (unsigned i=0; i<g_valid_address_space_len; i++) {
+    std::stringstream SS;
+    SS << STR_IND1 << i << " - " << g_valid_address_space[i] << "\n";
+    Msg += SS.str();
+  }
+  Msg += "\n" + STR_IND1 + STR_NOTE +
+    "Casts between address spaces is disallowed in " + STR_SPIR + "\n";
+
+  return Msg;
+}
+
+std::string getValidKernelArgAddressSpaceMsg() {
+  std::string Msg;
+  Msg += "Valid address spaces for kernel arguments in " + STR_SPIR + " are:\n";
+  std::stringstream SS;
+  SS << STR_IND1 << GLOBAL_ADDR_SPACE << " - " << g_valid_address_space[GLOBAL_ADDR_SPACE] << "\n";
+  SS << STR_IND1 << CONSTANT_ADDR_SPACE << " - " << g_valid_address_space[CONSTANT_ADDR_SPACE] << "\n";
+  SS << STR_IND1 << LOCAL_ADDR_SPACE << " - " << g_valid_address_space[LOCAL_ADDR_SPACE] << "\n";
+  Msg += SS.str();
+  return Msg;
+}
+
+extern std::string getValidGlobalAS3VariableMsg() {
+  std::string Msg = "Function-scope variables in the local address space\n";
+  Msg += STR_IND1 + "are represented by module-scope variables with addrspace(3).\n";
+  Msg += STR_IND1 + "The name of the variables has to have the following format:\n";
+  Msg += STR_IND1 + "@<function_name>.<variable_name>\n";
+  return Msg;
+}
+
+extern std::string getValidGlobalVarAddressSpacesMsg() {
+  std::string Msg;
+  Msg += "Valid address spaces for module-scope variables " + STR_SPIR + " are:\n";
+  std::stringstream SS;
+  SS << STR_IND1 << CONSTANT_ADDR_SPACE << " - " << g_valid_address_space[CONSTANT_ADDR_SPACE] << "\n";
+  SS << STR_IND1 << LOCAL_ADDR_SPACE << " - " << g_valid_address_space[LOCAL_ADDR_SPACE] << "\n";
+  Msg += SS.str();
+  return Msg;
+}
+
+std::string getValidCallingConventionMsg() {
+  std::string Msg;
+  Msg += "Valid user defined functions calling convention in ";
+  Msg += STR_SPIR + " are:\n";
+  for (unsigned i=0; i<g_valid_calling_convention_len; i++) {
+    Msg += STR_IND1 + g_valid_calling_convention[i] + "\n";
+  }
+  return Msg;
+}
+
+std::string getValidLinkageTypeMsg() {
+  std::string Msg;
+  Msg += "Valid linkage types in " + STR_SPIR + " are:\n";
+  for (unsigned i = 0; i < g_valid_linkage_type_len; i++) {
+    Msg += STR_IND1 + g_valid_linkage_type[i] + "\n";
+  }
+  return Msg;
+}
+
+std::string getValidIndirectCallMsg() {
+  std::string Msg;
+  Msg += "Indirect Calls are not allowed in " + STR_SPIR +"\n";
+  return Msg;
+}
+
+std::string getValidKernelArgInfoMsg() {
+  std::string Msg;
+  Msg += "Valid kernel arg info in " + STR_SPIR + " are:\n";
+  for (unsigned i=0; i<g_valid_kernel_arg_info_len; i++) {
+    Msg += STR_IND1 + g_valid_kernel_arg_info[i] + "\n";
+  }
+  Msg += "\n" + STR_IND1 + STR_NOTE +
+    "All above are mandatory for each metadata kernel,"
+    "except 'kernel_arg_name' which is optional.\n";
+  return Msg;
+}
+
+std::string getValidVersionMsg() {
+  std::string Msg;
+  Msg += "Module in " + STR_SPIR + " must have these metadata versions:\n";
+  for (unsigned i=0; i<g_valid_version_names_len; i++) {
+    Msg += STR_IND1 + g_valid_version_names[i] + "\n";
+  }
+  Msg += "\n";
+  Msg += STR_IND1 + "Valid versions for '" + OPENCL_OCL_VERSION + "'\n";
+  for (unsigned i=0; i<g_valid_ocl_versions_len; i++) {
+    const char *Major = g_valid_ocl_versions[i][0];
+    const char *Minor = g_valid_ocl_versions[i][1];
+    Msg += STR_IND2 + "{" + Major + "," + Minor + "}" + "\n";
+  }
+  Msg += "\n";
+  Msg += STR_IND1 + "Valid versions for '" + OPENCL_SPIR_VERSION + "'\n";
+  for (unsigned i=0; i<g_valid_spir_versions_len; i++) {
+    const char *Major = g_valid_spir_versions[i][0];
+    const char *Minor = g_valid_spir_versions[i][1];
+    Msg += STR_IND2 + "{" + Major + "," + Minor + "}" + "\n";
+  }
+
+  return Msg;
+}
+
+std::string getValidMemFenceMsg() {
+  std::string Msg;
+  Msg += "Synchronization functions accept 'cl_mem_fence_flags' enumeration "
+         "as an argument. This argument is i32 bitmap value.\n";
+  Msg += STR_IND1 + "Valid values for 'cl_mem_fence_flags' are:\n";
+  Msg += STR_IND2 + "1 - CLK_LOCAL_MEM_FENCE\n";
+  Msg += STR_IND2 + "2 - CLK_GLOBAL_MEM_FENCE\n";
+  Msg += STR_IND2 + "3 - CLK_LOCAL_MEM_FENCE | CLK_GLOBAL_MEM_FENCE\n";
+  return Msg;
+}
+
+std::string getMapOpenCLToLLVMMsg() {
+  std::string Msg;
+  Msg += "OpenCL C mapping to SPIR\n";
+  Msg += STR_IND1 + "Built-in Scalar Data Types:\n";
+  Msg += STR_IND2 + "bool                          -> i1\n";
+  Msg += STR_IND2 + "char, unsigned char, uchar    -> i8\n";
+  Msg += STR_IND2 + "short, unsigned short, ushort -> i16\n";
+  Msg += STR_IND2 + "int, unsigned int, uint       -> i32\n";
+  Msg += STR_IND2 + "long, unsigned long, ulong    -> i64\n";
+  Msg += STR_IND2 + "float                         -> float\n";
+  Msg += STR_IND2 + "double                        -> double\n";
+  Msg += STR_IND2 + "half                          -> half\n";
+  Msg += STR_IND2 + "void                          -> void\n";
+  Msg += "\n";
+  Msg += STR_IND1 + "Built-in Vector Data Types (n = 2, 3, 4, 8, and 16):\n";
+  Msg += STR_IND2 + "charn    -> < n x i8 >\n";
+  Msg += STR_IND2 + "ucharn   -> < n x i8 >\n";
+  Msg += STR_IND2 + "shortn   -> < n x i16 >\n";
+  Msg += STR_IND2 + "ushortn  -> < n x i16 >\n";
+  Msg += STR_IND2 + "intn     -> < n x i32 >\n";
+  Msg += STR_IND2 + "uintn    -> < n x i32 >\n";
+  Msg += STR_IND2 + "longn    -> < n x i64 >\n";
+  Msg += STR_IND2 + "ulongn   -> < n x i64 >\n";
+  Msg += STR_IND2 + "halfn    -> < n x half >\n";
+  Msg += STR_IND2 + "floatn   -> < n x float >\n";
+  Msg += STR_IND2 + "doublen  -> < n x double >\n";
+  Msg += "\n";
+  Msg += STR_IND1 + "Other Built-in Data Types:\n";
+  Msg += STR_IND2 + "image1d_t                  -> opencl.image1d_t\n";
+  Msg += STR_IND2 + "image1d_array_t            -> opencl.image1d_array_t\n";
+  Msg += STR_IND2 + "image1d_buffer_t           -> opencl.image1d_buffer_t\n";
+  Msg += STR_IND2 + "image2d_t                  -> opencl.image2d_t\n";
+  Msg += STR_IND2 + "image2d_depth_t            -> opencl.image2d_depth_t\n";
+  Msg += STR_IND2 + "image2d_msaa_t             -> opencl.image2d_msaa_t\n";
+  Msg += STR_IND2 + "image2d_msaa_depth_t       -> opencl.image2d_msaa_depth_t\n";
+  Msg += STR_IND2 + "image2d_array_t            -> opencl.image2d_array_t\n";
+  Msg += STR_IND2 + "image2d_array_depth_t      -> opencl.image2d_array_depth_t\n";
+  Msg += STR_IND2 + "image2d_array_msaa_t       -> opencl.image2d_array_msaa_t\n";
+  Msg += STR_IND2 + "image2d_array_msaa_depth_t -> opencl.image2d_array_msaa_depth_t\n";
+  Msg += STR_IND2 + "image3d_t                  -> opencl.image3d_t\n";
+  Msg += STR_IND2 + "event_t                    -> opencl.event_t\n";
+  Msg += STR_IND2 + "sampler_t                  -> i32\n";
+  Msg += STR_IND2 + "size_t                     -> i32 or i64\n";
+  Msg += STR_IND2 + "ptrdiff_t                  -> i32 or i64\n";
+  Msg += STR_IND2 + "uintptr_t                  -> i32 or i64\n";
+  Msg += STR_IND2 + "intptr_t                   -> i32 or i64\n";
+  return Msg;
+}
+
+std::string getValidNamedMetadataMsg() {
+  std::string Msg;
+  Msg += "Valid named metadata in " + STR_SPIR + " are:\n";
+  for (unsigned i=0; i<g_valid_named_metadata_len; i++) {
+    Msg += STR_IND1 + g_valid_named_metadata[i] + "\n";
+  }
+  Msg += "\n" + STR_IND1 + STR_NOTE +
+    "Except 'opencl.enable.FP_CONTRACT' all the others are mandatory.\n";
+  return Msg;
+}
+
+std::string getValidCoreFeaturesMsg() {
+  std::string Msg;
+  Msg += "Valid optional core features:\n";
+  for (unsigned i=0; i<g_valid_core_feature_len; i++) {
+    std::string Str = g_valid_core_feature[i];
+    Str = Str.substr(std::string(g_valid_core_feature_prefix).size());
+    Msg += STR_IND2 + g_valid_core_feature[i];
+    Msg += " - must be declared to indicate that " + Str + " types are used";
+    Msg += "\n";
+  }
+  return Msg;
+}
+
+std::string getValidKHRExtensionsMsg() {
+  std::string Msg;
+  Msg += "Valid KHR extensions:\n";
+  for (unsigned i=0; i<g_valid_khr_ext_len; i++) {
+    std::string Str = g_valid_khr_ext[i];
+    Str = Str.substr(std::string(g_valid_khr_ext_prefix).size());
+    Msg += STR_IND2 + g_valid_khr_ext[i];
+    //Msg += " - must be declared to indicate that " + Str + " extension is used";
+    Msg += "\n";
+  }
+  return Msg;
+}
+
+std::string getValidCompilerOptionsMsg() {
+  std::string Msg;
+  Msg += "Valid compiler options:\n";
+  for (unsigned i=0; i<g_valid_compiler_options_len; i++) {
+    Msg += STR_IND2 + g_valid_compiler_options[i];
+    Msg += "\n";
+  }
+  return Msg;
+}
+
+} // End SPIR namespace
+
diff --git a/llvm/lib/SPIRVerifier/SpirValidation.cpp b/llvm/lib/SPIRVerifier/SpirValidation.cpp
new file mode 100644
index 000000000000..01016b5ec448
--- /dev/null
+++ b/llvm/lib/SPIRVerifier/SpirValidation.cpp
@@ -0,0 +1,125 @@
+//===------------------------ SpirValidation.cpp -------------------------===//
+//
+//                              SPIR Tools
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===---------------------------------------------------------------------===//
+
+#include "llvm/SPIRVerifier/SpirValidation.h"
+#include "llvm/SPIRVerifier/SpirErrors.h"
+#include "llvm/SPIRVerifier/SpirIterators.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+namespace SPIR {
+
+//
+// SpirValidation class public methods.
+//
+
+char SpirValidation::ID = 0;
+
+SpirValidation::SpirValidation() : ModulePass(ID) {
+}
+
+SpirValidation::~SpirValidation() {
+}
+
+StringRef SpirValidation::getPassName() const {
+  return "Spir validation";
+}
+
+bool SpirValidation::runOnModule(Module& M) {
+  // Holder for initialized data in the module
+  DataHolder Data;
+
+  // Initialize instruction verifiers.
+  InstructionExecutorList iel;
+  // Bitcast instruction verifier.
+  VerifyBitcast vb(&ErrHolder);
+  iel.push_back(&vb);
+  // Call instruction verifier.
+  VerifyCall vc(&ErrHolder);
+  iel.push_back(&vc);
+  // Instruction type verifier.
+  VerifyInstructionType vit(&ErrHolder, &Data);
+  iel.push_back(&vit);
+
+  // Initialize function verifiers.
+  FunctionExecutorList fel;
+  // Function prototype verifier.
+  VerifyFunctionPrototype vfp(&ErrHolder, &Data);
+  fel.push_back(&vfp);
+  // Kernel prototype verifier
+  VerifyKernelPrototype vkp(&ErrHolder, &Data);
+  fel.push_back(&vkp);
+
+  // Initialize global variable verifiers
+  GlobalVariableExecutorList gel;
+  // Global variable verifier
+  VerifyGlobalVariable vgv(&ErrHolder, &Data);
+  gel.push_back(&vgv);
+
+  // Initialize module verifiers.
+  ModuleExecutorList mel;
+  // Module metadata kernels verifier.
+  VerifyMetadataKernels vkmd(&ErrHolder, &Data);
+  mel.push_back(&vkmd);
+  // Module OCL version verifier.
+  VerifyMetadataVersions voclv(
+    &ErrHolder, VerifyMetadataVersions::VERSION_OCL);
+  mel.push_back(&voclv);
+  // Module SPIR version verifier.
+  VerifyMetadataVersions vspirv(
+    &ErrHolder, VerifyMetadataVersions::VERSION_SPIR);
+  mel.push_back(&vspirv);
+  // Module metadata optional core features verifier.
+  VerifyMetadataCoreFeatures vmdcf(&ErrHolder, &Data);
+  mel.push_back(&vmdcf);
+  // Module metadata KHR extensions verifier.
+  VerifyMetadataKHRExtensions vmdext(&ErrHolder, &Data);
+  mel.push_back(&vmdext);
+  // Module metadata compiler options verifier.
+  VerifyMetadataCompilerOptions vmdco(&ErrHolder, &Data);
+  mel.push_back(&vmdco);
+
+  // Initialize basic block iterator.
+  BasicBlockIterator BBI(iel);
+
+  // Initialize function iterator.
+  FunctionIterator FI(fel, &BBI);
+
+  // Initialize global variable iterator.
+  GlobalVariableIterator GI(gel);
+
+  // Initialize module iterator.
+  ModuleIterator MI(mel, &FI, &GI);
+
+  // Run validation.
+  MI.execute(M);
+
+  // always print any errors that occured
+  const SPIR::ErrorPrinter *EP = getErrorPrinter();
+  if (EP->hasErrors()) {
+    errs() << "According to this SPIR Verifier, this is an invalid SPIR module.\n";
+    errs() << "The module contains the following errors:\n\n";
+    EP->print(errs(), false);
+  }
+
+  return false;
+}
+
+
+} // End SPIR namespace
+
+namespace llvm {
+  ModulePass *createSpirValidationPass() {
+    return new SPIR::SpirValidation();
+  }
+}
diff --git a/llvm/lib/Support/Signals.cpp b/llvm/lib/Support/Signals.cpp
index dd4dded4cd1d..23ea12584a3e 100644
--- a/llvm/lib/Support/Signals.cpp
+++ b/llvm/lib/Support/Signals.cpp
@@ -83,12 +83,15 @@ struct CallbackAndCookie {
   std::atomic<Status> Flag;
 };
 static constexpr size_t MaxSignalHandlerCallbacks = 8;
-static CallbackAndCookie CallBacksToRun[MaxSignalHandlerCallbacks];
+static CallbackAndCookie& getCallBacksToRun(size_t idx) {
+  static CallbackAndCookie CallBacksToRun[MaxSignalHandlerCallbacks];
+  return CallBacksToRun[idx];
+}
 
 // Signal-safe.
 void sys::RunSignalHandlers() {
   for (size_t I = 0; I < MaxSignalHandlerCallbacks; ++I) {
-    auto &RunMe = CallBacksToRun[I];
+    auto &RunMe = getCallBacksToRun(I);
     auto Expected = CallbackAndCookie::Status::Initialized;
     auto Desired = CallbackAndCookie::Status::Executing;
     if (!RunMe.Flag.compare_exchange_strong(Expected, Desired))
@@ -104,7 +107,7 @@ void sys::RunSignalHandlers() {
 static void insertSignalHandler(sys::SignalHandlerCallback FnPtr,
                                 void *Cookie) {
   for (size_t I = 0; I < MaxSignalHandlerCallbacks; ++I) {
-    auto &SetMe = CallBacksToRun[I];
+    auto &SetMe = getCallBacksToRun(I);
     auto Expected = CallbackAndCookie::Status::Empty;
     auto Desired = CallbackAndCookie::Status::Initializing;
     if (!SetMe.Flag.compare_exchange_strong(Expected, Desired))
diff --git a/llvm/lib/Support/Triple.cpp b/llvm/lib/Support/Triple.cpp
index df43ec15eb58..d0bd9930e280 100644
--- a/llvm/lib/Support/Triple.cpp
+++ b/llvm/lib/Support/Triple.cpp
@@ -67,6 +67,7 @@ StringRef Triple::getArchTypeName(ArchType Kind) {
   case sparcv9:        return "sparcv9";
   case spir64:         return "spir64";
   case spir:           return "spir";
+  case air64:          return "air64";
   case systemz:        return "s390x";
   case tce:            return "tce";
   case tcele:          return "tcele";
@@ -147,6 +148,7 @@ StringRef Triple::getArchTypePrefix(ArchType Kind) {
 
   case spir:
   case spir64:      return "spir";
+  case air64:       return "air64";
   case kalimba:     return "kalimba";
   case lanai:       return "lanai";
   case shave:       return "shave";
@@ -251,6 +253,8 @@ StringRef Triple::getEnvironmentTypeName(EnvironmentType Kind) {
   case MuslEABI: return "musleabi";
   case MuslEABIHF: return "musleabihf";
   case MuslX32: return "muslx32";
+  case Vulkan: return "vulkan";
+  case FloorHostCompute: return "floor_host_compute";
   case Simulator: return "simulator";
   }
 
@@ -323,6 +327,7 @@ Triple::ArchType Triple::getArchTypeForLLVMName(StringRef Name) {
     .Case("hsail64", hsail64)
     .Case("spir", spir)
     .Case("spir64", spir64)
+    .Case("air64", air64)
     .Case("kalimba", kalimba)
     .Case("lanai", lanai)
     .Case("shave", shave)
@@ -456,6 +461,7 @@ static Triple::ArchType parseArch(StringRef ArchName) {
     .Case("hsail64", Triple::hsail64)
     .Case("spir", Triple::spir)
     .Case("spir64", Triple::spir64)
+    .Case("air64", Triple::air64)
     .StartsWith("kalimba", Triple::kalimba)
     .Case("lanai", Triple::lanai)
     .Case("renderscript32", Triple::renderscript32)
@@ -558,6 +564,8 @@ static Triple::EnvironmentType parseEnvironment(StringRef EnvironmentName) {
       .StartsWith("musleabi", Triple::MuslEABI)
       .StartsWith("muslx32", Triple::MuslX32)
       .StartsWith("musl", Triple::Musl)
+      .StartsWith("vulkan", Triple::Vulkan)
+      .StartsWith("floor_host_compute", Triple::FloorHostCompute)
       .StartsWith("msvc", Triple::MSVC)
       .StartsWith("itanium", Triple::Itanium)
       .StartsWith("cygnus", Triple::Cygnus)
@@ -738,6 +746,7 @@ static Triple::ObjectFormatType getDefaultFormat(const Triple &T) {
   case Triple::sparcv9:
   case Triple::spir64:
   case Triple::spir:
+  case Triple::air64:
   case Triple::tce:
   case Triple::tcele:
   case Triple::thumbeb:
@@ -1354,6 +1363,7 @@ static unsigned getArchPointerBitWidth(llvm::Triple::ArchType Arch) {
   case llvm::Triple::riscv64:
   case llvm::Triple::sparcv9:
   case llvm::Triple::spir64:
+  case llvm::Triple::air64:
   case llvm::Triple::systemz:
   case llvm::Triple::ve:
   case llvm::Triple::wasm64:
@@ -1386,6 +1396,7 @@ Triple Triple::get32BitArchVariant() const {
   case Triple::msp430:
   case Triple::systemz:
   case Triple::ve:
+  case Triple::air64:
     T.setArch(UnknownArch);
     break;
 
@@ -1485,6 +1496,7 @@ Triple Triple::get64BitArchVariant() const {
   case Triple::riscv64:
   case Triple::sparcv9:
   case Triple::spir64:
+  case Triple::air64:
   case Triple::systemz:
   case Triple::ve:
   case Triple::wasm64:
@@ -1547,6 +1559,7 @@ Triple Triple::getBigEndianArchVariant() const {
   case Triple::shave:
   case Triple::spir64:
   case Triple::spir:
+  case Triple::air64:
   case Triple::wasm32:
   case Triple::wasm64:
   case Triple::x86:
@@ -1650,6 +1663,7 @@ bool Triple::isLittleEndian() const {
   case Triple::sparcel:
   case Triple::spir64:
   case Triple::spir:
+  case Triple::air64:
   case Triple::tcele:
   case Triple::thumb:
   case Triple::ve:
diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
index c3624058b42c..be018bc7eb32 100644
--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -5194,6 +5194,10 @@ CCAssignFn *AArch64TargetLowering::CCAssignFnForCall(CallingConv::ID CC,
   case CallingConv::Swift:
   case CallingConv::SwiftTail:
   case CallingConv::Tail:
+  case CallingConv::FLOOR_FUNC:
+  case CallingConv::FLOOR_KERNEL:
+  case CallingConv::FLOOR_VERTEX:
+  case CallingConv::FLOOR_FRAGMENT:
     if (Subtarget->isTargetWindows() && IsVarArg)
       return CC_AArch64_Win64_VarArg;
     if (!Subtarget->isTargetDarwin())
@@ -6306,7 +6310,7 @@ AArch64TargetLowering::LowerCall(CallLoweringInfo &CLI,
     }
   } else if (auto *S = dyn_cast<ExternalSymbolSDNode>(Callee)) {
     if (getTargetMachine().getCodeModel() == CodeModel::Large &&
-        Subtarget->isTargetMachO()) {
+        (Subtarget->isTargetMachO() || Subtarget->isTargetHostCompute())) {
       const char *Sym = S->getSymbol();
       Callee = DAG.getTargetExternalSymbol(Sym, PtrVT, AArch64II::MO_GOT);
       Callee = DAG.getNode(AArch64ISD::LOADgot, DL, PtrVT, Callee);
diff --git a/llvm/lib/Target/AArch64/AArch64Subtarget.cpp b/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
index 3072b5a41219..3e5e68ab29ab 100644
--- a/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
+++ b/llvm/lib/Target/AArch64/AArch64Subtarget.cpp
@@ -272,9 +272,10 @@ const RegisterBankInfo *AArch64Subtarget::getRegBankInfo() const {
 unsigned
 AArch64Subtarget::ClassifyGlobalReference(const GlobalValue *GV,
                                           const TargetMachine &TM) const {
-  // MachO large model always goes via a GOT, simply to get a single 8-byte
+  // MachO/Host-Compute large model always goes via a GOT, simply to get a single 8-byte
   // absolute relocation on all global addresses.
-  if (TM.getCodeModel() == CodeModel::Large && isTargetMachO())
+  if (TM.getCodeModel() == CodeModel::Large &&
+      (isTargetMachO() || isTargetHostCompute()))
     return AArch64II::MO_GOT;
 
   if (!TM.shouldAssumeDSOLocal(*GV->getParent(), GV)) {
@@ -304,9 +305,10 @@ AArch64Subtarget::ClassifyGlobalReference(const GlobalValue *GV,
 
 unsigned AArch64Subtarget::classifyGlobalFunctionReference(
     const GlobalValue *GV, const TargetMachine &TM) const {
-  // MachO large model always goes via a GOT, because we don't have the
+  // MachO/Host-Compute large model always goes via a GOT, because we don't have the
   // relocations available to do anything else..
-  if (TM.getCodeModel() == CodeModel::Large && isTargetMachO() &&
+  if (TM.getCodeModel() == CodeModel::Large &&
+      (isTargetMachO() || isTargetHostCompute()) &&
       !GV->hasInternalLinkage())
     return AArch64II::MO_GOT;
 
diff --git a/llvm/lib/Target/AArch64/AArch64Subtarget.h b/llvm/lib/Target/AArch64/AArch64Subtarget.h
index a34e14896192..98ee57a3e05d 100644
--- a/llvm/lib/Target/AArch64/AArch64Subtarget.h
+++ b/llvm/lib/Target/AArch64/AArch64Subtarget.h
@@ -525,6 +525,7 @@ public:
   bool isTargetWindows() const { return TargetTriple.isOSWindows(); }
   bool isTargetAndroid() const { return TargetTriple.isAndroid(); }
   bool isTargetFuchsia() const { return TargetTriple.isOSFuchsia(); }
+  bool isTargetHostCompute() const { return TargetTriple.getEnvironment() == Triple::FloorHostCompute; }
 
   bool isTargetCOFF() const { return TargetTriple.isOSBinFormatCOFF(); }
   bool isTargetELF() const { return TargetTriple.isOSBinFormatELF(); }
diff --git a/llvm/lib/Target/AMDGPU/CMakeLists.txt b/llvm/lib/Target/AMDGPU/CMakeLists.txt
index 6dd10affdfc9..2e10d19481ab 100644
--- a/llvm/lib/Target/AMDGPU/CMakeLists.txt
+++ b/llvm/lib/Target/AMDGPU/CMakeLists.txt
@@ -161,6 +161,7 @@ add_llvm_target(AMDGPUCodeGen
   CodeGen
   Core
   IPO
+  LibFloor
   MC
   Passes
   AMDGPUDesc
diff --git a/llvm/lib/Target/NVPTX/CMakeLists.txt b/llvm/lib/Target/NVPTX/CMakeLists.txt
index 4db593b1c37c..44a96e1ad46b 100644
--- a/llvm/lib/Target/NVPTX/CMakeLists.txt
+++ b/llvm/lib/Target/NVPTX/CMakeLists.txt
@@ -50,6 +50,7 @@ add_llvm_target(NVPTXCodeGen
   MC
   NVPTXDesc
   NVPTXInfo
+  LibFloor
   Scalar
   SelectionDAG
   Support
diff --git a/llvm/lib/Target/NVPTX/NVPTX.td b/llvm/lib/Target/NVPTX/NVPTX.td
index ecbf1e0aa4b4..ec0422947644 100644
--- a/llvm/lib/Target/NVPTX/NVPTX.td
+++ b/llvm/lib/Target/NVPTX/NVPTX.td
@@ -25,10 +25,12 @@ include "NVPTXInstrInfo.td"
 //===----------------------------------------------------------------------===//
 
 // SM Versions
+// Fermi
 def SM20 : SubtargetFeature<"sm_20", "SmVersion", "20",
                             "Target SM 2.0">;
 def SM21 : SubtargetFeature<"sm_21", "SmVersion", "21",
                             "Target SM 2.1">;
+// Kepler
 def SM30 : SubtargetFeature<"sm_30", "SmVersion", "30",
                             "Target SM 3.0">;
 def SM32 : SubtargetFeature<"sm_32", "SmVersion", "32",
@@ -37,28 +39,41 @@ def SM35 : SubtargetFeature<"sm_35", "SmVersion", "35",
                             "Target SM 3.5">;
 def SM37 : SubtargetFeature<"sm_37", "SmVersion", "37",
                             "Target SM 3.7">;
+// Maxwell
 def SM50 : SubtargetFeature<"sm_50", "SmVersion", "50",
                             "Target SM 5.0">;
 def SM52 : SubtargetFeature<"sm_52", "SmVersion", "52",
                             "Target SM 5.2">;
 def SM53 : SubtargetFeature<"sm_53", "SmVersion", "53",
                             "Target SM 5.3">;
+// Pascal
 def SM60 : SubtargetFeature<"sm_60", "SmVersion", "60",
                              "Target SM 6.0">;
 def SM61 : SubtargetFeature<"sm_61", "SmVersion", "61",
                              "Target SM 6.1">;
 def SM62 : SubtargetFeature<"sm_62", "SmVersion", "62",
                              "Target SM 6.2">;
+// Volta
 def SM70 : SubtargetFeature<"sm_70", "SmVersion", "70",
                              "Target SM 7.0">;
 def SM72 : SubtargetFeature<"sm_72", "SmVersion", "72",
                              "Target SM 7.2">;
+def SM73 : SubtargetFeature<"sm_73", "SmVersion", "73",
+                             "Target SM 7.3">;
+// Turing
 def SM75 : SubtargetFeature<"sm_75", "SmVersion", "75",
                              "Target SM 7.5">;
+// Ampere
 def SM80 : SubtargetFeature<"sm_80", "SmVersion", "80",
                              "Target SM 8.0">;
+def SM82 : SubtargetFeature<"sm_82", "SmVersion", "82",
+                             "Target SM 8.2">;
 def SM86 : SubtargetFeature<"sm_86", "SmVersion", "86",
                              "Target SM 8.6">;
+def SM87 : SubtargetFeature<"sm_87", "SmVersion", "87",
+                             "Target SM 8.7">;
+def SM88 : SubtargetFeature<"sm_88", "SmVersion", "88",
+                             "Target SM 8.8">;
 
 // PTX Versions
 def PTX32 : SubtargetFeature<"ptx32", "PTXVersion", "32",
@@ -77,6 +92,8 @@ def PTX60 : SubtargetFeature<"ptx60", "PTXVersion", "60",
                              "Use PTX version 6.0">;
 def PTX61 : SubtargetFeature<"ptx61", "PTXVersion", "61",
                              "Use PTX version 6.1">;
+def PTX62 : SubtargetFeature<"ptx62", "PTXVersion", "62",
+                             "Use PTX version 6.2">;
 def PTX63 : SubtargetFeature<"ptx63", "PTXVersion", "63",
                              "Use PTX version 6.3">;
 def PTX64 : SubtargetFeature<"ptx64", "PTXVersion", "64",
@@ -93,6 +110,8 @@ def PTX73 : SubtargetFeature<"ptx73", "PTXVersion", "73",
                              "Use PTX version 7.3">;
 def PTX74 : SubtargetFeature<"ptx74", "PTXVersion", "74",
                              "Use PTX version 7.4">;
+def PTX75 : SubtargetFeature<"ptx75", "PTXVersion", "75",
+                             "Use PTX version 7.5">;
 
 //===----------------------------------------------------------------------===//
 // NVPTX supported processors.
@@ -115,9 +134,13 @@ def : Proc<"sm_61", [SM61, PTX50]>;
 def : Proc<"sm_62", [SM62, PTX50]>;
 def : Proc<"sm_70", [SM70, PTX60]>;
 def : Proc<"sm_72", [SM72, PTX61]>;
+def : Proc<"sm_73", [SM73, PTX61]>;
 def : Proc<"sm_75", [SM75, PTX63]>;
 def : Proc<"sm_80", [SM80, PTX70]>;
+def : Proc<"sm_82", [SM82, PTX71]>;
 def : Proc<"sm_86", [SM86, PTX71]>;
+def : Proc<"sm_87", [SM87, PTX75]>;
+def : Proc<"sm_88", [SM88, PTX75]>;
 
 def NVPTXInstrInfo : InstrInfo {
 }
diff --git a/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp b/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp
index ea57f01b0ea5..a88c48b15f33 100644
--- a/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp
+++ b/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp
@@ -69,7 +69,7 @@ static std::atomic<unsigned> GlobalUniqueCallSite;
 
 static cl::opt<bool> sched4reg(
     "nvptx-sched4reg",
-    cl::desc("NVPTX Specific: schedule for register pressue"), cl::init(false));
+    cl::desc("NVPTX Specific: schedule for register pressue"), cl::init(true));
 
 static cl::opt<unsigned>
 FMAContractLevelOpt("nvptx-fma-level", cl::ZeroOrMore, cl::Hidden,
diff --git a/llvm/lib/Target/NVPTX/NVPTXInstrInfo.td b/llvm/lib/Target/NVPTX/NVPTXInstrInfo.td
index 4834985b1019..c901d30a26b8 100644
--- a/llvm/lib/Target/NVPTX/NVPTXInstrInfo.td
+++ b/llvm/lib/Target/NVPTX/NVPTXInstrInfo.td
@@ -142,6 +142,7 @@ def True : Predicate<"true">;
 def hasPTX31 : Predicate<"Subtarget->getPTXVersion() >= 31">;
 def hasPTX60 : Predicate<"Subtarget->getPTXVersion() >= 60">;
 def hasPTX61 : Predicate<"Subtarget->getPTXVersion() >= 61">;
+def hasPTX62 : Predicate<"Subtarget->getPTXVersion() >= 62">;
 def hasPTX63 : Predicate<"Subtarget->getPTXVersion() >= 63">;
 def hasPTX64 : Predicate<"Subtarget->getPTXVersion() >= 64">;
 def hasPTX65 : Predicate<"Subtarget->getPTXVersion() >= 65">;
diff --git a/llvm/lib/Target/NVPTX/NVPTXSubtarget.cpp b/llvm/lib/Target/NVPTX/NVPTXSubtarget.cpp
index 05c20369abf4..a17ae6eba00d 100644
--- a/llvm/lib/Target/NVPTX/NVPTXSubtarget.cpp
+++ b/llvm/lib/Target/NVPTX/NVPTXSubtarget.cpp
@@ -33,13 +33,13 @@ void NVPTXSubtarget::anchor() {}
 NVPTXSubtarget &NVPTXSubtarget::initializeSubtargetDependencies(StringRef CPU,
                                                                 StringRef FS) {
     // Provide the default CPU if we don't have one.
-    TargetName = std::string(CPU.empty() ? "sm_20" : CPU);
+    TargetName = std::string(CPU.empty() ? "sm_30" : CPU);
 
     ParseSubtargetFeatures(TargetName, /*TuneCPU*/ TargetName, FS);
 
-    // Set default to PTX 3.2 (CUDA 5.5)
+    // Set default to PTX 6.0 (CUDA 9.0)
     if (PTXVersion == 0) {
-      PTXVersion = 32;
+      PTXVersion = 60;
   }
 
   return *this;
diff --git a/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp b/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
index bca17f969370..315e53755611 100644
--- a/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
+++ b/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
@@ -41,10 +41,11 @@ using namespace llvm;
 
 // LSV is still relatively new; this switch lets us turn it off in case we
 // encounter (or suspect) a bug.
+// TODO/NOTE: don't want this when under register pressure
 static cl::opt<bool>
     DisableLoadStoreVectorizer("disable-nvptx-load-store-vectorizer",
                                cl::desc("Disable load/store vectorizer"),
-                               cl::init(false), cl::Hidden);
+                               cl::init(true), cl::Hidden);
 
 // TODO: Remove this flag when we are confident with no regressions.
 static cl::opt<bool> DisableRequireStructuredCFG(
diff --git a/llvm/lib/Target/NVPTX/NVPTXUtilities.cpp b/llvm/lib/Target/NVPTX/NVPTXUtilities.cpp
index 74d129d330f3..94442206ae51 100644
--- a/llvm/lib/Target/NVPTX/NVPTXUtilities.cpp
+++ b/llvm/lib/Target/NVPTX/NVPTXUtilities.cpp
@@ -276,7 +276,8 @@ bool isKernelFunction(const Function &F) {
   bool retval = findOneNVVMAnnotation(&F, "kernel", x);
   if (!retval) {
     // There is no NVVM metadata, check the calling convention
-    return F.getCallingConv() == CallingConv::PTX_Kernel;
+    return (F.getCallingConv() == CallingConv::PTX_Kernel ||
+            F.getCallingConv() == CallingConv::FLOOR_KERNEL);
   }
   return (x == 1);
 }
diff --git a/llvm/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp b/llvm/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
index b5351358e4e5..7f8fb3304ae8 100644
--- a/llvm/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
+++ b/llvm/lib/Target/X86/MCTargetDesc/X86MCAsmInfo.cpp
@@ -100,6 +100,12 @@ X86ELFMCAsmInfo::X86ELFMCAsmInfo(const Triple &T) {
 
   // Exceptions handling
   ExceptionsType = ExceptionHandling::DwarfCFI;
+
+  // host-compute:
+  // * don't emit .comment ident section
+  if (T.getEnvironment() == Triple::EnvironmentType::FloorHostCompute) {
+    HasIdentDirective = false;
+  }
 }
 
 const MCExpr *
diff --git a/llvm/lib/Transforms/CMakeLists.txt b/llvm/lib/Transforms/CMakeLists.txt
index dda5f6de11e3..036627f0329f 100644
--- a/llvm/lib/Transforms/CMakeLists.txt
+++ b/llvm/lib/Transforms/CMakeLists.txt
@@ -9,3 +9,4 @@ add_subdirectory(Hello)
 add_subdirectory(ObjCARC)
 add_subdirectory(Coroutines)
 add_subdirectory(CFGuard)
+add_subdirectory(LibFloor)
diff --git a/llvm/lib/Transforms/IPO/CMakeLists.txt b/llvm/lib/Transforms/IPO/CMakeLists.txt
index d9ada3b3e06a..2aba52c30953 100644
--- a/llvm/lib/Transforms/IPO/CMakeLists.txt
+++ b/llvm/lib/Transforms/IPO/CMakeLists.txt
@@ -23,6 +23,7 @@ add_llvm_component_library(LLVMipo
   IPO.cpp
   IROutliner.cpp
   InferFunctionAttrs.cpp
+  InlineEverything.cpp
   InlineSimple.cpp
   Inliner.cpp
   Internalize.cpp
@@ -64,9 +65,11 @@ add_llvm_component_library(LLVMipo
   InstCombine
   IRReader
   Linker
+  LibFloor
   Object
   ProfileData
   Scalar
+  SPIRVerifier
   Support
   TransformUtils
   Vectorize
diff --git a/llvm/lib/Transforms/IPO/FunctionAttrs.cpp b/llvm/lib/Transforms/IPO/FunctionAttrs.cpp
index 8918d01f3ec2..f89b3a042662 100644
--- a/llvm/lib/Transforms/IPO/FunctionAttrs.cpp
+++ b/llvm/lib/Transforms/IPO/FunctionAttrs.cpp
@@ -1573,6 +1573,15 @@ static bool addNoRecurseAttrs(const SCCNodeSet &SCCNodes) {
   if (!F || !F->hasExactDefinition() || F->doesNotRecurse())
     return false;
 
+  // set norecurse for all compute kernels and vertex/fragment shaders
+  if (F->getCallingConv() == CallingConv::FLOOR_KERNEL ||
+      F->getCallingConv() == CallingConv::FLOOR_VERTEX ||
+      F->getCallingConv() == CallingConv::FLOOR_FRAGMENT) {
+    F->setDoesNotRecurse();
+    ++NumNoRecurse;
+    return true;
+  }
+
   // If all of the calls in F are identifiable and are to norecurse functions, F
   // is norecurse. This check also detects self-recursion as F is not currently
   // marked norecurse, so any called from F to F will not be marked norecurse.
diff --git a/llvm/lib/Transforms/IPO/GlobalOpt.cpp b/llvm/lib/Transforms/IPO/GlobalOpt.cpp
index 1d40a3b52aae..4a752afec865 100644
--- a/llvm/lib/Transforms/IPO/GlobalOpt.cpp
+++ b/llvm/lib/Transforms/IPO/GlobalOpt.cpp
@@ -18,6 +18,7 @@
 #include "llvm/ADT/SmallPtrSet.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/Triple.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/ADT/iterator_range.h"
 #include "llvm/Analysis/BlockFrequencyInfo.h"
@@ -1711,6 +1712,16 @@ static void ChangeCalleesToFastCall(Function *F) {
   }
 }
 
+/// ChangeCalleesToFloorFunc - Walk all of the direct calls of the specified
+/// function, changing them to floor_func calling convention.
+static void ChangeCalleesToFloorFunc(Function *F) {
+  for (User *U : F->users()) {
+    if (isa<BlockAddress>(U))
+      continue;
+    cast<CallBase>(U)->setCallingConv(CallingConv::FLOOR_FUNC);
+  }
+}
+
 static AttributeList StripAttr(LLVMContext &C, AttributeList Attrs,
                                Attribute::AttrKind A) {
   unsigned AttrIndex;
@@ -2045,8 +2056,18 @@ OptimizeFunctions(Module &M,
       // If this function has a calling convention worth changing, is not a
       // varargs function, and is only called directly, promote it to use the
       // Fast calling convention.
-      F.setCallingConv(CallingConv::Fast);
-      ChangeCalleesToFastCall(&F);
+      // NOTE: with OpenCL/Metal/Vulkan/CUDA: change it to floor_func instead (fastcc is invalid)
+      const llvm::Triple triple(M.getTargetTriple());
+      if (triple.getArch() == llvm::Triple::ArchType::spir ||
+          triple.getArch() == llvm::Triple::ArchType::spir64 ||
+          triple.getArch() == llvm::Triple::ArchType::air64 ||
+          triple.getOS() == llvm::Triple::OSType::CUDA) {
+        F.setCallingConv(CallingConv::FLOOR_FUNC);
+        ChangeCalleesToFloorFunc(&F);
+      } else {
+        F.setCallingConv(CallingConv::Fast);
+        ChangeCalleesToFastCall(&F);
+      }
       ++NumFastCallFns;
       Changed = true;
     }
diff --git a/llvm/lib/Transforms/IPO/IPO.cpp b/llvm/lib/Transforms/IPO/IPO.cpp
index de1c1d379502..3e334483f1a3 100644
--- a/llvm/lib/Transforms/IPO/IPO.cpp
+++ b/llvm/lib/Transforms/IPO/IPO.cpp
@@ -39,6 +39,7 @@ void llvm::initializeIPO(PassRegistry &Registry) {
   initializeHotColdSplittingLegacyPassPass(Registry);
   initializeIROutlinerLegacyPassPass(Registry);
   initializeAlwaysInlinerLegacyPassPass(Registry);
+  initializeEverythingInlinerPass(Registry);
   initializeSimpleInlinerPass(Registry);
   initializeInferFunctionAttrsLegacyPassPass(Registry);
   initializeInternalizeLegacyPassPass(Registry);
@@ -98,6 +99,10 @@ void LLVMAddAlwaysInlinerPass(LLVMPassManagerRef PM) {
   unwrap(PM)->add(llvm::createAlwaysInlinerLegacyPass());
 }
 
+void LLVMAddEverythingInlinerPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(llvm::createEverythingInlinerPass());
+}
+
 void LLVMAddGlobalDCEPass(LLVMPassManagerRef PM) {
   unwrap(PM)->add(createGlobalDCEPass());
 }
diff --git a/llvm/lib/Transforms/IPO/InlineEverything.cpp b/llvm/lib/Transforms/IPO/InlineEverything.cpp
new file mode 100644
index 000000000000..c2bad9a4558a
--- /dev/null
+++ b/llvm/lib/Transforms/IPO/InlineEverything.cpp
@@ -0,0 +1,92 @@
+//===- InlineEverything.cpp - Code to inline all functions ----------------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements a custom inliner that inlines everything, unless it was
+// marked "noinline".
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/IPO.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/Analysis/AssumptionCache.h"
+#include "llvm/Analysis/CallGraph.h"
+#include "llvm/Analysis/InlineCost.h"
+#include "llvm/Analysis/TargetLibraryInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Transforms/IPO/Inliner.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "inline"
+
+namespace {
+
+/// \brief Inliner pass which inlines everything unless it was marked "noinline".
+class EverythingInliner : public LegacyInlinerBase {
+
+public:
+  EverythingInliner() : LegacyInlinerBase(ID, /*InsertLifetime*/ true) {
+    initializeEverythingInlinerPass(*PassRegistry::getPassRegistry());
+  }
+
+  EverythingInliner(bool InsertLifetime)
+      : LegacyInlinerBase(ID, InsertLifetime) {
+    initializeEverythingInlinerPass(*PassRegistry::getPassRegistry());
+  }
+
+  /// Main run interface method.  We override here to avoid calling skipSCC().
+  bool runOnSCC(CallGraphSCC &SCC) override { return inlineCalls(SCC); }
+
+  static char ID; // Pass identification, replacement for typeid
+
+  InlineCost getInlineCost(CallBase &CB) override;
+
+  using llvm::Pass::doFinalization;
+  bool doFinalization(CallGraph &CG) override {
+    return removeDeadFunctions(CG, /*AlwaysInlineOnly=*/ false);
+  }
+};
+
+}
+
+char EverythingInliner::ID = 0;
+INITIALIZE_PASS_BEGIN(EverythingInliner, "everything-inline",
+					  "everything inliner", false, false)
+INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
+INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(ProfileSummaryInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(TargetLibraryInfoWrapperPass)
+INITIALIZE_PASS_END(EverythingInliner, "everything-inline",
+                    "everything inliner", false, false)
+
+Pass *llvm::createEverythingInlinerPass() { return new EverythingInliner(); }
+
+Pass *llvm::createEverythingInlinerPass(bool InsertLifetime) {
+  return new EverythingInliner(InsertLifetime);
+}
+
+InlineCost EverythingInliner::getInlineCost(CallBase &CB) {
+  Function *Callee = CB.getCalledFunction();
+
+  if (Callee && !Callee->isDeclaration() &&
+      (CB.hasFnAttr(Attribute::NoInline) ||
+	   Callee->hasFnAttribute(Attribute::NoInline) ||
+	   !isInlineViable(*Callee).isSuccess())) {
+    return InlineCost::getNever("everything inliner");
+  }
+
+  return InlineCost::getAlways("everything inliner");
+}
diff --git a/llvm/lib/Transforms/IPO/Internalize.cpp b/llvm/lib/Transforms/IPO/Internalize.cpp
index 692e445cb7cb..dd4cebc7e460 100644
--- a/llvm/lib/Transforms/IPO/Internalize.cpp
+++ b/llvm/lib/Transforms/IPO/Internalize.cpp
@@ -24,6 +24,7 @@
 #include "llvm/ADT/StringSet.h"
 #include "llvm/ADT/Triple.h"
 #include "llvm/Analysis/CallGraph.h"
+#include "llvm/IR/Function.h"
 #include "llvm/IR/Module.h"
 #include "llvm/InitializePasses.h"
 #include "llvm/Pass.h"
@@ -115,6 +116,17 @@ bool InternalizePass::shouldPreserveGV(const GlobalValue &GV) {
   if (AlwaysPreserved.count(GV.getName()))
     return true;
 
+  // is this a compute (OpenCL/CUDA/Metal/Vulkan) kernel or graphics function?
+  if (isa<Function>(GV)) {
+    const Function* F = dyn_cast<Function>(&GV);
+    if (F &&
+        (F->getCallingConv() == CallingConv::FLOOR_KERNEL ||
+         F->getCallingConv() == CallingConv::FLOOR_VERTEX ||
+         F->getCallingConv() == CallingConv::FLOOR_FRAGMENT)) {
+      return true;
+    }
+  }
+
   return MustPreserveGV(GV);
 }
 
diff --git a/llvm/lib/Transforms/IPO/PassManagerBuilder.cpp b/llvm/lib/Transforms/IPO/PassManagerBuilder.cpp
index f0b5589347e2..1113e41cb737 100644
--- a/llvm/lib/Transforms/IPO/PassManagerBuilder.cpp
+++ b/llvm/lib/Transforms/IPO/PassManagerBuilder.cpp
@@ -39,6 +39,7 @@
 #include "llvm/Transforms/IPO/InferFunctionAttrs.h"
 #include "llvm/Transforms/InstCombine/InstCombine.h"
 #include "llvm/Transforms/Instrumentation.h"
+#include "llvm/Transforms/LibFloor.h"
 #include "llvm/Transforms/Scalar.h"
 #include "llvm/Transforms/Scalar/GVN.h"
 #include "llvm/Transforms/Scalar/InstSimplifyPass.h"
@@ -51,6 +52,7 @@
 #include "llvm/Transforms/Vectorize/LoopVectorize.h"
 #include "llvm/Transforms/Vectorize/SLPVectorizer.h"
 #include "llvm/Transforms/Vectorize/VectorCombine.h"
+#include "llvm/SPIRVerifier/SpirValidation.h"
 
 using namespace llvm;
 
@@ -220,6 +222,17 @@ PassManagerBuilder::PassManagerBuilder() {
     PerformThinLTO = EnablePerformThinLTO;
     DivergentTarget = false;
     CallGraphProfile = true;
+
+    EnableAddressSpaceFix = false;
+    EnableCUDAPasses = false;
+    EnableMetalPasses = false;
+    EnableMetalIntelWorkarounds = false;
+    EnableMetalNvidiaWorkarounds = false;
+    EnableSPIRPasses = false;
+    EnableSPIRIntelWorkarounds = false;
+    EnableVerifySPIR = false;
+    EnableVulkanPasses = false;
+    EnableVulkanLLVMPreStructurizationPass = false;
 }
 
 PassManagerBuilder::~PassManagerBuilder() {
@@ -366,7 +379,7 @@ void PassManagerBuilder::addPGOInstrPasses(legacy::PassManagerBase &MPM,
     MPM.add(createSROAPass());
     MPM.add(createEarlyCSEPass());             // Catch trivial redundancies
     MPM.add(createCFGSimplificationPass());    // Merge & remove BBs
-    MPM.add(createInstructionCombiningPass()); // Combine silly seq's
+    MPM.add(createInstructionCombiningPass(EnableVulkanPasses)); // Combine silly seq's
     addExtensionsToPM(EP_Peephole, MPM);
   }
   if ((EnablePGOInstrGen && !IsCS) || (EnablePGOCSInstrGen && IsCS)) {
@@ -422,7 +435,7 @@ void PassManagerBuilder::addFunctionSimplificationPasses(
   // Combine silly seq's
   if (OptLevel > 2)
     MPM.add(createAggressiveInstCombinerPass());
-  MPM.add(createInstructionCombiningPass());
+  MPM.add(createInstructionCombiningPass(EnableVulkanPasses));
   if (SizeLevel == 0 && !DisableLibCallsShrinkWrap)
     MPM.add(createLibCallsShrinkWrapPass());
   addExtensionsToPM(EP_Peephole, MPM);
@@ -466,7 +479,7 @@ void PassManagerBuilder::addFunctionSimplificationPasses(
   // simplifycfg. Eventually loop-simplifycfg should be enhanced to replace the
   // need for this.
   MPM.add(createCFGSimplificationPass());
-  MPM.add(createInstructionCombiningPass());
+  MPM.add(createInstructionCombiningPass(EnableVulkanPasses));
   // We resume loop passes creating a second loop pipeline here.
   if (EnableLoopFlatten) {
     MPM.add(createLoopFlattenPass()); // Flatten loops
@@ -506,7 +519,7 @@ void PassManagerBuilder::addFunctionSimplificationPasses(
 
   // Run instcombine after redundancy elimination to exploit opportunities
   // opened up by them.
-  MPM.add(createInstructionCombiningPass());
+  MPM.add(createInstructionCombiningPass(EnableVulkanPasses));
   addExtensionsToPM(EP_Peephole, MPM);
   if (OptLevel > 1) {
     if (EnableDFAJumpThreading && SizeLevel == 0)
@@ -533,7 +546,7 @@ void PassManagerBuilder::addFunctionSimplificationPasses(
   MPM.add(createCFGSimplificationPass(
       SimplifyCFGOptions().hoistCommonInsts(true).sinkCommonInsts(true)));
   // Clean up after everything.
-  MPM.add(createInstructionCombiningPass());
+  MPM.add(createInstructionCombiningPass(EnableVulkanPasses));
   addExtensionsToPM(EP_Peephole, MPM);
 
   if (EnableCHR && OptLevel >= 3 &&
@@ -544,6 +557,11 @@ void PassManagerBuilder::addFunctionSimplificationPasses(
 /// FIXME: Should LTO cause any differences to this set of passes?
 void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
                                          bool IsFullLTO) {
+  if (EnableMetalPasses) {
+    // Metal: enable ld/st vectorization
+    PM.add(createLoadStoreVectorizerPass());
+  }
+
   PM.add(createLoopVectorizePass(!LoopsInterleaved, !LoopVectorize));
 
   if (IsFullLTO) {
@@ -568,7 +586,7 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
     PM.add(createLoopLoadEliminationPass());
   }
   // Cleanup after the loop optimization passes.
-  PM.add(createInstructionCombiningPass());
+  PM.add(createInstructionCombiningPass(EnableVulkanPasses));
 
   if (OptLevel > 1 && ExtraVectorizerPasses) {
     // At higher optimization levels, try to clean up any runtime overlap and
@@ -579,11 +597,11 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
     // dead (or speculatable) control flows or more combining opportunities.
     PM.add(createEarlyCSEPass());
     PM.add(createCorrelatedValuePropagationPass());
-    PM.add(createInstructionCombiningPass());
+    PM.add(createInstructionCombiningPass(EnableVulkanPasses));
     PM.add(createLICMPass(LicmMssaOptCap, LicmMssaNoAccForPromotionCap));
     PM.add(createLoopUnswitchPass(SizeLevel || OptLevel < 3, DivergentTarget));
     PM.add(createCFGSimplificationPass());
-    PM.add(createInstructionCombiningPass());
+    PM.add(createInstructionCombiningPass(EnableVulkanPasses));
   }
 
   // Now that we've formed fast to execute loop structures, we do further
@@ -604,7 +622,7 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
 
   if (IsFullLTO) {
     PM.add(createSCCPPass());                 // Propagate exposed constants
-    PM.add(createInstructionCombiningPass()); // Clean up again
+    PM.add(createInstructionCombiningPass(EnableVulkanPasses)); // Clean up again
     PM.add(createBitTrackingDCEPass());
   }
 
@@ -616,11 +634,11 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
   }
 
   // Enhance/cleanup vector code.
-  PM.add(createVectorCombinePass());
+  PM.add(createVectorCombinePass(EnableVulkanPasses));
 
   if (!IsFullLTO) {
     addExtensionsToPM(EP_Peephole, PM);
-    PM.add(createInstructionCombiningPass());
+    PM.add(createInstructionCombiningPass(EnableVulkanPasses));
 
     if (EnableUnrollAndJam && !DisableUnrollLoops) {
       // Unroll and Jam. We do this before unroll but need to be in a separate
@@ -635,7 +653,7 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
 
     if (!DisableUnrollLoops) {
       // LoopUnroll may generate some redundency to cleanup.
-      PM.add(createInstructionCombiningPass());
+      PM.add(createInstructionCombiningPass(EnableVulkanPasses));
 
       // Runtime unrolling will introduce runtime check in loop prologue. If the
       // unrolled loop is a inner loop, then the prologue will be inside the
@@ -652,11 +670,14 @@ void PassManagerBuilder::addVectorPasses(legacy::PassManagerBase &PM,
   PM.add(createAlignmentFromAssumptionsPass());
 
   if (IsFullLTO)
-    PM.add(createInstructionCombiningPass());
+    PM.add(createInstructionCombiningPass(EnableVulkanPasses));
 }
 
 void PassManagerBuilder::populateModulePassManager(
     legacy::PassManagerBase &MPM) {
+  // Add LibraryInfo if we have some.
+  if (LibraryInfo) MPM.add(new TargetLibraryInfoWrapperPass(*LibraryInfo));
+
   // Whether this is a default or *LTO pre-link pipeline. The FullLTO post-link
   // is handled separately, so just check this is not the ThinLTO post-link.
   bool DefaultOrPreLinkPipeline = !PerformThinLTO;
@@ -675,6 +696,34 @@ void PassManagerBuilder::populateModulePassManager(
   // Allow forcing function attributes as a debugging and tuning aid.
   MPM.add(createForceFunctionAttrsLegacyPass());
 
+  if (EnableAddressSpaceFix) {
+    // address space fixing should be run as early as possible, but it also
+    // requires readonly/nocapture/etc function and argument attributes,
+    // which in turn requires certain alias analysis to be run first
+    // NOTE: the original FunctionAttrs pass should still be run later on,
+    // because the code will have changed significantly due to optimizations
+    // and other pass changes
+    addInitialAliasAnalysisPasses(MPM);
+    MPM.add(createAAResultsWrapperPass());
+    MPM.add(createPostOrderFunctionAttrsLegacyPass());
+    MPM.add(createAddressSpaceFixPass());
+    MPM.add(createInternalizePass()); // kill cloned functions
+    //MPM.add(createBarrierNoopPass());
+  }
+
+  // run "first" passes that should run before all else
+  // if(EnableCUDAPasses) --none
+  if(EnableMetalPasses) {
+    MPM.add(createMetalFirstPass(EnableMetalIntelWorkarounds, EnableMetalNvidiaWorkarounds));
+  }
+  // if(EnableSPIRPasses) --none
+
+  // run this before any other major optimizations (it will be helpful to them)
+  MPM.add(createPropagateRangeInfoPass());
+
+  // run this again, since functions might have changed
+  MPM.add(createForceFunctionAttrsLegacyPass());
+
   // If all optimizations are disabled, just run the always-inline pass and,
   // if enabled, the function merging pass.
   if (OptLevel == 0) {
@@ -722,7 +771,8 @@ void PassManagerBuilder::populateModulePassManager(
   if (LibraryInfo)
     MPM.add(new TargetLibraryInfoWrapperPass(*LibraryInfo));
 
-  addInitialAliasAnalysisPasses(MPM);
+  if (!EnableAddressSpaceFix)
+    addInitialAliasAnalysisPasses(MPM);
 
   // For ThinLTO there are two passes of indirect call promotion. The
   // first is during the compile phase when PerformThinLTO=false and
@@ -770,7 +820,7 @@ void PassManagerBuilder::populateModulePassManager(
 
   MPM.add(createDeadArgEliminationPass()); // Dead argument elimination
 
-  MPM.add(createInstructionCombiningPass()); // Clean up after IPCP & DAE
+  MPM.add(createInstructionCombiningPass(EnableVulkanPasses)); // Clean up after IPCP & DAE
   addExtensionsToPM(EP_Peephole, MPM);
   MPM.add(createCFGSimplificationPass()); // Clean up after IPCP & DAE
 
@@ -822,6 +872,35 @@ void PassManagerBuilder::populateModulePassManager(
   // we must insert a no-op module pass to reset the pass manager.
   MPM.add(createBarrierNoopPass());
 
+  // run image read/write function passes after inling everything,
+  // this way we can actually check each use of these functions and their arguments,
+  // with constants potentially changing/improving the behavior and allowing
+  // additional checking (like oob offsets).
+  if(EnableCUDAPasses || EnableMetalPasses || EnableSPIRPasses) {
+    if(EnableCUDAPasses) MPM.add(createCUDAImagePass(floor_image_capabilities));
+    if(EnableMetalPasses) MPM.add(createMetalImagePass(floor_image_capabilities));
+    if(EnableSPIRPasses) {
+      if(!EnableVulkanPasses) {
+        MPM.add(createSPIRImagePass(floor_image_capabilities, EnableSPIRIntelWorkarounds));
+      }
+      else {
+        MPM.add(createVulkanImagePass(floor_image_capabilities));
+      }
+    }
+    
+    // and cleanup afterwards, including loop and unrolling related things
+    MPM.add(createTailCallEliminationPass());
+    MPM.add(createLoopRotatePass());
+    MPM.add(createLICMPass());
+    MPM.add(createSimpleLoopUnrollPass());
+    
+    MPM.add(createGVNPass());
+    MPM.add(createAggressiveDCEPass());
+    MPM.add(createCFGSimplificationPass());
+    MPM.add(createInstructionCombiningPass());
+    addExtensionsToPM(EP_Peephole, MPM);
+  }
+
   if (RunPartialInlining)
     MPM.add(createPartialInliningPass());
 
@@ -930,7 +1009,12 @@ void PassManagerBuilder::populateModulePassManager(
   // llvm.loop.distribute=true or when -enable-loop-distribute is specified.
   MPM.add(createLoopDistributePass());
 
+  // NOTE: this is false by default, but we actually want additional
+  // optimizations for CUDA/OpenCL/Metal/Vulkan
   addVectorPasses(MPM, /* IsFullLTO */ false);
+  if (EnableCUDAPasses || EnableSPIRPasses || EnableMetalPasses || EnableVulkanPasses) {
+    addVectorPasses(MPM, true);
+  }
 
   // FIXME: We shouldn't bother with this anymore.
   MPM.add(createStripDeadPrototypesPass()); // Get rid of dead prototypes
@@ -976,6 +1060,74 @@ void PassManagerBuilder::populateModulePassManager(
 
   addExtensionsToPM(EP_OptimizerLast, MPM);
 
+  // run backend final passes at the very end, no IR should change after this point!
+  if (EnableCUDAPasses) MPM.add(createCUDAFinalPass());
+  if (EnableSPIRPasses) MPM.add(createSPIRFinalPass());
+  if (EnableVulkanPasses) { // must run after spir!
+    // initial cfg cleanup/simplification
+    MPM.add(createAggressiveDCEPass(true /* allow CFG removal here */));
+    // NOTE: we no longer need to or want to perform switch lowering (switch'es can be handled now!)
+    //MPM.add(createLowerSwitchPass());
+    MPM.add(createCFGSimplificationPass());
+    MPM.add(createSinkingPass());
+
+    // improve GEPs
+    MPM.add(createSeparateConstOffsetFromGEPPass());
+    MPM.add(createSpeculativeExecutionPass());
+    MPM.add(createStraightLineStrengthReducePass());
+    MPM.add(createGVNPass());
+    MPM.add(createNaryReassociatePass());
+    MPM.add(createEarlyCSEPass());
+
+    // builtin -> parameter replacement
+    MPM.add(createVulkanBuiltinParamHandlingPass());
+
+    // "pre-final" vulkanization (prior to cfg structurization)
+    MPM.add(createVulkanPreFinalPass());
+
+    // Vulkan requires structured control flow:
+    // -> hit it with LLVM passes/fixes first
+    MPM.add(createFixIrreduciblePass());
+    MPM.add(createUnifyLoopExitsPass());
+    // NOTE: while this may "fix" certain corner cases, it leads to trouble in the dxil-spirv structurizer,
+    // since it can generate incorrect control flow (e.g. might run through, but produce incorrect results)
+    // -> only enable it if requested
+    if (EnableVulkanLLVMPreStructurizationPass) {
+      MPM.add(createStructurizeCFGPass(false));
+    }
+    // -> finally use the CFG structurizer from dxil-spirv to get a proper conformant CFG
+    MPM.add(createCFGStructurizationPass());
+
+    // vulkanization
+    MPM.add(createVulkanFinalPass());
+    MPM.add(createVulkanFinalModuleCleanupPass());
+  }
+  if (EnableMetalPasses) {
+    MPM.add(createMetalFinalPass(EnableMetalIntelWorkarounds, EnableMetalNvidiaWorkarounds));
+    MPM.add(createMetalFinalModuleCleanupPass());
+  }
+
+  // cleanup
+  if (EnableMetalPasses || EnableCUDAPasses || (EnableSPIRPasses && !EnableVulkanPasses)) {
+    MPM.add(createTailCallEliminationPass());
+    MPM.add(createCFGSimplificationPass());
+    MPM.add(createInstructionCombiningPass());
+    MPM.add(createGVNPass());
+    MPM.add(createAggressiveDCEPass());
+  }
+  if (EnableVulkanPasses) {
+    // certain cleanup / post-final passes are disabled for Vulkan, because we don't
+    // want to change the CFG or introduce "invalid" instructions
+    MPM.add(createFMACombinerPass());
+    MPM.add(createTailCallEliminationPass());
+    MPM.add(createInstructionCombiningPass(true, true /* Vulkan */));
+    // NOTE: use new GVN here, b/c it doesn't change basic blocks
+    MPM.add(createNewGVNPass());
+    MPM.add(createAggressiveDCEPass(false /* don't allow CFG removal */));
+  }
+
+  if (EnableVerifySPIR) MPM.add(createSpirValidationPass());
+
   if (PrepareForLTO) {
     MPM.add(createCanonicalizeAliasesPass());
     // Rename anon globals to be able to handle them in the summary
@@ -1069,7 +1221,7 @@ void PassManagerBuilder::addLTOOptimizationPasses(legacy::PassManagerBase &PM) {
   // calls, etc, so let instcombine do this.
   if (OptLevel > 2)
     PM.add(createAggressiveInstCombinerPass());
-  PM.add(createInstructionCombiningPass());
+  PM.add(createInstructionCombiningPass(EnableVulkanPasses));
   addExtensionsToPM(EP_Peephole, PM);
 
   // Inline small functions
@@ -1103,7 +1255,7 @@ void PassManagerBuilder::addLTOOptimizationPasses(legacy::PassManagerBase &PM) {
   PM.add(createArgumentPromotionPass());
 
   // The IPO passes may leave cruft around.  Clean up after them.
-  PM.add(createInstructionCombiningPass());
+  PM.add(createInstructionCombiningPass(EnableVulkanPasses));
   addExtensionsToPM(EP_Peephole, PM);
   PM.add(createJumpThreadingPass(/*FreezeSelectCond*/ true));
 
diff --git a/llvm/lib/Transforms/InstCombine/InstCombineInternal.h b/llvm/lib/Transforms/InstCombine/InstCombineInternal.h
index 72e1b21e8d49..2faf849b8e61 100644
--- a/llvm/lib/Transforms/InstCombine/InstCombineInternal.h
+++ b/llvm/lib/Transforms/InstCombine/InstCombineInternal.h
@@ -63,12 +63,12 @@ class LLVM_LIBRARY_VISIBILITY InstCombinerImpl final
       public InstVisitor<InstCombinerImpl, Instruction *> {
 public:
   InstCombinerImpl(InstructionWorklist &Worklist, BuilderTy &Builder,
-                   bool MinimizeSize, AAResults *AA, AssumptionCache &AC,
+                   bool MinimizeSize, bool isVulkan, AAResults *AA, AssumptionCache &AC,
                    TargetLibraryInfo &TLI, TargetTransformInfo &TTI,
                    DominatorTree &DT, OptimizationRemarkEmitter &ORE,
                    BlockFrequencyInfo *BFI, ProfileSummaryInfo *PSI,
                    const DataLayout &DL, LoopInfo *LI)
-      : InstCombiner(Worklist, Builder, MinimizeSize, AA, AC, TLI, TTI, DT, ORE,
+      : InstCombiner(Worklist, Builder, MinimizeSize, isVulkan, AA, AC, TLI, TTI, DT, ORE,
                      BFI, PSI, DL, LI) {}
 
   virtual ~InstCombinerImpl() {}
diff --git a/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp b/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp
index 35739c3b9a21..d2fb39577a56 100644
--- a/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp
+++ b/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp
@@ -656,6 +656,11 @@ static bool isSafeAndProfitableToSinkLoad(LoadInst *L) {
 }
 
 Instruction *InstCombinerImpl::foldPHIArgLoadIntoPHI(PHINode &PN) {
+  if (isVulkan) {
+    // pointer PHIs are illegal with Vulkan
+    return nullptr;
+  }
+
   LoadInst *FirstLI = cast<LoadInst>(PN.getIncomingValue(0));
 
   // FIXME: This is overconservative; this transform is allowed in some cases
diff --git a/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp b/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp
index 1b401566dad7..624beb545d91 100644
--- a/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp
+++ b/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp
@@ -4020,7 +4020,8 @@ public:
 /// whose condition is a known constant, we only visit the reachable successors.
 static bool prepareICWorklistFromFunction(Function &F, const DataLayout &DL,
                                           const TargetLibraryInfo *TLI,
-                                          InstructionWorklist &ICWorklist) {
+                                          InstructionWorklist &ICWorklist,
+                                          bool isVulkan) {
   bool MadeIRChange = false;
   SmallPtrSet<BasicBlock *, 32> Visited;
   SmallVector<BasicBlock*, 256> Worklist;
@@ -4107,6 +4108,10 @@ static bool prepareICWorklistFromFunction(Function &F, const DataLayout &DL,
     if (Visited.count(&BB))
       continue;
 
+    // always keep everything in Vulkan fake continue blocks
+    if (isVulkan && BB.getName().endswith(".fake_continue"))
+      continue;
+
     unsigned NumDeadInstInBB;
     unsigned NumDeadDbgInstInBB;
     std::tie(NumDeadInstInBB, NumDeadDbgInstInBB) =
@@ -4145,7 +4150,7 @@ static bool combineInstructionsOverFunction(
     Function &F, InstructionWorklist &Worklist, AliasAnalysis *AA,
     AssumptionCache &AC, TargetLibraryInfo &TLI, TargetTransformInfo &TTI,
     DominatorTree &DT, OptimizationRemarkEmitter &ORE, BlockFrequencyInfo *BFI,
-    ProfileSummaryInfo *PSI, unsigned MaxIterations, LoopInfo *LI) {
+    ProfileSummaryInfo *PSI, unsigned MaxIterations, bool isVulkan, LoopInfo *LI) {
   auto &DL = F.getParent()->getDataLayout();
   MaxIterations = std::min(MaxIterations, LimitMaxIterations.getValue());
 
@@ -4187,9 +4192,9 @@ static bool combineInstructionsOverFunction(
     LLVM_DEBUG(dbgs() << "\n\nINSTCOMBINE ITERATION #" << Iteration << " on "
                       << F.getName() << "\n");
 
-    MadeIRChange |= prepareICWorklistFromFunction(F, DL, &TLI, Worklist);
+    MadeIRChange |= prepareICWorklistFromFunction(F, DL, &TLI, Worklist, isVulkan);
 
-    InstCombinerImpl IC(Worklist, Builder, F.hasMinSize(), AA, AC, TLI, TTI, DT,
+    InstCombinerImpl IC(Worklist, Builder, F.hasMinSize(), isVulkan, AA, AC, TLI, TTI, DT,
                         ORE, BFI, PSI, DL, LI);
     IC.MaxArraySizeForCombine = MaxArraySize;
 
@@ -4202,10 +4207,10 @@ static bool combineInstructionsOverFunction(
   return MadeIRChange;
 }
 
-InstCombinePass::InstCombinePass() : MaxIterations(LimitMaxIterations) {}
+InstCombinePass::InstCombinePass(bool isVulkan_) : MaxIterations(LimitMaxIterations), isVulkan(isVulkan_) {}
 
-InstCombinePass::InstCombinePass(unsigned MaxIterations)
-    : MaxIterations(MaxIterations) {}
+InstCombinePass::InstCombinePass(unsigned MaxIterations, bool isVulkan_)
+    : MaxIterations(MaxIterations), isVulkan(isVulkan_) {}
 
 PreservedAnalyses InstCombinePass::run(Function &F,
                                        FunctionAnalysisManager &AM) {
@@ -4225,7 +4230,7 @@ PreservedAnalyses InstCombinePass::run(Function &F,
       &AM.getResult<BlockFrequencyAnalysis>(F) : nullptr;
 
   if (!combineInstructionsOverFunction(F, Worklist, AA, AC, TLI, TTI, DT, ORE,
-                                       BFI, PSI, MaxIterations, LI))
+                                       BFI, PSI, MaxIterations, isVulkan, LI))
     // No changes, all analyses are preserved.
     return PreservedAnalyses::all();
 
@@ -4274,18 +4279,18 @@ bool InstructionCombiningPass::runOnFunction(Function &F) {
       nullptr;
 
   return combineInstructionsOverFunction(F, Worklist, AA, AC, TLI, TTI, DT, ORE,
-                                         BFI, PSI, MaxIterations, LI);
+                                         BFI, PSI, MaxIterations, isVulkan, LI);
 }
 
 char InstructionCombiningPass::ID = 0;
 
-InstructionCombiningPass::InstructionCombiningPass()
-    : FunctionPass(ID), MaxIterations(InstCombineDefaultMaxIterations) {
+InstructionCombiningPass::InstructionCombiningPass(bool isVulkan_)
+    : FunctionPass(ID), MaxIterations(InstCombineDefaultMaxIterations), isVulkan(isVulkan_) {
   initializeInstructionCombiningPassPass(*PassRegistry::getPassRegistry());
 }
 
-InstructionCombiningPass::InstructionCombiningPass(unsigned MaxIterations)
-    : FunctionPass(ID), MaxIterations(MaxIterations) {
+InstructionCombiningPass::InstructionCombiningPass(unsigned MaxIterations, bool isVulkan_)
+    : FunctionPass(ID), MaxIterations(MaxIterations), isVulkan(isVulkan_) {
   initializeInstructionCombiningPassPass(*PassRegistry::getPassRegistry());
 }
 
@@ -4312,14 +4317,14 @@ void LLVMInitializeInstCombine(LLVMPassRegistryRef R) {
   initializeInstructionCombiningPassPass(*unwrap(R));
 }
 
-FunctionPass *llvm::createInstructionCombiningPass() {
-  return new InstructionCombiningPass();
+FunctionPass *llvm::createInstructionCombiningPass(bool isVulkan) {
+  return new InstructionCombiningPass(isVulkan);
 }
 
-FunctionPass *llvm::createInstructionCombiningPass(unsigned MaxIterations) {
-  return new InstructionCombiningPass(MaxIterations);
+FunctionPass *llvm::createInstructionCombiningPass(unsigned MaxIterations, bool isVulkan) {
+  return new InstructionCombiningPass(MaxIterations, isVulkan);
 }
 
-void LLVMAddInstructionCombiningPass(LLVMPassManagerRef PM) {
-  unwrap(PM)->add(createInstructionCombiningPass());
+void LLVMAddInstructionCombiningPass(LLVMPassManagerRef PM, bool isVulkan) {
+  unwrap(PM)->add(createInstructionCombiningPass(isVulkan));
 }
diff --git a/llvm/lib/Transforms/LibFloor/AddressSpaceFix.cpp b/llvm/lib/Transforms/LibFloor/AddressSpaceFix.cpp
new file mode 100644
index 000000000000..f18d9b07538d
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/AddressSpaceFix.cpp
@@ -0,0 +1,613 @@
+//===- AddressSpaceFix.cpp - OpenCL/SPIR and related addrspace fixes ------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements an address space fixer for OpenCL/Metal/Vulkan.
+//
+// This is implemented as a module pass that iterates over all functions, then
+// over all call instructions in there, fixing all calls that require a
+// different address space then what is provided by the called function.
+// Since this requires the address space information "from the top",
+// this can't be implemented as a bottom-up SCC pass.
+// Note that this will duplicate any functions that don't have matching address
+// space parameters and thus heavily depends on proper inlining later on.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/AssumptionCache.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/TargetLibraryInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+#include <algorithm>
+#include <unordered_set>
+#include <cstdarg>
+#include <memory>
+#include <cxxabi.h>
+using namespace llvm;
+
+#define DEBUG_TYPE "AddressSpaceFix"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// AddressSpaceFix
+	struct AddressSpaceFix : public ModulePass, InstVisitor<AddressSpaceFix> {
+		friend class InstVisitor<AddressSpaceFix>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		bool was_modified { false };
+		
+		AddressSpaceFix() : ModulePass(ID) {
+			initializeAddressSpaceFixPass(*PassRegistry::getPassRegistry());
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<AAResultsWrapperPass>();
+			AU.addRequired<GlobalsAAWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<TargetLibraryInfoWrapperPass>();
+		}
+		
+		bool runOnModule(Module& Mod) override {
+			M = &Mod;
+			ctx = &M->getContext();
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			DBG(errs() << Mod << "\n");
+			
+			bool module_modified = false;
+			for(auto& func : Mod) {
+				// ignore non-c++ functions (e.g. ones that were created in here)
+				if(func.getName().count('.') > 0) continue;
+				module_modified |= runOnFunction(func);
+			}
+			return module_modified;
+		}
+		
+		bool runOnFunction(Function& F) {
+			// visit everything in this function
+			was_modified = false; // reset every time
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			visit(F);
+			if(was_modified) {
+				DBG(errs() << "!! modified function: ";)
+				DBG(errs().write_escaped(F.getName()) << '\n';)
+			}
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<AddressSpaceFix>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<AddressSpaceFix>::visit(I);
+		}
+
+		template <bool fix_call_instrs = true>
+		static void fix_users(AddressSpaceFix* asfix_pass, LLVMContext& ctx, Instruction* instr, Value* parent, const uint32_t address_space, std::vector<ReturnInst*>& returns) {
+			// fix instruction
+			switch(instr->getOpcode()) {
+				case Instruction::GetElementPtr: {
+					auto GEP = cast<GetElementPtrInst>(instr);
+					if(GEP->getType()->isPointerTy()) {
+						auto new_ptr_type = PointerType::get(GEP->getType()->getPointerElementType(), address_space);
+						DBG(errs() << ">> GEP: " << *GEP->getType();)
+						GEP->mutateType(new_ptr_type);
+						DBG(errs() << " -> " << *GEP->getType() << "\n";)
+					}
+					// else: can't happen?
+					break;
+				}
+				case Instruction::BitCast: {
+					auto BC = cast<BitCastInst>(instr);
+					if(BC->getDestTy()->isPointerTy()) {
+						auto new_ptr_type = PointerType::get(BC->getDestTy()->getPointerElementType(), address_space);
+						DBG(errs() << ">> BC: " << *BC->getType() << " -> ";)
+						BC->mutateType(new_ptr_type);
+						DBG(errs() << " -> " << *BC->getType() << "\n";)
+					}
+					// else: can't do anything (TODO: warn/error?)
+					break;
+				}
+				case Instruction::Call: {
+					if constexpr (fix_call_instrs) {
+						// TODO: should accumulate all users to *this* call instruction (there can be multiple), might want to delay this until done with the function?
+						auto CI = cast<CallInst>(instr);
+						DBG(errs() << ">> call: " << *CI << "\n";)
+						// -> recurse (note that the argument will already have the correct address space)
+						asfix_pass->fix_call_instr(*CI, false);
+					}
+					break;
+				}
+				case Instruction::Ret: {
+					// the function return type will be changed once all returns (and return types) have been accumulated
+					returns.push_back(cast<ReturnInst>(instr));
+					return;
+				}
+				case Instruction::Load: {
+					auto LD = cast<LoadInst>(instr);
+					if(LD->getType()->isPointerTy()) {
+						auto new_ptr_type = PointerType::get(LD->getType()->getPointerElementType(), address_space);
+						DBG(errs() << ">> LD: " << *LD->getType() << " -> ";)
+						LD->mutateType(new_ptr_type);
+						DBG(errs() << " -> " << *LD->getType() << "\n";)
+					}
+					break;
+				}
+				case Instruction::Store: {
+					auto ST = cast<StoreInst>(instr);
+					if(ST->getType()->isPointerTy()) {
+						auto new_ptr_type = PointerType::get(ST->getType()->getPointerElementType(), address_space);
+						DBG(errs() << ">> ST: " << *ST->getType() << " -> ";)
+						ST->mutateType(new_ptr_type);
+						DBG(errs() << " -> " << *ST->getType() << "\n";)
+					}
+					break;
+				}
+				case Instruction::PHI: {
+					auto phi = cast<PHINode>(instr);
+					if(phi->getType()->isPointerTy()) {
+						auto new_ptr_type = PointerType::get(phi->getType()->getPointerElementType(), address_space);
+						DBG(errs() << ">> PHI: " << *phi->getType();)
+						phi->mutateType(new_ptr_type);
+						DBG(errs() << " -> " << *phi->getType() << "\n";)
+					}
+					break;
+				}
+
+				case Instruction::AddrSpaceCast:
+				case Instruction::Invoke:
+					// bad, should never happen
+					ctx.emitError(instr, "encountered unsupported instruction");
+					return;
+
+				// uninteresting instructions:
+				default:
+					// nothing has changed, bail out
+					return;
+			}
+			
+			// recursively fix all users
+			for(auto user : instr->users()) {
+				DBG(errs() << ">> replacing rec use: " << *user << "\n";)
+				if(auto user_instr = dyn_cast<Instruction>(user)) {
+					switch(user_instr->getOpcode()) {
+						case Instruction::GetElementPtr:
+						case Instruction::BitCast:
+						case Instruction::Call:
+						case Instruction::Ret:
+						case Instruction::Load:
+						case Instruction::Store:
+						case Instruction::PHI:
+							fix_users<fix_call_instrs>(asfix_pass, ctx, user_instr, instr, address_space, returns);
+							break;
+						case Instruction::AddrSpaceCast:
+						case Instruction::Invoke:
+							// bad, should never happen
+							ctx.emitError(user_instr, "encountered unsupported instruction");
+							break;
+						default: break;
+					}
+				}
+			}
+		}
+		
+		struct as_fix_arg_info {
+			uint32_t index;
+			uint32_t address_space;
+			bool read_only_fix;
+		};
+		
+		// returns true if the return type changed
+		void fix_function(llvm::Function* func, const std::vector<as_fix_arg_info>& args, const bool is_top_call) {
+			std::vector<ReturnInst*> returns; // returns to fix
+			for(const auto& arg : args) {
+				if(arg.read_only_fix) continue;
+				
+				Argument& func_arg = *(std::next(func->arg_begin(), arg.index));
+				for(auto user : func_arg.users()) {
+					DBG(errs() << ">> replacing use: " << *user << "\n";)
+					if(auto instr = dyn_cast<Instruction>(user)) {
+						fix_users(this, *ctx, instr, &func_arg, arg.address_space, returns);
+					}
+					else {
+						DBG(errs() << "   not an instruction\n";)
+					}
+				}
+				DBG(errs() << "<< fixed arg: " << arg.index << "\n";)
+			}
+			
+			if(!returns.empty()) {
+				DBG(errs() << ">> fixing returns: " << returns.size() << "\n";)
+				std::unordered_set<Type*> ret_types;
+				for(const auto& ret : returns) {
+					// shouldn't occur (there'd be no initial user for this)
+					if(!ret->getReturnValue()) continue;
+					ret_types.emplace(ret->getReturnValue()->getType());
+				}
+				
+				// again, shouldn't occur, but still better to check
+				if(!ret_types.empty()) {
+					// if there is more than one expected return type we have a problem
+					if(ret_types.size() > 1) {
+						// TODO: should try and fix this properly (create alloca in caller + store result in there?)
+						// TODO: for targets with a generic address space, might want to use that instead
+						ctx->emitError("more than one return type in function " + func->getName().str());
+					}
+					else if((*ret_types.begin())->getPointerAddressSpace() !=
+							func->getReturnType()->getPointerAddressSpace()) {
+						// fix func return type
+						std::vector<Type*> param_types;
+						for(const auto& arg : func->args()) {
+							param_types.push_back(arg.getType());
+						}
+						auto new_ret_type = *ret_types.begin();
+						auto new_func_type = FunctionType::get(new_ret_type, param_types, false);
+						DBG({
+							auto old_func_type = func->getFunctionType();
+							errs() << ">> fixing return type: " << func->getName() << ": " << *old_func_type << " -> " << *new_func_type << "\n";
+						})
+						func->mutateType(PointerType::get(new_func_type, 0));
+						func->mutateFunctionType(new_func_type);
+					}
+				}
+			}
+			
+			DBG(errs() << "<< fixed func\n";)
+		}
+		
+		void fix_call(CallInst& CI, const std::vector<as_fix_arg_info>& args, const bool is_top_call) {
+			bool need_clone = false, need_read_only_fix = false;
+			for(const auto& arg : args) {
+				if(!arg.read_only_fix) need_clone = true;
+				else need_read_only_fix = true;
+			}
+			
+			// find first non-alloca instruction in the entry block of the function
+			// -> this will be the insert position for new alloca instructions
+			Instruction* alloca_insert { nullptr };
+			if(need_read_only_fix) {
+				for(auto& instr : *CI.getParent()->getParent()->begin()) {
+					if(!isa<AllocaInst>(instr)) {
+						alloca_insert = &instr;
+						break;
+					}
+				}
+			}
+			
+			// first: fix all read-only args
+			if(need_read_only_fix) {
+				for(const auto& arg : args) {
+					if(!arg.read_only_fix) continue;
+					
+					//  * create a temporary object (of the element/pointee type of the address space pointer)
+					//  * load data from the address space pointer to the temp object
+					//  * replace the respective call operand/argument with a pointer to the temp object
+					DBG(errs() << "\tread-only fix: arg #" << arg.index << "\n";)
+					
+					// TODO: handle alignment?
+					
+					auto call_arg = CI.getOperand(arg.index);
+					
+					builder->SetInsertPoint(alloca_insert); // insert alloca at function entry
+					auto tmp = builder->CreateAlloca(call_arg->getType()->getPointerElementType(),
+													 // what about arrays?
+													 nullptr,
+													 // give it a nice name
+													 "asfixtmp");
+					
+					builder->SetInsertPoint(&CI); // insert load before call
+					builder->CreateStore(builder->CreateLoad(call_arg->getType()->getPointerElementType(), call_arg), tmp);
+					
+					CI.setOperand(arg.index, tmp);
+				}
+			}
+			
+			// second: create cloned function if this is necessary
+			if(need_clone) {
+				// fix it:
+				//  * clone the called function and modify the appropriate argument so that it uses the correct address space
+				//  * recursively go through the cloned called function and appropriately change all uses of our modified argument
+				//    NOTE: this can very well recursively clone and fix called functions in there (and so on ...)
+				//  * modify this call so that it calls the fixed/cloned function
+				
+				auto called_func = CI.getCalledFunction();
+				DBG(errs() << "\tclone fix: " << called_func->getName() << "\n";)
+				
+				std::vector<Type*> param_types;
+				std::string func_name = called_func->getName().str();
+				uint32_t arg_num = 0;
+				for(const auto& func_arg : called_func->args()) {
+					auto call_arg = CI.getOperand(arg_num);
+					
+					bool arg_clone_fix = false;
+					for(const auto& arg : args) {
+						if(arg.index != arg_num ||
+						   arg.read_only_fix) {
+							continue;
+						}
+						
+						arg_clone_fix = true;
+						func_name += "." + std::to_string(arg.index) + "_" + std::to_string(arg.address_space);
+						break;
+					}
+					
+					if(!arg_clone_fix) {
+						// use original type
+						param_types.push_back(func_arg.getType());
+					}
+					else {
+						// use "called with" type
+						param_types.push_back(call_arg->getType());
+					}
+					
+					++arg_num;
+				}
+				
+				// check if cloned function already exists
+				auto cloned_func = M->getFunction(func_name);
+				if(cloned_func == nullptr) {
+					// only do this once
+					auto cloned_func_type = FunctionType::get(called_func->getReturnType(), param_types, false);
+					cloned_func = dyn_cast<Function>(M->getOrInsertFunction(func_name, cloned_func_type).getCallee());
+					
+					ValueToValueMapTy VMap;
+					Function::arg_iterator DestI = cloned_func->arg_begin();
+					for (const auto& I : called_func->args()) {
+						DestI->setName(I.getName());
+						VMap[&I] = &*DestI++;
+					}
+					
+					SmallVector<llvm::ReturnInst*, 8> returns;
+					llvm::CloneFunctionInto(cloned_func, called_func, VMap, CloneFunctionChangeType::GlobalChanges /* must be set for debug info */, returns);
+					
+#if 0 // for debugging purposes
+					cloned_func->addFnAttr(Attribute::NoInline);
+					cloned_func->removeFnAttr(Attribute::AlwaysInline);
+#endif
+					
+					DBG(errs() << "\n>> before <<\n" << *cloned_func);
+					
+					//
+					fix_function(cloned_func, args, is_top_call);
+					CI.setCalledFunction(cloned_func, true);
+					CI.mutateType(cloned_func->getReturnType());
+					
+					DBG(errs() << "\n>> after <<\n" << *cloned_func);
+				}
+				else {
+					DBG(errs() << "\t" << func_name << " already cloned\n";)
+					CI.setCalledFunction(cloned_func, true);
+					CI.mutateType(cloned_func->getReturnType());
+				}
+			}
+		}
+		
+		void visitCallInst(CallInst& CI) {
+			fix_call_instr(CI, true);
+		}
+		
+		void fix_call_instr(CallInst& CI, const bool is_top_call) {
+			PointerType* FPTy = cast<PointerType>(CI.getCalledOperand()->getType());
+			FunctionType* FTy = cast<FunctionType>(FPTy->getElementType());
+			
+			std::vector<as_fix_arg_info> fix_args;
+			for (unsigned i = 0, e = FTy->getNumParams(); i != e; ++i) {
+				// check if there is a type mismatch
+				if(CI.getOperand(i)->getType() != FTy->getParamType(i)) {
+					// both types must be pointers
+					auto arg = CI.getOperand(i);
+					auto called_arg_type = arg->getType();
+					auto expected_arg_type = FTy->getParamType(i);
+					if(!called_arg_type->isPointerTy() ||
+					   !expected_arg_type->isPointerTy()) {
+						// emit original verifier assertion (TODO: fix it there!)
+						assert(false && "#1: Call parameter type does not match function signature!");
+						continue;
+					}
+					
+					// check if the mismatch is _only_ due to the addrspace
+					auto as_ptr = cast<PointerType>(called_arg_type);
+					if(PointerType::get(as_ptr->getElementType(),
+										expected_arg_type->getPointerAddressSpace()) !=
+					   expected_arg_type) {
+						// emit original verifier assertion (TODO: fix it there!)
+						assert(false && "#2: Call parameter type does not match function signature!");
+						continue;
+					}
+					// else: yup, only addrspace mismatch
+					DBG(errs() << "#####################################################\n";)
+					DBG(errs() << "\t>> call to: "; CI.getCalledFunction()->llvm::Value::getType()->dump();)
+					DBG(errs() << "\n\t>> call: " << CI << "\n";)
+					DBG(errs() << "\t>> full: " << CI.getCalledFunction()->getName() << "\n";)
+					DBG(errs() << "\treplacing arg #" << i << "!\n";)
+					DBG(errs() << "\t" << called_arg_type->getPointerAddressSpace() << ", ";)
+					DBG(errs() << expected_arg_type->getPointerAddressSpace() << "\n";)
+					DBG(errs() << "\t"; called_arg_type->dump(); errs() << ", ";)
+					DBG(expected_arg_type->dump(); errs() << "\n";)
+					DBG({
+						int err = 0;
+						const char* demangled_name = abi::__cxa_demangle(CI.getCalledFunction()->getName().data(), 0, 0, &err);
+						errs() << "\tfunc: " << (demangled_name != nullptr ? demangled_name : CI.getCalledFunction()->getName().data()) << "\n";
+						if(demangled_name != nullptr) {
+							free((void*)demangled_name);
+						}
+					})
+					
+					// abort if arg is an addrspacecast and pretend everything is fine ("someone else" is already making it fit)
+					if(isa<AddrSpaceCastInst>(arg)) {
+						DBG(errs() << "\tabort due to existing addrspacecast: " << *arg << "\n";)
+						continue;
+					}
+					
+					// the same goes for bitcasts, unless src and dst have the same address space
+					if(const auto BCI = dyn_cast_or_null<BitCastInst>(arg)) {
+						if(BCI->getSrcTy()->getPointerAddressSpace() != BCI->getDestTy()->getPointerAddressSpace()) {
+							DBG(errs() << "\tabort due to existing bitcast: " << *arg << "\n";)
+							continue;
+						}
+						// else: perform address space fix, b/c this is a simple pointer/type cast
+						DBG(errs() << "\tkeeping bitcast: " << *arg << "\n";)
+					}
+					
+					// abort if expected param address space is not 0, this is not supported (or would end in a good way ...)
+					// TODO: figure out if it would be a good idea to allow things like loading from e.g. local AS and calling a global AS function
+					if(expected_arg_type->getPointerAddressSpace() != 0) {
+						DBG(errs() << "\tabort due to expected AS not being 0\n";)
+						continue;
+					}
+					
+					// abort if current argument is in address space 0 (can't be moved to an address space)
+					if(called_arg_type->getPointerAddressSpace() == 0) {
+						DBG(errs() << "\tabort due to arg already being in AS 0\n";)
+						continue;
+					}
+					
+					// query information that decides if a store would be necessary later on,
+					// i.e. the arg is not read-only and access to it must happen using the actual arg
+					const bool is_constant_as = (as_ptr->getPointerAddressSpace() == 2);
+					const bool is_readonly = CI.onlyReadsMemory(i);
+					const bool is_load = isa<LoadInst>(arg);
+					
+					fix_args.push_back(as_fix_arg_info {
+						i,
+						as_ptr->getPointerAddressSpace(),
+						is_constant_as || is_readonly || is_load,
+					});
+				}
+			}
+			
+			if(!fix_args.empty()) {
+				// retrieving AA directly in a module pass (as a dep) in llvm 3.8 is apparantly no longer possible,
+				// so we have to this localized AA instead (specific to the current function)
+				auto func = CI.getParent()->getParent();
+				BasicAAResult BAR(createLegacyPMBasicAAResult(*this, *func));
+				AAResults AA(createLegacyPMAAResults(*this, *func, BAR));
+				
+				// check if we have both clone and read-only fixes
+				bool has_ro_fix = false, has_clone_fix = false;
+				for(const auto& arg : fix_args) {
+					if(arg.read_only_fix) has_ro_fix = true;
+					else has_clone_fix = true;
+				}
+				if(has_ro_fix && has_clone_fix) {
+					// if so, check AA for all read-only fix args and check if these alias with any clone fix arg
+					for(auto& ro_arg : fix_args) {
+						if(!ro_arg.read_only_fix) continue;
+						for(const auto& clone_arg : fix_args) {
+							if(clone_arg.read_only_fix) continue;
+							const auto aa_res = AA.alias(CI.getOperand(ro_arg.index), CI.getOperand(clone_arg.index));
+							if(aa_res != AliasResult::NoAlias) {
+								// -> might or must alias
+								// disable the read-only fix for this arg and use clone instead
+								ro_arg.read_only_fix = false;
+								DBG(errs() << "\tdisabling read-only fix for arg #" << ro_arg.index << " due to aliasing with clone arg #" << clone_arg.index << "\n";)
+								break;
+							}
+						}
+					}
+				}
+				
+				// fix the call (+detect return type change)
+				auto orig_ret_type = CI.getCalledFunction()->getReturnType();
+				fix_call(CI, fix_args, is_top_call);
+				auto fixed_ret_type = CI.getCalledFunction()->getReturnType();
+				
+				if(is_top_call &&
+				   orig_ret_type != fixed_ret_type) {
+					// if this is a top call and the return type changed
+					DBG(errs() << "\ttop call return type changed: " << *orig_ret_type << " -> " << *fixed_ret_type << " (in: " << CI.getParent()->getParent()->getName() << ")\n");
+				}
+				
+				// done, signal that the function was modified
+				was_modified = true;
+			}
+		}
+	};
+}
+
+namespace llvm {
+	void fix_instruction_users(LLVMContext &ctx,
+	                           Instruction &instr,
+	                           Value &parent,
+	                           const uint32_t address_space,
+	                           std::vector<ReturnInst *> &returns) {
+		// NOTE: we can't fix call instructions here
+		AddressSpaceFix::fix_users<false>(nullptr, ctx, &instr, &parent, address_space, returns);
+	}
+}
+
+char AddressSpaceFix::ID = 0;
+INITIALIZE_PASS_BEGIN(AddressSpaceFix, "AddressSpaceFix", "AddressSpaceFix Pass", false, false)
+// add all the things (not fully depending on this in getAnalysisUsage)
+INITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(GlobalsAAWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
+INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(TargetLibraryInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(PostOrderFunctionAttrsLegacyPass) // TODO: what's the new method of doing this?
+INITIALIZE_PASS_END(AddressSpaceFix, "AddressSpaceFix", "AddressSpaceFix Pass", false, false)
+
+ModulePass *llvm::createAddressSpaceFixPass() {
+	return new AddressSpaceFix();
+}
diff --git a/llvm/lib/Transforms/LibFloor/CFGStructurization.cpp b/llvm/lib/Transforms/LibFloor/CFGStructurization.cpp
new file mode 100644
index 000000000000..d7e826af8cb2
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/CFGStructurization.cpp
@@ -0,0 +1,631 @@
+//===-- CFGStructurization.cpp - CFG Structurizer -------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//==-----------------------------------------------------------------------===//
+//
+// Transforms the CFG into a structurized CFG.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/IR/Verifier.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/Utils/BasicBlockUtils.h"
+#include "llvm/Transforms/Utils/LoopUtils.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <unordered_map>
+#include <unordered_set>
+#include <deque>
+#include <array>
+
+#include "llvm/Transforms/LibFloor/StructuralAnalysis.h"
+#include "llvm/Transforms/LibFloor/StructuralTransform.h"
+#include "llvm/Transforms/LibFloor/cfg/cfg_structurizer.hpp"
+#include "llvm/Transforms/LibFloor/cfg/cfg_translator.hpp"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "cfg-structurization"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	class CFGStructurization : public FunctionPass {
+	public:
+		static char ID; // Pass identification, replacement for typeid
+		
+		typedef StructuralAnalysis SA;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		
+		DominatorTree* DT;
+		
+		bool was_modified { false };
+
+		CFGStructurization() : FunctionPass(ID) {
+			initializeCFGStructurizationPass(*PassRegistry::getPassRegistry());
+		}
+		
+		StringRef getPassName() const override {
+			return "CFG structurization";
+		}
+
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<DominatorTreeWrapperPass>();
+		}
+		
+		// returns true if the BB only contains a terminator and doesn't have a successor
+		static bool is_simple_bb(const BasicBlock* bb) {
+			if(!succ_empty(bb)) return false;
+			
+			const Instruction* term = bb->getTerminator();
+			if(&bb->front() == term) {
+				return true;
+			}
+			// also need to handle special terminations (discard is a func call + unreachable/term inst)
+			if(const CallInst* CI = dyn_cast<CallInst>(&bb->front())) {
+				if(const Function* CF = CI->getCalledFunction()) {
+					if(CF->hasName() && CF->getName() == "floor.discard_fragment") {
+						// next must be term
+						if(&*std::next(bb->begin()) == term) {
+							return true;
+						}
+						assert(false && "invalid discard_fragment call (not followed by term instr)");
+					}
+				}
+			}
+			return false;
+		}
+		
+		CallInst* insert_merge_block_marker(BasicBlock* merge_block) {
+			Function* merge_block_func = M->getFunction("floor.merge_block");
+			if(merge_block_func == nullptr) {
+				FunctionType* merge_block_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), false);
+				merge_block_func = (Function*)M->getOrInsertFunction("floor.merge_block", merge_block_type).getCallee();
+				merge_block_func->setCallingConv(CallingConv::FLOOR_FUNC);
+				merge_block_func->setCannotDuplicate();
+				merge_block_func->setDoesNotThrow();
+				merge_block_func->setNotConvergent();
+				merge_block_func->setDoesNotRecurse();
+			}
+			CallInst* merge_block_call = CallInst::Create(merge_block_func, "", merge_block->getTerminator());
+			merge_block_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			return merge_block_call;
+		}
+		
+		CallInst* insert_continue_block_marker(BasicBlock* continue_block) {
+			Function* continue_block_func = M->getFunction("floor.continue_block");
+			if(continue_block_func == nullptr) {
+				FunctionType* continue_block_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), false);
+				continue_block_func = (Function*)M->getOrInsertFunction("floor.continue_block", continue_block_type).getCallee();
+				continue_block_func->setCallingConv(CallingConv::FLOOR_FUNC);
+				continue_block_func->setCannotDuplicate();
+				continue_block_func->setDoesNotThrow();
+				continue_block_func->setNotConvergent();
+				continue_block_func->setDoesNotRecurse();
+			}
+			CallInst* continue_block_call = CallInst::Create(continue_block_func, "", continue_block->getTerminator());
+			continue_block_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			return continue_block_call;
+		}
+		
+		void create_loop_merge(Instruction* insert_before, BasicBlock* bb_merge, BasicBlock* bb_continue) {
+			Function* loop_merge_func = M->getFunction("floor.loop_merge");
+			if(loop_merge_func == nullptr) {
+				llvm::Type* loop_merge_arg_types[] { llvm::Type::getLabelTy(*ctx), llvm::Type::getLabelTy(*ctx) };
+				FunctionType* loop_merge_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), loop_merge_arg_types, false);
+				loop_merge_func = (Function*)M->getOrInsertFunction("floor.loop_merge", loop_merge_type).getCallee();
+				loop_merge_func->setCallingConv(CallingConv::FLOOR_FUNC);
+				loop_merge_func->setCannotDuplicate();
+				loop_merge_func->setDoesNotThrow();
+				loop_merge_func->setNotConvergent();
+				loop_merge_func->setDoesNotRecurse();
+			}
+			llvm::Value* merge_vars[] { bb_merge, bb_continue };
+			CallInst* loop_merge_call = CallInst::Create(loop_merge_func, merge_vars, "", insert_before);
+			loop_merge_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			insert_merge_block_marker(bb_merge);
+			insert_continue_block_marker(bb_continue);
+		}
+		
+		void create_selection_merge(Instruction* insert_before, BasicBlock* merge_block) {
+			Function* sel_merge_func = M->getFunction("floor.selection_merge");
+			if(sel_merge_func == nullptr) {
+				llvm::Type* sel_merge_arg_types[] { llvm::Type::getLabelTy(*ctx) };
+				FunctionType* sel_merge_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), sel_merge_arg_types, false);
+				sel_merge_func = (Function*)M->getOrInsertFunction("floor.selection_merge", sel_merge_type).getCallee();
+				sel_merge_func->setCallingConv(CallingConv::FLOOR_FUNC);
+				sel_merge_func->setCannotDuplicate();
+				sel_merge_func->setDoesNotThrow();
+				sel_merge_func->setNotConvergent();
+				sel_merge_func->setDoesNotRecurse();
+			}
+			llvm::Value* merge_vars[] { merge_block };
+			CallInst* sel_merge_call = CallInst::Create(sel_merge_func, merge_vars, "", insert_before);
+			sel_merge_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			insert_merge_block_marker(merge_block);
+		}
+		
+		// recursively traverses a StructuralAnalysis node tree, applying the specified handler function on each combined node
+		template <typename F>
+		void node_recurse(const SA::NodeTy* node, SA& cfg_analysis, F&& handler) {
+			if(!node->isCombined) return;
+			
+			BasicBlock* this_block = cfg_analysis.mapNode2BB(node);
+			handler(node, this_block);
+			
+			for(const auto* child : node->childNode) {
+				node_recurse(child, cfg_analysis, std::forward<F&&>(handler));
+			}
+		}
+		
+		// this applies simple fixes on the CFG:
+		//  * split SelfLoops (single loops) into a header and body block
+		//  * clone / split off trivial exit blocks (to somewhat mitigate the "single merge block" problem)
+		void restructure_simple() {
+			DBG(errs() << "> simple CFG restructuring\n";)
+			StructuralAnalysis cfg_analysis;
+			cfg_analysis.analyze(*func);
+			DBG(cfg_analysis.write(errs());)
+			
+			node_recurse(*cfg_analysis.Net.begin(), cfg_analysis,
+						 [this](const SA::NodeTy* node, BasicBlock* this_block) {
+				switch(node->nodeType) {
+					case SA::SelfLoop: {
+						DBG(errs() << ">> splitting self loop: " << this_block->getName() << "\n";)
+						SplitBlock(this_block, this_block->getFirstNonPHI());
+						was_modified = true;
+						break;
+					}
+					case SA::IfThen:
+					case SA::IfThenElse: {
+						BranchInst* branch = dyn_cast<BranchInst>(this_block->getTerminator());
+						assert(branch != nullptr && "if-then/if-then-else block must have a branch instruction");
+						if(node->exitBB != nullptr &&
+						   is_simple_bb(node->exitBB) &&
+						   branch->isConditional()) {
+							// only do this for immediate branches to a trivial exit block, otherwise continue
+							BasicBlock* branches[] { branch->getSuccessor(0), branch->getSuccessor(1) };
+							const bool simple_bb[] {
+								is_simple_bb(branches[0]),
+								is_simple_bb(branches[1]),
+							};
+							if(simple_bb[0] || simple_bb[1]) {
+								const uint32_t simple_idx = (simple_bb[0] ? 0 : 1);
+								
+								DBG(errs() << ">> added new trivial exit: in: " << this_block->getName() << ", exit: " << branches[simple_idx]->getName() << "\n";)
+								
+								ValueToValueMapTy dummy_vmap;
+								auto new_exit = CloneBasicBlock(branches[simple_idx], dummy_vmap, ".exit", func);
+								branch->setSuccessor(simple_idx, new_exit);
+								was_modified = true;
+							}
+						}
+						break;
+					}
+					default: break;
+				}
+			});
+		}
+		
+		bool simplify_cfg(Function& F) {
+			DBG(errs() << "> CFG simplification\n";)
+			bool modified = false;
+			
+			std::vector<BasicBlock*> bbs;
+			for(auto bb_iter = F.begin(); bb_iter != F.end(); ++bb_iter) {
+				bbs.emplace_back(&*bb_iter);
+			}
+			
+			for(const auto& BB : bbs) {
+				modified |= DeleteDeadPHIs(BB);
+				
+				// if bb has a single predecessor, fold single entry phi nodes
+				if(BB->getUniquePredecessor() != nullptr) {
+					FoldSingleEntryPHINodes(BB);
+				}
+				
+				DBG(const auto bb_name = BB->getName();)
+				const auto did_merge = MergeBlockIntoPredecessor(BB, nullptr, nullptr, nullptr);
+				DBG(if(did_merge) { errs() << ">>> merged bb into pred: " << bb_name << "\n"; })
+				
+				modified |= did_merge;
+			}
+			
+			return modified;
+		}
+		
+		void restructure_and_annotate() {
+			DBG(errs() << "> final CFG restructuring and annotating\n";)
+			StructuralAnalysis cfg_analysis;
+			cfg_analysis.analyze(*func);
+			DBG(cfg_analysis.write(errs());)
+			
+			// compute LoopInfo from the DT (ideally we could use the StructuralAnalysis info for this,
+			// but it doesn't provide (correct) back-edge info)
+			if(was_modified) {
+				DT->recalculate(*func);
+			}
+			LoopInfo LI(*DT);
+			
+			// first: gather all merge block information
+			struct merge_info {
+				BasicBlock* header_block;
+				BasicBlock* continue_block; // nullptr if not a loop (TODO: still needed with node?)
+				const SA::NodeTy* node;
+			};
+			std::unordered_map<BasicBlock*, std::vector<merge_info>> merge_blocks;
+			std::unordered_set<BasicBlock*> loop_header_blocks, selection_header_blocks;
+			node_recurse(*cfg_analysis.Net.begin(), cfg_analysis, [this, /*&cfg_analysis,*/
+                                                             &loop_header_blocks,
+                                                             &selection_header_blocks,
+                                                             &LI, &merge_blocks](const SA::NodeTy* node,
+                                                                                 BasicBlock* this_block) {
+				switch(node->nodeType) {
+					case SA::NaturalLoop: {
+						assert(node->exitBB != nullptr && "must have an exit block");
+						DBG(errs() << ">> @loop: " << this_block->getName() << "\n";)
+						Loop* loop = LI.getLoopFor(this_block);
+						assert(loop != nullptr && "must have LoopInfo for this loop BB!");
+						BasicBlock* back_edge = loop->getLoopLatch();
+						if(back_edge == nullptr) {
+							report_fatal_error("did not find the loop back-edge");
+							llvm_unreachable("did not find the loop back-edge");
+						}
+						DBG(errs() << "loop back-edge: " << back_edge->getName() << "\n";)
+						DBG(errs() << "loop merge: " << node->exitBB->getName() << "\n";)
+						loop_header_blocks.emplace(this_block);
+						merge_blocks[node->exitBB].emplace_back(merge_info { this_block, back_edge, node });
+						break;
+					}
+					case SA::IfThen:
+					case SA::IfThenElse: {
+						BranchInst* branch = dyn_cast<BranchInst>(this_block->getTerminator());
+						assert(branch != nullptr && "if-then/if-then-else block must have a branch instruction");
+						if(branch->isConditional()) {
+							// if this selection header block is also a loop header block, split it before the branch,
+							// because blocks can't be both (according to the current spec)
+							// NOTE: will always encounter NaturalLoop before IfThen/IfThenElse of the same block
+							if(loop_header_blocks.count(this_block) > 0) {
+								this_block = SplitBlock(this_block, this_block->getTerminator(), DT, &LI);
+								was_modified = true;
+							}
+							
+							// check if either successor is a simple return block (only this block is it's predecessor)
+							// if so, make the other block the merge block (outcome doesn't matter if both return)
+							const bool is_simple_return_bb[] {
+								succ_empty(branch->getSuccessor(0)) &&
+								branch->getSuccessor(0)->getSinglePredecessor() == this_block,
+								succ_empty(branch->getSuccessor(1)) &&
+								branch->getSuccessor(1)->getSinglePredecessor() == this_block,
+							};
+							if(is_simple_return_bb[0] || is_simple_return_bb[1]) {
+								merge_blocks[is_simple_return_bb[0] ?
+											 branch->getSuccessor(1) :
+											 branch->getSuccessor(0)].emplace_back(merge_info { this_block, nullptr, node });
+							}
+							// if not, make the exit block the merge block
+							else {
+								assert(node->exitBB != nullptr && "must have an exit block");
+								merge_blocks[node->exitBB].emplace_back(merge_info { this_block, nullptr, node });
+							}
+							selection_header_blocks.emplace(this_block);
+						}
+						// else: don't need to handle unconditionals
+						break;
+					}
+					case SA::SelfLoop: {
+						report_fatal_error("CFG should no longer contain self-loops!");
+						llvm_unreachable("CFG should no longer contain self-loops!");
+					}
+					// TODO: how to handle other invalid types?
+					default: break;
+				}
+			});
+			
+			// then: add (loop|selection) merge annotations + possibly restructure the CFG if a merge block has multiple users
+			for(auto& merge_block : merge_blocks) {
+				// simple case: single-user merge block
+				if(merge_block.second.size() == 1) {
+					// loop
+					const auto& info = merge_block.second[0];
+					if(info.continue_block != nullptr) {
+						create_loop_merge(info.header_block->getTerminator(),
+										  merge_block.first, info.continue_block);
+					}
+					// selection
+					else {
+						create_selection_merge(info.header_block->getTerminator(), merge_block.first);
+					}
+					continue;
+				}
+				
+				// will always modify something after this
+				was_modified = true;
+				DBG(errs() << ">> multi-user merge block: " << merge_block.first->getName() << "\n";
+				for(const auto& info : merge_block.second) {
+					errs() << "\tuser: " << info.header_block->getName() << "\n";
+				}
+				errs() << "\n";)
+				
+				// figure out the merge tree we need to create
+				// (due to the merge-block dominator requirements we can't just simply create a chain or binary tree)
+				// NOTE: there will always be a single parent node and it will always be the first entry in
+				//       merge_block.second (unstructured -> structured cfg transformation makes sure of this)
+				struct merge_node {
+					const merge_info& info;
+					std::vector<merge_node> children;
+				};
+				const std::function<merge_node(const merge_info&)> merge_tree_recurse =
+					[&merge_tree_recurse, &merge_block/* , &cfg_analysis TODO: if needed */](const merge_info& info) {
+						merge_node node { info, {} };
+						for(auto* direct_child : info.node->childNode) {
+							if(!direct_child->isCombined) continue;
+							
+							std::deque<SA::NodeTy*> child_stack;
+							child_stack.emplace_back(direct_child);
+							while(!child_stack.empty()) {
+								auto* child = child_stack[0];
+								child_stack.pop_front();
+								if(!child->isCombined) continue;
+								
+								const auto iter = std::find_if(merge_block.second.cbegin(), merge_block.second.cend(),
+															   [child](const merge_info& entry) {
+																   return (entry.node == child);
+															   });
+								if(iter != merge_block.second.cend()) {
+									node.children.emplace_back(merge_tree_recurse(*iter));
+									break;
+								}
+								else {
+									for(auto* indirect_child : child->childNode) {
+										child_stack.emplace_back(indirect_child);
+									}
+								}
+							}
+						}
+						return node;
+					};
+				merge_node merge_tree = merge_tree_recurse(merge_block.second[0]);
+				
+				const std::function<void(const merge_node&, const uint32_t)> dump_merge_tree =
+					[&dump_merge_tree, &loop_header_blocks](const merge_node& node, const uint32_t level) {
+						for(uint32_t i = 0; i < level; ++i) errs() << "  ";
+						errs() << node.info.header_block->getName() << "(loop: " << (node.info.continue_block != nullptr ? 1 : 0) << ", " << loop_header_blocks.count(node.info.header_block) << ")\n";
+						for(const auto& child : node.children) {
+							dump_merge_tree(child, level + 1);
+						}
+					};
+				DBG(errs() << "## merge-tree:\n";
+				dump_merge_tree(merge_tree, 1);
+				errs() << "####\n";)
+				
+				// create the actual merge tree + insert resp. merge instructions
+				BasicBlock* merge_bb = merge_block.first;
+				const std::function<void(const merge_node&,
+										 BasicBlock*,
+										 const uint32_t,
+										 const uint32_t)> create_cfg_merge_tree =
+					[this, &create_cfg_merge_tree](const merge_node& node,
+												   BasicBlock* this_merge_block,
+												   const uint32_t level,
+												   const uint32_t child_idx) {
+						BasicBlock* header = node.info.header_block;
+						
+						std::vector<BasicBlock*> child_merge_bbs;
+						for(const auto& child : node.children) {
+							std::unordered_set<BasicBlock*> preds, visited;
+							// TODO: this is inefficient
+							const std::function<void(BasicBlock* bb)> get_preds =
+								[&get_preds, &preds, &visited, &this_merge_block](BasicBlock* bb) {
+									if(visited.count(bb) > 0) return;
+									visited.emplace(bb);
+									for(auto* succ : successors(bb)) {
+										if(succ == this_merge_block) {
+											preds.emplace(bb);
+										}
+										else {
+											get_preds(succ);
+										}
+									}
+								};
+							get_preds(child.info.header_block);
+							if(preds.empty()) continue;
+							
+							std::vector<BasicBlock*> preds_vec(preds.cbegin(), preds.cend());
+							DBG(errs() << "@" << header->getName() << ", split off:\n";
+							for(const auto* pred : preds_vec) {
+								errs() << "\tpred: " << pred->getName() << "\n";
+							})
+							
+							child_merge_bbs.emplace_back(SplitBlockPredecessors(this_merge_block, preds_vec,
+                                                                  ".merge", DT, nullptr, nullptr, false));
+						}
+						
+						uint32_t cidx = 0;
+						for(const auto& child : node.children) {
+							create_cfg_merge_tree(child, child_merge_bbs[cidx], level + 1, cidx);
+							cidx++;
+						}
+						
+						// add merge annotation
+						if(node.info.continue_block != nullptr) {
+							create_loop_merge(header->getTerminator(), this_merge_block, node.info.continue_block);
+						}
+						else {
+							create_selection_merge(header->getTerminator(), this_merge_block);
+						}
+						
+						return;
+					};
+				create_cfg_merge_tree(merge_tree, merge_bb, 0, 0);
+			}
+		}
+		
+		bool runOnFunction(Function &F) override {
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			was_modified = false;
+			
+#if 0 // enable to use the old CFG structurization
+			was_modified |= run_old_structurization(F);
+#else
+			was_modified |= run_structurization(F);
+#endif
+			
+			return was_modified;
+		}
+		
+		bool run_old_structurization(Function &F) {
+			DT = &getAnalysis<DominatorTreeWrapperPass>().getDomTree();
+			
+#if 0
+			DBG(
+				errs() << "#################### func before: " << F.getName() << " ####\n" << F << "\n";
+				//StructuralAnalysis::dumpCFGDot("func_initial.dot", F);
+			)
+#endif
+			
+			// initial unstructured -> structured control flow fixes
+			DBG(errs() << "> fixing unstructured control flow\n";)
+			StructuralTransform cfg_transform;
+			was_modified |= cfg_transform.transform(F);
+			
+			// after transformation/structurization (and sometimes in general), we might have very trivial BB branching (unconditional
+			// branch to a BB w/o any other incoming edges)
+			// -> this can throw off the analysis and following restructuring, so simplify these constructs
+			// NOTE: will also take care of folding PHIs
+			was_modified |= simplify_cfg(F);
+			
+			// apply simple cfg fixes
+			restructure_simple();
+			
+			// vulkan/spir-v specific annotations / cfg restructuring
+			restructure_and_annotate();
+			DBG({
+				StructuralAnalysis cfg_analysis;
+				cfg_analysis.analyze(F);
+				cfg_analysis.write(errs());
+			})
+			
+#if 0
+			DBG(
+				errs() << "#################### func after: " << F.getName() << " ####\n" << F << "\n";
+				//StructuralAnalysis::dumpCFGDot("func_after.dot", F);
+			)
+#endif
+				
+			DBG(errs() << "verifying ...\n";)
+#if !defined(NDEBUG)
+			const auto verify_failed = verifyFunction(F, &errs());
+			if (verify_failed) {
+				errs().flush();
+				assert(!verify_failed && "verification failed");
+			}
+#endif
+			DBG(errs() << "verifying done\n";)
+			
+			DBG(errs() << "## " << F.getName() << " modified? " << was_modified << "\n";)
+			
+			return was_modified;
+		}
+		
+		bool run_structurization(Function &F) {
+			// skip if we only have a single block that returns
+			if (auto ret_instr = dyn_cast_or_null<ReturnInst>(F.getEntryBlock().getTerminator())) {
+				return false;
+			}
+			was_modified = true;
+			
+			CFGNodePool node_pool(*ctx, F);
+			cfg_translator translator(F, *ctx, node_pool);
+			
+			CFGStructurizer structurizer(translator.get_entry_block(), node_pool, F, *ctx);
+			structurizer.run();
+			translator.cfg_to_llvm_ir(structurizer.get_entry_block(), true);
+			
+#if !defined(NDEBUG) || 1 // TODO: disable in release mode later on - for now, always keep it on
+			const auto verify_failed = verifyFunction(F, &errs());
+			if (verify_failed) {
+				errs().flush();
+				assert(!verify_failed && "verification failed");
+			}
+#endif
+			
+			return was_modified;
+		}
+	};
+
+}
+
+char CFGStructurization::ID = 0;
+FunctionPass *llvm::createCFGStructurizationPass() {
+	return new CFGStructurization();
+}
+INITIALIZE_PASS_BEGIN(CFGStructurization, "CFGStructurization", "CFGStructurization Pass", false, false)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(CFGStructurization, "CFGStructurization", "CFGStructurization Pass", false, false)
+
diff --git a/llvm/lib/Transforms/LibFloor/CMakeLists.txt b/llvm/lib/Transforms/LibFloor/CMakeLists.txt
new file mode 100644
index 000000000000..9d735328d897
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/CMakeLists.txt
@@ -0,0 +1,41 @@
+add_llvm_component_library(LLVMLibFloor
+  AddressSpaceFix.cpp
+  CFGStructurization.cpp
+  CMakeLists.txt
+  CUDAFinal.cpp
+  CUDAImage.cpp
+  FMACombiner.cpp
+  FloorImage.cpp
+  LibFloor.cpp
+  MetalFinal.cpp
+  MetalImage.cpp
+  PropagateRangeInfo.cpp
+  SPIRFinal.cpp
+  SPIRImage.cpp
+  StructuralAnalysis.cpp
+  StructuralTransform.cpp
+  VulkanFinal.cpp
+  VulkanImage.cpp
+  
+  cfg/cfg_structurizer.cpp
+  cfg/cfg_translator.cpp
+  cfg/node.cpp
+
+  ADDITIONAL_HEADER_DIRS
+  ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms
+  ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/LibFloor
+
+  DEPENDS
+  intrinsics_gen
+
+  COMPONENT_NAME
+  LibFloor
+
+  LINK_COMPONENTS
+  AggressiveInstCombine
+  Analysis
+  Core
+  InstCombine
+  Support
+  TransformUtils
+  )
diff --git a/llvm/lib/Transforms/LibFloor/CUDAFinal.cpp b/llvm/lib/Transforms/LibFloor/CUDAFinal.cpp
new file mode 100644
index 000000000000..6bc78ee318e6
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/CUDAFinal.cpp
@@ -0,0 +1,128 @@
+//===- CUDAFinal.cpp - CUDA final pass ------------------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// TODO
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <cxxabi.h>
+using namespace llvm;
+
+#define DEBUG_TYPE "CUDAFinal"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// CUDAFinal
+	struct CUDAFinal : public FunctionPass, InstVisitor<CUDAFinal> {
+		friend class InstVisitor<CUDAFinal>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		Instruction* alloca_insert { nullptr };
+		bool was_modified { false };
+		
+		CUDAFinal() : FunctionPass(ID) {
+			initializeCUDAFinalPass(*PassRegistry::getPassRegistry());
+		}
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			
+			// if not a kernel function, return (for now)
+			if(F.getCallingConv() != CallingConv::FLOOR_KERNEL) return false;
+			
+			// reset
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			was_modified = false;
+			
+			// visit everything in this function
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			visit(F);
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<CUDAFinal>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<CUDAFinal>::visit(I);
+		}
+		
+	};
+}
+
+char CUDAFinal::ID = 0;
+INITIALIZE_PASS_BEGIN(CUDAFinal, "CUDAFinal", "CUDAFinal Pass", false, false)
+INITIALIZE_PASS_END(CUDAFinal, "CUDAFinal", "CUDAFinal Pass", false, false)
+
+FunctionPass *llvm::createCUDAFinalPass() {
+	return new CUDAFinal();
+}
diff --git a/llvm/lib/Transforms/LibFloor/CUDAImage.cpp b/llvm/lib/Transforms/LibFloor/CUDAImage.cpp
new file mode 100644
index 000000000000..72714e5f1efb
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/CUDAImage.cpp
@@ -0,0 +1,819 @@
+//===- CUDAImage.cpp - CUDA-specific floor image transformations ----------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass implements the CUDA-specific floor image transformations, i.e.
+// floor.cuda.<read/write function>.*
+//   -> PTX texture/surface instructions (inline asm)
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/LibFloor/FloorImage.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <array>
+#include <optional>
+#include <cxxabi.h>
+using namespace llvm;
+
+#define DEBUG_TYPE "CUDAImage"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// CUDAImage
+	struct CUDAImage : public FloorImageBasePass {
+		static char ID; // Pass identification, replacement for typeid
+		
+		CUDAImage(const uint32_t image_capabilities_ = 0) :
+		FloorImageBasePass(ID, IMAGE_TYPE_ID::CUDA, image_capabilities_) {
+			initializeCUDAImagePass(*PassRegistry::getPassRegistry());
+		}
+		
+		static const char* type_to_geom(const COMPUTE_IMAGE_TYPE& image_type) {
+			switch (image_type & (COMPUTE_IMAGE_TYPE::__DIM_MASK |
+								  COMPUTE_IMAGE_TYPE::FLAG_ARRAY |
+								  COMPUTE_IMAGE_TYPE::FLAG_BUFFER |
+								  COMPUTE_IMAGE_TYPE::FLAG_CUBE |
+								  COMPUTE_IMAGE_TYPE::FLAG_DEPTH |
+								  COMPUTE_IMAGE_TYPE::FLAG_STENCIL |
+								  COMPUTE_IMAGE_TYPE::FLAG_MSAA)) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D:
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_BUFFER:
+					return "1d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_ARRAY:
+					return "a1d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D:
+					return "2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_ARRAY:
+					return "a2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+					return "2dms";
+				case COMPUTE_IMAGE_TYPE::IMAGE_3D:
+					return "3d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE:
+					return "cube";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE_ARRAY:
+					return "acube";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					return "a2dms";
+				default:
+					return nullptr;
+			}
+		}
+		
+		void handle_read_image(Instruction& I,
+							   const StringRef& func_name,
+							   llvm::Value* img_handle_arg,
+							   const COMPUTE_IMAGE_TYPE& image_type,
+							   llvm::ConstantInt* const_sampler_arg,
+							   llvm::Value* dyn_sampler_arg,
+							   llvm::Value* coord_arg,
+							   llvm::Value* layer_arg,
+							   llvm::Value* sample_arg,
+							   llvm::Value* offset_arg,
+							   const SmallVector<llvm::Value*, 3>& offset_elems,
+							   const bool is_offset,
+							   llvm::Value* lod_or_bias_arg,
+							   const bool is_lod_or_bias, // true: lod, false: bias
+							   llvm::Value* dpdx_arg,
+							   llvm::Value* dpdy_arg,
+							   const bool is_gradient,
+							   const COMPARE_FUNCTION& compare_function,
+							   llvm::Value* compare_value_arg,
+							   const bool is_compare) override {
+			SmallVector<llvm::Type*, 16> asm_arg_types;
+			SmallVector<llvm::Value*, 16> asm_args;
+			std::string constraints_str = "";
+			
+			// -> return data
+			std::string dtype;
+			llvm::Type* ret_type, *ret_vec_type;
+			if(func_name == "floor.cuda.read_image.float") {
+				dtype = "f32";
+				constraints_str = "=f,=f,=f,=f";
+				ret_type = llvm::StructType::get(*ctx, std::vector<llvm::Type*> {{
+					llvm::Type::getFloatTy(*ctx),
+					llvm::Type::getFloatTy(*ctx),
+					llvm::Type::getFloatTy(*ctx),
+					llvm::Type::getFloatTy(*ctx)
+				}});
+				ret_vec_type = llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4);
+			}
+			else if(func_name == "floor.cuda.read_image.int") {
+				dtype = "s32";
+				constraints_str = "=r,=r,=r,=r";
+				ret_type = llvm::StructType::get(*ctx, std::vector<llvm::Type*> {{
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx)
+				}});
+				ret_vec_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name == "floor.cuda.read_image.uint") {
+				dtype = "u32";
+				constraints_str = "=r,=r,=r,=r";
+				ret_type = llvm::StructType::get(*ctx, std::vector<llvm::Type*> {{
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx),
+					llvm::Type::getInt32Ty(*ctx)
+				}});
+				ret_vec_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			// unknown -> ignore
+			else return;
+			
+			constraints_str += ",l"; // u64 tex handle
+			asm_arg_types.push_back(llvm::Type::getInt64Ty(*ctx));
+			asm_args.push_back(img_handle_arg);
+			
+			// -> geom
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr; // .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(image_type);
+			const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type);
+			
+			// -> coords
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			const auto coord_dim = coord_vec_type->getNumElements();
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if(is_msaa && !coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer for msaa images");
+				return;
+			}
+			
+			// TODO: add s/w support for reading cube maps with integer coords (u, v, face, *layer)
+			if(is_cube && !coord_type->isFloatTy()) {
+				ctx->emitError(&I, "coordinate type must be float for cube images");
+				return;
+			}
+			
+			std::string ctype = (coord_type->isFloatTy() ? "f32" : "s32");
+			std::string coords_placeholders;
+			const std::string coord_type_str = (coord_type->isFloatTy() ? "f" : "r");
+			uint32_t asm_arg_idx = 5;
+			if(is_msaa) {
+				asm_arg_types.push_back(sample_arg->getType());
+				asm_args.push_back(sample_arg);
+				constraints_str += ",r";
+				coords_placeholders += " $";
+				coords_placeholders += std::to_string(asm_arg_idx++);
+			}
+			if(is_array) {
+				asm_arg_types.push_back(layer_arg->getType());
+				asm_args.push_back(layer_arg);
+				constraints_str += ",r";
+				coords_placeholders += (!is_msaa ? " $" : ", $");
+				coords_placeholders += std::to_string(asm_arg_idx++);
+			}
+			for(uint32_t i = 0; i < coord_dim; ++i) {
+				asm_arg_types.push_back(coord_type);
+				asm_args.push_back(builder->CreateExtractElement(coord_arg, builder->getInt32(i)));
+				constraints_str += "," + coord_type_str;
+				coords_placeholders += (i == 0 && asm_arg_idx == 5 ? " $" : ", $");
+				coords_placeholders += std::to_string(asm_arg_idx++);
+			}
+			
+			// append (ignored) 0 coordinate if #coordinates == 3
+			if((coord_dim + (is_msaa ? 1 : 0) + (is_array ? 1 : 0)) == 3) {
+				coords_placeholders += (coord_type->isFloatTy() ? ", 0.0" : ", 0");
+			}
+			
+			// -> lod
+			std::string mipmap_prefix = "";
+			std::string lod_str = "";
+			if(is_lod_or_bias) {
+				mipmap_prefix = "level.";
+				lod_str = ", ";
+				// NOTE: lod type must match coord elem type, cast if necessary
+				if(const auto const_lod = dyn_cast_or_null<ConstantInt>(lod_or_bias_arg)) {
+					if(coord_type->isIntegerTy()) {
+						lod_str += std::to_string(const_lod->getSExtValue());
+					}
+					else {
+						// convert to float
+						lod_str += std::to_string((float)const_lod->getSExtValue());
+					}
+				}
+				else if(const auto const_lod = dyn_cast_or_null<ConstantFP>(lod_or_bias_arg)) {
+					if(coord_type->isFloatTy()) {
+						lod_str += std::to_string(const_lod->getValueAPF().convertToFloat());
+					}
+					else {
+						// convert to int
+						lod_str += std::to_string((int32_t)round(const_lod->getValueAPF().convertToFloat()));
+					}
+				}
+				else {
+					asm_arg_types.push_back(coord_type);
+					if(coord_type == lod_or_bias_arg->getType()) {
+						asm_args.push_back(lod_or_bias_arg);
+					}
+					else {
+						// convert to appropriate type
+						asm_args.push_back(coord_type->isIntegerTy() ?
+										   builder->CreateFPToSI(lod_or_bias_arg, coord_type) :
+										   builder->CreateSIToFP(lod_or_bias_arg, coord_type));
+					}
+					lod_str += "$" + std::to_string(asm_arg_idx++);
+					constraints_str += (coord_type->isIntegerTy() ? ",r" : ",f");
+				}
+			}
+			
+			// -> gradient
+			std::string gradient_str = "";
+			if(is_gradient) {
+				mipmap_prefix = "grad.";
+				
+				// dpdx
+				gradient_str += ", { ";
+				for(uint32_t i = 0; i < coord_dim; ++i) {
+					asm_arg_types.push_back(builder->getFloatTy());
+					asm_args.push_back(builder->CreateExtractElement(dpdx_arg, builder->getInt32(i)));
+					gradient_str += (i == 0 ? "$" : ", $") + std::to_string(asm_arg_idx++);
+					constraints_str += ",f";
+				}
+				if(coord_dim == 3) gradient_str += ", 0.0";
+				
+				// dpdy
+				gradient_str += " }, { ";
+				for(uint32_t i = 0; i < coord_dim; ++i) {
+					asm_arg_types.push_back(builder->getFloatTy());
+					asm_args.push_back(builder->CreateExtractElement(dpdy_arg, builder->getInt32(i)));
+					gradient_str += (i == 0 ? "$" : ", $") + std::to_string(asm_arg_idx++);
+					constraints_str += ",f";
+				}
+				if(coord_dim == 3) gradient_str += ", 0.0";
+				gradient_str += " }";
+			}
+			
+			// -> offset
+			std::string offset_str = "";
+			if(is_offset) {
+				for(uint32_t i = 0; i < coord_dim; ++i) {
+					if(i != 0) offset_str += ", ";
+					if(const auto const_offset_elem = dyn_cast_or_null<ConstantInt>(offset_elems[i])) {
+						offset_str += std::to_string(const_offset_elem->getSExtValue());
+					}
+					else {
+						asm_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+						asm_args.push_back(offset_elems[i]);
+						offset_str += "$" + std::to_string(asm_arg_idx++);
+						constraints_str += ",r";
+					}
+				}
+				
+				// append ignored 0 offset
+				if(coord_dim == 3) {
+					offset_str += ", 0";
+				}
+			}
+			
+			// -> compare
+			std::string compare_str = "";
+			llvm::Value* compare_override = nullptr;
+			const bool has_hw_depth_compare = has_flag<IMAGE_CAPABILITY::DEPTH_COMPARE>(image_capabilities);
+			if(is_compare && has_hw_depth_compare) {
+				// must have float coords with depth compare
+				if(!coord_type->isFloatTy()) {
+					ctx->emitError(&I, "coordinate type must be float when using depth compare");
+					return;
+				}
+				
+				// check for unsupported image types
+				switch(image_type) {
+					case COMPUTE_IMAGE_TYPE::IMAGE_1D:
+					case COMPUTE_IMAGE_TYPE::IMAGE_1D_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_1D_BUFFER:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+					case COMPUTE_IMAGE_TYPE::IMAGE_2D:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_2D_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+					case COMPUTE_IMAGE_TYPE::IMAGE_CUBE:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_CUBE_ARRAY:
+						// all supported
+						break;
+						
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+					case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+					case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					case COMPUTE_IMAGE_TYPE::IMAGE_3D:
+					default:
+						ctx->emitError(&I, "image type does not support depth compare");
+						return;
+				}
+				
+				asm_arg_types.push_back(builder->getFloatTy());
+				asm_args.push_back(compare_value_arg);
+				constraints_str += ",f";
+				compare_str = "$" + std::to_string(asm_arg_idx++);
+				
+				// directly skip/eval NEVER and ALWAYS compare functions
+				// while this would be DCE'ed anyways, this is somewhat cleaner
+				// NOTE: we still want all of the usual error messages and checking, which is why we're not doing this earlier
+				if(compare_function == COMPARE_FUNCTION::NEVER) {
+					compare_override = ConstantFP::get(builder->getFloatTy(), 0.0f);
+				}
+				else if(compare_function == COMPARE_FUNCTION::ALWAYS) {
+					compare_override = ConstantFP::get(builder->getFloatTy(), 1.0f);
+				}
+			}
+			
+			// -> build asm string
+			std::string asm_str = "tex." + mipmap_prefix + geom + ".v4." + dtype + "." + ctype;
+			asm_str += " { $0, $1, $2, $3 },";
+			asm_str += " [$4, {" + coords_placeholders + " }]";
+			if(is_lod_or_bias) {
+				asm_str += lod_str;
+			}
+			if(is_gradient) {
+				asm_str += gradient_str;
+			}
+			if(is_offset) {
+				asm_str += ", { " + offset_str + " }";
+			}
+			if(is_compare && has_hw_depth_compare) {
+				asm_str += ", " + compare_str;
+			}
+			asm_str += ";";
+			
+			const auto asm_func_type = FunctionType::get(ret_type, asm_arg_types, false);
+			auto asm_func = InlineAsm::get(asm_func_type, asm_str, constraints_str, false /* non-volatile */);
+			auto asm_call = builder->CreateCall(asm_func, asm_args);
+			asm_call->setDoesNotAccessMemory(); // all reads are readnone (can be optimized away if unused)
+			asm_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			asm_call->setNotConvergent();
+			
+			//
+			llvm::Value* dst_vec = UndefValue::get(ret_vec_type);
+			if(compare_override == nullptr) {
+				// -> normal color read or h/w depth compare
+				if(!is_compare || has_hw_depth_compare) {
+					for(uint32_t i = 0; i < 4; ++i) {
+						auto scalar = builder->CreateExtractValue(asm_call, i);
+						dst_vec = builder->CreateInsertElement(dst_vec, scalar, builder->getInt32(i));
+					}
+				}
+				// -> s/w depth compare
+				else {
+					emulate_depth_compare(dst_vec, builder->CreateExtractValue(asm_call, 0), compare_function, compare_value_arg);
+				}
+			}
+			// -> compare override (NONE/NEVER/ALWAYS), don't emit tex instruction
+			else {
+				dst_vec = builder->CreateInsertElement(dst_vec, compare_override, builder->getInt32(0));
+			}
+			
+			//
+			I.replaceAllUsesWith(dst_vec);
+			I.eraseFromParent();
+		}
+		
+		CallInst* emit_write_image(llvm::Value* img_handle_arg,
+								   const COMPUTE_IMAGE_TYPE& format_type,
+								   const bool& is_normalized,
+								   const uint32_t& image_channel_count,
+								   llvm::Value* coord_arg,
+								   llvm::Type* coord_type,
+								   FixedVectorType* coord_vec_type,
+								   llvm::Value* layer_arg,
+								   const std::array<llvm::Value*, 4>& orig_data_args,
+								   const std::string& geom,
+								   const bool is_array,
+								   const bool is_float,
+								   const bool is_int,
+								   const DebugLoc& debug_loc) {
+			SmallVector<llvm::Type*, 16> asm_arg_types;
+			SmallVector<llvm::Value*, 16> asm_args;
+			
+			std::string constraints_str = "l"; // u64 surf handle
+			asm_arg_types.push_back(llvm::Type::getInt64Ty(*ctx));
+			asm_args.push_back(img_handle_arg);
+			
+			// -> coords
+			const auto coord_dim = coord_vec_type->getNumElements();
+			std::string coords_placeholders;
+			static const uint32_t coord_start_idx = 1;
+			uint32_t coord_idx = 0;
+			size_t x_coord_idx = 0;
+			if (is_array) {
+				asm_arg_types.push_back(layer_arg->getType());
+				asm_args.push_back(layer_arg);
+				constraints_str += ",r";
+				coords_placeholders += " $";
+				coords_placeholders += std::to_string(coord_start_idx + coord_idx++);
+			}
+			for (uint32_t i = 0; i < coord_dim; ++i) {
+				asm_arg_types.push_back(coord_type);
+				auto coord_elem = builder->CreateExtractElement(coord_arg, builder->getInt32(i));
+				if (i == 0) {
+					x_coord_idx = asm_args.size();
+				}
+				asm_args.push_back(coord_elem);
+				constraints_str += ",r";
+				coords_placeholders += (coord_idx == 0 ? " $" : ", $");
+				coords_placeholders += std::to_string(coord_start_idx + coord_idx++);
+			}
+			
+			// append (ignored) 0 coordinate if #coordinates == 3
+			if ((coord_dim + (is_array ? 1 : 0)) == 3) {
+				coords_placeholders += ", 0";
+			}
+			
+			// -> data
+			const auto write_channel_count = (image_channel_count == 3 ? 4 : image_channel_count);
+			
+			std::array<llvm::Value*, 4> data_args = orig_data_args;
+			std::string dtype, rtype;
+			if (is_normalized) {
+				// need to normalize 32-bit float -> 8-bit or 16-bit unsigned/signed int
+				if (format_type != COMPUTE_IMAGE_TYPE::FORMAT_8 && format_type != COMPUTE_IMAGE_TYPE::FORMAT_16) {
+					assert(false && "invalid normalized write format - should not be here!");
+					ctx->emitError("invalid normalized write format (expected 8-bit or 16-bit dst format");
+					return nullptr;
+				}
+				
+				bool is_8_bit = true;
+				if (format_type == COMPUTE_IMAGE_TYPE::FORMAT_8) {
+					dtype = "b8";
+				} else {
+					dtype = "b16";
+					is_8_bit = false;
+				}
+				rtype = "h"; // can't go lower than 16-bit
+				
+				for (uint32_t i = 0; i < write_channel_count; ++i) {
+					data_args[i] = builder->CreateFMul(data_args[i],
+													   ConstantFP::get(builder->getFloatTy(),
+																	   is_int ?
+																	   (is_8_bit ? 127.0 : 32767.0) :
+																	   (is_8_bit ? 255.0 : 65535.0)));
+					data_args[i] = builder->CreateFPToUI(data_args[i],
+														 is_8_bit ? builder->getInt8Ty() : builder->getInt16Ty());
+				}
+			} else {
+				switch (format_type) {
+					case COMPUTE_IMAGE_TYPE::FORMAT_8:
+						dtype = "b8";
+						rtype = "h"; // can't go lower than 16-bit
+						break;
+					case COMPUTE_IMAGE_TYPE::FORMAT_16:
+						dtype = "b16";
+						rtype = (is_float ? "f" : "h");
+						break;
+					case COMPUTE_IMAGE_TYPE::FORMAT_24:
+					case COMPUTE_IMAGE_TYPE::FORMAT_32_8:
+					case COMPUTE_IMAGE_TYPE::FORMAT_32:
+						dtype = "b32";
+						rtype = (is_float ? "f" : "r");
+						break;
+					default:
+						assert(false && "invalid write format - should not be here!");
+						ctx->emitError("invalid write format");
+						return nullptr;
+				}
+				
+				// need to trunc 32-bit data to 16-bit (for 8-bit/16-bit int/uint writes)
+				if (rtype == "h") {
+					for (uint32_t i = 0; i < write_channel_count; ++i) {
+						builder->CreateTrunc(data_args[i], builder->getInt16Ty());
+					}
+				}
+			}
+			
+			// we know the written binary data size now -> update x coordinate offset
+			asm_args[x_coord_idx] = builder->CreateMul(asm_args[x_coord_idx],
+													   builder->getInt32(write_channel_count *
+																		 (dtype == "b16" ? 2 :
+																		  (dtype == "b32" ? 4 : 1 /* b8 */))));
+			
+			std::string data_placeholders;
+			uint32_t data_idx = coord_start_idx + coord_idx;
+			for (uint32_t i = 0; i < write_channel_count; ++i) {
+				asm_arg_types.push_back(data_args[i]->getType());
+				asm_args.push_back(data_args[i]);
+				constraints_str += "," + rtype;
+				data_placeholders += (i == 0 ? " $" : ", $");
+				data_placeholders += std::to_string(data_idx++);
+			}
+			
+			// -> build asm string
+			std::string asm_str = "sust.b." + geom + ".";
+			asm_str += (image_channel_count == 1 ? "" : (image_channel_count == 2 ? "v2." : "v4."));
+			asm_str += dtype + ".";
+			asm_str += "zero"; // ignore out-of-bounds writes (TODO: might want to trap in debug mode?)
+			
+			asm_str += " [$0, {" + coords_placeholders + " }],";
+			asm_str += " {" + data_placeholders + " };";
+			
+			const auto asm_func_type = FunctionType::get(builder->getVoidTy(), asm_arg_types, false);
+			auto asm_func = InlineAsm::get(asm_func_type, asm_str, constraints_str, true /* volatile */);
+			auto asm_call = builder->CreateCall(asm_func, asm_args);
+			asm_call->setDebugLoc(debug_loc); // keep debug loc
+			asm_call->setNotConvergent();
+			
+			return asm_call;
+		}
+		
+		void handle_write_image(Instruction& I,
+								const StringRef& func_name,
+								llvm::Value* img_handle_arg,
+								const COMPUTE_IMAGE_TYPE& full_image_type,
+								const COMPUTE_IMAGE_TYPE& /* image_type */,
+								const COMPUTE_IMAGE_TYPE& /* format_type*/,
+								const COMPUTE_IMAGE_TYPE& /* data_type */,
+								llvm::Value* rt_image_type,
+								const bool& is_normalized,
+								const uint32_t& image_channel_count,
+								llvm::Value* coord_arg,
+								llvm::Value* layer_arg,
+								// NOTE: lod is handled on the library side
+								llvm::Value* /* lod_arg */,
+								const bool /* is_lod */,
+								llvm::Value* data_arg) override {
+			//// more arg checking
+			
+			// run-time type must be set
+			if (!rt_image_type) {
+				ctx->emitError(&I, "missing run-time image type");
+				return;
+			}
+			
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			if (!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if (!coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer");
+				return;
+			}
+			
+			const auto is_float = (func_name == "floor.cuda.write_image.float");
+			const auto is_int = (func_name == "floor.cuda.write_image.int");
+			const auto is_uint = (func_name == "floor.cuda.write_image.uint");
+			if (!is_float && !is_int && !is_uint) {
+				return; // unknown -> ignore
+			}
+			
+			// -> geom
+			const auto geom_cstr = type_to_geom(full_image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr; // .1d, .2d, .3d, .a1d, .a2d
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(full_image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(full_image_type);
+			const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(full_image_type);
+			
+			// cube and msaa formats are not writable by cuda/ptx
+			if (is_cube || is_msaa) {
+				ctx->emitError(&I, "invalid image type - type is not writable");
+				return;
+			}
+			
+			const auto& debug_loc = I.getDebugLoc();
+			const auto img_type_int_type = builder->getInt64Ty();
+			
+			std::array<llvm::Value*, 4> data_args {{
+				builder->CreateExtractElement(data_arg, builder->getInt32(0)),
+				builder->CreateExtractElement(data_arg, builder->getInt32(1)),
+				builder->CreateExtractElement(data_arg, builder->getInt32(2)),
+				builder->CreateExtractElement(data_arg, builder->getInt32(3))
+			}};
+			
+			// emits a large switch -> image asm write, for all valid run-time image types
+			const auto emit_writes = [&](const std::optional<uint32_t> image_channel_count) {
+				auto mask = uint64_t(COMPUTE_IMAGE_TYPE::__FORMAT_MASK | COMPUTE_IMAGE_TYPE::__DATA_TYPE_MASK | COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED);
+				if (!image_channel_count) {
+					mask |= uint64_t(COMPUTE_IMAGE_TYPE::__CHANNELS_MASK);
+				}
+				auto rt_cmp_val = builder->CreateAnd(rt_image_type, mask);
+				auto def_unreachable_block = BasicBlock::Create(*ctx, "write_unreachable", func);
+				new UnreachableInst(*ctx, def_unreachable_block);
+				
+				auto start_block = I.getParent();
+				auto end_block = start_block->splitBasicBlock(&I, "write_end");
+				builder->SetInsertPoint(start_block->getTerminator());
+				llvm::SwitchInst* sw = builder->CreateSwitch(rt_cmp_val, def_unreachable_block, is_float ? 7 : 4);
+				start_block->getTerminator()->eraseFromParent();
+				
+				const bool is_fixed_channel_count = image_channel_count.has_value();
+				const auto channel_bit = [&](const uint32_t channel_count) {
+					return (is_fixed_channel_count ? COMPUTE_IMAGE_TYPE::NONE :
+							COMPUTE_IMAGE_TYPE(uint64_t(channel_count - 1u) << uint64_t(COMPUTE_IMAGE_TYPE::__CHANNELS_SHIFT)));
+				};
+				const auto emit_block = [&](const std::string& name, COMPUTE_IMAGE_TYPE base_format, COMPUTE_IMAGE_TYPE case_rt_type, const uint32_t channel_count,
+											const bool is_float, const bool is_int) {
+					auto block = BasicBlock::Create(*ctx, "write_" + name, func);
+					sw->addCase(ConstantInt::get(img_type_int_type, uint64_t(case_rt_type | channel_bit(channel_count))), block);
+					builder->SetInsertPoint(block);
+					emit_write_image(img_handle_arg, base_format, has_flag<COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED>(case_rt_type), channel_count,
+									 coord_arg, coord_type, coord_vec_type, layer_arg, data_args, geom, is_array, is_float, is_int, debug_loc);
+					BranchInst::Create(end_block, block);
+					return block;
+				};
+				
+				for (uint32_t channel_count = (image_channel_count ? *image_channel_count : 1); channel_count <= (image_channel_count ? *image_channel_count : 4); ++channel_count) {
+					if (is_float) {
+						emit_block("unorm8", COMPUTE_IMAGE_TYPE::FORMAT_8, COMPUTE_IMAGE_TYPE::FORMAT_8 | COMPUTE_IMAGE_TYPE::UINT | COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED, channel_count, false, false);
+						emit_block("snorm8", COMPUTE_IMAGE_TYPE::FORMAT_8, COMPUTE_IMAGE_TYPE::FORMAT_8 | COMPUTE_IMAGE_TYPE::INT | COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED, channel_count, false, true);
+						emit_block("unorm16", COMPUTE_IMAGE_TYPE::FORMAT_16, COMPUTE_IMAGE_TYPE::FORMAT_16 | COMPUTE_IMAGE_TYPE::UINT | COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED, channel_count, false, false);
+						emit_block("snorm16", COMPUTE_IMAGE_TYPE::FORMAT_16, COMPUTE_IMAGE_TYPE::FORMAT_16 | COMPUTE_IMAGE_TYPE::INT | COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED, channel_count, false, true);
+						emit_block("f16", COMPUTE_IMAGE_TYPE::FORMAT_16, COMPUTE_IMAGE_TYPE::FORMAT_16 | COMPUTE_IMAGE_TYPE::FLOAT, channel_count, true, false);
+						auto f32_block = emit_block("f32", COMPUTE_IMAGE_TYPE::FORMAT_32, COMPUTE_IMAGE_TYPE::FORMAT_32 | COMPUTE_IMAGE_TYPE::FLOAT, channel_count, true, false);
+						// NOTE: 32-bit float + 8-bit stencil contains exactly the same code as only 32-bit float -> reuse "f32"
+						sw->addCase(ConstantInt::get(img_type_int_type, uint64_t(COMPUTE_IMAGE_TYPE::FORMAT_32_8 | COMPUTE_IMAGE_TYPE::FLOAT | channel_bit(channel_count))), f32_block);
+					} else if (is_uint) {
+						emit_block("ui8", COMPUTE_IMAGE_TYPE::FORMAT_8, COMPUTE_IMAGE_TYPE::FORMAT_8 | COMPUTE_IMAGE_TYPE::UINT, channel_count, false, false);
+						emit_block("ui16", COMPUTE_IMAGE_TYPE::FORMAT_16, COMPUTE_IMAGE_TYPE::FORMAT_16 | COMPUTE_IMAGE_TYPE::UINT, channel_count, false, false);
+						emit_block("ui24", COMPUTE_IMAGE_TYPE::FORMAT_24, COMPUTE_IMAGE_TYPE::FORMAT_24 | COMPUTE_IMAGE_TYPE::UINT, channel_count, false, false);
+						emit_block("ui32", COMPUTE_IMAGE_TYPE::FORMAT_32, COMPUTE_IMAGE_TYPE::FORMAT_32 | COMPUTE_IMAGE_TYPE::UINT, channel_count, false, false);
+					} else if (is_int) {
+						emit_block("i8", COMPUTE_IMAGE_TYPE::FORMAT_8, COMPUTE_IMAGE_TYPE::FORMAT_8 | COMPUTE_IMAGE_TYPE::INT, channel_count, false, true);
+						emit_block("i16", COMPUTE_IMAGE_TYPE::FORMAT_16, COMPUTE_IMAGE_TYPE::FORMAT_16 | COMPUTE_IMAGE_TYPE::INT, channel_count, false, true);
+						emit_block("i24", COMPUTE_IMAGE_TYPE::FORMAT_24, COMPUTE_IMAGE_TYPE::FORMAT_24 | COMPUTE_IMAGE_TYPE::INT, channel_count, false, true);
+						emit_block("i32", COMPUTE_IMAGE_TYPE::FORMAT_32, COMPUTE_IMAGE_TYPE::FORMAT_32 | COMPUTE_IMAGE_TYPE::INT, channel_count, false, true);
+					}
+				}
+			};
+			
+			if (has_flag<COMPUTE_IMAGE_TYPE::FLAG_FIXED_CHANNELS>(full_image_type)) {
+				// -> specific channel count is known
+				emit_writes(image_channel_count);
+			} else {
+				// -> channel count is not known, need to emit for { 1, 2, 3, 4 } channels
+				emit_writes({});
+			}
+			
+			// NOTE: write must/can not have any uses
+			assert(I.getNumUses() == 0);
+			I.eraseFromParent();
+		}
+		
+		void handle_get_image_dim(Instruction& I,
+								  const StringRef& func_name,
+								  llvm::Value* img_handle_arg,
+								  const COMPUTE_IMAGE_TYPE& full_image_type,
+								  const COMPUTE_IMAGE_TYPE& image_type,
+								  llvm::Value* lod_arg) override {
+			// gather info
+			const auto dim_count = image_dim_count(image_type);
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			// tex if readable, surf otherwise
+			const auto is_tex = has_flag<COMPUTE_IMAGE_TYPE::READ>(full_image_type);
+
+			// query/get function base
+			const auto query_image = [&](const std::string& query_name) {
+				SmallVector<llvm::Type*, 2> func_arg_types;
+				SmallVector<llvm::Value*, 2> func_args;
+				func_arg_types.push_back(img_handle_arg->getType());
+				func_args.push_back(img_handle_arg);
+				if (is_tex) {
+					func_arg_types.push_back(lod_arg->getType());
+					func_args.push_back(lod_arg);
+				}
+				
+				// -> build asm call
+				std::string asm_str = (is_tex ? "txq.level." : "suq.") + query_name + ".b32 ";
+				std::string constraints_str = "=r,l";
+				asm_str += "$0, [$1]";
+				if (is_tex) {
+					asm_str += ", $2";
+					constraints_str += ",r";
+				}
+				asm_str += ";";
+				
+				// create the asm call
+				const auto asm_func_type = FunctionType::get(llvm::Type::getInt32Ty(*ctx), func_arg_types, false);
+				auto asm_func = InlineAsm::get(asm_func_type, asm_str, constraints_str, false /* !volatile */);
+				auto get_call = builder->CreateCall(asm_func, func_args);
+				get_call->setConvergent();
+				get_call->setOnlyAccessesArgMemory();
+				get_call->setDoesNotThrow();
+				get_call->setOnlyReadsMemory(); // all get_* calls are readonly (can be optimized away if unused)
+				get_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+				return get_call;
+			};
+			
+			// we have to a return a full image dim query for all dims of the image (type dependent)
+			// order is: width [, height] [, depth], [, layer_count]
+			// non-existing dims are set to 0
+			const auto ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			llvm::Value* ret_vec = UndefValue::get(ret_type);
+			uint32_t ret_vec_idx = 0;
+			// all images have a width
+			ret_vec = builder->CreateInsertElement(ret_vec, query_image("width"), builder->getInt32(ret_vec_idx++));
+			if (dim_count >= 2) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("height"), builder->getInt32(ret_vec_idx++));
+			}
+			if (dim_count >= 3) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("depth"), builder->getInt32(ret_vec_idx++));
+			}
+			if (is_array) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("array_size"), builder->getInt32(ret_vec_idx++));
+			}
+			// NOTE: while cube maps technically have 6 layers, this number is not stored in the image dim
+			
+			// fill remaining components with 0
+			for (uint32_t vec_idx = ret_vec_idx; vec_idx < 4; ++vec_idx) {
+				ret_vec = builder->CreateInsertElement(ret_vec, builder->getInt32(0), builder->getInt32(ret_vec_idx++));
+			}
+			
+			//
+			I.replaceAllUsesWith(ret_vec);
+			I.eraseFromParent();
+		}
+		
+	};
+}
+
+char CUDAImage::ID = 0;
+INITIALIZE_PASS_BEGIN(CUDAImage, "CUDAImage", "CUDAImage Pass", false, false)
+INITIALIZE_PASS_END(CUDAImage, "CUDAImage", "CUDAImage Pass", false, false)
+
+FunctionPass *llvm::createCUDAImagePass(const uint32_t image_capabilities) {
+	return new CUDAImage(image_capabilities);
+}
diff --git a/llvm/lib/Transforms/LibFloor/FMACombiner.cpp b/llvm/lib/Transforms/LibFloor/FMACombiner.cpp
new file mode 100644
index 000000000000..2d36efa5f553
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/FMACombiner.cpp
@@ -0,0 +1,784 @@
+//===- FMACombiner.cpp - --------------------------------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass does fma combining/folding in fadd, fmul, fsub and fma instructions.
+//
+// NOTE:
+// * blatently copied from DAGCombiner, with some additions and modifications
+// * except for fpext this has all "aggressiveness" enabled
+//
+// TODO: canonicalize fadd/fsub/fmul/fneg
+// TODO: add general fmul/fneg handling
+// TODO: try combining in reverse, this might give more interesting results
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/IR/PatternMatch.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <unordered_map>
+#include <unordered_set>
+#include <deque>
+#include <array>
+using namespace llvm;
+
+#define DEBUG_TYPE "FMACombiner"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+// enable to add the line # to the name of new fma instructions
+//#define DEBUG_FMA 1
+
+namespace {
+	// FMACombiner
+	struct FMACombiner : public FunctionPass, InstVisitor<FMACombiner> {
+		friend class InstVisitor<FMACombiner>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		static constexpr const char fma_prefix[] { "_Z3fma" };
+		static constexpr const char fma_half[] { "_Z3fmaDhDhDh" };
+		static constexpr const char fma_float[] { "_Z3fmafff" };
+		static constexpr const char fma_double[] { "_Z3fmaddd" };
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		bool is_kernel_func { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		
+		std::unordered_set<Instruction*> unreachable_kill_list;
+		
+		FMACombiner() : FunctionPass(ID) {
+			initializeFMACombinerPass(*PassRegistry::getPassRegistry());
+		}
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			
+			// determine this function type + exit if it isn't a kernel or shader function
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			if(!is_kernel_func && !is_vertex_func && !is_fragment_func) return false;
+			
+			DBG(errs() << "> FMA combiner: "; errs().write_escaped(F.getName()) << '\n';)
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			// visit everything in this function
+			DBG(errs() << "> FMA combiner pass #1\n";)
+			visit(F);
+			
+			// for now: make a second pass over the function, there can still by folding opportunities
+			// TODO: try to already fold things when emitting new instructions, then this might not be necessary
+			DBG(errs() << "> FMA combiner pass #2\n";)
+			visit(F);
+			
+			DBG(errs() << "< FMA combiner done\n";)
+			
+			// always modified
+			return true; // TODO: only if actually modified
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<FMACombiner>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<FMACombiner>::visit(I);
+		}
+		
+		// fma is a function call -> forward it
+		void visitCallInst(CallInst &I) {
+			if(I.getCalledFunction()->getName().startswith(fma_prefix)) {
+				visitFMA(I);
+				return;
+			}
+		}
+		
+		// properly kills 'inst' and replaces all its uses with 'repl' if 'repl' is not nullptr
+		static void kill_instruction(Instruction* inst, Value* repl = nullptr) {
+			DBG(errs() << "removing: " << *inst;)
+			if(repl != nullptr) {
+				DBG(errs() << " => repl: " << *repl << "\n";)
+				inst->replaceAllUsesWith(repl);
+			}
+			DBG(else errs() << "\n";)
+			inst->dropAllReferences();
+			inst->removeFromParent();
+		}
+		// returns the opcode of 'V' if 'V' is an instruction, otherwise returns 0
+		static uint32_t get_opcode(llvm::Value* V) {
+			if(auto instr = dyn_cast<Instruction>(V)) {
+				return instr->getOpcode();
+			}
+			return 0;
+		};
+		// creates a negate instruction of 'op', inserted before 'insert_before'
+		Value* create_negate(Value* op, Instruction* insert_before) {
+			// use builder so that we can constant fold
+			builder->SetInsertPoint(insert_before);
+			return builder->CreateFNeg(op, op->hasName() ? op->getName() + ".neg" : "neg");
+		}
+		// creates a negate instruction of either 'op_0' or 'op_1', prefering the one which is a constant value,
+		// if neither is a constant, will use 'op_0', returns the instruction and 0 or 1, depending on which op was negated
+		std::pair<Value*, uint32_t> create_negate(Value* op_0, Value* op_1, Instruction* insert_before) {
+			builder->SetInsertPoint(insert_before);
+			uint32_t op_num = 0;
+			if(isa<ConstantFP>(op_0)) {
+				op_num = 0;
+			}
+			else if(isa<ConstantFP>(op_1)) {
+				op_num = 1;
+			}
+			auto op = (op_num == 0 ? op_0 : op_1);
+			return { builder->CreateFNeg(op, op->hasName() ? op->getName() + ".neg" : "neg"), op_num };
+		}
+#if !defined(DEBUG_FMA)
+		Instruction* emit_fma(
+#else
+#define emit_fma(...) emit_fma_(__LINE__, __VA_ARGS__)
+		Instruction* emit_fma_(uint32_t line,
+#endif
+							  Value* a, Value* b, Value* c,
+							  Instruction* repl,
+							  Instruction* opt_repl = nullptr,
+							  // if nullptr, insert before repl
+							  Instruction* insert_before = nullptr,
+							  bool is_remove = true) {
+			// TODO: already do simple constant folding in here
+			//      +fma(x, y, 0) -> fmul(x, y)
+			//      +fma(0, y, z) || fma(x, 0, z) -> z
+			//      +fma(c1, c2, z) -> fadd(z, c1*c2)
+			//      +fma(x, 1, z) || fma(1, y, z) -> fadd(x|y, z)
+			// -> add fma constant folder helper function?
+			
+			auto fp_type = a->getType();
+			if(fp_type != b->getType() || fp_type != c->getType()) {
+				// only replace with fma if all three operands have the same type
+				return nullptr;
+			}
+			if(!fp_type->isHalfTy() &&
+			   !fp_type->isFloatTy() &&
+			   !fp_type->isDoubleTy()) {
+				// must be 16-bit, 32-bit or 64-bit fp type
+				return nullptr;
+			}
+			
+			std::vector<Type*> param_types(3, fp_type);
+			const auto func_type = llvm::FunctionType::get(fp_type, param_types, false);
+			
+			std::string func_name;
+			if(fp_type->isHalfTy()) func_name = fma_half;
+			else if(fp_type->isFloatTy()) func_name = fma_float;
+			else if(fp_type->isDoubleTy()) func_name = fma_double;
+			else llvm_unreachable("invalid fp type");
+			
+			// emit in canonical form if a is const: fma(non-const a, const b, any c)
+			const auto is_a_const = isa<ConstantFP>(a);
+			const auto is_b_const = isa<ConstantFP>(b);
+			SmallVector<Value*, 3> args {
+				is_a_const && !is_b_const ? b : a,
+				is_a_const && !is_b_const ? a : b,
+				c
+			};
+			auto CI = CallInst::Create(M->getOrInsertFunction(func_name, func_type), args,
+									   (repl->hasName() ? repl->getName() + ".fma" : "fma")
+#if defined(DEBUG_FMA)
+									   + "_" + std::to_string(line) + "_"
+#endif
+									   , (insert_before == nullptr ? repl : insert_before));
+			CI->setCallingConv(CallingConv::FLOOR_FUNC);
+			CI->setDoesNotAccessMemory();
+			CI->setNotConvergent();
+			CI->setDebugLoc(repl->getDebugLoc()); // keep debug loc of first repl
+			DBG(errs() << "emitting: " << *CI << "\n";)
+			
+			if(is_remove) {
+				kill_instruction(repl, CI);
+			
+				if(opt_repl != nullptr &&
+				   opt_repl->getNumUses() == 0) {
+					kill_instruction(opt_repl);
+				}
+			}
+			
+			return CI;
+		}
+		void visitFAdd(BinaryOperator &I) {
+			// skip if fast-math/reassoc is not set
+			if(!I.hasAllowReassoc()) {
+				return;
+			}
+			
+			DBG(errs() << "@fadd: " << I << "\n";)
+			
+			auto N0 = I.getOperand(0);
+			auto N1 = I.getOperand(1);
+			auto N0I = dyn_cast<Instruction>(N0);
+			auto N1I = dyn_cast<Instruction>(N1);
+			
+			// If we have two choices trying to fold (fadd (fmul u, v), (fmul x, y)),
+			// prefer to fold the multiply with fewer uses.
+			if(get_opcode(N0) == Instruction::FMul &&
+			   get_opcode(N1) == Instruction::FMul) {
+				if(N0->getNumUses() > N1->getNumUses()) {
+					std::swap(N0, N1);
+					std::swap(N0I, N1I);
+				}
+			}
+			
+			// fold (fadd (fmul x, y), z) -> (fma x, y, z)
+			if(get_opcode(N0) == Instruction::FMul) {
+				if(emit_fma(N0I->getOperand(0), N0I->getOperand(1), N1, &I, N0I)) {
+					return;
+				}
+			}
+			
+			// fold (fadd x, (fmul y, z)) -> (fma y, z, x)
+			// Note: Commutes FADD operands.
+			if(get_opcode(N1) == Instruction::FMul) {
+				if(emit_fma(N1I->getOperand(0), N1I->getOperand(1), N0, &I, N1I)) {
+					return;
+				}
+			}
+			
+			// fold (fadd (fma x, y, (fmul u, v)), z) -> (fma x, y (fma u, v, z))
+			if(auto FMACI = dyn_cast<CallInst>(N0)) {
+				if(FMACI->getCalledFunction()->getName().startswith(fma_prefix) &&
+				   get_opcode(FMACI->getOperand(2)) == Instruction::FMul) {
+					auto mul_op = cast<Instruction>(FMACI->getOperand(2));
+					
+					DBG(errs() << "(fadd (fma x, y, (fmul u, v)), z) -> (fma x, y (fma u, v, z))\nfma:"
+							   << *FMACI << "\nmul: " << mul_op << "\n";)
+					
+					auto inner_fma = emit_fma(mul_op->getOperand(0), mul_op->getOperand(1), N1,
+											  mul_op, nullptr,
+											  // must insert before current instruction, b/c we don't know where 'z' came from
+											  &I,
+											  // don't remove yet, if sth fails, these are still needed
+											  // -> rely on DCE to handle this
+											  false);
+					
+					if(inner_fma) {
+						auto outer_fma = emit_fma(FMACI->getOperand(0), FMACI->getOperand(1), inner_fma, &I);
+						if(outer_fma) {
+							if(FMACI->getNumUses() == 0) {
+								kill_instruction(FMACI);
+							}
+							if(mul_op->getNumUses() == 0) {
+								kill_instruction(mul_op);
+							}
+							return;
+						}
+					}
+					DBG(errs() << "!! did not fold\n";)
+				}
+			}
+			
+			// fold (fadd x, (fma y, z, (fmul u, v)) -> (fma y, z (fma u, v, x))
+			if(auto FMACI = dyn_cast<CallInst>(N1)) {
+				if(FMACI->getCalledFunction()->getName().startswith(fma_prefix) &&
+				   get_opcode(FMACI->getOperand(2)) == Instruction::FMul) {
+					auto mul_op = cast<Instruction>(FMACI->getOperand(2));
+					
+					DBG(errs() << "(fadd x, (fma y, z, (fmul u, v)) -> (fma y, z (fma u, v, x)):\nfma:"
+							   << *FMACI << "\nmul: " << mul_op << "\n";)
+					
+					auto inner_fma = emit_fma(mul_op->getOperand(0), mul_op->getOperand(1), N0,
+											  mul_op, nullptr,
+											  // must insert before current instruction, b/c we don't know where 'x' came from
+											  &I,
+											  // don't remove yet, if sth fails, these are still needed
+											  // -> rely on DCE to handle this
+											  false);
+					
+					if(inner_fma) {
+						auto outer_fma = emit_fma(FMACI->getOperand(0), FMACI->getOperand(1), inner_fma, &I);
+						if(outer_fma) {
+							if(FMACI->getNumUses() == 0) {
+								kill_instruction(FMACI);
+							}
+							if(mul_op->getNumUses() == 0) {
+								kill_instruction(mul_op);
+							}
+							return;
+						}
+					}
+					DBG(errs() << "!! did not fold\n";)
+				}
+			}
+			
+			// fold (fadd x, (fma a, b, (fma u, v, (fmul s t)))), or swizzled fadd operands
+			// -> (fma s t (fma u v (fma a b x)))
+			{
+				auto FMACI0 = dyn_cast<CallInst>(N0);
+				auto FMACI1 = dyn_cast<CallInst>(N1);
+				auto FMACI = (FMACI0 != nullptr && FMACI0->getCalledFunction()->getName().startswith(fma_prefix) ? FMACI0 :
+							  FMACI1 != nullptr && FMACI1->getCalledFunction()->getName().startswith(fma_prefix) ? FMACI1 :
+							  nullptr);
+				if(FMACI) {
+					auto FMA2CI = dyn_cast<CallInst>(FMACI->getOperand(2));
+					if(FMA2CI != nullptr && FMA2CI->getCalledFunction()->getName().startswith(fma_prefix)) {
+						auto mul_op = dyn_cast<BinaryOperator>(FMA2CI->getOperand(2));
+						if(mul_op != nullptr && mul_op->getOpcode() == Instruction::FMul) {
+							DBG(errs() << "fold (fadd x, (fma a, b, (fma u, v, (fmul s t)))) -> (fma s t (fma u v (fma a b x))):\n"
+									   "fma1: " << *FMACI << "\nfma2: " << FMA2CI << "\nfmul: " << mul_op << "\n";)
+							
+							Value* x = (FMACI == FMACI0 ? N1 : N0);
+							Value* a = FMACI->getOperand(0);
+							Value* b = FMACI->getOperand(1);
+							Value* u = FMA2CI->getOperand(0);
+							Value* v = FMA2CI->getOperand(1);
+							Value* s = mul_op->getOperand(0);
+							Value* t = mul_op->getOperand(1);
+							
+							auto fma_abx = emit_fma(a, b, x, FMACI, nullptr, &I, false);
+							if(fma_abx) {
+								auto fma_uv_abx = emit_fma(u, v, fma_abx, FMA2CI, nullptr, &I, false);
+								if(fma_uv_abx) {
+									auto fma_st_uv_abx = emit_fma(s, t, fma_uv_abx, &I, nullptr);
+									if(fma_st_uv_abx) {
+										return;
+									}
+								}
+							}
+							DBG(errs() << "!! did not fold\n";)
+						}
+					}
+				}
+			}
+		}
+		void visitFSub(BinaryOperator &I) {
+			// skip if fast-math/reassoc is not set
+			if(!I.hasAllowReassoc()) {
+				return;
+			}
+			
+			DBG(errs() << "@fsub: " << I << "\n";)
+			
+			auto N0 = I.getOperand(0);
+			auto N1 = I.getOperand(1);
+			auto N0I = dyn_cast<Instruction>(N0);
+			auto N1I = dyn_cast<Instruction>(N1);
+			
+			// fold (fsub (fmul x, y), z) -> (fma x, y, (fneg z))
+			if(get_opcode(N0) == Instruction::FMul) {
+				DBG(errs() << "fsub fold #1\n";)
+				auto fneg_N1 = create_negate(N1, &I);
+				if(emit_fma(N0I->getOperand(0), N0I->getOperand(1), fneg_N1, &I, N0I)) {
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+			}
+			
+			// fold (fsub x, (fmul y, z)) -> (fma (fneg y), z, x)
+			// NOTE: Commutes FSUB operands.
+			// NOTE: can negate either y or z
+			if(get_opcode(N1) == Instruction::FMul) {
+				DBG(errs() << "fsub fold #2\n";)
+				auto N1op0 = N1I->getOperand(0);
+				auto N1op1 = N1I->getOperand(1);
+				auto neg = create_negate(N1op0, N1op1, &I);
+				if(emit_fma(neg.first, neg.second == 0 ? N1op1 : N1op0, N0, &I, N1I)) {
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+			}
+			
+			// fold (fsub (fneg (fmul, x, y)), z) -> (fma (fneg x), y, (fneg z))
+			// NOTE: can negate either x or y
+			Value* fneg_val;
+			if(PatternMatch::match(N0, PatternMatch::m_FNeg(PatternMatch::m_Value(fneg_val)))) { // TODO: is this correct?
+				auto N0op0I = dyn_cast<Instruction>(N0I->getOperand(0));
+				if(N0op0I != nullptr && N0op0I->getOpcode() == Instruction::FMul) {
+					DBG(errs() << "fsub fold #3\n";)
+					auto N00 = N0op0I->getOperand(0);
+					auto N01 = N0op0I->getOperand(1);
+					auto fneg_xy = create_negate(N00, N01, &I);
+					auto fneg_N1 = create_negate(N1, &I);
+					if(emit_fma(fneg_xy.first, fneg_xy.second == 0 ? N01 : N00, fneg_N1, &I, N0op0I)) {
+						// TODO: also kill fneg
+						return;
+					}
+					DBG(errs() << "!! did not fold\n";)
+				}
+			}
+			
+			// fold (fsub (fma x, y, (fmul u, v)), z)
+			//   -> (fma x, y (fma u, v, (fneg z)))
+			if(auto FMACI = dyn_cast<CallInst>(N0)) {
+				if(FMACI->getCalledFunction()->getName().startswith(fma_prefix) &&
+				   get_opcode(FMACI->getOperand(2)) == Instruction::FMul) {
+					auto mul_op = cast<Instruction>(FMACI->getOperand(2));
+					
+					DBG(errs() << "fold (fsub (fma x, y, (fmul u, v)), z) -> (fma x, y (fma u, v, (fneg z))):\n"
+							   << "fma: " << *FMACI << "\nfmul: " << mul_op << "\n";)
+					
+					auto fneg_N1 = create_negate(N1, &I);
+					auto inner_fma = emit_fma(mul_op->getOperand(0), mul_op->getOperand(1), fneg_N1,
+											  mul_op, nullptr,
+											  // must insert before current instruction, b/c we don't know where 'x' came from
+											  &I,
+											  // don't remove yet, if sth fails, these are still needed
+											  // -> rely on DCE to handle this
+											  false);
+					
+					if(inner_fma) {
+						auto outer_fma = emit_fma(FMACI->getOperand(0), FMACI->getOperand(1), inner_fma, &I);
+						if(outer_fma) {
+							if(FMACI->getNumUses() == 0) {
+								kill_instruction(FMACI);
+							}
+							if(mul_op->getNumUses() == 0) {
+								kill_instruction(mul_op);
+							}
+							return;
+						}
+					}
+					DBG(errs() << "!! did not fold\n";)
+				}
+			}
+	
+			// fold (fsub x, (fma y, z, (fmul u, v)))
+			//   -> (fma (fneg y), z, (fma (fneg u), v, x))
+			// NOTE: can negate either y or z, and either u or v
+			if(auto FMACI = dyn_cast<CallInst>(N1)) {
+				if(FMACI->getCalledFunction()->getName().startswith(fma_prefix) &&
+				   get_opcode(FMACI->getOperand(2)) == Instruction::FMul) {
+					auto mul_op = cast<Instruction>(FMACI->getOperand(2));
+					
+					DBG(errs() << "fold (fsub x, (fma y, z, (fmul u, v))) -> (fma (fneg y), z, (fma (fneg u), v, x)):\n"
+							   << "fma: " << *FMACI << "\nfmul: " << mul_op << "\n";)
+					
+					auto N20 = mul_op->getOperand(0);
+					auto N21 = mul_op->getOperand(1);
+					auto fneg_uv = create_negate(N20, N21, &I);
+					auto inner_fma = emit_fma(fneg_uv.first, fneg_uv.second == 0 ? N21 : N20, N0,
+											  mul_op, nullptr,
+											  // must insert before current instruction, b/c we don't know where 'x' came from
+											  &I,
+											  // don't remove yet, if sth fails, these are still needed
+											  // -> rely on DCE to handle this
+											  false);
+					
+					if(inner_fma) {
+						auto N1op0 = FMACI->getOperand(0);
+						auto N1op1 = FMACI->getOperand(1);
+						auto fneg_yz = create_negate(N1op0, N1op1, &I);
+						auto outer_fma = emit_fma(fneg_yz.first, fneg_yz.second == 0 ? N1op1 : N1op0, inner_fma, &I);
+						if(outer_fma) {
+							if(FMACI->getNumUses() == 0) {
+								kill_instruction(FMACI);
+							}
+							if(mul_op->getNumUses() == 0) {
+								kill_instruction(mul_op);
+							}
+							return;
+						}
+					}
+					DBG(errs() << "!! did not fold\n";)
+				}
+			}
+		}
+		void visitFMul(BinaryOperator &I) {
+			// skip if fast-math/reassoc is not set
+			if(!I.hasAllowReassoc()) {
+				return;
+			}
+			
+			DBG(errs() << "@fmul: " << I << "\n";)
+			
+			auto N0 = I.getOperand(0);
+			auto N1 = I.getOperand(1);
+			
+			// fold (fmul (fadd x, +1.0), y) -> (fma x, y, y)
+			// fold (fmul (fadd x, -1.0), y) -> (fma x, y, (fneg y))
+			auto FuseFADD = [&](Value* X, Value* Y) -> Instruction* {
+				if(get_opcode(X) == Instruction::FAdd) {
+					auto XInst = cast<Instruction>(X);
+					auto XC1 = dyn_cast<ConstantFP>(XInst->getOperand(1));
+					if(XC1 && XC1->isExactlyValue(+1.0)) {
+						return emit_fma(XInst->getOperand(0), Y, Y, &I, XInst);
+					}
+					if(XC1 && XC1->isExactlyValue(-1.0)) {
+						auto fneg_y = create_negate(Y, &I);
+						return emit_fma(XInst->getOperand(0), Y, fneg_y, &I, XInst);
+					}
+				}
+				return nullptr;
+			};
+			
+			if(FuseFADD(N0, N1) ||
+			   FuseFADD(N1, N0)) {
+				return;
+			}
+			
+			// fold (fmul (fsub +1.0, x), y) -> (fma (fneg x), y, y)
+			// NOTE: can fold either x or y
+			// fold (fmul (fsub -1.0, x), y) -> (fma (fneg x), y, (fneg y))
+			// NOTE: can folder either x or y, prefer y unless x is constant
+			// fold (fmul (fsub x, +1.0), y) -> (fma x, y, (fneg y))
+			// fold (fmul (fsub x, -1.0), y) -> (fma x, y, y)
+			auto FuseFSUB = [&](Value* X, Value* Y) -> Instruction* {
+				if(get_opcode(X) == Instruction::FSub) {
+					auto XInst = cast<Instruction>(X);
+					auto XC0 = dyn_cast<ConstantFP>(XInst->getOperand(0));
+					if(XC0 && XC0->isExactlyValue(+1.0)) {
+						auto xop1 = XInst->getOperand(1);
+						auto fneg = create_negate(xop1, Y, &I);
+						return emit_fma(fneg.first, fneg.second == 0 ? Y : xop1, Y, &I, XInst);
+					}
+					if(XC0 && XC0->isExactlyValue(-1.0)) {
+						auto fneg_y = create_negate(Y, &I);
+						auto xop1 = XInst->getOperand(1);
+						auto fneg = (isa<ConstantFP>(xop1) ? create_negate(xop1, &I) : fneg_y);
+						return emit_fma(fneg, (fneg == fneg_y ? xop1 : Y), fneg_y, &I, XInst);
+					}
+					
+					auto XC1 = dyn_cast<ConstantFP>(XInst->getOperand(1));
+					if(XC1 && XC1->isExactlyValue(+1.0)) {
+						auto fneg_y = create_negate(Y, &I);
+						return emit_fma(XInst->getOperand(0), Y, fneg_y, &I, XInst);
+					}
+					if(XC1 && XC1->isExactlyValue(-1.0)) {
+						return emit_fma(XInst->getOperand(0), Y, Y, &I, XInst);
+					}
+				}
+				return nullptr;
+			};
+			
+			if(FuseFSUB(N0, N1) ||
+			   FuseFSUB(N1, N0)) {
+				return;
+			}
+		}
+		void visitFMA(CallInst &I) {
+			auto N0 = I.getOperand(0);
+			auto N1 = I.getOperand(1);
+			auto N2 = I.getOperand(2);
+			
+			auto N0CFP = dyn_cast<ConstantFP>(N0);
+			auto N1CFP = dyn_cast<ConstantFP>(N1);
+			auto N2CFP = dyn_cast<ConstantFP>(N2);
+			
+			auto N0I = dyn_cast<Instruction>(N0);
+			auto N1I = dyn_cast<Instruction>(N1);
+			auto N2I = dyn_cast<Instruction>(N2);
+			
+			DBG(errs() << "@fma: " << I << "\n";)
+			
+			// Constant fold FMA.
+			if(N0CFP && N1CFP && N2CFP) {
+				DBG(errs() << "fma constant fold\n";)
+				APFloat a = N0CFP->getValueAPF();
+				const auto& b = N1CFP->getValueAPF();
+				const auto& c = N2CFP->getValueAPF();
+				if(a.fusedMultiplyAdd(b, c, APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					kill_instruction(&I, ConstantFP::get(*ctx, a));
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// mul with 0 -> N2
+			if((N0CFP && N0CFP->isZero()) ||
+			   (N1CFP && N1CFP->isZero())) {
+				DBG(errs() << "fma 0 mul\n";)
+				kill_instruction(&I, N2);
+				return;
+			}
+			
+			// (fma c1 c2 y) -> (fadd y c1*c2)
+			if(N0CFP && N1CFP) {
+				DBG(errs() << "fma -> fadd fold\n";)
+				APFloat a = N0CFP->getValueAPF();
+				const auto& b = N1CFP->getValueAPF();
+				if(a.multiply(b, APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					auto fadd = BinaryOperator::CreateFAdd(N2, ConstantFP::get(*ctx, a),
+														   I.hasName() ? I.getName() + ".fadd_c" : "fadd_c", &I);
+					fadd->setDebugLoc(I.getDebugLoc());
+					kill_instruction(&I, fadd);
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// (fma x y 0) -> (fmul x y)
+			if(N2CFP && N2CFP->isZero()) {
+				DBG(errs() << "fma -> add 0 fold\n";)
+				auto fmul = BinaryOperator::CreateFMul(N0, N1, I.hasName() ? I.getName() + ".fmul_a0" : "fmul_a0", &I);
+				fmul->setDebugLoc(I.getDebugLoc());
+				kill_instruction(&I, fmul);
+				return;
+			}
+			
+			// Canonicalize (fma c, x, y) -> (fma x, c, y)
+			if(N0CFP && !N1CFP) {
+				I.setOperand(0, N1);
+				I.setOperand(1, N0);
+				
+				// -> continue with swizzled ops
+				std::swap(N0, N1);
+				std::swap(N0CFP, N1CFP);
+				std::swap(N0I, N1I);
+			}
+			
+			// (fma x, 1, y) -> (fadd x, y)
+			if(N1CFP && N1CFP->isExactlyValue(1.0)) {
+				// N0 + N2
+				DBG(errs() << "fma -> fadd fold (0+2)\n";)
+				auto fadd = BinaryOperator::CreateFAdd(N0, N2, I.hasName() ? I.getName() + ".fadd" : "fadd", &I);
+				fadd->setDebugLoc(I.getDebugLoc());
+				kill_instruction(&I, fadd);
+				return;
+			}
+			
+			// (fma x, c1, (fmul x, c2)) -> (fmul x, c1+c2)
+			if(N2I && N2I->getOpcode() == Instruction::FMul &&
+			   N0 == N2I->getOperand(0) &&
+			   N1CFP &&
+			   isa<ConstantFP>(N2I->getOperand(1))) {
+				DBG(errs() << "fma -> x mul c1+c2 fold: " << *N2I << "\n";)
+				
+				APFloat c1 = N1CFP->getValueAPF();
+				const auto& c2 = cast<ConstantFP>(N2I->getOperand(1))->getValueAPF();
+				if(c1.add(c2, APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					auto fmul = BinaryOperator::CreateFMul(N0, ConstantFP::get(*ctx, c1),
+														   I.hasName() ? I.getName() + ".fmul_c" : "fmul_c", &I);
+					fmul->setDebugLoc(I.getDebugLoc());
+					kill_instruction(&I, fmul);
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// (fma (fmul x, c1), c2, y) -> (fma x, c1*c2, y)
+			if(N0I && N0I->getOpcode() == Instruction::FMul &&
+			   N1CFP &&
+			   isa<ConstantFP>(N0I->getOperand(1))) {
+				DBG(errs() << "fma -> fma x c1*c2 y fold: " << *N0I << "\n";)
+				
+				APFloat c1 = cast<ConstantFP>(N0I->getOperand(1))->getValueAPF();
+				const auto& c2 = N1CFP->getValueAPF();
+				if(c1.multiply(c2, APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					emit_fma(N0I->getOperand(0), ConstantFP::get(*ctx, c1), N2, &I, N0I);
+					return;
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// NOTE: don't fold: (fma x, -1, y) -> (fadd (fneg x), y)
+			
+			// creates a fp constant of "value" using the same type/semantics as "copy_sema"
+			const auto ap_constant_from_type = [](const APFloat& copy_sema, const double& value) {
+				if (&copy_sema.getSemantics() == &APFloat::IEEEdouble()) {
+					return APFloat(value);
+				} else {
+					return APFloat(float(value));
+				}
+			};
+			
+			// (fma x, c, x) -> (fmul x, (c+1))
+			if(N1CFP && N0 == N2) {
+				DBG(errs() << "fma -> fmul x c+1 fold\n";)
+				APFloat c = N1CFP->getValueAPF();
+				if(c.add(ap_constant_from_type(c, 1.0), APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					auto fmul = BinaryOperator::CreateFMul(N0, ConstantFP::get(*ctx, c),
+														   I.hasName() ? I.getName() + ".fmul_1c" : "fmul_1c", &I);
+					fmul->setDebugLoc(I.getDebugLoc());
+					kill_instruction(&I, fmul);
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// (fma x, c, (fneg x)) -> (fmul x, (c-1))
+			Value* fneg_val;
+			if(N1CFP &&
+			   N2I && N2I->getOperand(0) == N0 &&
+			   PatternMatch::match(N2I, PatternMatch::m_FNeg(PatternMatch::m_Value(fneg_val)))) { // TODO: is this correct?
+				DBG(errs() << "fma -> fmul x c-1 fold\n";)
+				APFloat c = N1CFP->getValueAPF();
+				if(c.subtract(ap_constant_from_type(c, 1.0), APFloat::rmNearestTiesToEven) != APFloat::opInvalidOp) {
+					auto fmul = BinaryOperator::CreateFMul(N0, ConstantFP::get(*ctx, c),
+														   I.hasName() ? I.getName() + ".fmul_s1c" : "fmul_s1c", &I);
+					fmul->setDebugLoc(I.getDebugLoc());
+					kill_instruction(&I, fmul);
+				}
+				DBG(errs() << "!! did not fold\n";)
+				return;
+			}
+			
+			// TODO: fold (fma (fadd x (fadd y c1)) c2 c3) -> (fma (fadd x y) c2 (c1*c2 + c3))
+			// TODO: same for sub and neg ops?
+		}
+	};
+	
+}
+
+char FMACombiner::ID = 0;
+constexpr const char FMACombiner::fma_prefix[];
+constexpr const char FMACombiner::fma_half[];
+constexpr const char FMACombiner::fma_float[];
+constexpr const char FMACombiner::fma_double[];
+
+FunctionPass *llvm::createFMACombinerPass() {
+	return new FMACombiner();
+}
+INITIALIZE_PASS_BEGIN(FMACombiner, "FMACombiner", "FMACombiner Pass", false, false)
+INITIALIZE_PASS_END(FMACombiner, "FMACombiner", "FMACombiner Pass", false, false)
diff --git a/llvm/lib/Transforms/LibFloor/FloorImage.cpp b/llvm/lib/Transforms/LibFloor/FloorImage.cpp
new file mode 100644
index 000000000000..d4215be4828f
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/FloorImage.cpp
@@ -0,0 +1,595 @@
+//===- FloorImage.cpp - base class for image transformations --------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This class defines and implements the base class for all
+// image transformations (CUDA and opaque, as used for Metal, OpenCL and Vulkan).
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/LibFloor/FloorImage.h"
+#include <cxxabi.h>
+using namespace llvm;
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+FloorImageBasePass::FloorImageBasePass(char &ID,
+									   const IMAGE_TYPE_ID& image_type_id_,
+									   const uint32_t& image_capabilities_)
+: FunctionPass(ID), image_type_id(image_type_id_),
+image_read_prefix(image_type_id == IMAGE_TYPE_ID::CUDA ? "floor.cuda.read_image." : "floor.opaque.read_image."),
+image_write_prefix(image_type_id == IMAGE_TYPE_ID::CUDA ? "floor.cuda.write_image." : "floor.opaque.write_image."),
+image_get_dim_prefix(image_type_id == IMAGE_TYPE_ID::CUDA ? "floor.cuda.get_image_dim" : "floor.opaque.get_image_dim"),
+image_capabilities((IMAGE_CAPABILITY)image_capabilities_) {
+}
+
+bool FloorImageBasePass::runOnFunction(Function &F) {
+	// exit if empty function
+	if(F.empty()) return false;
+	
+	// reset
+	M = F.getParent();
+	ctx = &M->getContext();
+	func = &F;
+	builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+	was_modified = false;
+	
+	// check triple
+	const auto triple = llvm::Triple(M->getTargetTriple());
+	if (triple.getArch() == Triple::ArchType::air64) {
+		is_metal = true;
+		uint32_t major_version = 0, minor_version = 0, micro_version = 0;
+		if (triple.getOS() == Triple::OSType::IOS) {
+			triple.getiOSVersion(major_version, minor_version, micro_version);
+			if (major_version >= 14) {
+				is_metal_2_3 = true;
+			}
+			if (major_version >= 15) {
+				is_metal_2_4 = true;
+			}
+		} else if (triple.getOS() == Triple::OSType::MacOSX) {
+			if (triple.getMacOSXVersion(major_version, minor_version, micro_version)) {
+				if (major_version >= 11) {
+					is_metal_2_3 = true;
+				}
+				if (major_version >= 12) {
+					is_metal_2_4 = true;
+				}
+			}
+		}
+	}
+	
+	{
+		AttrBuilder attr_builder;
+		attr_builder.addAttribute(llvm::Attribute::NoUnwind);
+		attr_builder.addAttribute(llvm::Attribute::ReadNone);
+		nounwind_readnone_attr = AttributeList::get(*ctx, ~0, attr_builder);
+	}
+	{
+		AttrBuilder attr_builder;
+		attr_builder.addAttribute(llvm::Attribute::NoUnwind);
+		nounwind_attr = AttributeList::get(*ctx, ~0, attr_builder);
+	}
+	
+	// visit everything in this function
+	DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+	is_fragment_shader = (F.getCallingConv() == CallingConv::FLOOR_FRAGMENT);
+	
+	if (!triple.isNVPTX()) {
+		visit(F);
+		return was_modified;
+	} else {
+		// for CUDA we may make drastic changes to the BBs and instructions in them
+		// -> can no longer use instruction vistor
+		// -> do this on our own with hard restart if we modified anything
+		bool was_modified_any = false, restart = true;
+		while (restart) {
+			restart = false;
+			for (auto bb = func->begin(); bb != func->end(); ++bb) {
+				for (auto instr = bb->begin(); instr != bb->end(); ++instr) {
+					if (auto CB = dyn_cast_or_null<CallBase>(&*instr); CB) {
+						visitCallBase(*CB);
+						
+						// restart!
+						if (was_modified) {
+							was_modified_any = true;
+							was_modified = false;
+							restart = true;
+							break;
+						}
+					}
+				}
+				if (restart) {
+					break;
+				}
+			}
+		}
+		return was_modified_any;
+	}
+}
+
+void FloorImageBasePass::visit(Instruction& I) {
+	InstVisitor<FloorImageBasePass>::visit(I);
+}
+
+void FloorImageBasePass::visitCallBase(CallBase& CB) {
+	const auto func = CB.getCalledFunction();
+	if(!func) return;
+	
+	const auto full_func_name = func->getName();
+	if(full_func_name.startswith(image_read_prefix) ||
+	   full_func_name.startswith(image_write_prefix)) {
+		// strip off .(i|f)(1|2|3) from the end of the func name
+		const auto func_name = full_func_name.rsplit('.').first;
+		handle_image(CB, func_name);
+		was_modified = true;
+	} else if (full_func_name.startswith(image_get_dim_prefix)) {
+		handle_image_query(CB, full_func_name);
+		was_modified = true;
+	}
+}
+
+void FloorImageBasePass::handle_image(CallBase& CB, const StringRef& func_name) {
+	builder->SetInsertPoint(&CB);
+	const bool is_image_read = func_name.startswith(image_read_prefix);
+	
+	/* args for cuda and opaque read/write functions:
+	 
+	 CUDA read:
+	 uint64_t tex, COMPUTE_IMAGE_TYPE type,
+	 coord_vec_type coord, uint32_t layer, uint32_t sample, offset_vec_type offset,
+	 int32_t lod_i, float lod_or_bias_f, bool is_lod, bool is_lod_float, bool is_bias,
+	 gradient_vec_type dpdx, gradient_vec_type dpdy, bool is_gradient,
+	 COMPARE_FUNCTION compare_function, float compare_value, bool is_compare
+	 
+	 CUDA write:
+	 uint64_t surf, COMPUTE_IMAGE_TYPE type, coord_vec_type coord, uint32_t layer, uint32_t lod, bool is_lod, data_vec_type data, COMPUTE_IMAGE_TYPE rt_type
+	 
+	 opaque read:
+	 image_t img, sampler_type smplr, COMPUTE_IMAGE_TYPE type,
+	 coord_vec_type coord, uint32_t layer, uint32_t sample, offset_vec_type offset,
+	 int32_t lod_i, float lod_or_bias_f, bool is_lod, bool is_lod_float, bool is_bias,
+	 gradient_vec_type dpdx, gradient_vec_type dpdy, bool is_gradient,
+	 COMPARE_FUNCTION compare_function, float compare_value, bool is_compare
+	 
+	 opaque write:
+	 image_t img, COMPUTE_IMAGE_TYPE type, coord_vec_type coord, uint32_t layer, uint32_t lod, bool is_lod, data_vec_type data
+	 
+	 */
+	
+	const uint32_t read_arg_count = (image_type_id == IMAGE_TYPE_ID::CUDA ? 17 : 18);
+	const uint32_t write_arg_count = (image_type_id == IMAGE_TYPE_ID::CUDA ? 8 : 7);
+	
+	// as args are largely the same for cuda and opaque, just offset the arg num by 1 for opaque,
+	// instead of doing this individually for each arg.
+	// also: only offset for image reads, as image writes are identical.
+	const uint32_t read_args_offset = (image_type_id == IMAGE_TYPE_ID::CUDA ? 0 : 1);
+	const uint32_t write_args_offset = 0;
+	const uint32_t args_offset = (is_image_read ? read_args_offset : write_args_offset);
+	
+	// check + get arguments
+	if(is_image_read && CB.arg_size() != read_arg_count) {
+		ctx->emitError(&CB, func_name + ": invalid argument count (expected " + std::to_string(read_arg_count) + ")");
+		return;
+	}
+	if(!is_image_read && CB.arg_size() != write_arg_count) {
+		ctx->emitError(&CB, func_name + ": invalid argument count (expected " + std::to_string(write_arg_count) + ")");
+		return;
+	}
+	
+	// -> tex/surf/img handle
+	const auto img_handle_arg = CB.getOperand(0);
+	if(image_type_id == IMAGE_TYPE_ID::CUDA &&
+	   !img_handle_arg->getType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image handle type (must be integer)");
+		return;
+	}
+	else if(image_type_id == IMAGE_TYPE_ID::OPAQUE) {
+		if(!img_handle_arg->getType()->isPointerTy()) {
+			ctx->emitError(&CB, "invalid image handle type (must be an image pointer)");
+			return;
+		}
+		// TODO: check opaque? check opencl image type?
+	}
+	
+	// -> type enum
+	const auto image_type_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(1 + args_offset));
+	if(!image_type_arg) {
+		ctx->emitError(&CB, "image type argument must be a constant value");
+		return;
+	}
+	if(!image_type_arg->getType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image-type type (must be enum/integer)");
+		return;
+	}
+	const uint32_t image_channel_count = ((uint32_t(image_type_arg->getZExtValue()) &
+										   uint32_t(COMPUTE_IMAGE_TYPE::__CHANNELS_MASK)) >>
+										  uint32_t(COMPUTE_IMAGE_TYPE::__CHANNELS_SHIFT)) + 1u;
+	const auto full_image_type = COMPUTE_IMAGE_TYPE(image_type_arg->getZExtValue());
+	const COMPUTE_IMAGE_TYPE image_type = full_image_type & COMPUTE_IMAGE_TYPE::BASE_TYPE_MASK;
+	const COMPUTE_IMAGE_TYPE format_type = full_image_type & COMPUTE_IMAGE_TYPE::__FORMAT_MASK;
+	const COMPUTE_IMAGE_TYPE image_data_type = full_image_type & COMPUTE_IMAGE_TYPE::__DATA_TYPE_MASK;
+	const bool is_normalized = has_flag<COMPUTE_IMAGE_TYPE::FLAG_NORMALIZED>(full_image_type);
+	const bool is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(full_image_type);
+	
+	// -> coord
+	const auto coord_arg = CB.getOperand(2 + args_offset);
+	const auto coord_arg_type = dyn_cast<FixedVectorType>(coord_arg->getType());
+	if (!coord_arg_type) {
+		ctx->emitError(&CB, "invalid image coordinate type");
+		return;
+	}
+	if (!coord_arg_type->getElementType()->isFloatTy() && !coord_arg_type->getElementType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image coordinate type");
+		return;
+	}
+	const auto coord_dim = coord_arg_type->getNumElements();
+	
+	// -> layer
+	const auto layer_arg = CB.getOperand(3 + args_offset);
+	if(!layer_arg->getType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image layer index type (must be integer)");
+		return;
+	}
+	
+	if(is_image_read) {
+		// -> sampler
+		// prefer const sampler / either const or dyn will be nullptr
+		llvm::ConstantInt* const_sampler_arg = nullptr;
+		llvm::Value* dyn_sampler_arg = nullptr;
+		if(image_type_id != IMAGE_TYPE_ID::CUDA) {
+			const_sampler_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(1));
+			if(const_sampler_arg == nullptr) {
+				dyn_sampler_arg = CB.getOperand(1);
+			}
+		}
+		
+		// -> sample
+		const auto sample_arg = CB.getOperand(4 + args_offset);
+		if(!sample_arg->getType()->isIntegerTy()) {
+			ctx->emitError(&CB, "invalid image sample index type (must be integer)");
+			return;
+		}
+		
+		// -> offset
+		const auto offset_arg = CB.getOperand(5 + args_offset);
+		const auto offset_arg_type = dyn_cast<FixedVectorType>(offset_arg->getType());
+		if(!offset_arg_type) {
+			ctx->emitError(&CB, "invalid offset type (must be an int vector)");
+			return;
+		}
+		if(!offset_arg_type->getElementType()->isIntegerTy()) {
+			ctx->emitError(&CB, "invalid offset type (must be an int vector)");
+			return;
+		}
+		if(coord_dim != offset_arg_type->getNumElements()) {
+			ctx->emitError(&CB, "invalid offset vector dimension: should be " + std::to_string(coord_dim));
+			return;
+		}
+		
+		SmallVector<llvm::Value*, 3> offset_elems;
+		bool is_offset = true;
+		if(const auto const_offset_arg = dyn_cast_or_null<Constant>(offset_arg)) {
+			// const 0 or undef -> no offset
+			if(const_offset_arg->isZeroValue() ||
+			   dyn_cast_or_null<UndefValue>(const_offset_arg)) {
+				is_offset = false;
+			}
+		}
+		
+		if(is_offset) {
+			// nobody supports this
+			if(has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type)) {
+				ctx->emitError(&CB, "image offset is not supported with cube maps");
+				return;
+			}
+			
+			// extract offset elems and check constant offset values
+			for(uint32_t i = 0; i < coord_dim; ++i) {
+				auto offset_elem = builder->CreateExtractElement(offset_arg, builder->getInt32(i));
+				offset_elems.push_back(offset_elem);
+				
+				if(const auto const_offset_elem = dyn_cast_or_null<ConstantInt>(offset_elem)) {
+					// can check if within required [-8, 7]
+					const auto val = const_offset_elem->getSExtValue();
+					if(val < -8 || val > 7) {
+						ctx->emitError(&CB, "offset out of range (must be in [-8, 7]): " + std::to_string(val));
+					}
+				}
+			}
+		}
+		
+		// -> misc flags
+		const auto is_lod_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(8 + args_offset));
+		const auto is_lod_float_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(9 + args_offset));
+		const auto is_bias_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(10 + args_offset));
+		const auto is_gradient_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(13 + args_offset));
+		const auto is_compare_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(16 + args_offset));
+		if(!is_lod_arg) {
+			ctx->emitError(&CB, "is_lod is not constant");
+			return;
+		}
+		if(!is_lod_float_arg) {
+			ctx->emitError(&CB, "is_lod_float is not constant");
+			return;
+		}
+		if(!is_bias_arg) {
+			ctx->emitError(&CB, "is_bias is not constant");
+			return;
+		}
+		if(!is_gradient_arg) {
+			ctx->emitError(&CB, "is_gradient is not constant");
+			return;
+		}
+		if(!is_compare_arg) {
+			ctx->emitError(&CB, "is_compare_arg is not constant");
+			return;
+		}
+		
+		const bool is_lod = is_lod_arg->isOne();
+		const bool is_lod_float = is_lod_float_arg->isOne();
+		const bool is_bias = is_bias_arg->isOne();
+		const bool is_gradient = is_gradient_arg->isOne();
+		const bool is_compare = is_compare_arg->isOne();
+		
+		if(is_lod && is_gradient) {
+			ctx->emitError(&CB, "lod and gradient are mutually exclusive");
+			return;
+		}
+		
+		// -> lod and bias
+		const auto lod_or_bias_arg = CB.getOperand(args_offset + (!is_bias && !is_lod_float ? 6 : 7));
+		
+		if(!lod_or_bias_arg->getType()->isIntegerTy() &&
+		   !lod_or_bias_arg->getType()->isFloatTy()) {
+			ctx->emitError(&CB, "lod must either be an integer or a float");
+			return;
+		}
+		
+		// -> gradient
+		const auto dpdx_arg = CB.getOperand(11 + args_offset);
+		const auto dpdy_arg = CB.getOperand(12 + args_offset);
+		
+		const auto dpdx_vec_type = dyn_cast<FixedVectorType>(dpdx_arg->getType());
+		const auto dpdy_vec_type = dyn_cast<FixedVectorType>(dpdy_arg->getType());
+		if(!dpdx_vec_type || !dpdy_vec_type) {
+			ctx->emitError(&CB, "dpdx and dpdy must be vector types");
+			return;
+		}
+		if(!dpdx_vec_type->getElementType()->isFloatTy() ||
+		   !dpdy_vec_type->getElementType()->isFloatTy()) {
+			ctx->emitError(&CB, "dpdx and dpdy element type must be float");
+			return;
+		}
+		
+		const auto dpdx_dim = dpdx_vec_type->getNumElements();
+		const auto dpdy_dim = dpdy_vec_type->getNumElements();
+		
+		if(dpdx_dim != coord_dim || dpdy_dim != coord_dim) {
+			ctx->emitError(&CB, "dpdx and dpdy vector dim must correspond to the coordinate dim");
+			return;
+		}
+		
+		// -> compare
+		const auto compare_function_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(14 + args_offset));
+		if(!compare_function_arg) {
+			ctx->emitError(&CB, "compare function arg is not constant");
+			return;
+		}
+		const COMPARE_FUNCTION compare_function = (COMPARE_FUNCTION)compare_function_arg->getZExtValue();
+		if(compare_function >= COMPARE_FUNCTION::__MAX_COMPARE_FUNCTION) {
+			ctx->emitError(&CB, "invalid compare function");
+			return;
+		}
+		
+		const auto compare_value_arg = CB.getOperand(15 + args_offset);
+		
+		handle_read_image(CB, func_name,
+						  img_handle_arg, image_type,
+						  const_sampler_arg, dyn_sampler_arg,
+						  coord_arg, layer_arg, sample_arg,
+						  offset_arg, offset_elems, is_offset,
+						  lod_or_bias_arg, is_lod /* if false, then it's always bias */,
+						  dpdx_arg, dpdy_arg, is_gradient,
+						  compare_function, compare_value_arg, is_compare);
+	}
+	else {
+		// -> data
+		const auto data_arg = CB.getOperand(6 + args_offset);
+		if (!is_depth || !is_metal) {
+			const auto data_vec_type = dyn_cast<FixedVectorType>(data_arg->getType());
+			if(!data_vec_type || data_vec_type->getNumElements() != 4) {
+				ctx->emitError(&CB, "invalid image data type (must be 4-component vector)");
+				return;
+			}
+			if(!data_vec_type->getElementType()->isFloatTy() &&
+			   !data_vec_type->getElementType()->isIntegerTy()) {
+				ctx->emitError(&CB, "invalid image data type (must be a float or integer vector)");
+				return;
+			}
+		} else {
+			const auto data_type = data_arg->getType();
+			if (!data_type->isFloatTy()) {
+				ctx->emitError(&CB, "invalid image data type (must be a single float when writing depth)");
+				return;
+			}
+		}
+		
+		// only writes with integer coordinates are allowed
+		if(!coord_arg_type->getElementType()->isIntegerTy()) {
+			ctx->emitError(&CB, "invalid coordinate type (must be integer)");
+			return;
+		}
+		
+		// -> lod
+		const auto is_lod_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(5 + args_offset));
+		const bool is_lod = is_lod_arg->isOne();
+		const auto lod_arg = CB.getOperand(4 + args_offset);
+		
+		if(!lod_arg->getType()->isIntegerTy()) {
+			ctx->emitError(&CB, "invalid lod type (must be integer)");
+			return;
+		}
+		
+		Value* image_rt_type_arg = nullptr;
+		if (image_type_id == IMAGE_TYPE_ID::CUDA) {
+			image_rt_type_arg = CB.getOperand(7 + args_offset);
+			if (!image_rt_type_arg->getType()->isIntegerTy()) {
+				ctx->emitError(&CB, "invalid run-time image-type type (must be enum/integer)");
+				return;
+			}
+		}
+		
+		handle_write_image(CB, func_name,
+						   img_handle_arg,
+						   full_image_type, image_type, format_type, image_data_type, image_rt_type_arg,
+						   is_normalized, image_channel_count,
+						   coord_arg, layer_arg,
+						   lod_arg, is_lod,
+						   data_arg);
+	}
+}
+
+void FloorImageBasePass::handle_image_query(CallBase& CB, const StringRef& func_name) {
+	builder->SetInsertPoint(&CB);
+	
+	/* args for CUDA and opaque get_image_dim functions:
+	 
+	 CUDA get_image_dim:
+	 uint64_t tex, COMPUTE_IMAGE_TYPE type, uint32_t lod
+	 
+	 opaque get_image_dim:
+	 image_t img, COMPUTE_IMAGE_TYPE type, uint32_t lod
+	 
+	 */
+	
+	// check + get arguments
+	constexpr const uint32_t arg_count = 3;
+	if(CB.arg_size() != arg_count) {
+		ctx->emitError(&CB, func_name + ": invalid argument count (expected " + std::to_string(arg_count) + ")");
+		return;
+	}
+	
+	// -> tex/surf/img handle
+	const auto img_handle_arg = CB.getOperand(0);
+	if(image_type_id == IMAGE_TYPE_ID::CUDA &&
+	   !img_handle_arg->getType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image handle type (must be integer)");
+		return;
+	}
+	else if(image_type_id == IMAGE_TYPE_ID::OPAQUE) {
+		if(!img_handle_arg->getType()->isPointerTy()) {
+			ctx->emitError(&CB, "invalid image handle type (must be an image pointer)");
+			return;
+		}
+	}
+	
+	// -> type enum
+	const auto image_type_arg = dyn_cast_or_null<ConstantInt>(CB.getOperand(1));
+	if(!image_type_arg) {
+		ctx->emitError(&CB, "image type argument must be a constant value");
+		return;
+	}
+	if(!image_type_arg->getType()->isIntegerTy()) {
+		ctx->emitError(&CB, "invalid image-type type (must be enum/integer)");
+		return;
+	}
+	const auto full_image_type = COMPUTE_IMAGE_TYPE(image_type_arg->getZExtValue());
+	const COMPUTE_IMAGE_TYPE image_type = full_image_type & COMPUTE_IMAGE_TYPE::BASE_TYPE_MASK;
+	
+	const auto lod_arg = CB.getOperand(2);
+	
+	handle_get_image_dim(CB, func_name, img_handle_arg, full_image_type, image_type, lod_arg);
+}
+
+void FloorImageBasePass::emulate_depth_compare(llvm::Value*& dst_vec,
+											   llvm::Value* tex_value,
+											   const COMPARE_FUNCTION& compare_function,
+											   llvm::Value* compare_value_arg) {
+	llvm::Value* insert_value = nullptr;
+	llvm::Value* condition = nullptr;
+	llvm::Constant* false_val = ConstantFP::get(builder->getFloatTy(), 0.0f);
+	llvm::Constant* true_val = ConstantFP::get(builder->getFloatTy(), 1.0f);
+	
+	switch(compare_function) {
+		case COMPARE_FUNCTION::NEVER:
+			insert_value = false_val;
+			break;
+		case COMPARE_FUNCTION::ALWAYS:
+			insert_value = true_val;
+			break;
+		case COMPARE_FUNCTION::LESS_OR_EQUAL:
+			condition = builder->CreateFCmpOLE(compare_value_arg, tex_value);
+			break;
+		case COMPARE_FUNCTION::GREATER_OR_EQUAL:
+			condition = builder->CreateFCmpOGE(compare_value_arg, tex_value);
+			break;
+		case COMPARE_FUNCTION::LESS:
+			condition = builder->CreateFCmpOLT(compare_value_arg, tex_value);
+			break;
+		case COMPARE_FUNCTION::GREATER:
+			condition = builder->CreateFCmpOGT(compare_value_arg, tex_value);
+			break;
+		case COMPARE_FUNCTION::EQUAL:
+			condition = builder->CreateFCmpOEQ(compare_value_arg, tex_value);
+			break;
+		case COMPARE_FUNCTION::NOT_EQUAL:
+			condition = builder->CreateFCmpONE(compare_value_arg, tex_value);
+			break;
+		default: llvm_unreachable("invalid compare function");
+	}
+	
+	if(condition != nullptr) {
+		insert_value = builder->CreateSelect(condition, true_val, false_val);
+	}
+	
+	dst_vec = builder->CreateInsertElement(dst_vec, insert_value, builder->getInt32(0));
+}
diff --git a/llvm/lib/Transforms/LibFloor/LibFloor.cpp b/llvm/lib/Transforms/LibFloor/LibFloor.cpp
new file mode 100644
index 000000000000..621aea3fd0c7
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/LibFloor.cpp
@@ -0,0 +1,122 @@
+//===-- LibFloor.cpp ------------------------------------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements common infrastructure for libLLVMLibFloor.a, which
+// implements various Compute/Graphics backend transformations.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/IR/Verifier.h"
+#include "llvm/InitializePasses.h"
+
+using namespace llvm;
+
+/// initializeLibFloor - Initialize all passes linked into the
+/// LibFloor library.
+void llvm::initializeLibFloor(PassRegistry &Registry) {
+  initializeAddressSpaceFixPass(Registry);
+  initializeEverythingInlinerPass(Registry);
+  initializeCUDAImagePass(Registry);
+  initializeCUDAFinalPass(Registry);
+  initializeMetalFirstPass(Registry);
+  initializeMetalFinalPass(Registry);
+  initializeMetalFinalModuleCleanupPass(Registry);
+  initializeMetalImagePass(Registry);
+  initializeSPIRFinalPass(Registry);
+  initializeSPIRImagePass(Registry);
+  initializeCFGStructurizationPass(Registry);
+  initializeVulkanImagePass(Registry);
+  initializeVulkanFinalPass(Registry);
+  initializeVulkanBuiltinParamHandlingPass(Registry);
+  initializeVulkanPreFinalPass(Registry);
+  initializeVulkanFinalModuleCleanupPass(Registry);
+  initializePropagateRangeInfoPass(Registry);
+  initializeFMACombinerPass(Registry);
+}
+
+void LLVMAddAddressSpaceFixPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createAddressSpaceFixPass());
+}
+
+void LLVMAddCUDAImagePass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createCUDAImagePass());
+}
+
+void LLVMAddCUDAFinalPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createCUDAFinalPass());
+}
+
+void LLVMAddMetalFirstPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createMetalFirstPass());
+}
+
+void LLVMAddMetalFinalPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createMetalFinalPass());
+}
+
+void LLVMAddMetalFinalModuleCleanupPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createMetalFinalModuleCleanupPass());
+}
+
+void LLVMAddMetalImagePass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createMetalImagePass());
+}
+
+void LLVMAddSPIRFinalPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createSPIRFinalPass());
+}
+
+void LLVMAddSPIRImagePass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createSPIRImagePass());
+}
+
+void LLVMAddCFGStructurizationPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createCFGStructurizationPass());
+}
+
+void LLVMAddVulkanImagePass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createVulkanImagePass());
+}
+
+void LLVMAddVulkanFinalPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createVulkanFinalPass());
+}
+
+void LLVMAddVulkanBuiltinParamHandlingPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createVulkanBuiltinParamHandlingPass());
+}
+
+void LLVMAddVulkanPreFinalPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createVulkanPreFinalPass());
+}
+
+void LLVMAddVulkanFinalModuleCleanupPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createVulkanFinalModuleCleanupPass());
+}
+
+void LLVMAddPropagateRangeInfoPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createPropagateRangeInfoPass());
+}
+
+void LLVMAddFMACombinerPass(LLVMPassManagerRef PM) {
+  unwrap(PM)->add(createFMACombinerPass());
+}
diff --git a/llvm/lib/Transforms/LibFloor/MetalFinal.cpp b/llvm/lib/Transforms/LibFloor/MetalFinal.cpp
new file mode 100644
index 000000000000..a65fadc972b8
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/MetalFinal.cpp
@@ -0,0 +1,1368 @@
+//===- MetalFinal.cpp - Metal final pass ----------------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file fixes certain post-codegen issues.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/AssumptionCache.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/TargetLibraryInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "../../IR/LLVMContextImpl.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <unordered_map>
+#include <unordered_set>
+#include <array>
+#include <optional>
+#include <cxxabi.h>
+using namespace llvm;
+
+#if defined(DEBUG_TYPE)
+#undef DEBUG_TYPE
+#endif
+#define DEBUG_TYPE "MetalFinal"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+//////////////////////////////////////////
+// blatantly copied/transplanted from SROA
+namespace {
+/// \brief A custom IRBuilder inserter which prefixes all names, but only in
+/// Assert builds.
+class IRBuilderPrefixedInserter : public IRBuilderDefaultInserter {
+  std::string Prefix;
+  const Twine getNameWithPrefix(const Twine &Name) const {
+    return Name.isTriviallyEmpty() ? Name : Prefix + Name;
+  }
+
+public:
+  void SetNamePrefix(const Twine &P) { Prefix = P.str(); }
+
+protected:
+  void InsertHelper(Instruction *I, const Twine &Name, BasicBlock *BB,
+                    BasicBlock::iterator InsertPt) const override {
+    IRBuilderDefaultInserter::InsertHelper(I, getNameWithPrefix(Name), BB,
+                                           InsertPt);
+  }
+};
+
+/// \brief Provide a typedef for IRBuilder that drops names in release builds.
+using IRBuilderTy = llvm::IRBuilder<ConstantFolder, IRBuilderPrefixedInserter>;
+}
+
+namespace {
+  /// \brief Generic recursive split emission class.
+  template <typename Derived>
+  class OpSplitter {
+  protected:
+    /// The builder used to form new instructions.
+    IRBuilderTy IRB;
+    /// The indices which to be used with insert- or extractvalue to select the
+    /// appropriate value within the aggregate.
+    SmallVector<unsigned, 4> Indices;
+    /// The indices to a GEP instruction which will move Ptr to the correct slot
+    /// within the aggregate.
+    SmallVector<Value *, 4> GEPIndices;
+    /// The base pointer of the original op, used as a base for GEPing the
+    /// split operations.
+    Value *Ptr;
+
+    /// Initialize the splitter with an insertion point, Ptr and start with a
+    /// single zero GEP index.
+    OpSplitter(Instruction *InsertionPoint, Value *Ptr)
+      : IRB(InsertionPoint), GEPIndices(1, IRB.getInt32(0)), Ptr(Ptr) {}
+
+  public:
+    /// \brief Generic recursive split emission routine.
+    ///
+    /// This method recursively splits an aggregate op (load or store) into
+    /// scalar or vector ops. It splits recursively until it hits a single value
+    /// and emits that single value operation via the template argument.
+    ///
+    /// The logic of this routine relies on GEPs and insertvalue and
+    /// extractvalue all operating with the same fundamental index list, merely
+    /// formatted differently (GEPs need actual values).
+    ///
+    /// \param Ty  The type being split recursively into smaller ops.
+    /// \param Agg The aggregate value being built up or stored, depending on
+    /// whether this is splitting a load or a store respectively.
+    void emitSplitOps(Type *Ty, Value *&Agg, const Twine &Name) {
+      if (Ty->isSingleValueType())
+        return static_cast<Derived *>(this)->emitFunc(Ty, Agg, Name);
+
+      if (ArrayType *ATy = dyn_cast<ArrayType>(Ty)) {
+        unsigned OldSize = Indices.size();
+        (void)OldSize;
+        for (unsigned Idx = 0, Size = ATy->getNumElements(); Idx != Size;
+             ++Idx) {
+          assert(Indices.size() == OldSize && "Did not return to the old size");
+          Indices.push_back(Idx);
+          GEPIndices.push_back(IRB.getInt32(Idx));
+          emitSplitOps(ATy->getElementType(), Agg, Name + "." + Twine(Idx));
+          GEPIndices.pop_back();
+          Indices.pop_back();
+        }
+        return;
+      }
+
+      if (StructType *STy = dyn_cast<StructType>(Ty)) {
+        unsigned OldSize = Indices.size();
+        (void)OldSize;
+        for (unsigned Idx = 0, Size = STy->getNumElements(); Idx != Size;
+             ++Idx) {
+          assert(Indices.size() == OldSize && "Did not return to the old size");
+          Indices.push_back(Idx);
+          GEPIndices.push_back(IRB.getInt32(Idx));
+          emitSplitOps(STy->getElementType(Idx), Agg, Name + "." + Twine(Idx));
+          GEPIndices.pop_back();
+          Indices.pop_back();
+        }
+        return;
+      }
+
+      llvm_unreachable("Only arrays and structs are aggregate loadable types");
+    }
+  };
+
+  struct LoadOpSplitter : public OpSplitter<LoadOpSplitter> {
+    LoadOpSplitter(Instruction *InsertionPoint, Value *Ptr)
+      : OpSplitter<LoadOpSplitter>(InsertionPoint, Ptr) {}
+
+    /// Emit a leaf load of a single value. This is called at the leaves of the
+    /// recursive emission to actually load values.
+    void emitFunc(Type *Ty, Value *&Agg, const Twine &Name) {
+      assert(Ty->isSingleValueType());
+      // Load the single value and insert it using the indices.
+      auto elem_type = Ptr->getType()->getScalarType()->getPointerElementType();
+      Value *GEP = IRB.CreateInBoundsGEP(elem_type, Ptr, GEPIndices, Name + ".gep");
+      Value *Load = IRB.CreateLoad(elem_type, GEP, Name + ".load");
+      Agg = IRB.CreateInsertValue(Agg, Load, Indices, Name + ".insert");
+      DBG(dbgs() << "          to: " << *Load << "\n");
+    }
+  };
+
+  struct StoreOpSplitter : public OpSplitter<StoreOpSplitter> {
+    StoreOpSplitter(Instruction *InsertionPoint, Value *Ptr)
+      : OpSplitter<StoreOpSplitter>(InsertionPoint, Ptr) {}
+
+    /// Emit a leaf store of a single value. This is called at the leaves of the
+    /// recursive emission to actually produce stores.
+    void emitFunc(Type *Ty, Value *&Agg, const Twine &Name) {
+      assert(Ty->isSingleValueType());
+      // Extract the single value and store it using the indices.
+      //
+      // The gep and extractvalue values are factored out of the CreateStore
+      // call to make the output independent of the argument evaluation order.
+      Value *ExtractValue =
+          IRB.CreateExtractValue(Agg, Indices, Name + ".extract");
+      Value *InBoundsGEP =
+          IRB.CreateInBoundsGEP(nullptr, Ptr, GEPIndices, Name + ".gep");
+      Value *Store = IRB.CreateStore(ExtractValue, InBoundsGEP);
+      (void)Store;
+      DBG(dbgs() << "          to: " << *Store << "\n");
+    }
+  };
+
+}
+//////////////////////////////////////////
+
+namespace {
+	// MetalFirst
+	struct MetalFirst : public FunctionPass, InstVisitor<MetalFirst> {
+		friend class InstVisitor<MetalFirst>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		const bool enable_intel_workarounds, enable_nvidia_workarounds;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		
+		bool was_modified { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		bool is_kernel_func { false };
+		
+		MetalFirst(const bool enable_intel_workarounds_ = false,
+				   const bool enable_nvidia_workarounds_ = false) :
+		FunctionPass(ID),
+		enable_intel_workarounds(enable_intel_workarounds_),
+		enable_nvidia_workarounds(enable_nvidia_workarounds_) {
+			initializeMetalFirstPass(*PassRegistry::getPassRegistry());
+		}
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			
+			// NOTE: for now, this is no longer needed
+			was_modified = false;
+			//visit(F);
+			
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<MetalFirst>::visit;
+		void visit(Instruction& /* I */) {
+			//InstVisitor<MetalFirst>::visit(I);
+		}
+	};
+	
+	// MetalFinal
+	struct MetalFinal : public FunctionPass, InstVisitor<MetalFinal> {
+		friend class InstVisitor<MetalFinal>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		const bool enable_intel_workarounds, enable_nvidia_workarounds;
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		Instruction* alloca_insert { nullptr };
+		bool was_modified { false };
+		bool is_kernel_func { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		
+		// added kernel function args
+		Argument* global_id { nullptr };
+		Argument* global_size { nullptr };
+		Argument* local_id { nullptr };
+		Argument* local_size { nullptr };
+		Argument* group_id { nullptr };
+		Argument* group_size { nullptr };
+		Argument* sub_group_id { nullptr };
+		Argument* sub_group_local_id { nullptr };
+		Argument* sub_group_size { nullptr };
+		Argument* num_sub_groups { nullptr };
+		
+		// added vertex function args
+		Argument* vertex_id { nullptr };
+		Argument* instance_id { nullptr };
+		
+		// added fragment function args
+		Argument* point_coord { nullptr };
+		Argument* primitive_id { nullptr };
+		Argument* barycentric_coord { nullptr };
+		
+		// any function args
+		Argument* soft_printf { nullptr };
+		
+		MetalFinal(const bool enable_intel_workarounds_ = false,
+				   const bool enable_nvidia_workarounds_ = false) :
+		FunctionPass(ID),
+		enable_intel_workarounds(enable_intel_workarounds_),
+		enable_nvidia_workarounds(enable_nvidia_workarounds_) {
+			initializeMetalFinalPass(*PassRegistry::getPassRegistry());
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<AAResultsWrapperPass>();
+			AU.addRequired<GlobalsAAWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<TargetLibraryInfoWrapperPass>();
+		}
+		
+		template <Instruction::CastOps cast_op, typename std::enable_if<(cast_op == llvm::Instruction::FPToSI ||
+																		 cast_op == llvm::Instruction::FPToUI ||
+																		 cast_op == llvm::Instruction::SIToFP ||
+																		 cast_op == llvm::Instruction::UIToFP), int>::type = 0>
+		llvm::Value* call_conversion_func(llvm::Value* from, llvm::Type* to_type) {
+			// metal only supports conversion of a specific set of integer and float types
+			// -> find and check them
+			const auto from_type = from->getType();
+			static const std::unordered_map<llvm::Type*, const char*> type_map {
+				{ llvm::Type::getInt1Ty(*ctx), ".i1" }, // not sure about signed/unsigned conversion here
+				{ llvm::Type::getInt8Ty(*ctx), ".i8" },
+				{ llvm::Type::getInt16Ty(*ctx), ".i16" },
+				{ llvm::Type::getInt32Ty(*ctx), ".i32" },
+				{ llvm::Type::getInt64Ty(*ctx), ".i64" },
+				{ llvm::Type::getHalfTy(*ctx), "f.f16" },
+				{ llvm::Type::getFloatTy(*ctx), "f.f32" },
+				{ llvm::Type::getDoubleTy(*ctx), "f.f64" },
+			};
+			const auto from_iter = type_map.find(from_type);
+			if(from_iter == end(type_map)) {
+				DBG(errs() << "failed to find conversion function for: " << *from_type << " -> " << *to_type << "\n";)
+				return from;
+			}
+			const auto to_iter = type_map.find(to_type);
+			if(to_iter == end(type_map)) {
+				DBG(errs() << "failed to find conversion function for: " << *from_type << " -> " << *to_type << "\n";)
+				return from;
+			}
+			
+			// figure out if from/to type is signed/unsigned
+			bool from_signed = false, to_signed = false;
+			switch(cast_op) {
+				case llvm::Instruction::FPToSI: from_signed = true; to_signed = true; break;
+				case llvm::Instruction::FPToUI: from_signed = true; to_signed = false; break;
+				case llvm::Instruction::SIToFP: from_signed = true; to_signed = true; break;
+				case llvm::Instruction::UIToFP: from_signed = false; to_signed = true; break;
+				default: __builtin_unreachable();
+			}
+			
+			DBG(errs() << "converting: " << *from_type << " (" << (from_signed ? "signed" : "unsigned") << ") -> " << *to_type << "(" << (to_signed ? "signed" : "unsigned") << ")\n";)
+			
+			// for intel gpus any conversion from/to float from/to i8 or i16 needs to go through a i32 first
+			if(enable_intel_workarounds && from_iter->second[0] == 'f') {
+				if(to_iter->first == llvm::Type::getInt8Ty(*ctx) ||
+				   to_iter->first == llvm::Type::getInt16Ty(*ctx)) {
+					// convert to i32 first, then trunc from i32 to i8/i16
+					const auto to_i32_cast = call_conversion_func<cast_op>(from, llvm::Type::getInt32Ty(*ctx));
+					return builder->CreateTrunc(to_i32_cast, to_iter->first);
+				}
+			}
+			
+			// air.convert.<to_type>.<from_type>
+			std::string func_name = "air.convert.";
+			
+			if(to_iter->second[0] == '.') {
+				func_name += (to_signed ? 's' : 'u');
+			}
+			func_name += to_iter->second;
+			
+			func_name += '.';
+			if(from_iter->second[0] == '.') {
+				func_name += (from_signed ? 's' : 'u');
+			}
+			func_name += from_iter->second;
+			
+			SmallVector<llvm::Type*, 1> params(1, from_type);
+			const auto func_type = llvm::FunctionType::get(to_type, params, false);
+			return builder->CreateCall(M->getOrInsertFunction(func_name, func_type), from);
+		}
+		
+		// dummy
+		template <Instruction::CastOps cast_op, typename std::enable_if<!(cast_op == llvm::Instruction::FPToSI ||
+																		  cast_op == llvm::Instruction::FPToUI ||
+																		  cast_op == llvm::Instruction::SIToFP ||
+																		  cast_op == llvm::Instruction::UIToFP), int>::type = 0>
+		llvm::Value* call_conversion_func(llvm::Value* from, llvm::Type*) {
+			return from;
+		}
+		
+		enum METAL_KERNEL_ARG_REV_IDX : int32_t {
+			METAL_KERNEL_ARG_COUNT = 6,
+			METAL_KERNEL_SUB_GROUPS_ARG_COUNT = 10,
+		};
+		
+		enum METAL_VERTEX_ARG_REV_IDX : int32_t {
+			METAL_VERTEX_ID = -2,
+			METAL_INSTANCE_ID = -1,
+			
+			METAL_VERTEX_ARG_COUNT = 2,
+		};
+		
+		enum METAL_FRAGMENT_ARG_REV_IDX : int32_t {
+			METAL_POINT_COORD = -1,
+			
+			METAL_FRAGMENT_ARG_COUNT = 1,
+		};
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			for(auto& instr : F.getEntryBlock().getInstList()) {
+				if(!isa<AllocaInst>(instr)) {
+					alloca_insert = &instr;
+					break;
+				}
+			}
+			
+			const auto get_arg_by_idx = [&F](const int32_t rev_idx) -> llvm::Argument* {
+				auto arg_iter = F.arg_end();
+				std::advance(arg_iter, rev_idx);
+				return &*arg_iter;
+			};
+			
+			// check for sub-group support
+			const auto triple = llvm::Triple(M->getTargetTriple());
+			bool has_sub_group_support = false;
+			if (triple.getArch() == Triple::ArchType::air64) {
+				if (triple.getOS() == Triple::OSType::MacOSX) {
+					has_sub_group_support = true;
+				}
+			}
+			
+			// check for optional features: soft-printf, primitive id, barycentric coord
+			bool has_soft_printf = false, has_primitive_id = false, has_barycentric_coord = false;
+			if (auto soft_printf_meta = M->getNamedMetadata("floor.soft_printf")) {
+				has_soft_printf = true;
+			}
+			if (auto primitive_id_meta = M->getNamedMetadata("floor.primitive_id")) {
+				has_primitive_id = true;
+			}
+			if (auto barycentric_coord_meta = M->getNamedMetadata("floor.barycentric_coord")) {
+				has_barycentric_coord = true;
+			}
+			
+			// get args if this is a kernel function
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			if(is_kernel_func) {
+				if (F.arg_size() >= (has_sub_group_support ? METAL_KERNEL_SUB_GROUPS_ARG_COUNT : METAL_KERNEL_ARG_COUNT) + (has_soft_printf ? 1 : 0)) {
+					int32_t rev_idx = -1;
+					if (has_sub_group_support) {
+						num_sub_groups = get_arg_by_idx(rev_idx--);
+						sub_group_size = get_arg_by_idx(rev_idx--);
+						sub_group_local_id = get_arg_by_idx(rev_idx--);
+						sub_group_id = get_arg_by_idx(rev_idx--);
+					}
+					group_size = get_arg_by_idx(rev_idx--);
+					group_id = get_arg_by_idx(rev_idx--);
+					local_size = get_arg_by_idx(rev_idx--);
+					local_id = get_arg_by_idx(rev_idx--);
+					global_size = get_arg_by_idx(rev_idx--);
+					global_id = get_arg_by_idx(rev_idx--);
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(rev_idx--);
+					}
+				} else {
+					errs() << "invalid kernel function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					global_id = nullptr;
+					global_size = nullptr;
+					local_id = nullptr;
+					local_size = nullptr;
+					group_id = nullptr;
+					group_size = nullptr;
+					sub_group_id = nullptr;
+					sub_group_local_id = nullptr;
+					sub_group_size = nullptr;
+					num_sub_groups = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// get args if this is a vertex function
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			if(is_vertex_func) {
+				if (F.arg_size() >= METAL_VERTEX_ARG_COUNT + (has_soft_printf ? 1 : 0)) {
+					// TODO: this should be optional / only happen on request
+					vertex_id = get_arg_by_idx(METAL_VERTEX_ID);
+					instance_id = get_arg_by_idx(METAL_INSTANCE_ID);
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(-(METAL_VERTEX_ARG_COUNT + 1));
+					}
+				} else {
+					errs() << "invalid vertex function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					vertex_id = nullptr;
+					instance_id = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// get args if this is a fragment function
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			if(is_fragment_func) {
+				const uint32_t opt_arg_count = (has_soft_printf ? 1u : 0u) + (has_primitive_id ? 1u : 0u) + (has_barycentric_coord ? 1u : 0u);
+				if (F.arg_size() >= METAL_FRAGMENT_ARG_COUNT + opt_arg_count) {
+					point_coord = get_arg_by_idx(METAL_POINT_COORD);
+					
+					// NOTE: reverse order!
+					uint32_t opt_arg_counter = 1;
+					if (has_barycentric_coord) {
+						barycentric_coord = get_arg_by_idx(-(METAL_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						barycentric_coord = nullptr;
+					}
+					if (has_primitive_id) {
+						primitive_id = get_arg_by_idx(-(METAL_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						primitive_id = nullptr;
+					}
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(-(METAL_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						soft_printf = nullptr;
+					}
+				} else {
+					errs() << "invalid fragment function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					point_coord = nullptr;
+					primitive_id = nullptr;
+					barycentric_coord = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// update function signature / param list
+			if(is_kernel_func || is_vertex_func || is_fragment_func) {
+				std::vector<Type*> param_types;
+				for(auto& arg : F.args()) {
+					param_types.push_back(arg.getType());
+				}
+				auto new_func_type = FunctionType::get(F.getReturnType(), param_types, false);
+				F.mutateType(PointerType::get(new_func_type, 0));
+				F.mutateFunctionType(new_func_type);
+				
+				// always remove "norecurse" and "min-legal-vector-width"
+				F.removeFnAttr(Attribute::NoRecurse);
+				F.removeFnAttr("min-legal-vector-width");
+			}
+			
+			// visit everything in this function
+			was_modified = false; // reset every time
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			visit(F);
+			
+			// always modified
+			return was_modified || is_kernel_func || is_vertex_func || is_fragment_func;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<MetalFinal>::visit;
+		void visit(Instruction& I) {
+			// remove fpmath metadata from all instructions
+			if (MDNode* MD = I.getMetadata(LLVMContext::MD_fpmath)) {
+				I.setMetadata(LLVMContext::MD_fpmath, nullptr);
+				was_modified = true;
+			}
+			
+			InstVisitor<MetalFinal>::visit(I);
+		}
+		
+		static std::optional<std::string> get_suffix_for_type(llvm::Type* type, const bool is_signed) {
+			std::string ret = ".";
+			switch (type->getTypeID()) {
+				case llvm::Type::IntegerTyID:
+					ret += (is_signed ? "s." : "u.");
+					ret += "i" + std::to_string(cast<IntegerType>(type)->getBitWidth());
+					break;
+				// NOTE: we generally omit the ".f" here, because it's usually not wanted
+				case llvm::Type::HalfTyID:
+					ret += "f16";
+					break;
+				case llvm::Type::FloatTyID:
+					ret += "f32";
+					break;
+				case llvm::Type::DoubleTyID:
+					ret += "f64";
+					break;
+				default:
+					return {};
+			}
+			return ret;
+		}
+		
+		void visitIntrinsicInst(IntrinsicInst &I) {
+			const auto print_instr = [](const Instruction& instr) {
+				std::string instr_str;
+				llvm::raw_string_ostream instr_stream(instr_str);
+				instr.print(instr_stream);
+				return instr_str;
+			};
+			
+			// kill or replace certain llvm.* instrinsic calls
+			switch (I.getIntrinsicID()) {
+				case Intrinsic::experimental_noalias_scope_decl:
+				case Intrinsic::lifetime_start:
+				case Intrinsic::lifetime_end:
+				case Intrinsic::assume:
+					I.eraseFromParent();
+					was_modified = true;
+					break;
+				case Intrinsic::memcpy:
+				case Intrinsic::memset:
+				case Intrinsic::memmove:
+				case Intrinsic::dbg_addr:
+				case Intrinsic::dbg_label:
+				case Intrinsic::dbg_value:
+				case Intrinsic::dbg_declare:
+					// pass
+					break;
+					
+				// single arguments cases
+				case Intrinsic::abs: {
+					auto op_val = I.getOperand(0);
+					
+					// handled signedness and AIR function name
+					bool is_signed = true;
+					std::string func_name = "air.";
+					switch (I.getIntrinsicID()) {
+						case Intrinsic::abs:
+							func_name += "abs";
+							break;
+						default:
+							ctx->emitError(&I, "unexpected intrinsic:\n" + print_instr(I));
+							return;
+					}
+					
+					auto suffix = get_suffix_for_type(op_val->getType(), is_signed);
+					if (!suffix) {
+						ctx->emitError(&I, "unexpected type in intrinsic:\n" + print_instr(I));
+						return;
+					}
+					func_name += *suffix;
+					
+					// create the new call
+					SmallVector<llvm::Type*, 1> param_types { op_val->getType() };
+					const auto func_type = llvm::FunctionType::get(I.getType(), param_types, false);
+					builder->SetInsertPoint(&I);
+					
+					auto call = builder->CreateCall(M->getOrInsertFunction(func_name, func_type), { op_val });
+					call->setDebugLoc(I.getDebugLoc());
+					
+					I.replaceAllUsesWith(call);
+					I.eraseFromParent();
+					was_modified = true;
+					break;
+				}
+					
+				// two arguments cases
+				case Intrinsic::umin:
+				case Intrinsic::smin:
+				case Intrinsic::umax:
+				case Intrinsic::smax:
+				case Intrinsic::minnum:
+				case Intrinsic::maxnum: {
+					auto op_lhs = I.getOperand(0);
+					auto op_rhs = I.getOperand(1);
+					
+					// handled signedness and AIR function name
+					bool is_signed = true;
+					std::string func_name = "air.";
+					switch (I.getIntrinsicID()) {
+						case Intrinsic::umin:
+							is_signed = false;
+							func_name += "min";
+							break;
+						case Intrinsic::smin:
+							func_name += "min";
+							break;
+						case Intrinsic::umax:
+							is_signed = false;
+							func_name += "max";
+							break;
+						case Intrinsic::smax:
+							func_name += "max";
+							break;
+						case Intrinsic::minnum:
+							func_name += (op_lhs->getType()->isFloatTy() ? "fast_fmin" : "fmin");
+							break;
+						case Intrinsic::maxnum:
+							func_name += (op_lhs->getType()->isFloatTy() ? "fast_fmax" : "fmax");
+							break;
+						default:
+							ctx->emitError(&I, "unexpected intrinsic:\n" + print_instr(I));
+							return;
+					}
+					
+					auto suffix = get_suffix_for_type(op_lhs->getType(), is_signed);
+					if (!suffix) {
+						ctx->emitError(&I, "unexpected type in intrinsic:\n" + print_instr(I));
+						return;
+					}
+					func_name += *suffix;
+					
+					// create the new call
+					SmallVector<llvm::Type*, 2> param_types { op_lhs->getType(), op_rhs->getType() };
+					const auto func_type = llvm::FunctionType::get(I.getType(), param_types, false);
+					builder->SetInsertPoint(&I);
+					
+					auto call = builder->CreateCall(M->getOrInsertFunction(func_name, func_type), { op_lhs, op_rhs });
+					call->setDebugLoc(I.getDebugLoc());
+					
+					I.replaceAllUsesWith(call);
+					I.eraseFromParent();
+					was_modified = true;
+					break;
+				}
+					
+#if 0 // TODO: implement these
+				case Intrinsic::vector_reduce_fadd: {
+					auto init = I.getOperand(0);
+					auto vec = I.getOperand(1);
+					const auto vec_type = dyn_cast_or_null<FixedVectorType>(vec->getType());
+					if (!vec_type) {
+						ctx->emitError(&I, "expected vector type in operand #1:\n" + print_instr(I));
+						return;
+					}
+					const auto elem_type = vec_type->getElementType();
+					if (!elem_type->isFloatTy()) {
+						ctx->emitError(&I, "expected element type of vector to be f32:\n" + print_instr(I));
+					}
+					
+					const auto width = vec_type->getNumElements();
+					if (width != 1 && width != 2 && width != 3 && width != 4 && width != 8 && width != 16) {
+						ctx->emitError(&I, "unexpected vector width " + std::to_string(width) + ":\n" + print_instr(I));
+						return;
+					}
+					
+					SmallVector<llvm::Type*, 2> func_arg_types;
+					SmallVector<llvm::Value*, 2> func_args;
+					func_arg_types.push_back(vec_type);
+					func_arg_types.push_back(vec_type);
+					func_args.push_back(vec);
+					func_args.push_back(ConstantVector::getSplat(ElementCount::getFixed(width), ConstantFP::get(elem_type, 1.0)));
+					
+					// -> build get func name
+					const std::string get_func_name = "air.dot.v" + std::to_string(width) + "f32";
+					
+					AttrBuilder attr_builder;
+					attr_builder.addAttribute(llvm::Attribute::NoUnwind);
+					attr_builder.addAttribute(llvm::Attribute::ReadOnly);
+					auto func_attrs = AttributeList::get(*ctx, ~0, attr_builder);
+					
+					// create the air call
+					const auto func_type = llvm::FunctionType::get(elem_type, func_arg_types, false);
+					builder->SetInsertPoint(&I);
+					llvm::CallInst* get_call = builder->CreateCall(M->getOrInsertFunction(get_func_name, func_type, func_attrs), func_args);
+					get_call->setDoesNotThrow();
+					get_call->setOnlyReadsMemory();
+					get_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+					
+					// TODO: handle "init" if not 0
+					
+					I.replaceAllUsesWith(get_call);
+					I.eraseFromParent();
+					was_modified = true;
+					break;
+				}
+#endif
+				case Intrinsic::vector_reduce_add:
+				case Intrinsic::vector_reduce_and:
+				case Intrinsic::vector_reduce_fadd:
+				case Intrinsic::vector_reduce_fmax:
+				case Intrinsic::vector_reduce_fmin:
+				case Intrinsic::vector_reduce_fmul:
+				case Intrinsic::vector_reduce_mul:
+				case Intrinsic::vector_reduce_or:
+				case Intrinsic::vector_reduce_smax:
+				case Intrinsic::vector_reduce_smin:
+				case Intrinsic::vector_reduce_umax:
+				case Intrinsic::vector_reduce_umin:
+				case Intrinsic::vector_reduce_xor:
+				default: {
+					ctx->emitError(&I, "unknown/unhandled intrinsic:\n" + print_instr(I));
+					break;
+				}
+			}
+		}
+		
+		//
+		void visitCallInst(CallInst &I) {
+			// if this isn't a kernel function we don't need to do anything here (yet)
+			if(!is_kernel_func && !is_vertex_func && !is_fragment_func) return;
+			
+			const auto func = I.getCalledFunction();
+			if(!func) return;
+			const auto func_name = func->getName();
+			if(!func_name.startswith("floor.")) return;
+			
+			builder->SetInsertPoint(&I);
+			
+			// figure out which one we need
+			Argument* id;
+			bool get_from_vector = false;
+			if(func_name == "floor.get_global_id.i32") {
+				id = global_id;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_global_size.i32") {
+				id = global_size;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_local_id.i32") {
+				id = local_id;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_local_size.i32") {
+				id = local_size;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_group_id.i32") {
+				id = group_id;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_group_size.i32") {
+				id = group_size;
+				get_from_vector = true;
+			}
+			else if(func_name == "floor.get_sub_group_id.i32") {
+				id = sub_group_id;
+			}
+			else if(func_name == "floor.get_sub_group_local_id.i32") {
+				id = sub_group_local_id;
+			}
+			else if(func_name == "floor.get_sub_group_size.i32") {
+				id = sub_group_size;
+			}
+			else if(func_name == "floor.get_num_sub_groups.i32") {
+				id = num_sub_groups;
+			}
+			else if(func_name == "floor.get_work_dim.i32") {
+				if(group_size == nullptr) {
+					DBG(printf("failed to get group_size arg, probably not in a kernel function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				// special case
+				// => group_size.z == 1 ? (group_size.y == 1 ? 1 : 2) : 3
+				const auto size_z = builder->CreateExtractElement(group_size, builder->getInt32(2));
+				const auto size_y = builder->CreateExtractElement(group_size, builder->getInt32(1));
+				const auto cmp_z = builder->CreateICmp(ICmpInst::ICMP_EQ, size_z, builder->getInt32(1));
+				const auto cmp_y = builder->CreateICmp(ICmpInst::ICMP_EQ, size_y, builder->getInt32(1));
+				const auto sel_x_or_y = builder->CreateSelect(cmp_y, builder->getInt32(1), builder->getInt32(2));
+				const auto sel_xy_or_z = builder->CreateSelect(cmp_z, sel_x_or_y, builder->getInt32(3));
+				I.replaceAllUsesWith(sel_xy_or_z);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.get_vertex_id.i32") {
+				if(vertex_id == nullptr) {
+					DBG(printf("failed to get vertex_id arg, probably not in a vertex function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(vertex_id);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.get_instance_id.i32") {
+				if(instance_id == nullptr) {
+					DBG(printf("failed to get instance_id arg, probably not in a vertex function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(instance_id);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.get_point_coord.float2") {
+				if(point_coord == nullptr) {
+					DBG(printf("failed to get point_coord arg, probably not in a fragment function?\n"); fflush(stdout);)
+					return;
+				}
+			
+				I.replaceAllUsesWith(point_coord);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.get_printf_buffer") {
+				if(soft_printf == nullptr) {
+					DBG(printf("failed to get printf_buffer arg, probably not in a kernel/vertex/fragment function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				// special case
+				I.replaceAllUsesWith(soft_printf);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.get_primitive_id.i32") {
+				if (primitive_id == nullptr) {
+					llvm::errs() << "failed to get primitive_id arg, not in a fragment function or feature is not enabled\n";
+					llvm::errs().flush();
+					return;
+				}
+				
+				I.replaceAllUsesWith(primitive_id);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.get_barycentric_coord.float3") {
+				if (barycentric_coord == nullptr) {
+					llvm::errs() << "failed to get barycentric_coord arg, not in a fragment function or feature is not enabled\n";
+					llvm::errs().flush();
+					return;
+				}
+			
+				I.replaceAllUsesWith(barycentric_coord);
+				I.eraseFromParent();
+				return;
+			}
+			// unknown -> ignore for now
+			else return;
+			
+			if(id == nullptr) {
+				DBG(printf("failed to get id arg, probably not in a kernel function?\n"); fflush(stdout);)
+				return;
+			}
+			
+			// replace call with vector load / elem extraction from the appropriate vector
+			I.replaceAllUsesWith(get_from_vector ? builder->CreateExtractElement(id, I.getOperand(0)) : id);
+			I.eraseFromParent();
+		}
+		
+		// like SPIR, Metal only supports scalar conversion ops ->
+		// * scalarize source vector
+		// * call conversion op for each scalar
+		// * reassemble a vector from the converted scalars
+		// * replace all uses of the original vector
+		template <Instruction::CastOps cast_op>
+		__attribute__((always_inline))
+		bool vec_to_scalar_ops(CastInst& I) {
+			if(!I.getType()->isVectorTy()) return false;
+			
+			// start insertion before instruction
+			builder->SetInsertPoint(&I);
+			
+			// setup
+			auto src_vec = I.getOperand(0);
+			const auto src_vec_type = dyn_cast<FixedVectorType>(src_vec->getType());
+			if (!src_vec_type) {
+				return false;
+			}
+			const auto dim = src_vec_type->getNumElements();
+			
+			const auto si_type = I.getDestTy();
+			const auto dst_scalar_type = si_type->getScalarType();
+			llvm::Value* dst_vec = UndefValue::get(si_type);
+			
+			// iterate over all vector components, emit a scalar instruction and insert into a new vector
+			for(uint32_t i = 0; i < dim; ++i) {
+				auto scalar = builder->CreateExtractElement(src_vec, builder->getInt32(i));
+				llvm::Value* cast;
+				switch(cast_op) {
+					case llvm::Instruction::FPToSI:
+					case llvm::Instruction::FPToUI:
+					case llvm::Instruction::SIToFP:
+					case llvm::Instruction::UIToFP:
+						cast = call_conversion_func<cast_op>(scalar, dst_scalar_type);
+						break;
+					default:
+						cast = builder->CreateCast(cast_op, scalar, dst_scalar_type);
+						break;
+				}
+				dst_vec = builder->CreateInsertElement(dst_vec, cast, builder->getInt32(i));
+			}
+			
+			// finally, replace all uses with the new vector and remove the old vec instruction
+			I.replaceAllUsesWith(dst_vec);
+			I.eraseFromParent();
+			was_modified = true;
+			return true;
+		}
+		
+		// si/ui/fp -> si/ui/fp conversions require a call to an intrinsic air function (air.convert.*)
+		template <Instruction::CastOps cast_op>
+		__attribute__((always_inline))
+		void scalar_conversion(CastInst& I) {
+			builder->SetInsertPoint(&I);
+			
+			// replace original conversion
+			I.replaceAllUsesWith(call_conversion_func<cast_op>(I.getOperand(0), I.getDestTy()));
+			I.eraseFromParent();
+			was_modified = true;
+		}
+		
+		void visitTruncInst(TruncInst &I) {
+			vec_to_scalar_ops<Instruction::Trunc>(I);
+		}
+		void visitZExtInst(ZExtInst &I) {
+			vec_to_scalar_ops<Instruction::ZExt>(I);
+		}
+		void visitSExtInst(SExtInst &I) {
+			vec_to_scalar_ops<Instruction::SExt>(I);
+		}
+		void visitFPTruncInst(FPTruncInst &I) {
+			vec_to_scalar_ops<Instruction::FPTrunc>(I);
+		}
+		void visitFPExtInst(FPExtInst &I) {
+			vec_to_scalar_ops<Instruction::FPExt>(I);
+		}
+		void visitFPToUIInst(FPToUIInst &I) {
+			if(!vec_to_scalar_ops<Instruction::FPToUI>(I)) {
+				scalar_conversion<Instruction::FPToUI>(I);
+			}
+		}
+		void visitFPToSIInst(FPToSIInst &I) {
+			if(!vec_to_scalar_ops<Instruction::FPToSI>(I)) {
+				scalar_conversion<Instruction::FPToSI>(I);
+			}
+		}
+		void visitUIToFPInst(UIToFPInst &I) {
+			if(!vec_to_scalar_ops<Instruction::UIToFP>(I)) {
+				scalar_conversion<Instruction::UIToFP>(I);
+			}
+		}
+		void visitSIToFPInst(SIToFPInst &I) {
+			if(!vec_to_scalar_ops<Instruction::SIToFP>(I)) {
+				scalar_conversion<Instruction::SIToFP>(I);
+			}
+		}
+		
+		// metal can only handle i32 indices
+		void visitExtractElement(ExtractElementInst& EEI) {
+			const auto idx_op = EEI.getIndexOperand();
+			const auto idx_type = idx_op->getType();
+			if(!idx_type->isIntegerTy(32)) {
+				if(const auto const_idx_op = dyn_cast_or_null<ConstantInt>(idx_op)) {
+					EEI.setOperand(1 /* idx op */, builder->getInt32((int32_t)const_idx_op->getValue().getZExtValue()));
+				}
+				else {
+					builder->SetInsertPoint(&EEI);
+					const auto i32_index = builder->CreateIntCast(idx_op, builder->getInt32Ty(), false);
+					EEI.setOperand(1 /* idx op */, i32_index);
+				}
+				was_modified = true;
+			}
+		}
+		
+		// metal can only handle i32 indices
+		void visitInsertElement(InsertElementInst& IEI) {
+			const auto idx_op = IEI.llvm::User::getOperand(2);
+			const auto idx_type = idx_op->getType();
+			if(!idx_type->isIntegerTy(32)) {
+				if(const auto const_idx_op = dyn_cast_or_null<ConstantInt>(idx_op)) {
+					IEI.setOperand(2 /* idx op */, builder->getInt32((int32_t)const_idx_op->getValue().getZExtValue()));
+				}
+				else {
+					builder->SetInsertPoint(&IEI);
+					const auto i32_index = builder->CreateIntCast(idx_op, builder->getInt32Ty(), false);
+					IEI.setOperand(2 /* idx op */, i32_index);
+				}
+				was_modified = true;
+			}
+		}
+		
+		void visitAllocaInst(AllocaInst &AI) {
+			if(!enable_intel_workarounds) return;
+			DBG(errs() << "alloca: " << AI << ", " << *AI.getType() << "\n";)
+			
+			BasicAAResult BAR(createLegacyPMBasicAAResult(*this, *func));
+			AAResults AA(createLegacyPMAAResults(*this, *func, BAR));
+			
+			// recursively find all users of this alloca + store all select and phi instructions that select/choose based on the alloca pointer
+			std::vector<Instruction*> users;
+			std::unordered_set<Instruction*> visited;
+			const std::function<void(Instruction&)> collect_users = [&users, &collect_users, &visited, &AI, &AA](Instruction& I) {
+				for(auto user : I.users()) {
+					auto instr = cast<Instruction>(user);
+					const auto has_visited = visited.insert(instr);
+					if(!has_visited.second) continue;
+					
+					// TODO: ideally, we want to track all GEPs and bitcasts to/of the alloca and only add select/phi instructions that
+					//       either use these or directly use the alloca (and not all pointers) - for now, AA will do
+					if(SelectInst* SI = dyn_cast<SelectInst>(user)) {
+						DBG(errs() << ">> select: " << *SI << "\n";)
+						DBG(errs() << "cond: " << *SI->getCondition() << "\n";)
+						DBG(errs() << "ops: " << *SI->getTrueValue() << ", " << *SI->getFalseValue() << "\n";)
+						
+						// skip immediately if not a pointer type
+						if(SI->getTrueValue()->getType()->isPointerTy() /* false val has the same type */) {
+							// check if either true or false alias with our alloca
+							const auto aa_res_true = AA.alias(SI->getTrueValue(), &AI);
+							const auto aa_res_false = AA.alias(SI->getFalseValue(), &AI);
+							DBG(errs() << "aa: " << aa_res_true << ", " << aa_res_false << "\n";)
+							if(aa_res_true != AliasResult::NoAlias ||
+							   aa_res_false != AliasResult::NoAlias) {
+								// if so, add this select
+								users.push_back(SI);
+							}
+						}
+					}
+					else if(PHINode* PHI = dyn_cast<PHINode>(user)) {
+						DBG(errs() << ">> phi: " << *PHI << "\n";)
+						DBG(errs() << "type: " << *PHI->getType() << "\n";)
+						
+						// skip immediately if not a pointer type
+						if(PHI->getType()->isPointerTy()) {
+							// check if it aliases with our alloca
+							const auto aa_res = AA.alias(PHI, &AI);
+							DBG(errs() << "aa: " << aa_res << "\n";)
+							if(aa_res != AliasResult::NoAlias) {
+								// if so, add this phi node
+								users.push_back(PHI);
+							}
+						}
+					}
+					collect_users(*instr);
+				}
+			};
+			collect_users(AI);
+			
+			DBG({
+				errs() << "####### users ##\n";
+				for(const auto& user : users) {
+					errs() << "user: " << *user << "\n";
+				}
+				errs() << "\n";
+			})
+			
+			// select replacement strategy:
+			// * create a tmp alloca that will later hold the selected data
+			// * replace the select with two branches (true/false)
+			// * depending on the select condition, branch to either true/false branch
+			// * inside these branches, store the corresponding true/false value into our tmp alloca, then branch back to after the select
+			// * remove the select
+			const auto select_replace = [&](SelectInst* SI) {
+				builder->SetInsertPoint(alloca_insert);
+				auto tmp_alloca = builder->CreateAlloca(AI.getType()->getPointerElementType(), nullptr, "sel_tmp");
+				tmp_alloca->setAlignment(AI.getAlign());
+				
+				// create our branch condition and true/false blocks that will replace the select
+				auto bb_true = BasicBlock::Create(*ctx, "sel.true", func);
+				auto bb_false = BasicBlock::Create(*ctx, "sel.false", func);
+				builder->SetInsertPoint(SI);
+				builder->CreateCondBr(SI->getCondition(), bb_true, bb_false);
+				
+				// split block before the select instruction so that we can branch back to it later
+				auto bb_start = SI->getParent();
+				auto bb_end = SI->getParent()->splitBasicBlock(SI);
+				// remove automatically inserted branch instruction from parent, since we already have a branch instruction
+				bb_start->getTerminator()->eraseFromParent();
+				
+				// create true/false branches that will copy the true/false data to our tmp alloca accordingly
+				// -> true branch
+				builder->SetInsertPoint(bb_true);
+				builder->CreateStore(builder->CreateLoad(SI->getTrueValue()->getType()->getPointerElementType(), SI->getTrueValue()), tmp_alloca);
+				builder->CreateBr(bb_end);
+				
+				// -> false branch
+				builder->SetInsertPoint(bb_false);
+				builder->CreateStore(builder->CreateLoad(SI->getFalseValue()->getType()->getPointerElementType(), SI->getFalseValue()), tmp_alloca);
+				builder->CreateBr(bb_end);
+				
+				// cleanup, replace select instruction with our new alloca
+				SI->replaceAllUsesWith(tmp_alloca);
+				SI->eraseFromParent();
+			};
+			
+			// phi replacement strategy:
+			// * create a tmp alloca (pointer), this will be used to store all phi pointers
+			// * iterate over all incoming values/pointers, then create a store of their pointer to the tmp pointer in their originating block
+			// * create a load from the tmp alloca and replace all uses of the phi node with it
+			// NOTE: loads and stores are volatile, so that no optimization can do any re-phi-ification(tm) later on
+			const auto phi_replace = [&](PHINode* PHI) {
+				auto phi_tmp_alloca = new AllocaInst(PHI->getType(), 0, nullptr, PHI->getName() + ".tmp", alloca_insert);
+				
+				for(uint32_t i = 0; i < PHI->getNumIncomingValues(); ++i) {
+					auto origin = PHI->getIncomingBlock(i);
+					new StoreInst(PHI->getIncomingValue(i), phi_tmp_alloca, true, origin->getTerminator());
+				}
+				
+				auto load_repl = new LoadInst(PHI->getType(), phi_tmp_alloca, PHI->getName() + ".repl", true,
+											  PHI->getParent()->getFirstNonPHI());
+				PHI->replaceAllUsesWith(load_repl);
+				PHI->eraseFromParent();
+			};
+			
+			for(const auto& user : users) {
+				if(SelectInst* SI = dyn_cast<SelectInst>(user)) {
+					select_replace(SI);
+				}
+				else if(PHINode* PHI = dyn_cast<PHINode>(user)) {
+					phi_replace(PHI);
+				}
+			}
+			was_modified = !users.empty();
+		}
+		
+	};
+	
+	// MetalFinalModuleCleanup:
+	// * image storage class name replacement
+	// * calling convention cleanup
+	// * strip unused functions/prototypes/externs
+	// * debug info cleanup
+	struct MetalFinalModuleCleanup : public ModulePass {
+		static char ID; // Pass identification, replacement for typeid
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		bool was_modified { false };
+		
+		MetalFinalModuleCleanup() : ModulePass(ID) {
+			initializeMetalFinalModuleCleanupPass(*PassRegistry::getPassRegistry());
+		}
+		
+		// this finds all libfloor image storage class structs and replaces their names with the appropriate Apple Metal texture struct type name
+		// NOTE: we need to do this, since Apple decided to handle these specially based on their name alone (e.g. no allocating additional registers)
+		bool run_array_of_images_name_replacement() {
+			std::vector<llvm::StructType*> image_storage_types;
+			for (const auto& st_type : ctx->pImpl->NamedStructTypes) {
+				if (st_type.first().startswith("class.floor_image::image")) {
+					image_storage_types.emplace_back(st_type.second);
+				}
+			}
+			for (auto& st_type : image_storage_types) {
+				if (st_type->getNumElements() != 1) {
+					// we only expect a single element
+					continue;
+				}
+				auto img_ptr_type = st_type->getElementType(0);
+				if (!img_ptr_type->isPointerTy()) {
+					// expected a pointer type
+					continue;
+				}
+				auto img_type = dyn_cast_or_null<llvm::StructType>(img_ptr_type->getPointerElementType());
+				if (!img_type || !img_type->isOpaque()) {
+					// expected an opaque struct type
+					continue;
+				}
+				
+				// we already emit the correct texture opaque texture type name -> find the corresponding Metal struct name
+				static const std::unordered_map<std::string, std::string> metal_name_lut {
+					{ "struct._texture_1d_t", "struct.metal::texture1d" },
+					{ "struct._texture_1d_array_t", "struct.metal::texture1d_array" },
+					{ "struct._texture_2d_t", "struct.metal::texture2d" },
+					{ "struct._texture_2d_array_t", "struct.metal::texture2d_array" },
+					{ "struct._depth_2d_t", "struct.metal::depth2d" },
+					{ "struct._depth_2d_array_t", "struct.metal::depth2d_array" },
+					{ "struct._texture_2d_ms_t", "struct.metal::texture2d_ms" },
+					{ "struct._texture_2d_ms_array_t", "struct.metal::texture2d_ms_array" },
+					{ "struct._depth_2d_ms_t", "struct.metal::depth2d_ms" },
+					{ "struct._depth_2d_ms_array_t", "struct.metal::depth2d_ms_array" },
+					{ "struct._texture_cube_t", "struct.metal::texturecube" },
+					{ "struct._texture_cube_array_t", "struct.metal::texturecube_array" },
+					{ "struct._depth_cube_t", "struct.metal::depthcube" },
+					{ "struct._depth_cube_array_t", "struct.metal::depthcube_array" },
+					{ "struct._texture_3d_t", "struct.metal::texture3d" },
+				};
+				auto repl_iter = metal_name_lut.find(img_type->getName().str());
+				if (repl_iter == metal_name_lut.end()) {
+					continue;
+				}
+				
+				//llvm::dbgs() << "replace " << st_type->getName().str() << " (" << img_type->getName().str() << ") -> " << repl_iter->second << "\n";
+				st_type->setName(repl_iter->second);
+			}
+			
+			return false;
+		}
+		
+		bool runOnModule(Module& Mod) override {
+			M = &Mod;
+			ctx = &M->getContext();
+			
+			bool module_modified = run_array_of_images_name_replacement();
+			
+			// * strip floor_* calling convention from all functions and their users (replace it with C CC)
+			// * kill all functions named floor.*
+			// * strip debug info from declarations
+			for (auto func_iter = Mod.begin(); func_iter != Mod.end();) {
+				auto& func = *func_iter;
+				if (func.getName().startswith("floor.")) {
+					if (func.getNumUses() != 0) {
+						errs() << func.getName() << " should not have any uses at this point!\n";
+					}
+					++func_iter; // inc before erase
+					func.eraseFromParent();
+					module_modified = true;
+					continue;
+				}
+				
+				if (func.getCallingConv() != CallingConv::C) {
+					func.setCallingConv(CallingConv::C);
+					for (auto user : func.users()) {
+						if (auto CB = dyn_cast<CallBase>(user)) {
+							CB->setCallingConv(CallingConv::C);
+						}
+					}
+					module_modified = true;
+				}
+				
+				if (func.isDeclaration()) {
+					if (DISubprogram* sub_prog_dbg = func.getSubprogram(); sub_prog_dbg) {
+						func.setSubprogram(nullptr);
+						module_modified = true;
+					}
+				}
+				
+				++func_iter;
+			}
+			return module_modified;
+		}
+		
+	};
+	
+}
+
+char MetalFirst::ID = 0;
+FunctionPass *llvm::createMetalFirstPass(const bool enable_intel_workarounds,
+										 const bool enable_nvidia_workarounds) {
+	return new MetalFirst(enable_intel_workarounds, enable_nvidia_workarounds);
+}
+INITIALIZE_PASS_BEGIN(MetalFirst, "MetalFirst", "MetalFirst Pass", false, false)
+INITIALIZE_PASS_END(MetalFirst, "MetalFirst", "MetalFirst Pass", false, false)
+
+char MetalFinal::ID = 0;
+FunctionPass *llvm::createMetalFinalPass(const bool enable_intel_workarounds,
+										 const bool enable_nvidia_workarounds) {
+	return new MetalFinal(enable_intel_workarounds, enable_nvidia_workarounds);
+}
+INITIALIZE_PASS_BEGIN(MetalFinal, "MetalFinal", "MetalFinal Pass", false, false)
+INITIALIZE_PASS_END(MetalFinal, "MetalFinal", "MetalFinal Pass", false, false)
+
+char MetalFinalModuleCleanup::ID = 0;
+ModulePass *llvm::createMetalFinalModuleCleanupPass() {
+	return new MetalFinalModuleCleanup();
+}
+INITIALIZE_PASS_BEGIN(MetalFinalModuleCleanup, "MetalFinal module cleanup", "MetalFinal module cleanup Pass", false, false)
+INITIALIZE_PASS_END(MetalFinalModuleCleanup, "MetalFinal module cleanup", "MetalFinal module cleanup Pass", false, false)
diff --git a/llvm/lib/Transforms/LibFloor/MetalImage.cpp b/llvm/lib/Transforms/LibFloor/MetalImage.cpp
new file mode 100644
index 000000000000..66da29964474
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/MetalImage.cpp
@@ -0,0 +1,741 @@
+//===- MetalImage.cpp - Metal-specific floor image transformations --------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass implements the Metal-specific floor image transformations, i.e.
+// floor.opaque.<read/write function>.* -> air.<read/write function>.*
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/LibFloor/FloorImage.h"
+#include <unordered_map>
+using namespace llvm;
+
+#define DEBUG_TYPE "MetalImage"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	struct MetalImage : public FloorImageBasePass {
+		static char ID; // Pass identification, replacement for typeid
+		
+		static constexpr const uint32_t Metal_ConstantAS = 2;
+		
+		MetalImage(const uint32_t image_capabilities_ = 0) :
+		FloorImageBasePass(ID, IMAGE_TYPE_ID::OPAQUE, image_capabilities_) {
+			initializeMetalImagePass(*PassRegistry::getPassRegistry());
+		}
+		
+		static const char* type_to_geom(const COMPUTE_IMAGE_TYPE& image_type) {
+			switch(image_type) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D:
+					return "texture_1d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_ARRAY:
+					return "texture_1d_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_BUFFER:
+					return "texture_1d_buffer";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+					return "depth_2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D:
+					return "texture_2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+					return "depth_2d_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_ARRAY:
+					return "texture_2d_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+					return "depth_2d_ms";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+					return "texture_2d_ms";
+				case COMPUTE_IMAGE_TYPE::IMAGE_3D:
+					return "texture_3d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+					return "depth_cube";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+					return "depth_cube_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE:
+					return "texture_cube";
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE_ARRAY:
+					return "texture_cube_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY: // since Metal 2.1
+					return "depth2d_ms_array";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY: // since Metal 2.1
+					return "texture2d_ms_array";
+				default:
+					return nullptr;
+			}
+		}
+		
+		void handle_read_image(Instruction& I,
+							   const StringRef& func_name,
+							   llvm::Value* img_handle_arg,
+							   const COMPUTE_IMAGE_TYPE& image_type,
+							   llvm::ConstantInt* const_sampler_arg,
+							   llvm::Value* dyn_sampler_arg,
+							   llvm::Value* coord_arg,
+							   llvm::Value* layer_arg,
+							   llvm::Value* sample_arg,
+							   llvm::Value* offset_arg,
+							   const SmallVector<llvm::Value*, 3>& offset_elems,
+							   const bool is_offset,
+							   llvm::Value* lod_or_bias_arg,
+							   const bool is_lod_or_bias, // true: lod, false: bias
+							   llvm::Value* dpdx_arg,
+							   llvm::Value* dpdy_arg,
+							   const bool is_gradient,
+							   const COMPARE_FUNCTION& compare_function,
+							   llvm::Value* compare_value_arg,
+							   const bool is_compare) override {
+			SmallVector<llvm::Type*, 16> func_arg_types;
+			SmallVector<llvm::Value*, 16> func_args;
+			
+			// -> return data
+			std::string dtype;
+			llvm::Type* ret_type;
+			if(func_name.endswith(".float")) {
+				dtype = "v4f32";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4);
+			}
+			else if(func_name.endswith(".int")) {
+				dtype = "s.v4i32";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".uint")) {
+				dtype = "u.v4i32";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".half")) {
+				dtype = "v4f16";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getHalfTy(*ctx), 4);
+			}
+			else if(func_name.endswith(".short")) {
+				dtype = "s.v4i16";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt16Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".ushort")) {
+				dtype = "u.v4i16";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt16Ty(*ctx), 4);
+			}
+			// unknown -> ignore
+			else return;
+			
+			// -> geom
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(image_type);
+			const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type);
+			const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			// -> coord type
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if(is_msaa && !coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer for msaa images");
+				return;
+			}
+			if(is_cube && !coord_type->isFloatTy()) {
+				ctx->emitError(&I, "coordinate type must be float for cube images");
+				return;
+			}
+			
+			// air.read_* or air.sample_*?
+			// for msaa: always read
+			// for all else: read if int coords, sample if float coords
+			const bool is_sample_call = (!is_msaa && !coord_type->isIntegerTy());
+			
+			// img handle and sampler
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			// only add the sampler arg if this is a sample call
+			bool set_sampler_attrs = false;
+			uint32_t sampler_param_idx = 0;
+			if(is_sample_call) {
+				// sampler handling since Metal 2.0+ is more complex
+				// -> need to transform sampler into a global constant
+				// -> need to add this constant to !air.sampler_states metadata
+				if (dyn_sampler_arg == nullptr) {
+					ctx->emitError(&I, "sampler arg should be dynamic");
+					return;
+				}
+				
+				auto ce_sampler = dyn_cast<ConstantExpr>(dyn_sampler_arg);
+				if (ce_sampler == nullptr) {
+					ctx->emitError(&I, "sampler arg must be a constant expression");
+					return;
+				}
+				
+				if (!ce_sampler->isCast() || ce_sampler->getOpcode() != llvm::Instruction::IntToPtr) {
+					ctx->emitError(&I, "sampler arg must be a constant inttoptr expression");
+					return;
+				}
+				
+				auto sampler_constant_value = dyn_cast<ConstantInt>(ce_sampler->getOperand(0));
+				if (sampler_constant_value == nullptr) {
+					ctx->emitError(&I, "sampler arg must contain a constant value");
+					return;
+				}
+				
+				// create global sampler state
+				// NOTE: since we're often using the same sampler -> cache them
+				static std::unordered_map<uint64_t, GlobalVariable*> sample_state_cache;
+				auto sampler_constant_value_u64 = sampler_constant_value->getZExtValue();
+				auto cache_iter = sample_state_cache.find(sampler_constant_value_u64);
+				GlobalVariable* sampler_state = nullptr;
+				if (cache_iter != sample_state_cache.end()) {
+					sampler_state = cache_iter->second;
+				} else {
+					sampler_state = new GlobalVariable(*M,
+													   sampler_constant_value->getType(),
+													   true,
+													   GlobalVariable::InternalLinkage,
+													   sampler_constant_value,
+													   "__air_sampler_state",
+													   nullptr,
+													   GlobalValue::NotThreadLocal,
+													   Metal_ConstantAS);
+					sampler_state->setAlignment(MaybeAlign { 8u }); // always 8-byte aligned
+					sample_state_cache.emplace(sampler_constant_value_u64, sampler_state);
+					
+					// insert metadata
+					auto ctx = &M->getContext();
+					SmallVector<llvm::Metadata*, 2> sampler_md_info;
+					sampler_md_info.push_back(llvm::MDString::get(*ctx, "air.sampler_state"));
+					sampler_md_info.push_back(llvm::ConstantAsMetadata::get(sampler_state));
+					
+					auto samplers_md = M->getOrInsertNamedMetadata("air.sampler_states");
+					samplers_md->addOperand(llvm::MDNode::get(*ctx, sampler_md_info));
+				}
+				
+				// still need to bitcast to %struct._sampler_t*
+				auto sampler_type = dyn_cast<PointerType>(dyn_sampler_arg->getType());
+				auto cast_sampler_state = ConstantExpr::getBitCast(sampler_state, sampler_type);
+				
+				func_arg_types.push_back(cast_sampler_state->getType());
+				func_args.push_back(cast_sampler_state);
+				
+				set_sampler_attrs = true;
+				sampler_param_idx = func_args.size() - 1;
+			}
+			
+			if(is_depth) {
+				// must always add the depth type 1 (== float)
+				func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+				func_args.push_back(builder->getInt32(1));
+				
+				// depth return type is always a float
+				ret_type = llvm::Type::getFloatTy(*ctx);
+				dtype = "f32";
+				
+				if(is_compare) {
+					if(!is_sample_call) {
+						ctx->emitError(&I, "compare must be a sample call");
+						return;
+					}
+				}
+			}
+			else {
+				if(is_compare) {
+					ctx->emitError(&I, "compare is only allowed with depth types");
+					return;
+				}
+			}
+			
+			// handle offset
+			llvm::Value* offset_coord_arg = coord_arg;
+			if(!is_sample_call && is_offset) {
+				offset_coord_arg = builder->CreateAdd(coord_arg, offset_arg);
+			}
+			
+			// -> coords: coords, sample, face, layer
+			if(coord_vec_type->getNumElements() == 1) {
+				// 1D coord, make it scalar
+				func_arg_types.push_back(coord_type);
+				func_args.push_back(builder->CreateExtractElement(offset_coord_arg, builder->getInt32(0)));
+			}
+			else {
+				// normal coord arg
+				if(!is_cube || coord_type->isFloatTy()) {
+					func_arg_types.push_back(offset_coord_arg->getType());
+					func_args.push_back(offset_coord_arg);
+				}
+				// cube with int coords
+				else {
+					// extract the face integer from the coords
+					const auto face_arg = builder->CreateExtractElement(coord_arg, builder->getInt32(2));
+					
+					// create new int2 arg
+					llvm::Value* coord_i2_arg = UndefValue::get(llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 2));
+					coord_i2_arg = builder->CreateInsertElement(coord_i2_arg, builder->CreateExtractElement(offset_coord_arg, builder->getInt32(0)), builder->getInt32(0));
+					coord_i2_arg = builder->CreateInsertElement(coord_i2_arg, builder->CreateExtractElement(offset_coord_arg, builder->getInt32(1)), builder->getInt32(1));
+					
+					func_arg_types.push_back(coord_i2_arg->getType());
+					func_args.push_back(coord_i2_arg);
+					
+					func_arg_types.push_back(face_arg->getType());
+					func_args.push_back(face_arg);
+				}
+			}
+			
+			if(is_msaa) {
+				func_arg_types.push_back(sample_arg->getType());
+				func_args.push_back(sample_arg);
+			}
+			
+			if(is_array) {
+				func_arg_types.push_back(layer_arg->getType());
+				func_args.push_back(layer_arg);
+			}
+			
+			if(is_compare) {
+				func_arg_types.push_back(compare_value_arg->getType());
+				func_args.push_back(compare_value_arg);
+			}
+			
+			// -> additional args: lod, bias, gradient, offset
+			if(!is_sample_call) {
+				// -> read
+				if(!is_msaa) { // msaa is always lod 0, hence needs no arg
+					// -> lod
+					if(is_lod_or_bias) {
+						// for read, only int lods are allowed
+						if(!lod_or_bias_arg->getType()->isIntegerTy()) {
+							// convert to int
+							const auto int_lod = builder->CreateFPToSI(lod_or_bias_arg, llvm::Type::getInt32Ty(*ctx));
+							func_arg_types.push_back(int_lod->getType());
+							func_args.push_back(int_lod);
+						}
+						else {
+							func_arg_types.push_back(lod_or_bias_arg->getType());
+							func_args.push_back(lod_or_bias_arg);
+						}
+					}
+					else {
+						// if no lod is specified (bias is not allowed here), add a 0 lod
+						func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+						func_args.push_back(builder->getInt32(0));
+					}
+				}
+				
+				if (is_metal_2_3) {
+					// Metal 2.3+ has an additional (unknown) i32 0 argument
+					func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+					func_args.push_back(builder->getInt32(0));
+				}
+			}
+			else {
+				// -> sample
+				
+				// -> gradient
+				if(is_gradient) {
+					func_arg_types.push_back(dpdx_arg->getType());
+					func_args.push_back(dpdx_arg);
+					func_arg_types.push_back(dpdy_arg->getType());
+					func_args.push_back(dpdy_arg);
+					
+					if (is_metal_2_4) {
+						// Metal 2.4 has an additional "min LOD clamp" f32 argument -> set to 0.0 for now
+						func_arg_types.push_back(llvm::Type::getFloatTy(*ctx));
+						func_args.push_back(ConstantFP::get(llvm::Type::getFloatTy(*ctx), 0.0f));
+					}
+				}
+				
+				// -> offset
+				if(!is_cube) { // cube allows no offset
+					func_arg_types.push_back(llvm::Type::getInt1Ty(*ctx));
+					func_args.push_back(builder->getInt1(is_offset));
+					
+					func_arg_types.push_back(offset_arg->getType());
+					func_args.push_back(offset_arg);
+				}
+				
+				// -> lod / bias
+				if(!is_gradient) {
+					// lod or bias?
+					func_arg_types.push_back(llvm::Type::getInt1Ty(*ctx));
+					func_args.push_back(builder->getInt1(is_lod_or_bias));
+					
+					// for sample, only float lods are allowed
+					if(!lod_or_bias_arg->getType()->isFloatTy()) {
+						// convert to float
+						const auto float_lod = builder->CreateSIToFP(lod_or_bias_arg, llvm::Type::getFloatTy(*ctx));
+						func_arg_types.push_back(float_lod->getType());
+						func_args.push_back(float_lod);
+					}
+					else {
+						func_arg_types.push_back(lod_or_bias_arg->getType());
+						func_args.push_back(lod_or_bias_arg);
+					}
+				}
+				
+				if (is_metal_2_3) {
+					// Metal 2.3 has an additional (unknown) f32 0.0 argument
+					// NOTE: do not emit this for Metal 2.4+ when a gradient is used
+					if (!is_metal_2_4 || (is_metal_2_4 && !is_gradient)) {
+						func_arg_types.push_back(llvm::Type::getFloatTy(*ctx));
+						func_args.push_back(ConstantFP::get(llvm::Type::getFloatTy(*ctx), 0.0f));
+					}
+				}
+				
+				// Metal 2.0+ has an additional (unknown) i32 0 argument
+				func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+				func_args.push_back(builder->getInt32(0));
+			}
+			
+			// Metal 2.3+: return type is now always { <sample/read-type>, i8 }
+			if (is_metal_2_3) {
+				ret_type = llvm::StructType::get(*ctx, { ret_type, llvm::Type::getInt8Ty(*ctx) }, false /* !packed */);
+				assert(((llvm::StructType*)ret_type)->isLiteral() && "must be literal");
+			}
+			
+			// -> build read func name
+			std::string read_func_name = "air.";
+			
+			read_func_name += (is_sample_call ? "sample_" : "read_");
+			if(is_compare) read_func_name += "compare_";
+			read_func_name += geom;
+			if(is_gradient) read_func_name += "_grad";
+			
+			read_func_name += '.' + dtype;
+			
+			AttrBuilder attr_builder;
+			attr_builder.addAttribute(llvm::Attribute::Convergent);
+			attr_builder.addAttribute(llvm::Attribute::ArgMemOnly);
+			attr_builder.addAttribute(llvm::Attribute::NoUnwind);
+			attr_builder.addAttribute(llvm::Attribute::ReadOnly);
+			auto func_attrs = AttributeList::get(*ctx, ~0, attr_builder);
+			
+			// create the air call
+			const auto func_type = llvm::FunctionType::get(ret_type, func_arg_types, false);
+			llvm::CallInst* read_call = builder->CreateCall(M->getOrInsertFunction(read_func_name, func_type, func_attrs), func_args);
+			read_call->setConvergent();
+			read_call->setOnlyAccessesArgMemory();
+			read_call->setDoesNotThrow();
+			read_call->setOnlyReadsMemory(); // all reads are readonly (can be optimized away if unused)
+			read_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			
+			// set sampler* cast attributes if necessary
+			if (set_sampler_attrs) {
+				read_call->addParamAttr(sampler_param_idx, Attribute::NoCapture);
+				read_call->addParamAttr(sampler_param_idx, Attribute::ReadOnly);
+			}
+			
+			// if this is a depth read/sample, the return type is a float -> create a float4
+			llvm::Value* read_call_result = read_call;
+			if (is_metal_2_3) {
+				// we always ignore the i8 return value -> extract are wanted result
+				read_call_result = builder->CreateExtractValue(read_call_result, { 0 });
+			}
+			if(is_depth) {
+				auto undef_vec = UndefValue::get(llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4));
+				read_call_result = builder->CreateInsertElement(undef_vec, read_call_result, builder->getInt32(0));
+				// rest is undef/zero (and will be stripped away again anyways)
+			}
+			
+			//
+			I.replaceAllUsesWith(read_call_result);
+			I.eraseFromParent();
+		}
+		
+		void handle_write_image(Instruction& I,
+								const StringRef& func_name,
+								llvm::Value* img_handle_arg,
+								const COMPUTE_IMAGE_TYPE& full_image_type,
+								const COMPUTE_IMAGE_TYPE& image_type,
+								const COMPUTE_IMAGE_TYPE& format_type,
+								const COMPUTE_IMAGE_TYPE& data_type,
+								llvm::Value* /* rt_image_type */,
+								const bool& is_normalized,
+								const uint32_t& image_channel_count,
+								llvm::Value* coord_arg,
+								llvm::Value* layer_arg,
+								llvm::Value* lod_arg,
+								const bool is_lod,
+								llvm::Value* data_arg) override {
+			SmallVector<llvm::Type*, 16> func_arg_types;
+			SmallVector<llvm::Value*, 16> func_args;
+			
+			//// more arg checking
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if(!coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer");
+				return;
+			}
+			
+			std::string dtype;
+			if (func_name.endswith(".float.depth")) {
+				dtype = "f32";
+			} else if (func_name.endswith(".float")) {
+				dtype = "v4f32";
+			} else if (func_name.endswith(".int")) {
+				dtype = "s.v4i32";
+			} else if (func_name.endswith(".uint")) {
+				dtype = "u.v4i32";
+			} else if (func_name.endswith(".half")) {
+				dtype = "v4f16";
+			} else if (func_name.endswith(".short")) {
+				dtype = "s.v4i16";
+			} else if (func_name.endswith(".ushort")) {
+				dtype = "u.v4i16";
+			}
+			// unknown -> ignore
+			else return;
+			
+			//// func replacement
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			// -> geom
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type);
+			const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			// filter types that are not allowed, b/c they can't be directly written to
+			switch (image_type) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					ctx->emitError(&I, "invalid image type - type is not writable");
+					return;
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+					// TODO/NOTE: not officially exposed by Metal headers, but exists in metalfe -> figure out what the min version is
+					break;
+				default:
+					break;
+			}
+			
+			// -> coords: coords, face, layer
+			if(coord_vec_type->getNumElements() == 1) {
+				// 1D coord, make it scalar
+				func_arg_types.push_back(coord_type);
+				func_args.push_back(builder->CreateExtractElement(coord_arg, builder->getInt32(0)));
+			}
+			else {
+				// normal coord arg
+				if(!is_cube) {
+					func_arg_types.push_back(coord_vec_type);
+					func_args.push_back(coord_arg);
+				}
+				// cube with int coords
+				else {
+					// extract the face integer from the coords
+					const auto face_arg = builder->CreateExtractElement(coord_arg, builder->getInt32(2));
+					
+					// create new int2 arg
+					llvm::Value* coord_i2_arg = UndefValue::get(llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 2));
+					coord_i2_arg = builder->CreateInsertElement(coord_i2_arg, builder->CreateExtractElement(coord_arg, builder->getInt32(0)), builder->getInt32(0));
+					coord_i2_arg = builder->CreateInsertElement(coord_i2_arg, builder->CreateExtractElement(coord_arg, builder->getInt32(1)), builder->getInt32(1));
+					
+					func_arg_types.push_back(coord_i2_arg->getType());
+					func_args.push_back(coord_i2_arg);
+					
+					func_arg_types.push_back(face_arg->getType());
+					func_args.push_back(face_arg);
+				}
+			}
+			
+			if(is_array) {
+				func_arg_types.push_back(layer_arg->getType());
+				func_args.push_back(layer_arg);
+			}
+			
+			// -> data (a 4-component vector or a single float if depth)
+			func_arg_types.push_back(data_arg->getType());
+			func_args.push_back(data_arg);
+			
+			// -> lod
+			func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+			if (!is_lod) {
+				func_args.push_back(builder->getInt32(0));
+			} else {
+				func_args.push_back(lod_arg);
+			}
+			
+			if (is_metal_2_3) {
+				// Metal 2.3+ has an additional rounding mode argument (always set to 0 for now, or 2 if depth)
+				func_arg_types.push_back(llvm::Type::getInt32Ty(*ctx));
+				func_args.push_back(builder->getInt32(!is_depth ? 0 : 2));
+			}
+			
+			// -> build write func name
+			const std::string write_func_name = "air.write_" + geom + '.' + dtype;
+			
+			// create the air call
+			const auto func_type = llvm::FunctionType::get(builder->getVoidTy(), func_arg_types, false);
+			llvm::CallInst* write_call = builder->CreateCall(M->getOrInsertFunction(write_func_name, func_type, nounwind_attr), func_args);
+			write_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			
+			//
+			I.replaceAllUsesWith(write_call);
+			I.eraseFromParent();
+		}
+		
+		void handle_get_image_dim(Instruction& I,
+								  const StringRef& func_name,
+								  llvm::Value* img_handle_arg,
+								  const COMPUTE_IMAGE_TYPE& /* full_image_type */,
+								  const COMPUTE_IMAGE_TYPE& image_type,
+								  llvm::Value* lod_arg) override {
+			// gather info
+			const auto dim_count = image_dim_count(image_type);
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			const std::string geom = geom_cstr;
+
+			// query/get function base
+			const auto query_image = [&](const std::string& query_name) {
+				SmallVector<llvm::Type*, 2> func_arg_types;
+				SmallVector<llvm::Value*, 2> func_args;
+				func_arg_types.push_back(img_handle_arg->getType());
+				func_args.push_back(img_handle_arg);
+				func_arg_types.push_back(lod_arg->getType());
+				func_args.push_back(lod_arg);
+				
+				// -> build get func name
+				const std::string get_func_name = "air.get_" + query_name + '_' + geom;
+				
+				AttrBuilder attr_builder;
+				attr_builder.addAttribute(llvm::Attribute::Convergent);
+				attr_builder.addAttribute(llvm::Attribute::ArgMemOnly);
+				attr_builder.addAttribute(llvm::Attribute::NoUnwind);
+				attr_builder.addAttribute(llvm::Attribute::ReadOnly);
+				auto func_attrs = AttributeList::get(*ctx, ~0, attr_builder);
+				
+				// create the air call
+				const auto ret_type = llvm::Type::getInt32Ty(*ctx);
+				const auto func_type = llvm::FunctionType::get(ret_type, func_arg_types, false);
+				llvm::CallInst* get_call = builder->CreateCall(M->getOrInsertFunction(get_func_name, func_type, func_attrs), func_args);
+				get_call->setConvergent();
+				get_call->setOnlyAccessesArgMemory();
+				get_call->setDoesNotThrow();
+				get_call->setOnlyReadsMemory(); // all get_* calls are readonly (can be optimized away if unused)
+				get_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+				return get_call;
+			};
+			
+			// we have to a return a full image dim query for all dims of the image (type dependent)
+			// order is: width [, height] [, depth], [, layer_count]
+			// non-existing dims are set to 0
+			const auto ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			llvm::Value* ret_vec = UndefValue::get(ret_type);
+			uint32_t ret_vec_idx = 0;
+			// all images have a width
+			ret_vec = builder->CreateInsertElement(ret_vec, query_image("width"), builder->getInt32(ret_vec_idx++));
+			if (dim_count >= 2) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("height"), builder->getInt32(ret_vec_idx++));
+			}
+			if (dim_count >= 3) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("depth"), builder->getInt32(ret_vec_idx++));
+			}
+			if (is_array) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("array_size"), builder->getInt32(ret_vec_idx++));
+			}
+			// NOTE: while cube maps technically have 6 layers, this number is not stored in the image dim
+			
+			// fill remaining components with 0
+			for (uint32_t vec_idx = ret_vec_idx; vec_idx < 4; ++vec_idx) {
+				ret_vec = builder->CreateInsertElement(ret_vec, builder->getInt32(0), builder->getInt32(ret_vec_idx++));
+			}
+			
+			//
+			I.replaceAllUsesWith(ret_vec);
+			I.eraseFromParent();
+		}
+		
+	};
+}
+
+char MetalImage::ID = 0;
+INITIALIZE_PASS_BEGIN(MetalImage, "MetalImage", "MetalImage Pass", false, false)
+INITIALIZE_PASS_END(MetalImage, "MetalImage", "MetalImage Pass", false, false)
+
+FunctionPass *llvm::createMetalImagePass(const uint32_t image_capabilities) {
+	return new MetalImage(image_capabilities);
+}
diff --git a/llvm/lib/Transforms/LibFloor/PropagateRangeInfo.cpp b/llvm/lib/Transforms/LibFloor/PropagateRangeInfo.cpp
new file mode 100644
index 000000000000..bbabd068ef1b
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/PropagateRangeInfo.cpp
@@ -0,0 +1,219 @@
+//===- PropagateRangeInfo.cpp - Vulkan final pass -------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// TODO
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/PostDominators.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/Utils/BasicBlockUtils.h"
+#include "llvm/Transforms/Utils/LoopUtils.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <unordered_map>
+#include <unordered_set>
+#include <deque>
+#include <array>
+using namespace llvm;
+
+#define DEBUG_TYPE "PropagateRangeInfo"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// PropagateRangeInfo
+	struct PropagateRangeInfo : public FunctionPass, InstVisitor<PropagateRangeInfo> {
+		friend class InstVisitor<PropagateRangeInfo>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		
+		bool was_modified { false };
+		
+		PropagateRangeInfo() :
+		FunctionPass(ID) {
+			initializePropagateRangeInfoPass(*PassRegistry::getPassRegistry());
+		}
+		
+		StringRef getPassName() const override {
+			return "Propagate Range Info";
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.setPreservesCFG();
+			FunctionPass::getAnalysisUsage(AU);
+		}
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			was_modified = false;
+			
+			visit(F);
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<PropagateRangeInfo>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<PropagateRangeInfo>::visit(I);
+		}
+		
+		// we always start at call instructions, because this is where range info originates from
+		void visitCallInst(CallInst &CI) {
+			MDNode* range = CI.getMetadata(LLVMContext::MD_range);
+			if(range == nullptr) return;
+			
+			for(User* user : CI.users()) {
+				if(Instruction* I = dyn_cast<Instruction>(user)) {
+					DBG(errs() << "adding range info to user: " << *I << "\n";)
+					addRangeInfo(*I, range);
+				}
+			}
+		}
+		
+		void addRangeInfo(Instruction& I, MDNode* range) {
+			// can't handle non-integer types or int bit-width > 64 yet
+			IntegerType* int_type = dyn_cast<IntegerType>(I.getType());
+			if(int_type == nullptr ||
+			   int_type->getBitWidth() > 64) {
+				return;
+			}
+			
+			// can only propagate to certain instruction
+			switch(I.getOpcode()) {
+				// trunc/extend are obvious
+				case llvm::Instruction::Trunc:
+				case llvm::Instruction::SExt:
+				case llvm::Instruction::ZExt:
+					break;
+				
+				// TODO: integer/other ops when the operand is a ConstantInt
+				// add/sub/mul/div/rem/shl/shr -> obvious
+				// and -> [0, currrent max]?
+				// or/xor -> remove range info, b/c [0, int max]?
+				// select -> merge
+				// phi -> merge if all have range info
+				
+				// else: can't propagate
+				default: return;
+			}
+			
+			// will modify now
+			was_modified = true;
+			
+			// check if we need to convert the integer type
+			// TODO: how to handle signed values? can't retrieve that info from APInt or ConstantInt
+			const auto range_type = mdconst::extract<ConstantInt>(range->getOperand(0))->getType();
+			MDNode* apply_range = range;
+			if(range_type != int_type) {
+				const uint64_t max_value = int_type->getBitMask();
+				
+				SmallVector<Metadata*, 2> conv_range_ints;
+				conv_range_ints.reserve(range->getNumOperands());
+				for(const auto& op : range->operands()) {
+					const auto val = mdconst::extract<ConstantInt>(op)->getValue();
+					const auto clamped_val = std::min(val.getZExtValue(), max_value);
+					conv_range_ints.push_back(llvm::ConstantAsMetadata::get(ConstantInt::get(int_type, clamped_val, false)));
+				}
+				apply_range = MDNode::get(*ctx, conv_range_ints);
+			}
+			
+			// if the instruction already contains range info, merge it
+			MDNode* instr_range = I.getMetadata(LLVMContext::MD_range);
+			if(instr_range == nullptr) {
+				I.setMetadata(LLVMContext::MD_range, apply_range);
+			}
+			else {
+				I.setMetadata(LLVMContext::MD_range, MDNode::getMostGenericRange(apply_range, instr_range));
+			}
+			
+			// continue the recursion
+			for(User* user : I.users()) {
+				if(Instruction* UI = dyn_cast<Instruction>(user)) {
+					DBG(errs() << "> adding range info to user: " << *UI << "\n";)
+					addRangeInfo(*UI, apply_range);
+				}
+			}
+		}
+		
+	};
+	
+}
+
+char PropagateRangeInfo::ID = 0;
+FunctionPass *llvm::createPropagateRangeInfoPass() {
+	return new PropagateRangeInfo();
+}
+INITIALIZE_PASS_BEGIN(PropagateRangeInfo, "PropagateRangeInfo", "PropagateRangeInfo Pass", false, false)
+INITIALIZE_PASS_END(PropagateRangeInfo, "PropagateRangeInfo", "PropagateRangeInfo Pass", false, false)
+
diff --git a/llvm/lib/Transforms/LibFloor/SPIRFinal.cpp b/llvm/lib/Transforms/LibFloor/SPIRFinal.cpp
new file mode 100644
index 000000000000..e95b51d07638
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/SPIRFinal.cpp
@@ -0,0 +1,347 @@
+//===- SPIRFinal.cpp - OpenCL/SPIR fixes ----------------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file tries to fix the LLVM IR so that it is SPIR-conformant.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <cxxabi.h>
+using namespace llvm;
+
+#define DEBUG_TYPE "SPIRFinal"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// SPIRFinal
+	struct SPIRFinal : public FunctionPass, InstVisitor<SPIRFinal> {
+		friend class InstVisitor<SPIRFinal>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		Instruction* alloca_insert { nullptr };
+		bool was_modified { false };
+		
+		SPIRFinal() : FunctionPass(ID) {
+			initializeSPIRFinalPass(*PassRegistry::getPassRegistry());
+		}
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if (F.empty()) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			// visit everything in this function
+			was_modified = false; // reset every time
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			
+			// update function signature / param list
+			if(F.getCallingConv() == CallingConv::FLOOR_KERNEL) {
+				std::vector<Type*> param_types;
+				for(auto& arg : F.args()) {
+					// add "byval" attribute for all kernel struct parameters
+					if(arg.getType()->isPointerTy() &&
+					   arg.getType()->getPointerElementType()->isAggregateType() &&
+					   arg.getType()->getPointerAddressSpace() == 0 &&
+					   !F.getAttributes().hasAttributeAtIndex(arg.getArgNo() + 1, Attribute::ByVal)) {
+						arg.addAttr(Attribute::getWithByValType(*ctx, arg.getType()->getPointerElementType()));
+						was_modified = true;
+					}
+					
+					param_types.push_back(arg.getType());
+				}
+				if(was_modified) {
+					auto new_func_type = PointerType::get(FunctionType::get(F.getReturnType(), param_types, false), 0);
+					F.mutateType(new_func_type);
+				}
+			}
+			
+			//
+			visit(F);
+			if(was_modified) {
+				DBG(errs() << "!! modified function: ";)
+				DBG(errs().write_escaped(F.getName()) << '\n';)
+			}
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<SPIRFinal>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<SPIRFinal>::visit(I);
+		}
+		
+		// SPIR only supports scalar conversion ops ->
+		// * scalarize source vector
+		// * call conversion op for each scalar
+		// * reassemble a vector from the converted scalars
+		// * replace all uses of the original vector
+		template <Instruction::CastOps cast_op>
+		__attribute__((always_inline))
+		void vec_to_scalar_ops(CastInst& I) {
+			if (!I.getType()->isVectorTy()) {
+				return;
+			}
+			
+			// setup
+			auto* src_vec = I.getOperand(0);
+			auto src_vec_type = dyn_cast_or_null<FixedVectorType>(src_vec->getType());
+			if (!src_vec_type) {
+				return;
+			}
+			const auto dim = src_vec_type->getNumElements();
+			const auto si_type = I.getDestTy();
+			const auto si_scalar_type = si_type->getScalarType();
+			llvm::Value* dst_vec = UndefValue::get(si_type);
+			
+			// start insertion before instruction
+			builder->SetInsertPoint(&I);
+			
+			// iterate over all vector components, emit a scalar instruction and insert into a new vector
+			auto DL = I.getDebugLoc();
+			for(uint32_t i = 0; i < dim; ++i) {
+				auto scalar = builder->CreateExtractElement(src_vec, builder->getInt32(i));
+				dst_vec = builder->CreateInsertElement(dst_vec,
+													   builder->CreateCast(cast_op, scalar, si_scalar_type),
+													   builder->getInt32(i));
+				((ExtractElementInst*)scalar)->setDebugLoc(DL);
+				((InsertElementInst*)dst_vec)->setDebugLoc(DL);
+			}
+			
+			// finally, replace all uses with the new vector and remove the old vec instruction
+			I.replaceAllUsesWith(dst_vec);
+			I.eraseFromParent();
+			was_modified = true;
+		}
+		
+		void visitTruncInst(TruncInst &I) {
+			vec_to_scalar_ops<Instruction::Trunc>(I);
+		}
+		void visitZExtInst(ZExtInst &I) {
+			vec_to_scalar_ops<Instruction::ZExt>(I);
+		}
+		void visitSExtInst(SExtInst &I) {
+			vec_to_scalar_ops<Instruction::SExt>(I);
+		}
+		void visitFPTruncInst(FPTruncInst &I) {
+			vec_to_scalar_ops<Instruction::FPTrunc>(I);
+		}
+		void visitFPExtInst(FPExtInst &I) {
+			vec_to_scalar_ops<Instruction::FPExt>(I);
+		}
+		void visitFPToUIInst(FPToUIInst &I) {
+			vec_to_scalar_ops<Instruction::FPToUI>(I);
+		}
+		void visitFPToSIInst(FPToSIInst &I) {
+			vec_to_scalar_ops<Instruction::FPToSI>(I);
+		}
+		void visitUIToFPInst(UIToFPInst &I) {
+			vec_to_scalar_ops<Instruction::UIToFP>(I);
+		}
+		void visitSIToFPInst(SIToFPInst &I) {
+			vec_to_scalar_ops<Instruction::SIToFP>(I);
+		}
+		
+		// SPIR doesn't support LLVM lifetime and assume intrinsics
+		// -> simply remove them
+		// TODO: should probably kill the global decl as well
+		void visitIntrinsicInst(IntrinsicInst &I) {
+			if (I.getIntrinsicID() == Intrinsic::lifetime_start ||
+				I.getIntrinsicID() == Intrinsic::lifetime_end ||
+				I.getIntrinsicID() == Intrinsic::assume) {
+				I.eraseFromParent();
+				was_modified = true;
+			}
+		}
+		
+		// "ashr" instructions may not be "exact"
+		void visitAShr(BinaryOperator& O) {
+			auto* ashr = dyn_cast_or_null<PossiblyExactOperator>(&O);
+			if(ashr && ashr->isExact()) {
+				// -> replace with a non-exact version
+				builder->SetInsertPoint(&O);
+				auto new_ashr = builder->CreateAShr(O.getOperand(0), O.getOperand(1));
+				((Instruction*)new_ashr)->setDebugLoc(O.getDebugLoc());
+				O.replaceAllUsesWith(new_ashr);
+				O.eraseFromParent();
+				was_modified = true;
+			}
+		}
+		
+		// unsupported LLVM IR instructions - fail on these
+		void visitIndirectBrInst(IndirectBrInst &I) {
+			ctx->emitError(&I, "indirect-br instruction is not supported by SPIR");
+		}
+		void visitInvokeInst(InvokeInst &I) {
+			ctx->emitError(&I, "invoke instruction is not supported by SPIR");
+		}
+		// NOTE: unwind no longer exists
+		void visitResumeInst(ResumeInst &I) {
+			ctx->emitError(&I, "resume instruction is not supported by SPIR");
+		}
+		void visitFenceInst(FenceInst &I) {
+			ctx->emitError(&I, "fence instruction is not supported by SPIR");
+		}
+		void visitAtomicCmpXchgInst(AtomicCmpXchgInst &I) {
+			ctx->emitError(&I, "atomic-cmp-xchg instruction is not supported by SPIR - use atomic_* function calls instead!");
+		}
+		void visitAtomicRMWInst(AtomicRMWInst &I) {
+			ctx->emitError(&I, "atomic-rmv instruction is not supported by SPIR - use atomic_* function calls instead!");
+		}
+		void visitVAArgInst(VAArgInst &I) {
+			ctx->emitError(&I, "va-arg instruction is not supported by SPIR");
+		}
+		void visitLandingPadInst(LandingPadInst &I) {
+			ctx->emitError(&I, "landing-pad instruction is not supported by SPIR");
+		}
+		
+		// calls to function pointers are not allowed
+		void visitCallInst(CallInst &I) {
+			if(I.getCalledFunction() == nullptr) {
+				ctx->emitError(&I, "indirect function call / call to function pointer is not supported by SPIR");
+			}
+		}
+		
+		// atomic load/store instructions are not allowed
+		void visitLoadInst(LoadInst &I) {
+			if(I.isAtomic()) {
+				ctx->emitError(&I, "atomic-load instruction is not supported by SPIR - use atomic_* function calls instead!");
+			}
+		}
+		void visitStoreInst(StoreInst &I) {
+			if(I.isAtomic()) {
+				ctx->emitError(&I, "atomic-store instruction is not supported by SPIR - use atomic_* function calls instead!");
+			}
+		}
+		
+		// convert illegal integer types in select and switch instructions to legal ones
+		// TODO/NOTE: ideally, this should be done for all instructions in the module, but right now this
+		//            is the only place where illegal int types get emitted
+		void visitSelectInst(SelectInst& SI) {
+			if(!SI.getType()->isIntegerTy()) return;
+			
+			const auto bit_width = SI.getType()->getIntegerBitWidth();
+			if(M->getDataLayout().isLegalInteger(bit_width) || bit_width == 1 /* always allow bool */) {
+				return;
+			}
+			
+			const bool mutate_true_val = (SI.getTrueValue()->getType()->getIntegerBitWidth() == bit_width);
+			const bool mutate_false_val = (SI.getFalseValue()->getType()->getIntegerBitWidth() == bit_width);
+			if((mutate_true_val && !isa<ConstantInt>(SI.getTrueValue())) ||
+			   (mutate_false_val && !isa<ConstantInt>(SI.getFalseValue()))) {
+				ctx->emitError(&SI, "select uses an illegal integer bit width (" + std::to_string(bit_width) + ") " +
+							   "and true/false values can not be in-place converted to a legal integer width, " +
+							   "because they are not constant values!");
+				SI.print(errs());
+				errs() << '\n';
+				return;
+			}
+			
+			const auto smallest_legal_type = M->getDataLayout().getSmallestLegalIntType(*ctx, bit_width);
+			if(mutate_true_val) SI.getTrueValue()->mutateType(smallest_legal_type);
+			if(mutate_false_val) SI.getFalseValue()->mutateType(smallest_legal_type);
+			SI.mutateType(smallest_legal_type);
+		}
+		void visitSwitchInst(SwitchInst& SI) {
+			if(!SI.getCondition()->getType()->isIntegerTy()) return;
+			
+			const auto bit_width = SI.getCondition()->getType()->getIntegerBitWidth();
+			if(M->getDataLayout().isLegalInteger(bit_width)) {
+				return;
+			}
+			
+			const auto smallest_legal_type = M->getDataLayout().getSmallestLegalIntType(*ctx, bit_width);
+			SI.getCondition()->mutateType(smallest_legal_type);
+		}
+		
+		void visitGetElementPtrInst(GetElementPtrInst& GEP) {
+			// fix internal GEP address space mismatch
+			if (auto *PTy = dyn_cast<PointerType>(GEP.getType())) {
+				if (GEP.getAddressSpace() != PTy->getAddressSpace()) {
+					auto new_ptr_type = PointerType::get(GEP.getType()->getPointerElementType(), GEP.getAddressSpace());
+					DBG(errs() << ">> fix GEP/PTy mismatch: " << GEP.getAddressSpace() << " != " << PTy->getAddressSpace() << ": " << *GEP.getType();)
+					GEP.mutateType(new_ptr_type);
+					DBG(errs() << " -> " << *GEP.getType() << "\n";)
+					was_modified = true;
+				}
+			}
+		}
+	};
+}
+
+char SPIRFinal::ID = 0;
+INITIALIZE_PASS_BEGIN(SPIRFinal, "SPIRFinal", "SPIRFinal Pass", false, false)
+INITIALIZE_PASS_END(SPIRFinal, "SPIRFinal", "SPIRFinal Pass", false, false)
+
+FunctionPass *llvm::createSPIRFinalPass() {
+	return new SPIRFinal();
+}
diff --git a/llvm/lib/Transforms/LibFloor/SPIRImage.cpp b/llvm/lib/Transforms/LibFloor/SPIRImage.cpp
new file mode 100644
index 000000000000..ed640c5cde1f
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/SPIRImage.cpp
@@ -0,0 +1,769 @@
+//===- SPIRImage.cpp - SPIR-specific floor image transformations ----------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass implements the SPIR-specific floor image transformations, i.e.
+// floor.opaque.<read/write function>.* -> spir image function call
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/LibFloor/FloorImage.h"
+#include <unordered_map>
+using namespace llvm;
+
+#define DEBUG_TYPE "SPIRImage"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	struct SPIRImage : public FloorImageBasePass {
+		static char ID; // Pass identification, replacement for typeid
+		bool enable_intel_workarounds { false };
+		
+		SPIRImage(const uint32_t image_capabilities_ = 0,
+				  const bool enable_intel_workarounds_ = false) :
+		FloorImageBasePass(ID, IMAGE_TYPE_ID::OPAQUE, image_capabilities_),
+		enable_intel_workarounds(enable_intel_workarounds_) {
+			initializeSPIRImagePass(*PassRegistry::getPassRegistry());
+		}
+		
+		static const char* type_to_geom(const COMPUTE_IMAGE_TYPE& image_type) {
+			switch(image_type) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D:
+					return "11ocl_image1d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_ARRAY:
+					return "16ocl_image1darray";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_BUFFER:
+					return "17ocl_image1dbuffer";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+					return "16ocl_image2ddepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D:
+					return "11ocl_image2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+					return "21ocl_image2darraydepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_ARRAY:
+					return "16ocl_image2darray";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+					return "20ocl_image2dmsaadepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+					return "15ocl_image2dmsaa";
+				case COMPUTE_IMAGE_TYPE::IMAGE_3D:
+					return "11ocl_image3d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+					return "25ocl_image2darraymsaadepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					return "20ocl_image2darraymsaa";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE:
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE_ARRAY:
+				default:
+					return nullptr;
+			}
+		}
+		
+		llvm::Function* get_or_create_spir_function(std::string func_name,
+													llvm::Type* ret_type,
+													const SmallVector<llvm::Type*, 8>& func_arg_types,
+													const bool is_readnone = false) {
+			if(enable_intel_workarounds) {
+				// intel name mangling is a bit off sometimes, so use their name mangling instead
+				// correct -> intel
+				static const std::unordered_map<std::string, std::string> intel_func_repl {
+					{ "_Z11read_imagef11ocl_image2d11ocl_samplerDv2_fDv2_fDv2_f", "_Z11read_imagef11ocl_image2d11ocl_samplerDv2_fS_S_" },
+					{ "_Z11read_imagei11ocl_image2d11ocl_samplerDv2_fDv2_fDv2_f", "_Z11read_imagei11ocl_image2d11ocl_samplerDv2_fS_S_" },
+					{ "_Z12read_imageui11ocl_image2d11ocl_samplerDv2_fDv2_fDv2_f", "_Z12read_imageui11ocl_image2d11ocl_samplerDv2_fS_S_" },
+					{ "_Z11read_imagef16ocl_image2ddepth11ocl_samplerDv2_fDv2_fDv2_f", "_Z11read_imagef16ocl_image2ddepth11ocl_samplerDv2_fS_S_" },
+					{ "_Z11read_imagef11ocl_image3d11ocl_samplerDv4_fDv4_fDv4_f", "_Z11read_imagef11ocl_image3d11ocl_samplerDv4_fS_S_" },
+					{ "_Z11read_imagei11ocl_image3d11ocl_samplerDv4_fDv4_fDv4_f", "_Z11read_imagei11ocl_image3d11ocl_samplerDv4_fS_S_" },
+					{ "_Z12read_imageui11ocl_image3d11ocl_samplerDv4_fDv4_fDv4_f", "_Z12read_imageui11ocl_image3d11ocl_samplerDv4_fS_S_" },
+					{ "_Z12write_imagei11ocl_image3dDv4_iDv4_i", "_Z12write_imagei11ocl_image3dDv4_iS_" },
+					{ "_Z12write_imagei11ocl_image3dDv4_iiDv4_i", "_Z12write_imagei11ocl_image3dDv4_iiS_" },
+					{ "_Z12write_imagei16ocl_image2darrayDv4_iDv4_i", "_Z12write_imagei16ocl_image2darrayDv4_iS_" },
+					{ "_Z12write_imagei16ocl_image2darrayDv4_iiDv4_i", "_Z12write_imagei16ocl_image2darrayDv4_iiS_" },
+				};
+				const auto iter = intel_func_repl.find(func_name);
+				if(iter != intel_func_repl.end()) {
+					func_name = iter->second;
+				}
+			}
+			
+			const auto func_type = llvm::FunctionType::get(ret_type, func_arg_types, false);
+			auto func = M->getFunction(func_name);
+			if(func == nullptr) { // only do this once
+				func = dyn_cast<Function>(M->getOrInsertFunction(func_name, func_type,
+																 is_readnone ?
+																 nounwind_readnone_attr :
+																 nounwind_attr).getCallee());
+				func->setCallingConv(CallingConv::FLOOR_FUNC);
+			}
+			return func;
+		}
+		
+		SmallVector<llvm::Value*, 3> get_image_dim(llvm::Value* img_handle_arg,
+												   llvm::FixedVectorType* coord_vec_type,
+												   const std::string& geom) {
+			SmallVector<llvm::Value*, 3> ret;
+			
+			static const char* img_dim_funcs[] {
+				"_Z15get_image_width",
+				"_Z16get_image_height",
+				"_Z15get_image_depth"
+			};
+			
+			const auto dim = coord_vec_type->getNumElements();
+			SmallVector<llvm::Type*, 8> get_dim_arg_types;
+			SmallVector<llvm::Value*, 8> get_dim_func_args;
+			get_dim_arg_types.push_back(img_handle_arg->getType());
+			get_dim_func_args.push_back(img_handle_arg);
+			for(uint32_t i = 0; i < dim; ++i) {
+				auto get_dim_func = get_or_create_spir_function(img_dim_funcs[i] + geom,
+																builder->getInt32Ty(),
+																get_dim_arg_types,
+																true);
+				llvm::CallInst* get_dim_call = builder->CreateCall(get_dim_func, get_dim_func_args);
+				get_dim_call->setDoesNotAccessMemory();
+				get_dim_call->setConvergent();
+				get_dim_call->setDoesNotThrow();
+				get_dim_call->setCallingConv(CallingConv::FLOOR_FUNC);
+				ret.push_back(get_dim_call);
+			}
+			
+			return ret;
+		}
+		
+		void handle_cl_coord(Instruction& I,
+							 llvm::Value* coord_arg,
+							 llvm::Value* layer_arg,
+							 const bool is_array,
+							 const bool is_msaa,
+							 const bool must_have_int_args,
+							 std::string& cl_func_name,
+							 SmallVector<llvm::Type*, 8>& func_arg_types,
+							 SmallVector<llvm::Value*, 8>& func_args) {
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			const auto coord_dim = coord_vec_type->getNumElements();
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			const auto is_int_coord = coord_type->isIntegerTy();
+			if(is_msaa && !is_int_coord) {
+				ctx->emitError(&I, "coordinate type must be integer for msaa images");
+				return;
+			}
+			
+			if(must_have_int_args && !is_int_coord) {
+				ctx->emitError(&I, "coordinate type must be integer");
+				return;
+			}
+			
+			// opencl only knows scalar, vector2 and vector4 coordinates -> need to create them if necessary
+			auto cl_coord_dim = coord_dim + (is_array ? 1 : 0);
+			if(cl_coord_dim == 3) cl_coord_dim = 4;
+			const auto cl_coord_scalar_type = (is_int_coord ? llvm::Type::getInt32Ty(*ctx) : llvm::Type::getFloatTy(*ctx));
+			const auto cl_coord_type = (cl_coord_dim == 1 ?
+										cl_coord_scalar_type :
+										llvm::FixedVectorType::get(cl_coord_scalar_type, cl_coord_dim));
+			
+			// start with the specified coord arg, there are some cases where we can just use it without rebuilding
+			auto cl_coord_arg = coord_arg;
+			if(cl_coord_type != coord_vec_type) {
+				if(cl_coord_dim == 1) {
+					// just a scalar
+					cl_coord_arg = builder->CreateExtractElement(coord_arg, builder->getInt32(0));
+				}
+				else {
+					// create a new tmp coord, then copy coord elements (keep unused undef)
+					cl_coord_arg = UndefValue::get(cl_coord_type);
+					uint32_t coord_idx = 0;
+					for(; coord_idx < coord_dim; ++coord_idx) {
+						cl_coord_arg = builder->CreateInsertElement(cl_coord_arg,
+																	builder->CreateExtractElement(coord_arg,
+																								  builder->getInt32(coord_idx)),
+																	builder->getInt32(coord_idx));
+					}
+					
+					// need to pull the layer index into the coordinate, including possible int -> float conversion
+					if(is_array) {
+						auto layer = layer_arg;
+						if(!is_int_coord) {
+							// need to convert
+							layer = builder->CreateUIToFP(layer_arg, cl_coord_scalar_type);
+						}
+						cl_coord_arg = builder->CreateInsertElement(cl_coord_arg, layer, builder->getInt32(coord_idx++));
+					}
+				}
+			}
+			func_arg_types.push_back(cl_coord_arg->getType());
+			func_args.push_back(cl_coord_arg);
+			
+			if(cl_coord_dim > 1) {
+				cl_func_name += "Dv" + std::to_string(cl_coord_dim) + "_";
+			}
+			cl_func_name += (is_int_coord ? "i" : "f");
+		}
+		
+		void handle_read_image(Instruction& I,
+							   const StringRef& func_name,
+							   llvm::Value* img_handle_arg,
+							   const COMPUTE_IMAGE_TYPE& image_type,
+							   llvm::ConstantInt* const_sampler_arg,
+							   llvm::Value* dyn_sampler_arg,
+							   llvm::Value* coord_arg,
+							   llvm::Value* layer_arg,
+							   llvm::Value* sample_arg,
+							   llvm::Value* offset_arg,
+							   const SmallVector<llvm::Value*, 3>& offset_elems,
+							   const bool is_offset,
+							   llvm::Value* lod_or_bias_arg,
+							   const bool is_lod_or_bias, // true: lod, false: bias
+							   llvm::Value* dpdx_arg,
+							   llvm::Value* dpdy_arg,
+							   const bool is_gradient_,
+							   const COMPARE_FUNCTION& compare_function,
+							   llvm::Value* compare_value_arg,
+							   const bool is_compare) override {
+			SmallVector<llvm::Type*, 8> func_arg_types;
+			SmallVector<llvm::Value*, 8> func_args;
+			
+			// must be constant/constexpr for now
+			if(const_sampler_arg == nullptr) {
+				ctx->emitError(&I, "sampler must be a constant");
+				return;
+			}
+			auto sampler_arg = const_sampler_arg;
+			
+			// NOTE: opencl is rather limited when it comes to image functionality, hence not all types of image reads
+			//       are supported, though some can simply be ignored or emulated in s/w
+			
+			// TODO: add an option to disable advanced image functions and silently fallback to simple ones (at the loss of functionality)
+			
+			// get geom string / mangled name + flags
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown/incorrect/unsupported image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(image_type);
+			const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			auto coord_vec_type = dyn_cast<FixedVectorType>(coord_arg->getType());
+			if (!coord_vec_type) {
+				ctx->emitError(&I, "coordinate must be a vector type");
+				return;
+			}
+			
+			// -> caps check
+			if(is_lod_or_bias) {
+				if(!has_flag<IMAGE_CAPABILITY::MIPMAP_READ>(image_capabilities)) {
+					ctx->emitError(&I, "lod read not supported by device");
+					return;
+				}
+				
+				// *sigh* will be supported with opencl 2.1 though
+				// -> convert int coords to float coords and swap out sampler
+				if (coord_vec_type->getElementType()->isIntegerTy()) {
+					const auto coord_dim = coord_vec_type->getNumElements();
+					const auto fp_coord_type = llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), coord_dim);
+					
+					auto img_dims = get_image_dim(img_handle_arg, coord_vec_type, geom);
+					
+					llvm::Value* fp_coord = UndefValue::get(fp_coord_type);
+					for(uint32_t i = 0; i < coord_dim; ++i) {
+						// fp_coord_i = (float(int_coord_i) + 0.5) / float(img_dim_i)
+						auto elem = builder->CreateExtractElement(coord_arg, builder->getInt32(i));
+						auto fp_elem = builder->CreateSIToFP(elem, builder->getFloatTy());
+						auto fp_elem_half = builder->CreateFAdd(fp_elem, ConstantFP::get(builder->getFloatTy(), 0.5));
+						auto fp_dim_i = builder->CreateSIToFP(img_dims[i], builder->getFloatTy());
+						auto div = builder->CreateFDiv(fp_elem_half, fp_dim_i);
+						fp_coord = builder->CreateInsertElement(fp_coord, div, builder->getInt32(i));
+					}
+					coord_arg = fp_coord;
+					
+					// sampler: set NORMALIZED/CLK_NORMALIZED_COORDS_TRUE flag
+					// NOTE: PIXEL/CLK_NORMALIZED_COORDS_FALSE is 0, so we don't need to clear anything
+					sampler_arg = ConstantInt::get(const_sampler_arg->getType(), const_sampler_arg->getZExtValue() | 0x1);
+				}
+			}
+			
+			bool is_gradient = is_gradient_;
+			if(is_gradient_) {
+				if(!has_flag<IMAGE_CAPABILITY::MIPMAP_READ>(image_capabilities)) {
+					ctx->emitError(&I, "gradient read not supported by device");
+					return;
+				}
+				
+				// again, not supported (also: doesn't make much sense?)
+				// -> not going to s/w emulate this for now
+				// -> silently ignore for now
+#if 0
+				if(coord_arg->getType()->getVectorElementType()->isIntegerTy()) {
+					ctx->emitError(&I, "gradient read not supported with integer coordinates");
+					return;
+				}
+#else
+				if(coord_vec_type->getElementType()->isIntegerTy()) {
+					is_gradient = false;
+				}
+#endif
+			}
+			
+			// -> return data and cl function name
+			// NOTE: we don't have a c++ mangling support in here, so do it manually
+			// (this is actually easy enough, since everything is very static)
+			std::string cl_func_name;
+			llvm::Type* ret_type;
+			if(func_name.endswith(".float")) {
+				cl_func_name = "_Z11read_imagef";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4);
+			}
+			else if(func_name.endswith(".int")) {
+				cl_func_name = "_Z11read_imagei";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".uint")) {
+				cl_func_name = "_Z12read_imageui";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".half")) {
+				cl_func_name = "_Z11read_imageh";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getHalfTy(*ctx), 4);
+			}
+			// unknown -> ignore
+			else return;
+			
+			// -> geom
+			cl_func_name += geom;
+			
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			if(is_depth) {
+				// depth return type is always a float
+				ret_type = llvm::Type::getFloatTy(*ctx);
+			}
+			
+			// except for msaa, we always have a sampler
+			if(!is_msaa) {
+				cl_func_name += "11ocl_sampler";
+				func_arg_types.push_back(sampler_arg->getType());
+				func_args.push_back(sampler_arg);
+			}
+			
+			// -> offset
+			// opencl has no offset support, so always add it
+			llvm::Value* offset_coord_arg = coord_arg;
+			if(is_offset) {
+				if(coord_vec_type->getElementType()->isIntegerTy()) {
+					offset_coord_arg = builder->CreateAdd(coord_arg, offset_arg);
+				}
+				else {
+					// need to fallback to s/w for fp coords
+					auto offset_dim = coord_vec_type->getNumElements();
+					auto img_dims = get_image_dim(img_handle_arg, coord_vec_type, geom);
+					
+					// float_offset_i = float(offset_i) / float(dim_i)
+					llvm::Value* fp_offset = UndefValue::get(coord_vec_type);
+					for(uint32_t i = 0; i < offset_dim; ++i) {
+						auto offset_i = builder->CreateExtractElement(offset_arg, builder->getInt32(i));
+						
+						// one offset elem is often 0
+						// -> add some special handling since the si->fp conversion and fdiv are unnecessary here
+						// (this might later also get rid of unnecessary get_image_* calls)
+						if(const auto const_offset_i = dyn_cast_or_null<ConstantInt>(offset_i)) {
+							if(const_offset_i->getSExtValue() == 0) {
+								builder->CreateInsertElement(fp_offset, ConstantFP::get(builder->getFloatTy(), 0.0),
+															 builder->getInt32(i));
+								continue;
+							}
+						}
+						
+						auto offset_i_fp = builder->CreateSIToFP(offset_i, builder->getFloatTy());
+						auto dim_i = builder->CreateSIToFP(img_dims[i], builder->getFloatTy());
+						fp_offset = builder->CreateInsertElement(fp_offset,
+																 builder->CreateFDiv(offset_i_fp, dim_i),
+																 builder->getInt32(i));
+					}
+					
+					// finally: add the compute fp offset
+					offset_coord_arg = builder->CreateFAdd(coord_arg, fp_offset);
+				}
+			}
+			
+			// -> coord
+			handle_cl_coord(I,
+							offset_coord_arg,
+							layer_arg,
+							is_array,
+							is_msaa,
+							false, // can have either int or float coords
+							cl_func_name,
+							func_arg_types,
+							func_args);
+			
+			// -> sample
+			if(is_msaa) {
+				if(!sample_arg->getType()->isIntegerTy()) {
+					ctx->emitError(&I, "msaa sample index must be integer");
+					return;
+				}
+				
+				cl_func_name += "i";
+				func_arg_types.push_back(sample_arg->getType());
+				func_args.push_back(sample_arg);
+			}
+			
+			// -> gradient
+			if(is_gradient) {
+				const auto coord_dim = coord_vec_type->getNumElements();
+				if(coord_dim == 1) {
+					// extract scalar
+					cl_func_name += "ff";
+					func_arg_types.push_back(builder->getFloatTy());
+					func_args.push_back(builder->CreateExtractElement(dpdx_arg, builder->getInt32(0)));
+					func_arg_types.push_back(builder->getFloatTy());
+					func_args.push_back(builder->CreateExtractElement(dpdy_arg, builder->getInt32(0)));
+				}
+				else if(coord_dim == 2) {
+					// just pass-through
+					cl_func_name += "Dv2_fDv2_f";
+					func_arg_types.push_back(dpdx_arg->getType());
+					func_args.push_back(dpdx_arg);
+					func_arg_types.push_back(dpdy_arg->getType());
+					func_args.push_back(dpdy_arg);
+				}
+				else if(coord_dim == 3) {
+					// need to create a vector4
+					cl_func_name += "Dv4_fDv4_f";
+					
+					const auto grad_type = llvm::FixedVectorType::get(builder->getFloatTy(), 4);
+					func_arg_types.push_back(grad_type);
+					func_arg_types.push_back(grad_type);
+					
+					llvm::Value* dpdx4 = UndefValue::get(grad_type);
+					llvm::Value* dpdy4 = UndefValue::get(grad_type);
+					for(uint32_t i = 0; i < 3; ++i) {
+						auto idx = builder->getInt32(i);
+						dpdx4 = builder->CreateInsertElement(dpdx4, builder->CreateExtractElement(dpdx_arg, idx), idx);
+						dpdy4 = builder->CreateInsertElement(dpdy4, builder->CreateExtractElement(dpdy_arg, idx), idx);
+					}
+					func_args.push_back(dpdx4);
+					func_args.push_back(dpdy4);
+				}
+				else llvm_unreachable("invalid coord dim");
+			}
+			
+			// -> lod
+			// NOTE: bias is never supported
+			if(is_lod_or_bias) {
+				cl_func_name += "f";
+				
+				auto lod = lod_or_bias_arg;
+				if(lod_or_bias_arg->getType()->isIntegerTy()) {
+					// only float is supported, convert it
+					lod = builder->CreateSIToFP(lod_or_bias_arg, builder->getFloatTy());
+				}
+				func_arg_types.push_back(lod->getType());
+				func_args.push_back(lod);
+			}
+			
+			// create the opencl call
+			auto read_func = get_or_create_spir_function(cl_func_name, ret_type, func_arg_types, true);
+			llvm::CallInst* read_call = builder->CreateCall(read_func, func_args);
+			read_call->setDoesNotAccessMemory(); // all reads are readnone (can be optimized away if unused)
+			read_call->setConvergent();
+			read_call->setDoesNotThrow();
+			read_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			read_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			// if this is a depth read/sample, the return type is a float -> create a float4
+			llvm::Value* read_call_result = read_call;
+			if(is_depth) {
+				read_call_result = UndefValue::get(llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4));
+				if(!is_compare) {
+					read_call_result = builder->CreateInsertElement(read_call_result, read_call, builder->getInt32(0));
+				}
+				// -> compare
+				else {
+					emulate_depth_compare(read_call_result, read_call, compare_function, compare_value_arg);
+				}
+				// NOTE: rest of vector is undef/zero (and will be stripped away again anyways)
+			}
+			
+			//
+			I.replaceAllUsesWith(read_call_result);
+			I.eraseFromParent();
+		}
+		
+		void handle_write_image(Instruction& I,
+								const StringRef& func_name,
+								llvm::Value* img_handle_arg,
+								const COMPUTE_IMAGE_TYPE& full_image_type,
+								const COMPUTE_IMAGE_TYPE& image_type,
+								const COMPUTE_IMAGE_TYPE& format_type,
+								const COMPUTE_IMAGE_TYPE& data_type,
+								llvm::Value* /* rt_image_type */,
+								const bool& is_normalized,
+								const uint32_t& image_channel_count,
+								llvm::Value* coord_arg,
+								llvm::Value* layer_arg,
+								llvm::Value* lod_arg,
+								const bool is_lod,
+								llvm::Value* data_arg) override {
+			SmallVector<llvm::Type*, 8> func_arg_types;
+			SmallVector<llvm::Value*, 8> func_args;
+			
+			//// more arg checking
+			auto coord_vec_type = dyn_cast_or_null<VectorType>(coord_arg->getType());
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if(!coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer");
+				return;
+			}
+			
+			std::string cl_func_name, dtype;
+			if(func_name.endswith(".float")) {
+				cl_func_name = "_Z12write_imagef";
+				dtype = "f";
+			}
+			else if(func_name.endswith(".int")) {
+				cl_func_name = "_Z12write_imagei";
+				dtype = "i";
+			}
+			else if(func_name.endswith(".uint")) {
+				cl_func_name = "_Z13write_imageui";
+				dtype = "j";
+			}
+			else if(func_name.endswith(".half")) {
+				cl_func_name = "_Z12write_imageh";
+				dtype = "h";
+			}
+			// unknown -> ignore
+			else return;
+			
+			//// func replacement
+			// -> geom
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown/incorrect/unsupported image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			// filter types that are not allowed, b/c they can't be directly written to
+			switch (image_type) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					ctx->emitError(&I, "invalid image type - type is not writable");
+					return;
+				default:
+					break;
+			}
+			
+			cl_func_name += geom;
+			
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			// -> coord
+			handle_cl_coord(I,
+							coord_arg,
+							layer_arg,
+							is_array,
+							false, // no msaa
+							true, // must always have int coords for writes
+							cl_func_name,
+							func_arg_types,
+							func_args);
+			
+			// -> lod
+			if(is_lod) {
+				// always int
+				cl_func_name += "i";
+				func_arg_types.push_back(lod_arg->getType());
+				func_args.push_back(lod_arg);
+			}
+			
+			// -> data
+			// data is always a vector4, unless we're writing depth
+			if(!is_depth) {
+				cl_func_name += "Dv4_";
+				func_arg_types.push_back(data_arg->getType());
+				func_args.push_back(data_arg);
+			}
+			else {
+				// extract and use depth elem
+				auto depth_arg = builder->CreateExtractElement(data_arg, builder->getInt32(0));
+				func_arg_types.push_back(depth_arg->getType());
+				func_args.push_back(depth_arg);
+			}
+			cl_func_name += dtype;
+			
+			// create the opencl call
+			auto write_func = get_or_create_spir_function(cl_func_name, builder->getVoidTy(), func_arg_types, false);
+			llvm::CallInst* write_call = builder->CreateCall(write_func, func_args);
+			write_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			write_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			//
+			I.replaceAllUsesWith(write_call);
+			I.eraseFromParent();
+			
+		}
+		
+		void handle_get_image_dim(Instruction& I,
+								  const StringRef& func_name,
+								  llvm::Value* img_handle_arg,
+								  const COMPUTE_IMAGE_TYPE& /* full_image_type */,
+								  const COMPUTE_IMAGE_TYPE& image_type,
+								  llvm::Value* lod_arg) override {
+			// gather info
+			const auto dim_count = image_dim_count(image_type);
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			const std::string geom = geom_cstr;
+			
+			// TODO/NOTE: LOD arg is not supported with OpenCL/SPIR, only with OpenCL/SPIR-V (but not properly handled yet)
+			const bool with_lod = false;
+
+			// query/get function base
+			const auto query_image = [&](const std::string& query_name) {
+				SmallVector<llvm::Type*, 8> func_arg_types;
+				SmallVector<llvm::Value*, 8> func_args;
+				func_arg_types.push_back(img_handle_arg->getType());
+				func_args.push_back(img_handle_arg);
+				if (with_lod) {
+					func_arg_types.push_back(lod_arg->getType());
+					func_args.push_back(lod_arg);
+				}
+				
+				// -> build get func name
+				const std::string get_func_name = query_name + geom + (with_lod ? "i" : "");
+				
+				// create the air call
+				const auto ret_type = llvm::Type::getInt32Ty(*ctx);
+				auto get_func = get_or_create_spir_function(get_func_name, ret_type, func_arg_types, true);
+				llvm::CallInst* get_call = builder->CreateCall(get_func, func_args);
+				get_call->setConvergent();
+				get_call->setDoesNotThrow();
+				get_call->setDoesNotAccessMemory(); // all get_* calls are readnone (can be optimized away if unused)
+				get_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+				get_call->setCallingConv(CallingConv::FLOOR_FUNC);
+				return get_call;
+			};
+			
+			// we have to a return a full image dim query for all dims of the image (type dependent)
+			// order is: width [, height] [, depth], [, layer_count]
+			// non-existing dims are set to 0
+			const auto ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			llvm::Value* ret_vec = UndefValue::get(ret_type);
+			uint32_t ret_vec_idx = 0;
+			// all images have a width
+			ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z15get_image_width"), builder->getInt32(ret_vec_idx++));
+			if (dim_count >= 2) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z16get_image_height"), builder->getInt32(ret_vec_idx++));
+			}
+			if (dim_count >= 3) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z15get_image_depth"), builder->getInt32(ret_vec_idx++));
+			}
+			if (is_array) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z20get_image_array_size"), builder->getInt32(ret_vec_idx++));
+			}
+			// NOTE: while cube maps technically have 6 layers, this number is not stored in the image dim
+			
+			// fill remaining components with 0
+			for (uint32_t vec_idx = ret_vec_idx; vec_idx < 4; ++vec_idx) {
+				ret_vec = builder->CreateInsertElement(ret_vec, builder->getInt32(0), builder->getInt32(ret_vec_idx++));
+			}
+			
+			//
+			I.replaceAllUsesWith(ret_vec);
+			I.eraseFromParent();
+		}
+		
+	};
+}
+
+char SPIRImage::ID = 0;
+INITIALIZE_PASS_BEGIN(SPIRImage, "SPIRImage", "SPIRImage Pass", false, false)
+INITIALIZE_PASS_END(SPIRImage, "SPIRImage", "SPIRImage Pass", false, false)
+
+FunctionPass *llvm::createSPIRImagePass(const uint32_t image_capabilities,
+										const bool intel_workarounds) {
+	return new SPIRImage(image_capabilities, intel_workarounds);
+}
diff --git a/llvm/lib/Transforms/LibFloor/StructuralAnalysis.cpp b/llvm/lib/Transforms/LibFloor/StructuralAnalysis.cpp
new file mode 100644
index 000000000000..80013a7198ef
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/StructuralAnalysis.cpp
@@ -0,0 +1,1938 @@
+//===- StructuralAnalysis.cpp - -------------------------------------------===//
+//
+// Copyright (c) 2015, Computer Architecture and Systems Laboratory at Georgia Tech
+// Copyright (c) 2016 - 2017, Florian Ziesche (LLVM port + general fixes/cleanup)
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice, this
+//   list of conditions and the following disclaimer.
+//
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+//
+// * Neither the name of gpuocelot nor the names of its
+//   contributors may be used to endorse or promote products derived from
+//   this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE
+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+//===----------------------------------------------------------------------===//
+// \author  Haicheng Wu <hwu36@gatech.edu>
+// \date    Monday April 4, 2011
+// \brief   The source file for the StructuralAnalysis pass.
+//===----------------------------------------------------------------------===//
+//
+// This file defines the class of Structural Analysis which will return the
+// control tree and unstructured branches of a function
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor/StructuralAnalysis.h"
+#include "llvm/Support/GraphWriter.h"
+#include "llvm/Support/FileSystem.h"
+#include <algorithm>
+
+namespace llvm {
+
+void StructuralAnalysis::dumpCFGDot(const std::string &filename,
+                                    const Function &F) {
+  std::error_code EC;
+  raw_fd_ostream File(filename, EC, sys::fs::OF_Text);
+
+  if (!EC)
+    WriteGraph(File, (const Function *)&F);
+  else
+    errs() << "failed to dump cfg!";
+}
+
+void StructuralAnalysis::dumpIR(const std::string &filename,
+                                const Function &F) {
+  std::error_code EC;
+  raw_fd_ostream File(filename, EC, sys::fs::OF_Text);
+
+  if (!EC)
+    File << F;
+  else
+    errs() << "failed to dump IR!";
+}
+
+StructuralAnalysis::NodeTy::~NodeTy() {
+  for (auto n = childNode.begin(); n != childNode.end(); ++n) {
+    delete *n;
+  }
+}
+
+static StructuralAnalysis::NodeTy *
+get_first_node_or_null(const std::vector<StructuralAnalysis::NodeTy *> &nodes) {
+  if (nodes.empty()) {
+    return nullptr;
+  }
+  return nodes[0];
+}
+
+// buildSimpleCFG - Build a Simple CFG out of the LLVM CFG
+void StructuralAnalysis::buildSimpleCFG(NodeVecTy &Nodes) {
+  // Create a simple CFG node for every Basic Block
+  for (BasicBlock &i : *_function) {
+    NodeTy *n = new NodeTy();
+    n->BB = &i;
+    n->containedBB.emplace_back(&i);
+    Nodes.emplace_back(n);
+    BB2NodeMap[&i] = n;
+  }
+
+  // Setup the edges of the simple CFG
+  for (BasicBlock &block : *_function) {
+    NodeTy *n = BB2NodeMap[&block];
+
+    // Setup the predecessor of every node
+    for (BasicBlock *pred : predecessors(&block)) {
+      n->predNode.emplace_back(BB2NodeMap[pred]);
+    }
+
+    // Setup the successor of every node
+    for (BasicBlock *succ : successors(&block)) {
+      n->succNode.emplace_back(BB2NodeMap[succ]);
+    }
+  }
+
+  // Remove unreachable node
+  NodeTy *entry = BB2NodeMap[&_function->getEntryBlock()];
+
+  deleteUnreachableNodes(Nodes, entry);
+}
+
+// structuralAnalysis - Follow Fig 7.39 of Muchnick book
+void StructuralAnalysis::structuralAnalysis(NodeVecTy &Nodes, NodeTy *entry) {
+  NodeTy *n = nullptr, *p = nullptr, *entryNode = nullptr, *exitNode = nullptr;
+  RegionTy rType;
+  NodeVecTy nodeSet, reachUnder;
+
+  unstructuredBRVec.clear();
+
+  // Handle the case if the Function has only one Basic Block
+  if (Nodes.size() == 1) {
+    NodeTy *node = new NodeTy();
+    NodeTy *singleNode = Nodes.front();
+    node->isCombined = true;
+    node->childNode.emplace_back(singleNode);
+    node->entryNode = singleNode;
+    node->exitBB = singleNode->BB;
+    node->containedBB.emplace_back(singleNode->BB);
+    node->nodeType = Block;
+
+    singleNode->parentNode = node;
+
+    Nodes[0] = node; // replace singleNode with node
+
+    return;
+  }
+
+  do {
+    bool change = false;
+
+    post.clear();
+    preTree.clear();
+    postTree.clear();
+
+    visit.clear();
+    visitPath.clear();
+    postMax = 0;
+    postCtr = 1;
+    preMax = 0;
+
+    for (NodeTy *node : Nodes) {
+      preTree[node] = 0;
+      postTree[node] = 0;
+    }
+
+    DFSPostorder(Nodes, entry);
+
+    while (Nodes.size() > 1 && postCtr <= postMax) {
+      n = post[postCtr];
+
+      if (!containsNode(Nodes, n))
+        continue;
+
+      // Locate an acyclic region, if present
+      if (n->isLoopHeader && n->loopExitNode) {
+        visitPath.clear();
+
+        if (path(n, n->loopExitNode, Nodes, nullptr)) {
+          NodeTy *tmpNode = n->loopExitNode;
+
+          while (tmpNode->parentNode) {
+            tmpNode = tmpNode->parentNode;
+          }
+
+          n->remove_successor(tmpNode);
+          tmpNode->remove_predecessor(n);
+          n->loopExitNode = tmpNode;
+        } else
+          n->loopExitNode = nullptr;
+      }
+
+      rType =
+          acyclicRegionType(Nodes, n, nodeSet, &entryNode, &exitNode, entry);
+
+      if (n->isLoopHeader && n->loopExitNode) {
+        n->add_successor(n->loopExitNode);
+        n->loopExitNode->add_predecessor(n);
+      }
+
+      if (rType == Improper) {
+        change = true;
+
+        break;
+      } else if (rType != Nil) {
+        p = reduce(Nodes, rType, nodeSet, entryNode, exitNode);
+        change = true;
+
+        if (containsNode(nodeSet, entry)) {
+          entry = p;
+        }
+
+        break;
+      } else {
+        if (NodeTy *backEdgeNode = pathBack(n, Nodes, reachUnder)) {
+          rType = cyclicRegionType(Nodes, reachUnder, n, backEdgeNode,
+                                   &exitNode, entry);
+
+          if (rType == Improper) {
+            change = true;
+
+            break;
+          } else if (rType != Nil) {
+            change = true;
+            p = reduce(Nodes, rType, reachUnder, n, exitNode);
+
+            if (containsNode(reachUnder, entry)) {
+              entry = p;
+            }
+
+            break;
+          } else
+            postCtr++;
+        } else
+          postCtr++;
+      }
+    }
+
+    if (!change) {
+      for (uint32_t i = 1; i <= postMax; i++) {
+        NodeTy *node = post[i];
+
+        if (node->predNode.size() > 1 && node->succNode.empty()) {
+          uint32_t min = postMax + 1;
+
+          for (NodeTy *predNode : node->predNode) {
+            if (postTree[predNode] < min)
+              min = postTree[predNode];
+          }
+
+          for (NodeTy *predNode : node->predNode) {
+            if (postTree[predNode] != min) {
+              if (isStillReachableFromEntry(Nodes, entry, node, predNode)) {
+                findUnstructuredBR(predNode, node, true, true);
+                change = true;
+              }
+            }
+          }
+
+          if (change)
+            break;
+        }
+      }
+    }
+
+    if (!change) {
+      for (uint32_t i = 1; i <= postMax; i++) {
+        NodeTy *node = post[i];
+
+        if (node->predNode.size() > 1 && !node->isBackEdge) {
+          NodeTy *tmpNode = nullptr;
+          bool processThisNode = true;
+
+          for (NodeTy *predNode : node->predNode) {
+            if (edge2ClassMap[std::make_pair(predNode, node)] == BACK) {
+              processThisNode = false;
+
+              break;
+            }
+
+            if (tmpNode == nullptr)
+              tmpNode = predNode;
+            else {
+              visitPath.clear();
+
+              if (path(tmpNode, predNode, Nodes, node))
+                continue;
+              else {
+                visitPath.clear();
+
+                if (path(predNode, tmpNode, Nodes, node))
+                  tmpNode = predNode;
+                else {
+                  processThisNode = false;
+
+                  break;
+                }
+              }
+            }
+          }
+
+          if (processThisNode) {
+            for (NodeTy *predNode : node->predNode) {
+              if (predNode == tmpNode) {
+                if (isStillReachableFromEntry(Nodes, entry, node, predNode)) {
+                  findUnstructuredBR(predNode, node, true, true);
+                  change = true;
+                }
+              }
+            }
+
+            if (change)
+              break;
+          }
+        }
+      }
+    }
+
+    if (!change) {
+      for (uint32_t i = 1; i <= postMax; i++) {
+        NodeTy *node = post[i];
+
+        if (node->predNode.size() > 1 && !node->isBackEdge) {
+          bool processThisNode = true;
+          uint32_t min = postMax + 1;
+
+          for (NodeTy *predNode : node->predNode) {
+            if (edge2ClassMap[std::make_pair(predNode, node)] == BACK) {
+              processThisNode = false;
+
+              break;
+            }
+
+            if (postTree[predNode] < min)
+              min = postTree[predNode];
+          }
+
+          if (processThisNode) {
+
+            for (NodeTy *predNode : node->predNode) {
+              if (postTree[predNode] != min) {
+                if (isStillReachableFromEntry(Nodes, entry, node, predNode)) {
+                  findUnstructuredBR(predNode, node, true, true);
+                  change = true;
+                }
+              }
+            }
+
+            if (change)
+              break;
+          }
+        }
+      }
+    }
+
+    // TODO: properly report this instead of asserting
+    if (!change) {
+      StructuralAnalysis::dumpCFGDot("failed_function.dot", *_function);
+    }
+    assert(change != false && "Cannot reduce any more in structural analysis");
+    if (!change)
+      break;
+  } while (Nodes.size() != 1);
+}
+
+// DFSPostorder - Follow Fig 7.40 of Muchnick book
+void StructuralAnalysis::DFSPostorder(const NodeVecTy &Nodes, NodeTy *x) {
+  visit.emplace(x);
+  preTree[x] = ++preMax;
+
+  for (NodeTy *y : x->succNode) {
+    if (visit.count(y) == 0) {
+      DFSPostorder(Nodes, y);
+      edge2ClassMap[std::make_pair(x, y)] = TREE;
+    } else if (preTree[x] < preTree[y]) {
+      edge2ClassMap[std::make_pair(x, y)] = FORWARD;
+    } else if (postTree[y] == 0 || preTree[x] == preTree[y]) {
+      edge2ClassMap[std::make_pair(x, y)] = BACK;
+    } else {
+      edge2ClassMap[std::make_pair(x, y)] = CROSS;
+    }
+  }
+
+  postMax++;
+  post[postMax] = x;
+  postTree[x] = postMax;
+}
+
+// acyclicRegionType - Follow Fig 7.41 of Muchnick book
+StructuralAnalysis::RegionTy
+StructuralAnalysis::acyclicRegionType(const NodeVecTy &Nodes, NodeTy *node,
+                                      NodeVecTy &nset, NodeTy **entryNode,
+                                      NodeTy **exitNode, NodeTy *entry) {
+  NodeTy *m, *n;
+  bool p, s;
+
+  nset.clear();
+
+  // Check for a block containing node
+  NodeTy *firstNode, *lastNode;
+  firstNode = lastNode = n = node;
+  p = true;
+  s = (n->succNode.size() == 1);
+
+  while (p && s) {
+    lastNode = n;
+
+    if (!containsNode(nset, n)) {
+      nset.emplace_back(n);
+    } else {
+      return Nil;
+    }
+
+    n = get_first_node_or_null(n->succNode);
+    p = (n->predNode.size() == 1);
+    s = (n->succNode.size() == 1);
+  }
+
+  if (p) {
+    if (!containsNode(nset, n)) {
+      nset.emplace_back(n);
+      lastNode = n;
+    } else {
+      return Nil;
+    }
+  }
+
+  n = node;
+  p = (n->predNode.size() == 1);
+  s = true;
+
+  while (p && s) {
+    firstNode = n;
+
+    if (!containsNode(nset, n) || n == node) {
+      insertNode(nset, n);
+    } else {
+      return Nil;
+    }
+    n = get_first_node_or_null(n->predNode);
+    p = (n->predNode.size() == 1);
+    s = (n->succNode.size() == 1);
+  }
+
+  if (s) {
+    if (!containsNode(nset, n) || n == node) {
+      firstNode = n;
+      insertNode(nset, n);
+    } else {
+      return Nil;
+    }
+  }
+
+  if (firstNode->has_predecessor(lastNode)) {
+    if (nset.size() == 2) {
+      return Nil;
+    } else {
+      eraseNode(nset, firstNode);
+    }
+  }
+
+  *entryNode = n;
+
+  if (nset.size() >= 2) {
+    if (!containsNode(nset, *entryNode)) {
+      for (NodeTy *succ : (*entryNode)->succNode) {
+        if (containsNode(nset, succ)) {
+          *entryNode = succ;
+        }
+      }
+    }
+
+    *exitNode = lastNode;
+
+    return Block;
+  }
+
+  *entryNode = node;
+
+  if ((*entryNode)->succNode.size() == 2) {
+    auto i = (*entryNode)->succNode.begin();
+    m = *i;
+    ++i;
+    n = *i;
+
+    if (m == *entryNode || n == *entryNode)
+      return Nil;
+
+    if (edge2ClassMap[std::make_pair(*entryNode, m)] == BACK)
+      return Nil;
+    if (edge2ClassMap[std::make_pair(*entryNode, n)] == BACK)
+      return Nil;
+
+    // Check for a normal IfThenElse
+    if (m->succNode.size() == 1 && n->succNode.size() == 1 &&
+        m->predNode.size() == 1 && n->predNode.size() == 1 &&
+        get_first_node_or_null(m->succNode) ==
+            get_first_node_or_null(n->succNode) &&
+        get_first_node_or_null(m->succNode) != *entryNode) {
+
+      if (edge2ClassMap[std::make_pair(m, *entryNode)] == BACK)
+        return Nil;
+      if (edge2ClassMap[std::make_pair(n, *entryNode)] == BACK)
+        return Nil;
+
+      insertNode(nset, *entryNode);
+      insertNode(nset, m);
+      insertNode(nset, n);
+      *exitNode = get_first_node_or_null(m->succNode);
+
+      return IfThenElse;
+    }
+    // Check for an IfThenElse with no exit block
+    if (m->succNode.empty() && n->succNode.empty() && m->predNode.size() == 1 &&
+        n->predNode.size() == 1) {
+      insertNode(nset, *entryNode);
+      insertNode(nset, m);
+      insertNode(nset, n);
+      *exitNode = nullptr;
+
+      return IfThenElse;
+    }
+    // Check for an IfThen
+    // n is the Then part
+    else if (n->succNode.size() == 1 && n->predNode.size() == 1 &&
+             m == get_first_node_or_null(n->succNode)) {
+      if (edge2ClassMap[std::make_pair(n, m)] != BACK) {
+        if (edge2ClassMap[std::make_pair(n, *entryNode)] == BACK) {
+          return Nil;
+        }
+
+        insertNode(nset, *entryNode);
+        insertNode(nset, n);
+        *exitNode = m;
+
+        return IfThen;
+      }
+    }
+    // m is the Then part
+    else if (m->succNode.size() == 1 && m->predNode.size() == 1 &&
+             n == get_first_node_or_null(m->succNode)) {
+      if (edge2ClassMap[std::make_pair(m, n)] != BACK) {
+        if (edge2ClassMap[std::make_pair(m, *entryNode)] == BACK) {
+          return Nil;
+        }
+
+        insertNode(nset, *entryNode);
+        insertNode(nset, m);
+        *exitNode = n;
+
+        return IfThen;
+      }
+    }
+    // n is the Then part w/o exiting edge
+    else if (n->succNode.empty() && n->predNode.size() == 1) {
+      visitPath.clear();
+
+      if (!path(m, *entryNode, Nodes, nullptr)) {
+        insertNode(nset, *entryNode);
+        insertNode(nset, n);
+        *exitNode = nullptr;
+
+        return IfThen;
+      }
+    }
+    // m is the Then part w/o exiting edge
+    else if (m->succNode.empty() && m->predNode.size() == 1) {
+      visitPath.clear();
+
+      if (!path(n, *entryNode, Nodes, nullptr)) {
+        insertNode(nset, *entryNode);
+        insertNode(nset, m);
+        *exitNode = nullptr;
+
+        return IfThen;
+      }
+    }
+    // Check for an IfThenElse with incoming edges
+    else if (m->succNode.size() == 1 && n->succNode.size() == 1 &&
+             get_first_node_or_null(m->succNode) ==
+                 get_first_node_or_null(n->succNode) &&
+             get_first_node_or_null(m->succNode) != *entryNode) {
+
+      if (edge2ClassMap[std::make_pair(m, *entryNode)] == BACK)
+        return Nil;
+      if (edge2ClassMap[std::make_pair(n, *entryNode)] == BACK)
+        return Nil;
+
+      if (!n->has_predecessor(get_first_node_or_null(n->succNode)) &&
+          !m->has_predecessor(get_first_node_or_null(m->succNode))) {
+
+        if (m->has_predecessor(get_first_node_or_null(m->succNode)) ||
+            n->has_predecessor(get_first_node_or_null(n->succNode))) {
+          return Nil;
+        }
+
+        bool improperFlag = false;
+
+        if (m->predNode.size() > 1) {
+          for (auto pi = m->predNode.begin(), pe = m->predNode.end(); pi != pe;
+               ++pi) {
+            if (*pi != *entryNode &&
+                isStillReachableFromEntry(Nodes, entry, m, *pi) &&
+                edge2ClassMap[std::make_pair(*pi, m)] != BACK) {
+              findUnstructuredBR(*pi, m, true, true);
+              improperFlag = true;
+            }
+          }
+        }
+
+        if (n->predNode.size() > 1) {
+          for (auto pi = n->predNode.begin(), pe = n->predNode.end(); pi != pe;
+               ++pi) {
+            if (*pi != *entryNode &&
+                isStillReachableFromEntry(Nodes, entry, n, *pi) &&
+                edge2ClassMap[std::make_pair(*pi, n)] != BACK) {
+              findUnstructuredBR(*pi, n, true, true);
+              improperFlag = true;
+            }
+          }
+        }
+
+        if (improperFlag)
+          return Improper;
+      }
+    }
+    // Check for an IfThen with incoming edges
+    // n is the Then part
+    else if (n->succNode.size() == 1 && n->predNode.size() > 1 &&
+             m == get_first_node_or_null(n->succNode)) {
+      if (edge2ClassMap[std::make_pair(n, *entryNode)] == BACK)
+        return Nil;
+
+      if (edge2ClassMap[std::make_pair(n, m)] != BACK) {
+        if (n->has_predecessor(m))
+          return Nil;
+
+        bool improperFlag = false;
+
+        for (auto pi = n->predNode.begin(), pe = n->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, n, *pi) &&
+              edge2ClassMap[std::make_pair(*pi, n)] != BACK) {
+            findUnstructuredBR(*pi, n, true, true);
+            improperFlag = true;
+          }
+        }
+
+        if (improperFlag)
+          return Improper;
+      }
+    }
+    // m is the Then part
+    else if (m->succNode.size() == 1 && m->predNode.size() > 1 &&
+             n == get_first_node_or_null(m->succNode)) {
+      if (edge2ClassMap[std::make_pair(m, *entryNode)] == BACK)
+        return Nil;
+
+      if (edge2ClassMap[std::make_pair(m, n)] != BACK) {
+        if (m->has_predecessor(n))
+          return Nil;
+
+        bool improperFlag = false;
+
+        for (auto pi = m->predNode.begin(), pe = m->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, m, *pi) &&
+              edge2ClassMap[std::make_pair(*pi, m)] != BACK) {
+            findUnstructuredBR(*pi, m, true, true);
+            improperFlag = true;
+          }
+        }
+
+        if (improperFlag)
+          return Improper;
+      }
+    }
+    // Check for an IfThenElse (w/o exit block) with incoming edges
+    else if (m->succNode.empty() && n->succNode.empty()) {
+      bool improperFlag = false;
+
+      if (m->predNode.size() > 1) {
+        for (auto pi = m->predNode.begin(), pe = m->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, m, *pi)) {
+            findUnstructuredBR(*pi, m, true, true);
+            improperFlag = true;
+          }
+        }
+      }
+
+      if (n->predNode.size() > 1) {
+        for (auto pi = n->predNode.begin(), pe = n->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, n, *pi)) {
+            findUnstructuredBR(*pi, n, true, true);
+            improperFlag = true;
+          }
+        }
+      }
+
+      if (improperFlag)
+        return Improper;
+    }
+    // n is the Then part (w/o exiting edge) with incoming edges
+    else if (n->succNode.empty() && n->predNode.size() > 1) {
+      visitPath.clear();
+
+      if (!path(m, *entryNode, Nodes, nullptr)) {
+        if (n->has_predecessor(m))
+          return Nil;
+
+        bool improperFlag = false;
+
+        for (auto pi = n->predNode.begin(), pe = n->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, n, *pi)) {
+            findUnstructuredBR(*pi, n, true, true);
+            improperFlag = true;
+          }
+        }
+
+        if (improperFlag)
+          return Improper;
+      }
+    }
+    // m is the Then part w/o exiting edge with incoming edges
+    else if (m->succNode.empty() && m->predNode.size() > 1) {
+      visitPath.clear();
+
+      if (!path(n, *entryNode, Nodes, nullptr)) {
+        if (m->has_predecessor(n))
+          return Nil;
+
+        bool improperFlag = false;
+
+        for (auto pi = m->predNode.begin(), pe = m->predNode.end(); pi != pe;
+             ++pi) {
+          if (*pi != *entryNode &&
+              isStillReachableFromEntry(Nodes, entry, n, *pi)) {
+            findUnstructuredBR(*pi, m, true, true);
+            improperFlag = true;
+          }
+        }
+
+        if (improperFlag)
+          return Improper;
+      }
+    }
+  }
+  // Check for Case
+  else if ((*entryNode)->succNode.size() > 2) {
+    if (isCaseWithDefault(Nodes, *entryNode, exitNode, entry)) {
+      insertNode(nset, *entryNode);
+
+      for (auto i = (*entryNode)->succNode.begin(),
+                e = (*entryNode)->succNode.end();
+           i != e; ++i) {
+        insertNode(nset, *i);
+      }
+
+      return Case;
+    } else if (isCaseWithoutDefault(Nodes, *entryNode, exitNode, entry)) {
+      insertNode(nset, *entryNode);
+
+      for (auto i = (*entryNode)->succNode.begin(),
+                e = (*entryNode)->succNode.end();
+           i != e; ++i) {
+        if (*i != *exitNode) {
+          insertNode(nset, *i);
+        }
+      }
+
+      return Case;
+    } else if (isImproperCaseWithDefault(Nodes, *entryNode, entry)) {
+      return Improper;
+    } else if (isImproperCaseWithoutDefault(Nodes, *entryNode, exitNode,
+                                            entry)) {
+      return Improper;
+    }
+  }
+
+  return Nil;
+}
+
+// isCaseWithDefault - Check if node leads a case block
+bool StructuralAnalysis::isCaseWithDefault(const NodeVecTy &Nodes,
+                                           NodeTy *entryNode, NodeTy **exitNode,
+                                           NodeTy *entry) {
+  *exitNode = nullptr;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    // Check if every successor node has only one successor
+    if ((*i)->succNode.size() > 1)
+      return false;
+
+    if (edge2ClassMap[std::make_pair(entryNode, *i)] == BACK)
+      return false;
+
+    // If successor has only one predessor, it has to be the entry node
+    if ((*i)->predNode.size() == 1) {
+      if (entryNode != get_first_node_or_null((*i)->predNode))
+        return false;
+    }
+    // If successor has two predessors, one has to be the entry node
+    // and the other has to be another successor node
+    else if ((*i)->predNode.size() == 2) {
+      auto pi = (*i)->predNode.begin();
+      NodeTy *predNode1 = *pi;
+      ++pi;
+      NodeTy *predNode2 = *pi;
+
+      if (predNode1 != entryNode || !entryNode->has_successor(predNode2))
+        if (!entryNode->has_successor(predNode1) || predNode2 != entryNode)
+          return false;
+    }
+    // The predecessor node number has to be less than 3
+    else
+      return false;
+
+    NodeTy *succNode = get_first_node_or_null((*i)->succNode);
+
+    if (succNode == nullptr)
+      continue;
+
+    if (succNode == entryNode)
+      return false;
+
+    // Check if the successor of the successor node is not another successor
+    if (!entryNode->has_successor(succNode)) {
+      // Check if the successor of the successor is the only exit node
+      if (!*exitNode)
+        *exitNode = succNode;
+      else if (*exitNode != succNode)
+        return false;
+    }
+    // There is no loop between successors
+    else if (succNode->has_successor(*i))
+      return false;
+  }
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if ((*i)->succNode.empty()) {
+      visitPath.clear();
+
+      if (path(*exitNode, entryNode, Nodes, nullptr))
+        return false;
+    }
+  }
+
+  return true;
+}
+
+// isImproperCaseWithDefault - Check if node leads a case block
+bool StructuralAnalysis::isImproperCaseWithDefault(const NodeVecTy &Nodes,
+                                                   NodeTy *entryNode,
+                                                   NodeTy *entry) {
+  NodeTy *exitNode = nullptr;
+  EdgeSetTy improperEdgeSet;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    // Check if every successor node has only one successor
+    if ((*i)->succNode.size() > 1)
+      return false;
+
+    if (edge2ClassMap[std::make_pair(entryNode, *i)] == BACK)
+      return false;
+
+    NodeTy *succNode = get_first_node_or_null((*i)->succNode);
+
+    if (succNode) {
+      if (succNode == entryNode)
+        return false;
+
+      // Check if the successor of the successor node
+      // is not another successor node
+      if (!entryNode->has_successor(succNode)) {
+        // Is the successor of the successor node is the only exit node?
+        if (!exitNode)
+          exitNode = succNode;
+        else if (exitNode != succNode)
+          return false;
+      }
+      // There is no loop between successors
+      else if (succNode->has_successor(*i))
+        return false;
+    }
+
+    // If successor has only one predessor, it has to be the entry node
+    if ((*i)->predNode.size() == 1) {
+      if (entryNode != get_first_node_or_null((*i)->predNode))
+        return false;
+    }
+    // If successor has two predessors, one has to be the entry node
+    // and the other has to be another successor node
+    else if ((*i)->predNode.size() == 2) {
+      auto pi = (*i)->predNode.begin();
+      NodeTy *predNode1 = *pi;
+      ++pi;
+      NodeTy *predNode2 = *pi;
+
+      if (predNode1 != entryNode || !entryNode->has_successor(predNode2))
+        if (!entryNode->has_successor(predNode1) || predNode2 != entryNode)
+          return false;
+    }
+    // The predecessor node number has to be less than 3
+    else {
+      int insideIncomingNum = 0;
+
+      for (auto pi = (*i)->predNode.begin(), pe = (*i)->predNode.end();
+           pi != pe; ++pi) {
+
+        if (edge2ClassMap[std::make_pair(*pi, *i)] != BACK && *pi != exitNode &&
+            *pi != entryNode) {
+          if (!entryNode->has_successor(*pi))
+            improperEdgeSet.insert(std::make_pair(*pi, *i));
+          else {
+            insideIncomingNum++;
+
+            if (insideIncomingNum > 1)
+              improperEdgeSet.insert(std::make_pair(*pi, *i));
+          }
+        } else
+          return false;
+      }
+    }
+  }
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if ((*i)->succNode.empty()) {
+      visitPath.clear();
+
+      if (path(exitNode, entryNode, Nodes, nullptr))
+        return false;
+    }
+  }
+
+  bool improperFlag = false;
+
+  for (EdgeSetTy::iterator i = improperEdgeSet.begin(),
+                           e = improperEdgeSet.end();
+       i != e; ++i)
+    if (isStillReachableFromEntry(Nodes, entry, i->second, i->first)) {
+      findUnstructuredBR(i->first, i->second, true, true);
+      improperFlag = true;
+    }
+
+  return improperFlag;
+}
+
+// isCaseWithoutDefault - Check if node leads a case block
+bool StructuralAnalysis::isCaseWithoutDefault(const NodeVecTy &Nodes,
+                                              NodeTy *entryNode,
+                                              NodeTy **exitNode,
+                                              NodeTy *entry) {
+  // Find the exit node first
+  *exitNode = nullptr;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    NodeTy *node1 = *i;
+    bool foundExit = true;
+
+    // all of successors of exit node are not within the switch block
+    for (auto si = node1->succNode.begin(), se = node1->succNode.end();
+         si != se; ++si) {
+      NodeTy *succNode = *si;
+
+      if (succNode) {
+        if (entryNode->has_successor(succNode)) {
+          foundExit = false;
+
+          break;
+        } else if (succNode == entryNode)
+          return false;
+      }
+    }
+
+    if (!foundExit)
+      continue;
+
+    foundExit = false;
+
+    // at least one of predcessors of exit node comes from switch block
+    for (auto pi = node1->predNode.begin(), pe = node1->predNode.end();
+         pi != pe; ++pi) {
+      NodeTy *predNode = *pi;
+
+      if (predNode != entryNode && entryNode->has_successor(predNode)) {
+        foundExit = true;
+      }
+    }
+
+    if (foundExit) {
+      *exitNode = node1;
+
+      break;
+    }
+  }
+
+  if (!(*exitNode))
+    return false;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if (*i == *exitNode)
+      continue;
+
+    // Check if every successor node has only one successor
+    if ((*i)->succNode.size() > 1)
+      return false;
+
+    NodeTy *succNode = get_first_node_or_null((*i)->succNode);
+
+    if (succNode) {
+      if (succNode == nullptr)
+        continue;
+
+      if (edge2ClassMap[std::make_pair(entryNode, *i)] == BACK)
+        return false;
+
+      // The successor of the successor node should be the the another
+      // successor node of node
+      if (!entryNode->has_successor(succNode))
+        return false;
+      // There is no loop between successors
+      else if (succNode != *exitNode && succNode->has_successor(*i))
+        return false;
+    }
+
+    // If successor has only one predessor, it has to be the entry node
+    if ((*i)->predNode.size() == 1) {
+      if (entryNode != get_first_node_or_null((*i)->predNode))
+        return false;
+    }
+    // If successor has two predessors, one has to be the entry node
+    // and the other has to be another successor node
+    else if ((*i)->predNode.size() == 2) {
+      auto pi = (*i)->predNode.begin();
+      NodeTy *predNode1 = *pi;
+      ++pi;
+      NodeTy *predNode2 = *pi;
+
+      if (predNode1 != entryNode || !entryNode->has_successor(predNode2))
+        if (!entryNode->has_successor(predNode1) || predNode2 != entryNode)
+          return false;
+    }
+    // The predecessor node number has to be less than 3
+    else
+      return false;
+  }
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if ((*i)->succNode.empty()) {
+      visitPath.clear();
+
+      if (path(*exitNode, entryNode, Nodes, nullptr))
+        return false;
+    }
+  }
+
+  return true;
+}
+
+// isImproperCaseoutWithDefault - Check if node leads a case block with incoming
+// edges
+bool StructuralAnalysis::isImproperCaseWithoutDefault(const NodeVecTy &Nodes,
+                                                      NodeTy *entryNode,
+                                                      NodeTy **exitNode,
+                                                      NodeTy *entry) {
+  EdgeSetTy improperEdgeSet;
+
+  // Find the exit node first
+  *exitNode = nullptr;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    NodeTy *node1 = *i;
+    bool foundExit = true;
+
+    // all of successors of exit node are not within the switch block
+    for (auto si = node1->succNode.begin(), se = node1->succNode.end();
+         si != se; ++si) {
+      NodeTy *succNode = *si;
+
+      if (succNode) {
+        if (entryNode->has_successor(succNode)) {
+          foundExit = false;
+
+          break;
+        } else if (succNode == entryNode)
+          return false;
+      }
+    }
+
+    if (!foundExit)
+      continue;
+
+    foundExit = false;
+
+    // at least one of predcessors of exit node comes from switch block
+    for (auto pi = node1->predNode.begin(), pe = node1->predNode.end();
+         pi != pe; ++pi) {
+      NodeTy *predNode = *pi;
+
+      if (predNode != entryNode && entryNode->has_successor(predNode)) {
+        foundExit = true;
+      }
+    }
+
+    if (foundExit) {
+      *exitNode = node1;
+
+      break;
+    }
+  }
+
+  if (!(*exitNode))
+    return false;
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if (*i == *exitNode)
+      continue;
+
+    // Check if every successor node has only one successor
+    if ((*i)->succNode.size() > 1)
+      return false;
+
+    if (edge2ClassMap[std::make_pair(entryNode, *i)] == BACK)
+      return false;
+
+    NodeTy *succNode = get_first_node_or_null((*i)->succNode);
+
+    if (succNode) {
+      if (succNode == nullptr)
+        continue;
+
+      // The successor of the successor node should be the the another
+      // successor node of node
+      if (!entryNode->has_successor(succNode))
+        return false;
+      // There is no loop between successors
+      else if (succNode != *exitNode && succNode->has_successor(*i))
+        return false;
+    }
+
+    // If successor has only one predessor, it has to be the entry node
+    if ((*i)->predNode.size() == 1) {
+      if (entryNode != get_first_node_or_null((*i)->predNode))
+        return false;
+    }
+    // If successor has two predessors, one has to be the entry node
+    // and the other has to be another successor node
+    else if ((*i)->predNode.size() == 2) {
+      auto pi = (*i)->predNode.begin();
+      NodeTy *predNode1 = *pi;
+      ++pi;
+      NodeTy *predNode2 = *pi;
+
+      if (predNode1 != entryNode || !entryNode->has_successor(predNode2))
+        if (!entryNode->has_successor(predNode1) || predNode2 != entryNode)
+          return false;
+
+      if (predNode1 == *exitNode)
+        return false;
+
+      if (predNode2 == *exitNode)
+        return false;
+    }
+    // The predecessor node number has to be less than 3
+    else {
+      int insideIncomingNum = 0;
+
+      for (auto pi = (*i)->predNode.begin(), pe = (*i)->predNode.end();
+           pi != pe; ++pi) {
+
+        if (edge2ClassMap[std::make_pair(*pi, *i)] != BACK &&
+            (*i) != *exitNode && (*pi) != entryNode && (*pi) != *exitNode) {
+          if (!entryNode->has_successor(*pi))
+            improperEdgeSet.insert(std::make_pair(*pi, *i));
+          else {
+            insideIncomingNum++;
+
+            if (insideIncomingNum > 1)
+              improperEdgeSet.insert(std::make_pair(*pi, *i));
+          }
+        } else
+          return false;
+      }
+    }
+  }
+
+  for (auto i = entryNode->succNode.begin(), e = entryNode->succNode.end();
+       i != e; ++i) {
+    if ((*i)->succNode.empty()) {
+      visitPath.clear();
+
+      if (path(*exitNode, entryNode, Nodes, nullptr))
+        return false;
+    }
+  }
+
+  bool improperFlag = false;
+
+  for (EdgeSetTy::iterator i = improperEdgeSet.begin(),
+                           e = improperEdgeSet.end();
+       i != e; ++i) {
+    if (isStillReachableFromEntry(Nodes, entry, i->second, i->first)) {
+      findUnstructuredBR(i->first, i->second, true, true);
+      improperFlag = true;
+    }
+  }
+
+  return improperFlag;
+}
+
+// cyclicRegionType - Follow Fig 7.42 of Muchnick book
+StructuralAnalysis::RegionTy StructuralAnalysis::cyclicRegionType(
+    const NodeVecTy &Nodes, NodeVecTy &nset, NodeTy *loopHeaderNode,
+    NodeTy *backEdgeNode, NodeTy **exitNode, NodeTy *entry) {
+  // Check for a SelfLoop
+  if (nset.size() == 1) {
+    if (loopHeaderNode == backEdgeNode) {
+      *exitNode = get_first_node_or_null(backEdgeNode->succNode);
+
+      return SelfLoop;
+    } else
+      return Nil;
+  }
+
+  if (isImproper(Nodes, nset, loopHeaderNode, backEdgeNode, exitNode, entry))
+    // It is an Improper region
+    return Improper;
+
+  if (nset.size() == 2) {
+    if (backEdgeNode->succNode.size() == 1) {
+      for (NodeTy *pred : loopHeaderNode->predNode) {
+        if (pred != backEdgeNode) {
+          if (edge2ClassMap[std::make_pair(pred, loopHeaderNode)] == BACK)
+            return Nil;
+        }
+      }
+
+      for (NodeTy *succ : loopHeaderNode->succNode) {
+        if (succ != backEdgeNode) {
+          if (edge2ClassMap[std::make_pair(loopHeaderNode, succ)] == BACK)
+            return Nil;
+        }
+      }
+
+      if (backEdgeNode->predNode.size() != 1 ||
+          backEdgeNode->succNode.size() != 1)
+        return Nil;
+
+      return NaturalLoop;
+    } else if (backEdgeNode->succNode.size() > 1) {
+      for (NodeTy *pred : loopHeaderNode->predNode) {
+        if (pred != backEdgeNode) {
+          if (edge2ClassMap[std::make_pair(pred, loopHeaderNode)] == BACK)
+            return Nil;
+        }
+      }
+
+      for (NodeTy *succ : loopHeaderNode->succNode) {
+        if (succ != backEdgeNode) {
+          if (edge2ClassMap[std::make_pair(loopHeaderNode, succ)] == BACK)
+            return Nil;
+        }
+      }
+
+      if (backEdgeNode->predNode.size() != 1)
+        return Nil;
+
+      return NaturalLoop;
+    }
+  }
+
+  return Nil;
+}
+
+// reduce - Follow Fig 7.43 of Muchnick book
+StructuralAnalysis::NodeTy *
+StructuralAnalysis::reduce(NodeVecTy &N, RegionTy rType, NodeVecTy &nodeSet,
+                           NodeTy *entryNode, NodeTy *exitNode) {
+  NodeTy *node = new NodeTy();
+
+  node->isCombined = true;
+
+  if (entryNode) {
+    node->entryNode = entryNode;
+    node->entryBB = findEntryBB(entryNode);
+  }
+
+  replace(N, node, nodeSet /*, addSelfEdge*/);
+
+  node->isLoopHeader = false;
+  node->loopExitNode = nullptr;
+  node->isBackEdge = false;
+  node->parentNode = nullptr;
+
+  if (exitNode)
+    node->exitBB = findEntryBB(exitNode);
+  else
+    node->exitBB = nullptr;
+
+  for (auto &i : nodeSet) {
+    findBB(i, node->containedBB);
+  }
+
+  node->nodeType = rType;
+
+  return node;
+}
+
+// replace - Follow Fig 7.44 of Muchnick book
+void StructuralAnalysis::replace(NodeVecTy &N, NodeTy *node,
+                                 NodeVecTy &nodeSet /*, bool addSelfEdge*/) {
+  // Link region node into abstract flowgraph, adjust the postorder traversal
+  // and predecessor and successor functions, and augment the control tree
+  compact(N, node, nodeSet /*, addSelfEdge*/);
+
+  for (auto i = nodeSet.begin(), e = nodeSet.end(); i != e; ++i) {
+    insertNode(node->childNode, *i);
+    (*i)->parentNode = node;
+  }
+}
+
+// isImproper - Follow Fig 7.45 of Muchnick book
+bool StructuralAnalysis::isImproper(const NodeVecTy &Nodes, NodeVecTy &nset,
+                                    NodeTy *loopHeaderNode,
+                                    NodeTy *backEdgeNode, NodeTy **exitNode,
+                                    NodeTy *entry) {
+  bool improperFlag = false;
+
+  // Check loopHeaderNode first
+  for (auto i = loopHeaderNode->predNode.begin(),
+            e = loopHeaderNode->predNode.end();
+       i != e; ++i) {
+    NodeTy *predNode = *i;
+
+    if (edge2ClassMap[std::make_pair(predNode, loopHeaderNode)] == BACK) {
+      if (!containsNode(nset, predNode) &&
+          isStillReachableFromEntry(Nodes, entry, loopHeaderNode, predNode)) {
+        findUnstructuredBR(predNode, loopHeaderNode, true, true);
+        improperFlag = true;
+      } else if (containsNode(nset, predNode) && predNode != backEdgeNode) {
+        findUnstructuredBR(predNode, loopHeaderNode, false, false);
+        improperFlag = true;
+      }
+    }
+  }
+
+  // Check the incoming edges
+  for (auto i = nset.begin(), e = nset.end(); i != e; ++i) {
+    NodeTy *node = *i;
+
+    if (node != loopHeaderNode)
+      for (auto ii = node->predNode.begin(), ee = node->predNode.end();
+           ii != ee; ++ii) {
+        if (!containsNode(nset, *ii)
+            /*&& isStillReachableFromEntry(N, entry, node, *ii)*/) {
+          improperFlag = true;
+
+          findUnstructuredBR(*ii, node, false, true);
+          deleteUnreachableNodes(const_cast<NodeVecTy&>(Nodes), entry);
+        }
+      }
+  }
+
+  EdgeSetTy exitEdgeSet;
+  NodeTy *exitNodeOfHeader = nullptr;
+  NodeTy *exitNodeOfBackEdge = nullptr;
+  NodeTy *mainExitNode = nullptr;
+
+  for (auto i = nset.begin(), e = nset.end(); i != e; ++i) {
+    NodeTy *node = *i;
+
+    for (auto ii = node->succNode.begin(), ee = node->succNode.end(); ii != ee;
+         ++ii) {
+      if (!containsNode(nset, *ii)) {
+        exitEdgeSet.insert(std::make_pair(node, *ii));
+
+        if (node == loopHeaderNode) {
+          if (exitNodeOfHeader == nullptr)
+            exitNodeOfHeader = *ii;
+        } else if (node == backEdgeNode) {
+          if (exitNodeOfBackEdge == nullptr)
+            exitNodeOfBackEdge = *ii;
+        }
+      }
+    }
+  }
+
+  if (exitNodeOfHeader)
+    mainExitNode = exitNodeOfHeader;
+  else if (exitNodeOfBackEdge)
+    mainExitNode = exitNodeOfBackEdge;
+
+  for (EdgeSetTy::iterator i = exitEdgeSet.begin(), e = exitEdgeSet.end();
+       i != e; ++i) {
+    EdgeTy exitEdge = *i;
+
+    if (exitEdge.second != mainExitNode) {
+      findUnstructuredBR(exitEdge.first, exitEdge.second, false, true);
+      deleteUnreachableNodes(const_cast<NodeVecTy&>(Nodes), entry);
+      improperFlag = true;
+    }
+  }
+
+  if (exitNodeOfHeader) {
+    for (EdgeSetTy::iterator i = exitEdgeSet.begin(), e = exitEdgeSet.end();
+         i != e; ++i) {
+      if (i->first != loopHeaderNode && (*i).second == mainExitNode) {
+        findUnstructuredBR(i->first, i->second, false, false);
+        improperFlag = true;
+      }
+    }
+  } else if (exitNodeOfBackEdge) {
+    for (EdgeSetTy::iterator i = exitEdgeSet.begin(), e = exitEdgeSet.end();
+         i != e; ++i) {
+      if (i->first != backEdgeNode && i->second == mainExitNode) {
+        findUnstructuredBR(i->first, i->second, false, false);
+        improperFlag = true;
+      }
+    }
+  }
+
+  *exitNode = mainExitNode;
+  loopHeaderNode->isLoopHeader = true;
+  backEdgeNode->isBackEdge = true;
+  loopHeaderNode->loopExitNode = mainExitNode;
+
+  return improperFlag;
+}
+
+// pathBack - Check if there is a node k such that there is a path from
+// m to k that does not pass through n and an edge k->n that is a back edge
+StructuralAnalysis::NodeTy *
+StructuralAnalysis::pathBack(NodeTy *n, NodeVecTy &N, NodeVecTy &reachUnder) {
+  NodeTy *backEdgeNode = nullptr;
+
+  reachUnder.clear();
+
+  // Find backedge first
+  for (NodeTy *predNode : n->predNode) {
+    if (edge2ClassMap[std::make_pair(predNode, n)] == BACK) {
+      if (!containsNode(reachUnder, predNode)) {
+        backEdgeNode = predNode;
+
+        // Locate a cyclic region, if present
+        reachUnder.clear();
+        insertNode(reachUnder, n);
+        insertNode(reachUnder, backEdgeNode);
+
+        for (NodeTy *m : N) {
+          // Check if there is a path from m to loop exit node
+          visitPath.clear();
+          if (path(m, backEdgeNode, N, n)) {
+            visitPath.clear();
+
+            if (path(n, m, N, backEdgeNode)) {
+              insertNode(reachUnder, m);
+            }
+          }
+        }
+      }
+    }
+  }
+
+  return backEdgeNode;
+}
+
+// path(n, m, I) - Return true if there is a path from from n to m
+// such that all the nodes in it are in I and false otherwise
+bool StructuralAnalysis::path(NodeTy *n, NodeTy *m, const NodeVecTy &Nodes,
+                              NodeTy *esc) {
+  visitPath.emplace(n);
+
+  if (n == esc || m == esc)
+    return false;
+
+  if (n == m)
+    return true;
+
+  for (auto i = n->succNode.begin(), e = n->succNode.end(); i != e; ++i) {
+    if (containsNode(Nodes, *i) && *i != esc && visitPath.count(*i) == 0) {
+      if (*i == m) {
+        return true;
+      } else if (path(*i, m, Nodes, esc)) {
+        return true;
+      }
+    }
+  }
+
+  return false;
+}
+
+// path(n, m, I, src, dst) - Return true if there is a path from from n to m
+// such that all the nodes in it are in I without going through edge src->dst
+// and false otherwise
+bool StructuralAnalysis::path(NodeTy *n, NodeTy *m, const NodeVecTy &Nodes,
+                              NodeTy *src, NodeTy *dst) {
+  visitPath.emplace(n);
+
+  if (n == m)
+    return true;
+
+  for (auto i = n->succNode.begin(), e = n->succNode.end(); i != e; ++i)
+    if (containsNode(Nodes, *i) && visitPath.count(*i) == 0) {
+      if (*i == dst && n == src)
+        continue;
+
+      if (*i == m)
+        return true;
+      else if (path(*i, m, Nodes, src, dst))
+        return true;
+    }
+
+  return false;
+}
+
+// compact - Compact nodes in nset into n;
+void StructuralAnalysis::compact(NodeVecTy &N, NodeTy *n,
+                                 NodeVecTy &nset /*, bool addSelfEdge*/) {
+  // Adds node n to N
+  insertNode(N, n);
+
+  // Remove the nodes in nset from both N and post()
+  for (auto nset_node : nset) {
+    for (auto succ : nset_node->succNode) {
+      if (!containsNode(nset, succ)) {
+        n->add_successor(succ);
+        succ->add_predecessor(n);
+        succ->remove_predecessor(nset_node);
+      }
+    }
+
+    for (auto pred : nset_node->predNode) {
+      if (!containsNode(nset, pred)) {
+        n->add_predecessor(pred);
+        pred->add_successor(n);
+        pred->remove_successor(nset_node);
+      }
+    }
+
+    eraseNode(N, nset_node);
+  }
+}
+
+// mapNode2BB - Return the corresponding BasicBlock* of the node
+BasicBlock *StructuralAnalysis::mapNode2BB(const NodeTy *node) const {
+  const NodeTy *tmpNode = node;
+  while (tmpNode->isCombined) {
+    tmpNode = tmpNode->entryNode;
+  }
+  return tmpNode->BB;
+}
+
+// mapBB2Node - Return the corresponding sturcture node of the basic block
+StructuralAnalysis::NodeTy *StructuralAnalysis::mapBB2Node(BasicBlock *bb) {
+  NodeTy *node, *tmpNode;
+
+  node = BB2NodeMap[bb];
+
+  while ((tmpNode = node->parentNode) != nullptr)
+    node = tmpNode;
+
+  return node;
+}
+
+// dumpCTNode - dump one CT node
+void StructuralAnalysis::dumpCTNode(llvm::raw_ostream &stream,
+                                    NodeTy *node) const {
+  if (!node->isCombined)
+    return;
+
+  stream << "\t";
+
+  switch (node->nodeType) {
+  case Block:
+    stream << "Block      ";
+    break;
+  case IfThen:
+    stream << "IfThen     ";
+    break;
+  case IfThenElse:
+    stream << "IfThenElse ";
+    break;
+  case Case:
+    stream << "Case       ";
+    break;
+  case SelfLoop:
+    stream << "SelfLoop   ";
+    break;
+  case NaturalLoop:
+    stream << "NaturalLoop";
+    break;
+  default:
+    break;
+  }
+
+  stream << "\t";
+
+  dumpNode(stream, node);
+  stream << " -> exit: "
+         << (node->exitBB != nullptr ? node->exitBB->getName().str() : "null");
+
+  stream << '\n';
+
+  for (auto i = node->childNode.begin(), e = node->childNode.end(); i != e;
+       ++i) {
+    dumpCTNode(stream, *i);
+  }
+}
+
+// dumpNode - dump one node
+void StructuralAnalysis::dumpNode(llvm::raw_ostream &stream,
+                                  NodeTy *node) const {
+  BBVecTy BBVec;
+
+  BasicBlock *this_block = mapNode2BB(node);
+  stream << "[" << this_block->getName().str() << "]\t";
+
+  findBB(node, BBVec);
+  for (BasicBlock *BB : BBVec) {
+    if (BB == this_block)
+      continue;
+    stream << BB->getName().str() << "\t";
+  }
+}
+
+// findUnstructuredBR - Record the branch and remove it from CFG
+void StructuralAnalysis::findUnstructuredBR(NodeTy *srcNode, NodeTy *dstNode,
+                                            bool needForwardCopy, bool isGoto) {
+  BBVecTy srcNodeVec, dstNodeVec;
+  findBB(srcNode, srcNodeVec);
+  findBB(dstNode, dstNodeVec);
+
+  for (BasicBlock *srcBB : srcNodeVec) {
+    for (BasicBlock *succ : successors(srcBB)) {
+      for (BasicBlock *dstBB : dstNodeVec) {
+        if (dstBB == succ) {
+          if (isGoto) {
+            if (checkUnique(unstructuredBRVec, srcBB, dstBB)) {
+              unstructuredBRVec.push_back(std::make_pair(srcBB, dstBB));
+            }
+          }
+
+          if (needForwardCopy) {
+            if (checkUnique(dstNode->incomingForwardBR, srcBB, dstBB)) {
+              dstNode->incomingForwardBR.push_back(
+                  std::make_pair(srcBB, dstBB));
+            }
+          }
+        }
+      }
+    }
+  }
+
+  srcNode->remove_successor(dstNode);
+  dstNode->remove_predecessor(srcNode);
+
+  if (edge2ClassMap[std::make_pair(srcNode, dstNode)] == BACK) {
+    dstNode->isLoopHeader = false;
+    dstNode->loopExitNode = nullptr;
+  }
+}
+
+// findBB - put all Basic Blocks in node into nodeVec
+void StructuralAnalysis::findBB(NodeTy *node, BBVecTy &nodeVec) const {
+  if (!node->isCombined) {
+    if (find(nodeVec.cbegin(), nodeVec.cend(), node->BB) != nodeVec.cend())
+      return;
+    nodeVec.emplace_back(node->BB);
+  } else {
+    for (NodeTy *child : node->childNode)
+      findBB(child, nodeVec);
+  }
+}
+
+// dumpUnstructuredBR - Dump all found unstructured branches
+void StructuralAnalysis::dumpUnstructuredBR(llvm::raw_ostream &stream) const {
+  stream << "\nUnstructured Branches:\n";
+
+  for (EdgeVecTy::const_iterator i = unstructuredBRVec.begin(),
+                                 e = unstructuredBRVec.end();
+       i != e; ++i) {
+    stream << "\t" << i->first->getName().str() << "\t"
+           << i->second->getName().str() << "\n";
+  }
+
+  stream << "\n";
+}
+
+void StructuralAnalysis::dumpForwardCopy(llvm::raw_ostream &stream) const {
+  stream << "\nneed forward copy:\n";
+
+  const std::function<void(const NodeTy *)> node_iter =
+      [&node_iter, &stream](const NodeTy *node) {
+        for (const auto &edge : node->incomingForwardBR) {
+          stream << "\t" << edge.first->getName().str() << "\t"
+                 << edge.second->getName().str() << "\n";
+        }
+
+        for (const auto &child : node->childNode) {
+          node_iter(child);
+        }
+      };
+
+  if (!Net.empty()) {
+    node_iter(Net.front());
+  }
+
+  stream << "\n";
+}
+
+// True if after erasing edge src->dst, dst is still reachable from entry
+bool StructuralAnalysis::isStillReachableFromEntry(const NodeVecTy &Nodes,
+                                                   NodeTy *entry,
+                                                   NodeTy *dstNode,
+                                                   NodeTy *srcNode) {
+  visitPath.clear();
+
+  return path(entry, dstNode, Nodes, srcNode, dstNode);
+}
+
+// findEntryBB - find the entry Basic Block of the node
+BasicBlock *StructuralAnalysis::findEntryBB(NodeTy *node) {
+  if (!node->isCombined)
+    return node->BB;
+  else
+    return findEntryBB(node->entryNode);
+}
+
+void StructuralAnalysis::cleanupUnreachable() {
+  for (auto i = unreachableNodeSet.begin(), e = unreachableNodeSet.end();
+       i != e; ++i) {
+    cleanup(*i);
+  }
+}
+
+// clean - fill in the element of incoming branches and outgoing branches
+void StructuralAnalysis::cleanup(NodeTy *node) {
+  if (!node->isCombined)
+    return;
+  else {
+    if ((node->nodeType == NaturalLoop || node->nodeType == SelfLoop) &&
+        node->containedBB.size() > 1) {
+      for (auto i = node->containedBB.begin(), e = node->containedBB.end();
+           i != e; ++i) {
+        BasicBlock *BB = *i;
+
+        if (BB != node->entryBB) {
+          for (BasicBlock *Pred : predecessors(BB)) {
+            if (!containsBB(node->containedBB, Pred)) {
+              node->incomingBR.push_back(std::make_pair(Pred, BB));
+            }
+          }
+
+          for (BasicBlock *Succ : successors(BB)) {
+            if (!containsBB(node->containedBB, Succ) && Succ != node->exitBB) {
+              node->outgoingBR.push_back(std::make_pair(BB, Succ));
+            }
+          }
+        }
+      }
+    }
+
+    NodeVecTy nodeSet = node->childNode;
+
+    for (auto i = nodeSet.begin(), e = nodeSet.end(); i != e; ++i) {
+      cleanup(*i);
+    }
+  }
+}
+
+// deleteUnreachableNode - delete nodes that are no longer reachable from the
+// entry
+void StructuralAnalysis::deleteUnreachableNodes(NodeVecTy &Nodes,
+                                                NodeTy *entry) {
+  for (auto iter = Nodes.begin(); iter != Nodes.end();) {
+    visitPath.clear();
+    NodeTy *node = *iter;
+
+    if (!path(entry, node, Nodes, nullptr)) {
+      for (auto pi = node->predNode.begin(), pe = node->predNode.end();
+           pi != pe; ++pi) {
+        (*pi)->remove_successor(node);
+      }
+
+      for (auto si = node->succNode.begin(), se = node->succNode.end();
+           si != se; ++si) {
+        (*si)->remove_predecessor(node);
+      }
+
+      insertNode(unreachableNodeSet, node);
+
+      iter = Nodes.erase(iter);
+    } else
+      ++iter;
+  }
+}
+
+void StructuralAnalysis::reconstructUnreachable() {
+BEGIN:
+  bool merge = false;
+
+  for (auto i = unreachableNodeSet.begin(), e = unreachableNodeSet.end();
+       i != e; ++i) {
+    NodeTy *node1 = *i;
+
+    for (auto ii = unreachableNodeSet.begin(), ee = unreachableNodeSet.end();
+         ii != ee; ++ii) {
+      NodeTy *node2 = *i;
+
+      if (node1 == node2)
+        continue;
+
+      for (auto pi = node1->predNode.begin(), pe = node1->predNode.end();
+           pi != pe; ++pi) {
+        NodeTy *pred = *pi;
+
+        if ((pred->isCombined &&
+             containsBB(node2->containedBB, pred->entryBB)) ||
+            (!pred->isCombined && containsBB(node2->containedBB, pred->BB))) {
+          pred->add_successor(node1);
+          merge = true;
+        }
+      }
+
+      for (auto si = node1->succNode.begin(), se = node1->succNode.end();
+           si != se; ++si) {
+        NodeTy *succ = *si;
+
+        if ((succ->isCombined &&
+             containsBB(node2->containedBB, succ->entryBB)) ||
+            (!succ->isCombined && containsBB(node2->containedBB, succ->BB))) {
+          succ->add_predecessor(node1);
+          merge = true;
+        }
+      }
+
+      if (merge) {
+        NodeVecTy nodeSet;
+
+        insertNode(nodeSet, node1);
+        insertNode(nodeSet, node2);
+
+        reduce(unreachableNodeSet, Unreachable, nodeSet, nullptr, nullptr);
+
+        goto BEGIN;
+      }
+    }
+  }
+}
+
+StructuralAnalysis::~StructuralAnalysis() {
+  for (auto n = Net.begin(); n != Net.end(); ++n) {
+    delete *n;
+  }
+}
+
+void StructuralAnalysis::analyze(Function &F) {
+  _function = &F;
+
+  // build a Simple CFG out of the LLVM CFG
+  buildSimpleCFG(Net);
+
+  NodeTy *entry = BB2NodeMap[&_function->getEntryBlock()];
+
+  // Follow the Fig 7.39 of Muchnick book
+  structuralAnalysis(Net, entry);
+
+  if (!Net.empty()) {
+    cleanup(*(Net.begin()));
+  }
+
+  reconstructUnreachable();
+
+  cleanupUnreachable();
+}
+
+void StructuralAnalysis::write(llvm::raw_ostream &stream) const {
+  stream << _function->getName().str() << ":\n";
+
+  if (!Net.empty()) {
+    dumpCTNode(stream, *(Net.begin()));
+  }
+
+  dumpUnstructuredBR(stream);
+  dumpForwardCopy(stream);
+}
+
+bool StructuralAnalysis::checkUnique(EdgeVecTy &edgeVec, BasicBlock *srcBB,
+                                     BasicBlock *dstBB) {
+  for (EdgeVecTy::iterator i = edgeVec.begin(), e = edgeVec.end(); i != e;
+       ++i) {
+
+    if (i->first == srcBB && i->second == dstBB)
+      return false;
+  }
+
+  return true;
+}
+
+bool StructuralAnalysis::containsNode(const NodeVecTy &Nodes,
+                                      const NodeTy *node) {
+  return find(cbegin(Nodes), cend(Nodes), node) != cend(Nodes);
+}
+
+bool StructuralAnalysis::containsBB(const BBVecTy &BBs, const BasicBlock *BB) {
+  return find(cbegin(BBs), cend(BBs), BB) != cend(BBs);
+}
+
+bool StructuralAnalysis::insertNode(NodeVecTy &Nodes, NodeTy *node) {
+  if (containsNode(Nodes, node)) {
+    return false;
+  }
+  Nodes.emplace_back(node);
+  return true;
+}
+
+bool StructuralAnalysis::eraseNode(NodeVecTy &Nodes, NodeTy *node) {
+  const auto iter = find(cbegin(Nodes), cend(Nodes), node);
+  if (iter == cend(Nodes)) {
+    return false;
+  }
+  Nodes.erase(iter);
+  return true;
+}
+}
diff --git a/llvm/lib/Transforms/LibFloor/StructuralTransform.cpp b/llvm/lib/Transforms/LibFloor/StructuralTransform.cpp
new file mode 100644
index 000000000000..ee4af6a5c663
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/StructuralTransform.cpp
@@ -0,0 +1,676 @@
+//===- StructuralTransform.cpp - ------------------------------------------===//
+//
+// Copyright (c) 2015, Computer Architecture and Systems Laboratory at Georgia Tech
+// Copyright (c) 2016 - 2017, Florian Ziesche (LLVM port + general fixes/cleanup)
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+// * Redistributions of source code must retain the above copyright notice, this
+//   list of conditions and the following disclaimer.
+//
+// * Redistributions in binary form must reproduce the above copyright notice,
+//   this list of conditions and the following disclaimer in the documentation
+//   and/or other materials provided with the distribution.
+//
+// * Neither the name of gpuocelot nor the names of its
+//   contributors may be used to endorse or promote products derived from
+//   this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE
+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+//===----------------------------------------------------------------------===//
+// \author  Haicheng Wu <hwu36@gatech.edu>
+// \date    Monday April 4, 2011
+// \brief   The source file for the StructuralTransform pass.
+//===----------------------------------------------------------------------===//
+//
+// This file implements an Structural Transform based on Zhang's paper
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor/StructuralTransform.h"
+#include "llvm/Transforms/Utils/BasicBlockUtils.h"
+#include "llvm/Transforms/Utils/LoopUtils.h"
+#include "llvm/IR/Verifier.h"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+#define report(x) DBG(errs() << x << "\n"; errs().flush();)
+
+namespace llvm {
+
+typedef StructuralAnalysis SA;
+
+// Algorithm 2 of Zhang's paper -- elimination of outgoing branches
+bool StructuralTransform::Cut(NodeTy *N) {
+  report("Applying cut transform");
+  bool change = false;
+
+  for (auto *child : N->childNode) {
+    change |= Cut(child);
+  }
+
+  if (!stopCut) {
+    if (N->isCombined &&
+        (N->nodeType == SA::NaturalLoop || N->nodeType == SA::SelfLoop) &&
+        N->containedBB.size() > 1 && !N->outgoingBR.empty()) {
+      change = true;
+
+      BasicBlock *TopExitBB = N->exitBB;
+
+      for (auto &edge : N->outgoingBR) {
+        // 1. Before loop, insert fp = false
+        auto fp = new AllocaInst(condition_type, 0, nullptr, "fp", alloca_insert);
+
+        // TODO: "before loop" doesn't equal "in the entry block", or does it?
+        // what if it is contained by another loop?
+        new StoreInst(ConstantInt::get(condition_type, 0), fp,
+                      condition_init_insert);
+
+        // 2. replace branch to targe t by if (B) then {fp = true; exit}
+        new StoreInst(ConstantInt::get(condition_type, 1), fp,
+                      edge.first->getFirstNonPHI());
+
+        for (BasicBlock *succ : successors(edge.first)) {
+          if (!SA::containsBB(N->containedBB, succ)) {
+            continue;
+          }
+
+          new StoreInst(ConstantInt::get(condition_type, 0), fp,
+                        succ->getFirstNonPHI());
+        }
+
+        // 3. After loop, insert if (fp) goto t
+        BasicBlock *NewCmpBB = BasicBlock::Create(
+            _function->getContext(), "NewCmpBB", _function, nullptr);
+
+        // ValueMap
+        BBMapper ValueMap;
+        ValueMap[edge.second] = NewCmpBB;
+
+        if (TopExitBB != nullptr) {
+          ValueMap[TopExitBB] = NewCmpBB;
+        }
+
+        // For each BasicBlock
+        // Remap Values here for Instructions
+        for (BasicBlock *BB : N->containedBB) {
+          // TODO: handle switches
+          assert(!isa<ReturnInst>(BB->getTerminator()) &&
+                 "terminator can not be a return instruction");
+          BranchInst *branch = dyn_cast<BranchInst>(BB->getTerminator());
+          assert(branch != nullptr &&
+                 "terminator must be a branch instruction");
+
+          if (branch->isUnconditional()) {
+            auto it = ValueMap.find(branch->getSuccessor(0));
+
+            // found in ValueMap
+            if (it != ValueMap.end()) {
+              DBG(errs() << "changing successor of " << BB->getName()
+                         << " from " << branch->getSuccessor(0)->getName()
+                         << " to " << it->second->getName() << "\n";)
+              branch->setSuccessor(0, it->second);
+            }
+          } else if (branch->isConditional()) {
+            auto it1 = ValueMap.find(branch->getSuccessor(0));
+            auto it2 = ValueMap.find(branch->getSuccessor(1));
+
+            const bool found1 = (it1 != ValueMap.end());
+            const bool found2 = (it2 != ValueMap.end());
+
+            // both found in ValueMap
+            if (found1 && found2) {
+              branch->eraseFromParent();
+              BranchInst::Create(it1->second, BB);
+            }
+            // edge 1 is found in ValueMap & edge 2 is not
+            else if (found1 && !found2) {
+              branch->setSuccessor(0, it1->second);
+            }
+            // edge 2 is found in ValueMap & edge 1 is not
+            else if (!found1 && found2) {
+              branch->setSuccessor(1, it2->second);
+            }
+          }
+        }
+
+        if (TopExitBB != nullptr) {
+          const auto insert_point = NewCmpBB->getFirstNonPHI();
+          auto rhs = ConstantInt::get(condition_type, 1);
+          Value *cmp = nullptr;
+          if (insert_point != nullptr) {
+            auto lhs = new LoadInst(condition_type, fp, "fp_load", false, insert_point);
+            cmp = new ICmpInst(insert_point, CmpInst::ICMP_EQ, lhs, rhs,
+                               "fp_cmp");
+          } else {
+            // no instructions in bb yet -> insert at BB end
+            auto lhs = new LoadInst(condition_type, fp, "fp_load", false, NewCmpBB);
+            cmp = new ICmpInst(*NewCmpBB, CmpInst::ICMP_EQ, lhs, rhs, "fp_cmp");
+          }
+          BranchInst::Create(edge.second, TopExitBB, cmp, NewCmpBB);
+        } else {
+          BranchInst::Create(edge.second, NewCmpBB);
+        }
+
+        TopExitBB = NewCmpBB;
+      }
+
+      if (N->exitBB == nullptr) {
+        // TODO: can this be an unconditional? also: handle switch?
+        BranchInst *branch = dyn_cast<BranchInst>(N->entryBB->getTerminator());
+        assert(branch != nullptr && branch->isUnconditional() &&
+               "invalid branch in entryBB");
+        branch->setSuccessor(0, TopExitBB);
+      }
+
+      stopCut = true;
+      return change;
+    }
+  }
+
+  return change;
+}
+
+// Algorithm 3 of Zhang's paper -- elimination of backward branches
+bool StructuralTransform::BackwardCopy(NodeTy *N) {
+  // report("Applying backward copy");
+  assert(false && "BackwardCopy not implemented yet");
+  // TODO: !
+  return false;
+}
+
+// Algorithm 4 of Zhang's paper -- elimination of Forward branches
+bool StructuralTransform::ForwardCopy(NodeTy *N, uint32_t level) {
+  DBG(errs() << "Applying forward copy: " << std::string(level, ' ')
+             << SA.findEntryBB(N)->getName() << "\n";)
+  for (auto *child : N->childNode) {
+    // we can only safely apply one forward copy, so abort if this has happened
+    if (ForwardCopy(child, level + 1)) {
+      return true;
+    }
+  }
+
+  if (!N->incomingForwardBR.empty()) {
+    DBG(errs() << ">> @" << SA.findEntryBB(N)->getName() << ": needs forward copy\n";)
+    for (auto &edge : N->incomingForwardBR) {
+      BBMapper BBMap; // local
+
+      // Clone BasicBlocks to the new function
+      for (BasicBlock *BB : N->containedBB) {
+        auto vmap = std::make_unique<ValueToValueMapTy>();
+        BasicBlock *ClonedBB = CloneBasicBlock(BB, *vmap.get(), ".scfg.cloned",
+                                               _function, nullptr);
+
+        // NOTE: instructions will be remapped later
+
+        const auto vmap_iter = ClonedVMap.find(ClonedBB);
+        if (vmap_iter == ClonedVMap.end()) {
+          ClonedVMap[ClonedBB] = std::move(vmap);
+        } else {
+          // can this ever occur?
+          errs() << "\033[31m MARK: did occur \033[m\n";
+          auto insert_vmap_ptr = vmap_iter->second.get();
+          for (const auto &elem : *vmap.get()) {
+            insert_vmap_ptr->insert(elem);
+          }
+        }
+        BBMap[BB] = ClonedBB;
+      }
+
+      // update global map
+      for (auto &bb_entry : BBMap) {
+        ClonedBBMap[bb_entry.first] = bb_entry.second;
+      }
+
+      // general value remapping function (handling remappings across bbs)
+      const auto map_value = [this, &BBMap](
+          Value *value, const ValueToValueMapTy *direct_vmap) -> Value * {
+        // not mapping BBs, constants or function arguments
+        if (isa<BasicBlock>(value) || isa<Constant>(value) ||
+            isa<Argument>(value)) {
+          return value;
+        }
+
+        const auto direct_iter = direct_vmap->find(value);
+        // direct BB mapping, i.e. instruction is defined in this BB
+        if (direct_iter != direct_vmap->end()) {
+          DBG(errs() << "\033[32m direct mapping: " << *value << " -> "
+                     << *direct_iter->second << " \033[m\n";)
+          return direct_iter->second;
+        }
+        // indirect BB mapping, i.e. instruction is defined in another cloned BB
+        else if (Instruction *instr = dyn_cast<Instruction>(value)) {
+          const auto orig_par_BB = instr->getParent();
+          const auto orig_par_BB_iter = BBMap.find(orig_par_BB);
+          if (orig_par_BB_iter != BBMap.end()) {
+            // map it to the cloned BB
+            const auto par_vmap_iter =
+                ClonedVMap.find(orig_par_BB_iter->second);
+            if (par_vmap_iter != ClonedVMap.end()) {
+              const auto mapped_iter = par_vmap_iter->second->find(instr);
+              if (mapped_iter != par_vmap_iter->second->end()) {
+                // found it
+                DBG(errs() << "\033[32m mapping instr from other BB: " << *instr
+                           << " -> " << *mapped_iter->second << " \033[m\n";)
+                return mapped_iter->second;
+              }
+            }
+          }
+        }
+        DBG(errs() << "\033[31m can't map: " << *value << "\033[m\n";)
+        return value;
+      };
+
+      // replace instruction uses of instructions that were cloned in the cloned
+      // block (CloneBasicBlock will only clone the "lhs", not the uses/operands
+      // inside instructions)
+      // note that we also need to replace/map instructions that come from other
+      // BBs that were also cloned
+      // -> need to do this after cloning everything and having created the
+      // value/bb maps for everything
+      for (BasicBlock *BB : N->containedBB) {
+        const auto cloned_bb_iter = BBMap.find(BB);
+        if (cloned_bb_iter == BBMap.end()) {
+          assert(false && "failed to find cloned BB");
+        }
+        auto ClonedBB = cloned_bb_iter->second;
+
+        const auto vmap_iter = ClonedVMap.find(ClonedBB);
+        if (vmap_iter == ClonedVMap.end()) {
+          assert(false && "failed to find vmap for cloned BB");
+        }
+        const auto vmap = vmap_iter->second.get();
+
+        for (Instruction &instr : *ClonedBB) {
+          for (uint32_t i = 0, count = instr.getNumOperands(); i < count; ++i) {
+            instr.setOperand(i, map_value(instr.getOperand(i), vmap));
+          }
+        }
+      }
+
+      // For each BasicBlock
+      // Remap Values here for Instructions
+      for (auto &bb_entry : BBMap) {
+        // nothing to do here
+        if (isa<ReturnInst>(bb_entry.first->getTerminator())) {
+          continue;
+        }
+
+        BranchInst *branch =
+            dyn_cast<BranchInst>(bb_entry.first->getTerminator());
+        assert(branch != nullptr && "terminator must be a branch instruction");
+        BranchInst *sec_branch =
+            dyn_cast<BranchInst>(bb_entry.second->getTerminator());
+        assert(sec_branch != nullptr && "invalid branch");
+
+        if (branch->isUnconditional()) {
+          auto it = BBMap.find(branch->getSuccessor(0));
+
+          BasicBlock *new_sec_branch_target = nullptr;
+          // found in BBMap
+          if (it != BBMap.end()) {
+            new_sec_branch_target = it->second;
+          }
+          // not found in BBMap
+          else {
+            new_sec_branch_target = branch->getSuccessor(0);
+          }
+
+          // transform conditional branch to an unconditional one
+          if (sec_branch->isConditional()) {
+            BranchInst::Create(new_sec_branch_target, sec_branch);
+            sec_branch->eraseFromParent();
+          }
+          // if already unconditional, simply set the new target
+          else {
+            sec_branch->setSuccessor(0, new_sec_branch_target);
+          }
+        } else if (branch->isConditional()) {
+          auto it1 = BBMap.find(branch->getSuccessor(0));
+          auto it2 = BBMap.find(branch->getSuccessor(1));
+
+          // both found in BBMap
+          if (it1 != BBMap.end() && it2 != BBMap.end()) {
+            sec_branch->setSuccessor(0, it1->second);
+            sec_branch->setSuccessor(1, it2->second);
+          }
+          // edge 1 is found in BBMap & edge 2 is not
+          else if (it1 != BBMap.end() && it2 == BBMap.end()) {
+            sec_branch->setSuccessor(0, it1->second);
+            sec_branch->setSuccessor(1, branch->getSuccessor(1));
+          }
+          // edge 2 is found in BBMap & edge 1 is not
+          else if (it1 == BBMap.end() && it2 != BBMap.end()) {
+// TODO: correct order? or reverse here?
+#if 1
+            sec_branch->setSuccessor(0, it2->second);
+            sec_branch->setSuccessor(1, branch->getSuccessor(0));
+#else
+            sec_branch->setSuccessor(0, branch->getSuccessor(0));
+            sec_branch->setSuccessor(1, it2->second);
+#endif
+          }
+          // neither is in BBMap
+          else if (it1 == BBMap.end() && it2 == BBMap.end()) {
+            sec_branch->setSuccessor(0, branch->getSuccessor(0));
+            sec_branch->setSuccessor(1, branch->getSuccessor(1));
+          }
+        }
+      }
+
+      BranchInst *branch = dyn_cast<BranchInst>(edge.first->getTerminator());
+      BasicBlock *newDst = BBMap[edge.second];
+      assert(branch != nullptr && "terminator must be a branch instruction");
+      if (branch->getSuccessor(0) == edge.second) {
+        branch->setSuccessor(0, newDst);
+      } else if (branch->isConditional() &&
+                 branch->getSuccessor(1) == edge.second) {
+        branch->setSuccessor(1, newDst);
+      } else {
+        assert(false && "initial edge doesn't point to edge target");
+      }
+
+      // update phi nodes of cloned blocks + successors preds / phi nodes
+      for (auto &bb_entry : BBMap) {
+        BasicBlock *orig = bb_entry.first;
+        BasicBlock *clone = bb_entry.second;
+        const auto update_phis = [this, &map_value](BasicBlock *bb,
+                                                    BasicBlock *orig_bb) {
+          // update incoming
+          std::unordered_set<BasicBlock *> preds;
+          for (BasicBlock *pred : predecessors(bb)) {
+            preds.emplace(pred);
+          }
+          for (Instruction *instr = &bb->front(),
+                           *next_instr = instr->getNextNode(),
+                           *non_phi = bb->getFirstNonPHI();
+               instr != non_phi;
+               instr = next_instr, next_instr = next_instr->getNextNode()) {
+            if (PHINode *phi = dyn_cast<PHINode>(instr)) {
+              std::unordered_set<BasicBlock *> rem_bbs;
+              std::unordered_set<BasicBlock *> existing_bbs;
+              for (BasicBlock *bb_in : phi->blocks()) {
+                existing_bbs.emplace(bb_in);
+                if (preds.count(bb_in) == 0) {
+                  rem_bbs.emplace(bb_in);
+                }
+              }
+
+              // add incoming phi edges for non-existing preds (map based on
+              // original BB value -> clone vmap)
+              for (BasicBlock *pred : preds) {
+                if (existing_bbs.count(pred) == 0) {
+                  // find original bb
+                  const auto orig_bb_iter = ClonedBBMap.find_value(pred);
+                  if (orig_bb_iter == ClonedBBMap.end()) {
+                    assert(false && "no original BB for cloned BB!");
+                  }
+
+                  const auto bb_index =
+                      phi->getBasicBlockIndex(orig_bb_iter->first);
+                  assert(bb_index != -1 &&
+                         "original BB not part of this phi node?");
+
+                  const auto orig_val = phi->getIncomingValue(bb_index);
+                  auto mapped_value =
+                      map_value(orig_val, ClonedVMap[pred].get());
+
+                  // add new incoming edge
+                  phi->addIncoming(mapped_value, pred);
+                }
+              }
+
+              // remove incoming BB values from BBs that no longer point to this
+              for (BasicBlock *rem_bb : rem_bbs) {
+                phi->removeIncomingValue(rem_bb);
+              }
+            }
+          }
+        };
+        update_phis(orig, nullptr);
+        update_phis(clone, orig);
+      }
+
+      // update outgoing / successors
+      for (auto &bb_entry : BBMap) {
+        BasicBlock *orig_bb = bb_entry.first;
+        BasicBlock *bb = bb_entry.second;
+
+        for (BasicBlock *succ : successors(bb)) {
+          if (BBMap.find_value(succ) != BBMap.end()) {
+            // already handled
+            continue;
+          }
+
+          for (Instruction *instr = &succ->front(),
+                           *non_phi = succ->getFirstNonPHI();
+               instr != non_phi; instr = instr->getNextNode()) {
+            if (PHINode *phi = dyn_cast<PHINode>(instr)) {
+              // add new incoming
+              const auto orig_val = phi->getIncomingValueForBlock(orig_bb);
+              assert(orig_val != nullptr &&
+                     "original block was not using this phi/block?");
+              auto mapped_value = map_value(orig_val, ClonedVMap[bb].get());
+
+              // add new incoming edge
+              phi->addIncoming(mapped_value, bb);
+            }
+          }
+        }
+      }
+
+      // cleanup / optimize
+      for (auto &bb_entry : BBMap) {
+        DeleteDeadPHIs(bb_entry.first);
+        DeleteDeadPHIs(bb_entry.second);
+
+        if (bb_entry.first->getUniquePredecessor() != nullptr) {
+          FoldSingleEntryPHINodes(bb_entry.first);
+        }
+        if (bb_entry.second->getUniquePredecessor() != nullptr) {
+          FoldSingleEntryPHINodes(bb_entry.second);
+        }
+      }
+
+#if 0
+      static uint32_t transform_idx = 0;
+      errs() << "## dumping #" << transform_idx << " ...\n";
+      StructuralAnalysis::dumpIR("func_transform_" +
+                                     std::to_string(transform_idx) + ".ll",
+                                 *_function);
+      StructuralAnalysis::dumpCFGDot("func_transform_" +
+                                         std::to_string(transform_idx) + ".dot",
+                                     *_function);
+      ++transform_idx;
+
+      errs() << "verifying ...\n";
+      assert(!verifyFunction(*_function, &errs()) && "verification failed");
+      errs() << "verifying done\n";
+#endif
+    }
+
+    return true; // applied forward copy
+  }
+
+  return false; // nothing happened
+}
+
+bool StructuralTransform::transform(Function &F) {
+  _function = &F;
+
+  alloca_insert = &F.getEntryBlock().front();
+  condition_init_insert = F.getEntryBlock().getTerminator();
+
+  // TODO: can we merge multiple conditions to a larger integer and then use a
+  // switch instead?
+  if (condition_type == nullptr) {
+    condition_type = llvm::Type::getInt1Ty(F.getContext());
+  }
+
+  SA.unstructuredBRVec.clear();
+  SA.unreachableNodeSet.clear();
+  SA.Net.clear();
+  SA.analyze(F);
+
+  DBG(SA.write(errs());)
+
+  bool was_modified = false;
+  while (!SA.unstructuredBRVec.empty()) {
+    NodeTy *entry = *(SA.Net.begin());
+
+    stopCut = false;
+
+    for (NodeTy *node : SA.unreachableNodeSet) {
+      if (Cut(node)) {
+        was_modified = true;
+        goto ANALYSIS;
+      }
+    }
+
+    if (ForwardCopy(entry)) {
+      was_modified = true;
+      goto ANALYSIS;
+    }
+
+    if (Cut(entry)) {
+      was_modified = true;
+      goto ANALYSIS;
+    }
+
+    if (BackwardCopy(entry)) {
+      was_modified = true;
+      goto ANALYSIS;
+    }
+
+  ANALYSIS:
+    DBG(errs() << "#### NEW ANALYSIS ####\n";)
+    SA.unstructuredBRVec.clear();
+    SA.unreachableNodeSet.clear();
+    SA.Net.clear();
+    SA.analyze(*_function);
+  }
+
+  SA.Net.clear();
+  // SA.analyze(*_function); // TODO: needed?
+
+  return was_modified;
+}
+
+const BasicBlock *StructuralTransform::bb(NodeTy *node) const {
+  return (node->isCombined ? nullptr : node->BB);
+}
+
+const StructuralTransform::NodeListTy &
+StructuralTransform::children(const NodeTy *node) const {
+  NodeListTy *nList = new NodeListTy;
+  NodeTy *tmp = nullptr;
+
+  switch (node->nodeType) {
+  case SA::Block:
+    tmp = node->entryNode;
+    nList->push_back(tmp);
+
+    while (tmp->succNode.size() == 1) {
+      tmp = *(tmp->succNode.begin());
+      nList->push_back(tmp);
+    }
+
+    break;
+  case SA::IfThen:
+    nList->push_back(cond(node));
+    nList->push_back(*(node->childNode.begin()));
+
+    break;
+  case SA::IfThenElse:
+    nList->push_back(cond(node));
+    nList->push_back(ifTrue(node));
+    nList->push_back(ifFalse(node));
+
+    break;
+  case SA::SelfLoop:
+    nList->push_back(*(node->childNode.begin()));
+    break;
+  case SA::NaturalLoop:
+    tmp = node->entryNode;
+    nList->push_back(tmp);
+
+    tmp = *(tmp->succNode.begin());
+    nList->push_back(tmp);
+
+    break;
+  default:
+    break;
+  }
+
+  return *nList;
+}
+
+const SA::NodeTy *StructuralTransform::cond(const NodeTy *node) const {
+  return node->entryNode;
+}
+
+const SA::NodeTy *StructuralTransform::ifTrue(const NodeTy *node) const {
+  if (node->nodeType == SA::IfThen) {
+    return *(node->childNode.begin());
+  }
+
+  const NodeTy *lastChild = node;
+
+  while (!lastChild->isCombined) {
+    lastChild = children(lastChild).back();
+  }
+
+  BranchInst *branch = dyn_cast<BranchInst>(lastChild->BB->getTerminator());
+  assert(branch != nullptr && "terminator must be a branch instruction");
+
+  auto tmpNode = node->childNode.begin();
+  NodeTy *childNode1 = *tmpNode;
+  ++tmpNode;
+  NodeTy *childNode2 = *tmpNode;
+
+  if (branch->getSuccessor(0) == childNode1->entryBB) {
+    return childNode2;
+  } else {
+    return childNode1;
+  }
+}
+
+const SA::NodeTy *StructuralTransform::ifFalse(const NodeTy *node) const {
+  const NodeTy *lastChild = node;
+
+  while (!lastChild->isCombined) {
+    lastChild = children(lastChild).back();
+  }
+
+  BranchInst *branch = dyn_cast<BranchInst>(lastChild->BB->getTerminator());
+  assert(branch != nullptr && "terminator must be a branch instruction");
+
+  auto tmpNode = node->childNode.begin();
+  NodeTy *childNode1 = *tmpNode;
+  ++tmpNode;
+  NodeTy *childNode2 = *tmpNode;
+
+  if (branch->getSuccessor(0) == childNode1->entryBB) {
+    return childNode1;
+  } else {
+    return childNode2;
+  }
+}
+}
diff --git a/llvm/lib/Transforms/LibFloor/VulkanFinal.cpp b/llvm/lib/Transforms/LibFloor/VulkanFinal.cpp
new file mode 100644
index 000000000000..6444c7b4e4bd
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/VulkanFinal.cpp
@@ -0,0 +1,1782 @@
+//===- VulkanFinal.cpp - Vulkan final pass --------------------------------===//
+//
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file fixes certain post-codegen issues and transforms specific builtin
+// functions with uses of new function arguments, as well as transforming
+// input/output variables/arguments for later use in the SPIR-V backend.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/Analysis/BasicAliasAnalysis.h"
+#include "llvm/Analysis/GlobalsModRef.h"
+#include "llvm/Analysis/PostDominators.h"
+#include "llvm/Analysis/LoopInfo.h"
+#include "llvm/Analysis/ValueTracking.h"
+#include "llvm/Analysis/TargetLibraryInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/Utils/BasicBlockUtils.h"
+#include "llvm/Transforms/Utils/LoopUtils.h"
+#include "llvm/Transforms/Utils/Cloning.h"
+#include "llvm/Transforms/LibFloor/AddressSpaceFix.h"
+#include <algorithm>
+#include <cstdarg>
+#include <memory>
+#include <unordered_map>
+#include <unordered_set>
+#include <deque>
+#include <array>
+using namespace llvm;
+
+#define DEBUG_TYPE "VulkanFinal"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	// -> SPIRVInternal.h (can't include, b/c it's not in a proper path)
+	static const uint32_t SPIRAS_Constant = 2;
+	static const uint32_t SPIRAS_Uniform = 5;
+	//static const uint32_t SPIRAS_Input = 6;
+	static const uint32_t SPIRAS_Output = 7;
+	static const uint32_t SPIRAS_StorageBuffer = 12;
+	static const uint32_t SPIRAS_PhysicalStorageBuffer = 5349;
+
+	// VulkanBuiltinParamHandling
+	struct VulkanBuiltinParamHandling : public FunctionPass, InstVisitor<VulkanBuiltinParamHandling> {
+		friend class InstVisitor<VulkanBuiltinParamHandling>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		bool is_kernel_func { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		
+		// added kernel function args
+		Argument* global_id { nullptr };
+		Argument* local_id { nullptr };
+		Argument* group_id { nullptr };
+		Argument* group_size { nullptr };
+		GlobalVariable* workgroup_size { nullptr };
+		
+		// added general shader function args
+		Argument* view_index { nullptr };
+		
+		// added vertex function args
+		Argument* vertex_id { nullptr };
+		Argument* instance_id { nullptr };
+		
+		// added fragment function args
+		Argument* point_coord { nullptr };
+		Argument* frag_coord { nullptr };
+		Argument* primitive_id { nullptr };
+		Argument* barycentric_coord { nullptr };
+		
+		// any function args
+		Argument* soft_printf { nullptr };
+		
+		VulkanBuiltinParamHandling() :
+		FunctionPass(ID) {
+			initializeVulkanBuiltinParamHandlingPass(*PassRegistry::getPassRegistry());
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<AAResultsWrapperPass>();
+			AU.addRequired<GlobalsAAWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<TargetLibraryInfoWrapperPass>();
+		}
+		
+		enum VULKAN_KERNEL_ARG_REV_IDX : int32_t {
+			VULKAN_GLOBAL_ID = -4,
+			VULKAN_LOCAL_ID = -3,
+			VULKAN_GROUP_ID = -2,
+			VULKAN_GROUP_SIZE = -1,
+			
+			VULKAN_KERNEL_ARG_COUNT = 4,
+		};
+		
+		enum VULKAN_VERTEX_ARG_REV_IDX : int32_t {
+			VULKAN_VERTEX_ID = -3,
+			VULKAN_VERTEX_VIEW_INDEX = -2,
+			VULKAN_INSTANCE_ID = -1,
+			
+			VULKAN_VERTEX_ARG_COUNT = 3,
+		};
+		
+		enum VULKAN_FRAGMENT_ARG_REV_IDX : int32_t {
+			VULKAN_POINT_COORD = -3,
+			VULKAN_FRAG_COORD = -2,
+			VULKAN_FRAGMENT_VIEW_INDEX = -1,
+			
+			VULKAN_FRAGMENT_ARG_COUNT = 3,
+		};
+		
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			
+			// determine this function type + exit if it isn't a kernel or shader function
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			if(!is_kernel_func && !is_vertex_func && !is_fragment_func) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			const auto get_arg_by_idx = [&F](const int32_t& rev_idx) -> llvm::Argument* {
+				auto arg_iter = F.arg_end();
+				std::advance(arg_iter, rev_idx);
+				return &*arg_iter;
+			};
+			
+			// check for optional features: soft-printf, primitive id, barycentric coord
+			bool has_soft_printf = false, has_primitive_id = false, has_barycentric_coord = false;
+			if (auto soft_printf_meta = M->getNamedMetadata("floor.soft_printf")) {
+				has_soft_printf = true;
+			}
+			if (auto primitive_id_meta = M->getNamedMetadata("floor.primitive_id")) {
+				has_primitive_id = true;
+			}
+			if (auto barycentric_coord_meta = M->getNamedMetadata("floor.barycentric_coord")) {
+				has_barycentric_coord = true;
+			}
+			
+			DBG(errs() << "> adding built-in args ...\n";)
+			// add args if this is a kernel function
+			if(is_kernel_func) {
+				if (F.arg_size() >= VULKAN_KERNEL_ARG_COUNT + (has_soft_printf ? 1 : 0)) {
+					global_id = get_arg_by_idx(VULKAN_GLOBAL_ID);
+					local_id = get_arg_by_idx(VULKAN_LOCAL_ID);
+					group_id = get_arg_by_idx(VULKAN_GROUP_ID);
+					group_size = get_arg_by_idx(VULKAN_GROUP_SIZE);
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(-(VULKAN_KERNEL_ARG_COUNT + 1));
+					}
+				} else {
+					errs() << "invalid kernel function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					global_id = nullptr;
+					local_id = nullptr;
+					group_id = nullptr;
+					group_size = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// add args if this is a vertex function
+			if(is_vertex_func) {
+				if (F.arg_size() >= VULKAN_VERTEX_ARG_COUNT + (has_soft_printf ? 1 : 0)) {
+					// TODO: this should be optional / only happen on request
+					vertex_id = get_arg_by_idx(VULKAN_VERTEX_ID);
+					view_index = get_arg_by_idx(VULKAN_VERTEX_VIEW_INDEX);
+					instance_id = get_arg_by_idx(VULKAN_INSTANCE_ID);
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(-(VULKAN_VERTEX_ARG_COUNT + 1));
+					}
+				} else {
+					errs() << "invalid vertex function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					vertex_id = nullptr;
+					view_index = nullptr;
+					instance_id = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// add args if this is a fragment function
+			if(is_fragment_func) {
+				const uint32_t opt_arg_count = (has_soft_printf ? 1u : 0u) + (has_primitive_id ? 1u : 0u) + (has_barycentric_coord ? 1u : 0u);
+				if (F.arg_size() >= VULKAN_FRAGMENT_ARG_COUNT + opt_arg_count) {
+					point_coord = get_arg_by_idx(VULKAN_POINT_COORD);
+					frag_coord = get_arg_by_idx(VULKAN_FRAG_COORD);
+					view_index = get_arg_by_idx(VULKAN_FRAGMENT_VIEW_INDEX);
+					
+					// NOTE: reverse order!
+					uint32_t opt_arg_counter = 1;
+					if (has_barycentric_coord) {
+						barycentric_coord = get_arg_by_idx(-(VULKAN_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						barycentric_coord = nullptr;
+					}
+					if (has_primitive_id) {
+						primitive_id = get_arg_by_idx(-(VULKAN_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						primitive_id = nullptr;
+					}
+					if (has_soft_printf) {
+						soft_printf = get_arg_by_idx(-(VULKAN_FRAGMENT_ARG_COUNT + opt_arg_counter++));
+					} else {
+						soft_printf = nullptr;
+					}
+				} else {
+					errs() << "invalid fragment function (" << F.getName() << ") argument count: " << F.arg_size() << "\n";
+					point_coord = nullptr;
+					frag_coord = nullptr;
+					view_index = nullptr;
+					primitive_id = nullptr;
+					barycentric_coord = nullptr;
+					soft_printf = nullptr;
+				}
+			}
+			
+			// emit work-group size variable for this function (initialized externally at "run-time" / SPIR-V spec)
+			if(is_kernel_func) {
+				auto workgroup_size_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 3);
+				workgroup_size = new GlobalVariable(*M,
+													workgroup_size_type,
+													true,
+													GlobalVariable::ExternalLinkage,
+													nullptr,
+													F.getName().str() + ".vulkan_constant.workgroup_size",
+													nullptr,
+													GlobalValue::NotThreadLocal,
+													0,
+													true);
+			}
+			
+			// visit everything in this function
+			DBG(errs() << "> handling instructions ...\n";)
+			visit(F);
+			
+			// always modified
+			DBG(errs() << "> " << F.getName() << " done\n";)
+			return true;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<VulkanBuiltinParamHandling>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<VulkanBuiltinParamHandling>::visit(I);
+		}
+		
+		//
+		void visitCallInst(CallInst &I) {
+			const auto func_name = I.getCalledFunction()->getName();
+			if(!func_name.startswith("floor.builtin.")) return;
+			
+			builder->SetInsertPoint(&I);
+			
+			// figure out which one we need
+			Argument* id;
+			if(func_name == "floor.builtin.global_id.i32") {
+				id = global_id;
+			}
+			else if(func_name == "floor.builtin.local_id.i32") {
+				id = local_id;
+			}
+			else if(func_name == "floor.builtin.group_id.i32") {
+				id = group_id;
+			}
+			else if(func_name == "floor.builtin.group_size.i32") {
+				id = group_size;
+			}
+			else if(func_name == "floor.builtin.local_size.i32") {
+				// this doesn't have a direct built-in equivalent, but must be loaded from the WorkgroupSize constant
+				I.replaceAllUsesWith(builder->CreateExtractElement(builder->CreateLoad(workgroup_size->getType()->getPointerElementType(), workgroup_size), I.getOperand(0)));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.global_size.i32") {
+				// this doesn't have a direct built-in equivalent, but must be computed from the WorkgroupSize constant
+				// TODO/NOTE: this might need some more work on the spir-v side, right now this is always constant folded
+				auto dim_idx = I.getOperand(0);
+				auto wg_size_dim = builder->CreateExtractElement(builder->CreateLoad(workgroup_size->getType()->getPointerElementType(), workgroup_size), dim_idx);
+				auto grp_count_dim = builder->CreateExtractElement(builder->CreateLoad(group_size->getType()->getPointerElementType(), group_size), dim_idx);
+				auto global_size_dim = builder->CreateMul(wg_size_dim, grp_count_dim);
+				I.replaceAllUsesWith(global_size_dim);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.work_dim.i32") {
+				if(group_size == nullptr) {
+					DBG(printf("failed to get group_size arg, probably not in a kernel function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				// special case
+				// => group_size.z == 1 ? (group_size.y == 1 ? 1 : 2) : 3
+				const auto loaded_group_size = builder->CreateLoad(group_size->getType()->getPointerElementType(), group_size);
+				const auto size_z = builder->CreateExtractElement(loaded_group_size, builder->getInt32(2));
+				const auto size_y = builder->CreateExtractElement(loaded_group_size, builder->getInt32(1));
+				const auto cmp_z = builder->CreateICmp(ICmpInst::ICMP_EQ, size_z, builder->getInt32(1));
+				const auto cmp_y = builder->CreateICmp(ICmpInst::ICMP_EQ, size_y, builder->getInt32(1));
+				const auto sel_x_or_y = builder->CreateSelect(cmp_y, builder->getInt32(1), builder->getInt32(2));
+				const auto sel_xy_or_z = builder->CreateSelect(cmp_z, sel_x_or_y, builder->getInt32(3));
+				I.replaceAllUsesWith(sel_xy_or_z);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.vertex_id.i32") {
+				if(vertex_id == nullptr) {
+					DBG(printf("failed to get vertex_id arg, probably not in a vertex function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(vertex_id->getType()->getPointerElementType(), vertex_id, "vertex_index"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.instance_id.i32") {
+				if(instance_id == nullptr) {
+					DBG(printf("failed to get instance_id arg, probably not in a vertex function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(instance_id->getType()->getPointerElementType(), instance_id, "instance_index"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.point_coord.float2") {
+				if(point_coord == nullptr) {
+					DBG(printf("failed to get point_coord arg, probably not in a fragment function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(point_coord->getType()->getPointerElementType(), point_coord, "point_coord"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.frag_coord.float4") {
+				if(frag_coord == nullptr) {
+					DBG(printf("failed to get frag_coord arg, probably not in a fragment function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(frag_coord->getType()->getPointerElementType(), frag_coord, "frag_coord"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.get_printf_buffer") {
+				if(soft_printf == nullptr) {
+					DBG(printf("failed to get printf_buffer arg, probably not in a kernel/vertex/fragment function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				// special case
+				I.replaceAllUsesWith(soft_printf);
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.view_index.i32") {
+				if(view_index == nullptr) {
+					DBG(printf("failed to get view_index arg, probably not in a shader function?\n"); fflush(stdout);)
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(view_index->getType()->getPointerElementType(), view_index, "view_index"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.primitive_id.i32") {
+				if (primitive_id == nullptr) {
+					llvm::errs() << "failed to get primitive_id arg, not in a fragment function or feature is not enabled\n";
+					llvm::errs().flush();
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(primitive_id->getType()->getPointerElementType(), primitive_id, "primitive_id"));
+				I.eraseFromParent();
+				return;
+			}
+			else if(func_name == "floor.builtin.barycentric_coord.float3") {
+				if (barycentric_coord == nullptr) {
+					llvm::errs() << "failed to get barycentric_coord arg, not in a fragment function or feature is not enabled\n";
+					llvm::errs().flush();
+					return;
+				}
+				
+				I.replaceAllUsesWith(builder->CreateLoad(barycentric_coord->getType()->getPointerElementType(), barycentric_coord, "barycentric_coord"));
+				I.eraseFromParent();
+				return;
+			}
+			// unknown -> ignore for now
+			else return;
+			
+			if(id == nullptr) {
+				DBG(printf("failed to get id arg, probably not in a kernel function?\n"); fflush(stdout);)
+				return;
+			}
+			
+			// replace call with vector load / elem extraction from the appropriate vector
+			I.replaceAllUsesWith(builder->CreateExtractElement(builder->CreateLoad(id->getType()->getPointerElementType(), id), I.getOperand(0)));
+			I.eraseFromParent();
+		}
+	};
+	
+	// VulkanFinal
+	struct VulkanFinal : public FunctionPass, InstVisitor<VulkanFinal> {
+		friend class InstVisitor<VulkanFinal>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		std::shared_ptr<llvm::IRBuilder<>> builder;
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		bool is_kernel_func { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		
+		VulkanFinal() :
+		FunctionPass(ID) {
+			initializeVulkanFinalPass(*PassRegistry::getPassRegistry());
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<AAResultsWrapperPass>();
+			AU.addRequired<GlobalsAAWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<TargetLibraryInfoWrapperPass>();
+		}
+		
+		// retrieves the target Vulkan version from metadata
+		static std::pair<uint32_t, uint32_t> get_vulkan_version(Module& M) {
+			const llvm::NamedMDNode *VulkanVersion =
+			M.getNamedMetadata("vulkan.version");
+			if (VulkanVersion == nullptr || VulkanVersion->getNumOperands() != 1) {
+				return {};
+			}
+			
+			const MDNode *vulkan_version_md = VulkanVersion->getOperand(0);
+			if (vulkan_version_md->getNumOperands() < 2) {
+				return {};
+			}
+			
+			uint64_t version_major = 0, version_minor = 0;
+			
+			const MDOperand &version_major_op = vulkan_version_md->getOperand(0);
+			if (const ConstantAsMetadata *version_major_md =
+				dyn_cast_or_null<ConstantAsMetadata>(version_major_op.get())) {
+				if (const ConstantInt *version_major_int =
+					dyn_cast_or_null<ConstantInt>(version_major_md->getValue())) {
+					version_major = version_major_int->getZExtValue();
+				} else {
+					return {};
+				}
+			} else {
+				return {};
+			}
+			
+			const MDOperand &version_minor_op = vulkan_version_md->getOperand(1);
+			if (const ConstantAsMetadata *version_minor_md =
+				dyn_cast_or_null<ConstantAsMetadata>(version_minor_op.get())) {
+				if (const ConstantInt *version_minor_int =
+					dyn_cast_or_null<ConstantInt>(version_minor_md->getValue())) {
+					version_minor = version_minor_int->getZExtValue();
+				} else {
+					return {};
+				}
+			} else {
+				return {};
+			}
+			
+			return { version_major, version_minor };
+		}
+
+		bool runOnFunction(Function &F) override {
+			// exit if empty function
+			if(F.empty()) return false;
+			DBG(errs() << "in func: "; errs().write_escaped(F.getName()) << '\n';)
+			
+			// determine this function type + exit if it isn't a kernel or shader function
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			if(!is_kernel_func && !is_vertex_func && !is_fragment_func) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			builder = std::make_shared<llvm::IRBuilder<>>(*ctx);
+			
+			//const auto vulkan_version = get_vulkan_version(*M);
+			
+			// handle return value / output
+			DBG(errs() << "> handling return values ...\n";)
+			if(is_vertex_func || is_fragment_func) {
+				const auto emit_output_var = [this](const std::string& var_name,
+													llvm::Type* global_type,
+													uint32_t address_space,
+													bool is_constant,
+													Constant* initializer) {
+					auto GV = new GlobalVariable(*M,
+												 global_type,
+												 is_constant,
+												 GlobalVariable::InternalLinkage,
+												 initializer,
+												 var_name,
+												 nullptr,
+												 GlobalValue::NotThreadLocal,
+												 address_space);
+					return GV;
+				};
+				
+				const auto ret_type = F.getReturnType();
+				const auto func_name = F.getName().str();
+				uint32_t return_idx = 0;
+				if(ret_type->isStructTy()) {
+					const auto st_type = cast<llvm::StructType>(ret_type);
+					for(const auto& elem : st_type->elements()) {
+						emit_output_var(func_name + ".vulkan_output." + std::to_string(return_idx++),
+										elem, SPIRAS_Output, false, nullptr);
+					}
+				}
+				else if(ret_type->isArrayTy()) {
+					emit_output_var(func_name + ".vulkan_output.0", ret_type, SPIRAS_Output, false, nullptr);
+				}
+				else if(!ret_type->isVoidTy()) {
+					emit_output_var(func_name + ".vulkan_output.0", ret_type, SPIRAS_Output, false, nullptr);
+				}
+				// else: nothing/passthrough
+			}
+			
+			// visit everything in this function
+			DBG(errs() << "> handling instructions ...\n";)
+			visit(F);
+			
+			DBG(errs() << "> updating function signature / parameters ...\n";)
+			{
+				// update function signature / parameters:
+				//  * change the return type to void (vs/fs returns have already been modified)
+				//  * transform constant AS pointers to either Uniform or StorageBuffer AS
+				//  * transform StorageBuffer AS image pointers to Uniform AS
+				//  * enclose non-struct Uniform parameters in a struct (note that enclosing SSBOs happens later)
+				//  * transform pointers inside argument buffers to PhysicalStorageBuffer AS
+				// NOTE: must be called after visiting rets and other ret type/val users
+				std::vector<Type*> param_types;
+				DBG(errs() << "func: " << F.getName().str() << "\n";)
+				for (auto& arg : F.args()) {
+					auto arg_type = arg.getType();
+					DBG(errs() << "\targ: " << arg.getName().str() << ": " << *arg_type;)
+					
+					// arg type must be a pointer
+					if (const auto ptr_type = dyn_cast<llvm::PointerType>(arg_type);
+						ptr_type && (ptr_type->getAddressSpace() == SPIRAS_Constant || ptr_type->getAddressSpace() == SPIRAS_StorageBuffer)) {
+						const auto ptr_as = ptr_type->getAddressSpace();
+						auto elem_type = ptr_type->getElementType();
+						const auto arg_idx = param_types.size();
+						
+						// handle IUBs
+						const auto iub_attr = F.getAttributeAtIndex(llvm::AttributeList::FirstArgIndex + arg_idx, "vulkan_iub");
+						const auto is_iub = (iub_attr.getRawPointer() != nullptr);
+						DBG(
+							if (is_iub) {
+								errs() << " (IUB)";
+							}
+						)
+						
+						// handle arg buffers
+						const auto ab_attr = F.getAttributeAtIndex(llvm::AttributeList::FirstArgIndex + arg_idx, "vulkan_arg_buffer");
+						const auto is_arg_buffer = (ab_attr.getRawPointer() != nullptr);
+						DBG(
+							if (is_arg_buffer) {
+								errs() << " (arg-buffer)";
+							}
+						)
+						assert(!(is_arg_buffer && is_iub) && "IUB and AB are mutually exclusive");
+						assert(!is_arg_buffer || (is_arg_buffer && elem_type->isStructTy()) && "AB element type must be a struct");
+						
+						// since there is a limit on how many IUBs we can have and how large they can be, some arguments might fall back to using SSBOs
+						const auto is_ssbo_uniform = (!is_iub && !is_arg_buffer && arg.onlyReadsMemory() &&
+													  (arg.hasAttribute(Attribute::Dereferenceable) ||
+													   arg.hasAttribute(Attribute::DereferenceableOrNull)));
+						
+						// any image/opaque type is unsized
+						const auto is_sized = elem_type->isSized();
+						
+						// decide storage class
+						const auto storage_class = (is_iub || !is_sized ? SPIRAS_Uniform : SPIRAS_StorageBuffer);
+						
+						// transform storage class if necessary +
+						// for IUBs/SSBO-Uniform: enclose in struct if the element type is not a struct
+						if ((is_iub || is_ssbo_uniform) && !elem_type->isStructTy()) {
+							llvm::Type* st_elems[] { elem_type };
+							elem_type = llvm::StructType::create(*ctx, st_elems, "enclose." + arg.getName().str());
+							arg_type = elem_type->getPointerTo(storage_class);
+							arg.mutateType(arg_type);
+							
+							// replace users, should usually only have one load
+							llvm::Value* idx_list[] {
+								llvm::ConstantInt::get(llvm::Type::getInt32Ty(*ctx), 0),
+								llvm::ConstantInt::get(llvm::Type::getInt32Ty(*ctx), 0),
+							};
+							std::vector<User*> users;
+							for(auto* user : arg.users()) {
+								users.emplace_back(user);
+							}
+							for (auto& user : users) {
+								if (auto instr = dyn_cast<Instruction>(user)) {
+									if (isa<LoadInst>(instr)) {
+										auto elem_gep = llvm::GetElementPtrInst::CreateInBounds(arg.getType()->getScalarType()->getPointerElementType(),
+																								&arg, idx_list, "", instr);
+										auto repl_instr = new LoadInst(elem_gep->getType()->getPointerElementType(), elem_gep, instr->getName(), false, instr);
+										repl_instr->setDebugLoc(instr->getDebugLoc());
+										instr->replaceAllUsesWith(repl_instr);
+										instr->eraseFromParent();
+									} else {
+										DBG(errs() << "\nunhandled arg user: " << *instr << "\n";)
+										DBG(errs().flush();)
+										assert(false && "unhandled arg user");
+									}
+								} else {
+									DBG(errs() << "\narg user is not an instruction\n";)
+									DBG(errs().flush();)
+									assert(false && "arg user is not an instruction");
+								}
+							}
+						} else if (storage_class != ptr_as) {
+							arg_type = elem_type->getPointerTo(storage_class);
+							arg.mutateType(arg_type);
+							
+							// update users
+							std::vector<User*> users;
+							for (auto* user : arg.users()) {
+								users.emplace_back(user);
+							}
+							std::vector<ReturnInst*> returns; // returns to fix -> there shouldn't be any here
+							for (auto& user : users) {
+								if (auto instr = dyn_cast<Instruction>(user)) {
+									fix_instruction_users(*ctx, *instr, arg, storage_class, returns);
+								}
+							}
+							assert(returns.empty() && "unexpected return type change");
+						}
+
+						// transform argument buffer struct and uses
+						if (is_arg_buffer) {
+							// note that we only need to cover the pointers at the top-level of the struct (i.e. either direct pointers or array of pointers)
+							auto elem_struct_type = dyn_cast<llvm::StructType>(elem_type);
+							std::vector<Type*> adj_struct_member_types;
+							bool struct_needs_adjustment = false;
+							for (auto& member_type : elem_struct_type->elements()) {
+								if (member_type->isPointerTy()) {
+									auto member_ptr_type = dyn_cast<llvm::PointerType>(member_type);
+									if (member_ptr_type->getAddressSpace() != SPIRAS_StorageBuffer) {
+										ctx->emitError("unsupported pointer address space in argument buffer of function " + func->getName().str());
+										return false;
+									}
+									
+									member_ptr_type = member_ptr_type->getElementType()->getPointerTo(SPIRAS_PhysicalStorageBuffer);
+									adj_struct_member_types.emplace_back(member_ptr_type);
+									struct_needs_adjustment = true;
+								} else if (member_type->isArrayTy()) {
+									auto arr_elem_type = member_type->getArrayElementType();
+									if (!arr_elem_type->isPointerTy()) {
+										adj_struct_member_types.emplace_back(member_type);
+										continue;
+									}
+									
+									auto arr_elem_ptr_type = dyn_cast<llvm::PointerType>(arr_elem_type);
+									if (arr_elem_ptr_type->getAddressSpace() != SPIRAS_StorageBuffer) {
+										ctx->emitError("unsupported pointer address space in buffer array in argument buffer of function " + func->getName().str());
+										return false;
+									}
+									
+									arr_elem_ptr_type = arr_elem_ptr_type->getElementType()->getPointerTo(SPIRAS_PhysicalStorageBuffer);
+									arr_elem_type = llvm::ArrayType::get(arr_elem_ptr_type, member_type->getArrayNumElements());
+									adj_struct_member_types.emplace_back(arr_elem_type);
+									struct_needs_adjustment = true;
+								} else {
+									adj_struct_member_types.emplace_back(member_type);
+								}
+							}
+							if (struct_needs_adjustment) {
+								auto adj_struct_type = llvm::StructType::create(*ctx, adj_struct_member_types, elem_struct_type->getName().str() + ".adj");
+								arg_type = adj_struct_type->getPointerTo(storage_class);
+								arg.mutateType(arg_type);
+								
+								// update users
+								std::vector<User*> users;
+								for (auto* user : arg.users()) {
+									users.emplace_back(user);
+								}
+								for (auto& user : users) {
+									// all users should just be GEPs
+									if (auto GEP = dyn_cast<GetElementPtrInst>(user)) {
+										const auto result_type = GEP->getResultElementType();
+										if (!result_type->isPointerTy() || result_type->getPointerAddressSpace() != SPIRAS_StorageBuffer) {
+											continue;
+										}
+										
+										// modify GEP types to use the correct address spaces
+										auto new_result_type = result_type->getPointerElementType()->getPointerTo(SPIRAS_PhysicalStorageBuffer);
+										auto new_gep_type = PointerType::get(new_result_type, SPIRAS_StorageBuffer);
+										GEP->mutateType(new_gep_type);
+										GEP->setResultElementType(new_result_type);
+										
+										// fix GEP users
+										std::vector<User*> gep_users;
+										for (auto* gep_user : GEP->users()) {
+											gep_users.emplace_back(gep_user);
+										}
+										for (auto& gep_user : gep_users) {
+											if (auto gep_user_instr = dyn_cast<Instruction>(gep_user)) {
+												std::vector<ReturnInst*> returns; // returns to fix -> there shouldn't be any here
+												fix_instruction_users(*ctx, *gep_user_instr, *GEP, SPIRAS_PhysicalStorageBuffer, returns);
+												assert(returns.empty() && "unexpected return type change");
+											}
+										}
+									} else {
+										DBG(errs() << "\nunhandled AB user: " << *user << "\n";)
+										DBG(errs().flush();)
+										assert(false && "unhandled AB user");
+									}
+								}
+							}
+						}
+
+						DBG(errs() << " -> " << *arg_type;)
+					}
+					DBG(errs() << "\n";)
+					
+					param_types.push_back(arg_type);
+				}
+
+				auto new_func_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), param_types, false);
+				F.mutateType(PointerType::get(new_func_type, 0));
+				F.mutateFunctionType(new_func_type);
+			}
+			
+			// always modified
+			DBG(errs() << "> " << F.getName() << " done\n";)
+			return true;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<VulkanFinal>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<VulkanFinal>::visit(I);
+		}
+		
+		// prefer i32 indices so that we don't need the Int64 capability
+		void visitExtractElement(ExtractElementInst& EEI) {
+			const auto idx_op = EEI.getIndexOperand();
+			const auto idx_type = idx_op->getType();
+			if(!idx_type->isIntegerTy(32)) {
+				if(const auto const_idx_op = dyn_cast_or_null<ConstantInt>(idx_op)) {
+					EEI.setOperand(1 /* idx op */, builder->getInt32((int32_t)const_idx_op->getValue().getZExtValue()));
+				}
+				// else: can't do anything, b/c other int type would still be used when casting
+			}
+		}
+		
+		// prefer i32 indices so that we don't need the Int64 capability
+		void visitInsertElement(InsertElementInst& IEI) {
+			const auto idx_op = IEI.llvm::User::getOperand(2);
+			const auto idx_type = idx_op->getType();
+			if(!idx_type->isIntegerTy(32)) {
+				if(const auto const_idx_op = dyn_cast_or_null<ConstantInt>(idx_op)) {
+					IEI.setOperand(2 /* idx op */, builder->getInt32((int32_t)const_idx_op->getValue().getZExtValue()));
+				}
+				// else: can't do anything, b/c other int type would still be used when casting
+			}
+		}
+		
+		void visitAllocaInst(AllocaInst &AI) {
+			// TODO: no pointers to pointers in vulkan
+		}
+		
+		void visitReturnInst(ReturnInst &RI) {
+			if(!is_vertex_func && !is_fragment_func) return;
+			
+			auto ret_val = RI.getReturnValue();
+			const auto ret_type = ret_val->getType();
+			if(ret_val == nullptr) return;
+			
+			DebugLoc DL;
+			if(auto ret_instr = dyn_cast<Instruction>(ret_val)) {
+				DL = ret_instr->getDebugLoc();
+			}
+			
+			const auto get_output_var = [this](uint32_t& idx) -> GlobalValue* {
+				const auto name = func->getName().str() + ".vulkan_output." + std::to_string(idx++);
+				auto output_var = M->getNamedValue(name);
+				if(output_var == nullptr) {
+					errs() << "output variable \"" << name << "\" doesn't exist\n";
+					return nullptr;
+				}
+				return output_var;
+			};
+			
+			uint32_t ret_idx = 0;
+			if(ret_type->isStructTy()) {
+				// struct -> split up to individual field stores
+				const auto st_type = cast<llvm::StructType>(ret_type);
+				for(uint32_t i = 0; i < st_type->getNumElements(); ++i) {
+					auto output_var = get_output_var(ret_idx);
+					if(output_var == nullptr) return;
+					
+					SmallVector<uint32_t, 1> idx_list { ret_idx - 1 };
+					auto st_extract = ExtractValueInst::Create(ret_val, idx_list, "ret.extract", &RI);
+					auto st = new StoreInst(st_extract, output_var, false, &RI);
+					if(DL) st->setDebugLoc(DL);
+				}
+			}
+			else {
+				// else: array, scalars, vectors -> direct store
+				auto output_var = get_output_var(ret_idx);
+				if(output_var == nullptr) return;
+				auto st = new StoreInst(ret_val, output_var, false, &RI);
+				if(DL) st->setDebugLoc(DL);
+			}
+			
+			// clear return
+			ReturnInst::Create(*ctx, RI.getParent());
+			RI.eraseFromParent();
+		}
+	};
+	
+	// VulkanPreFinal
+	struct VulkanPreFinal : public FunctionPass, InstVisitor<VulkanPreFinal> {
+		friend class InstVisitor<VulkanPreFinal>;
+		
+		static char ID; // Pass identification, replacement for typeid
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		Function* func { nullptr };
+		bool is_kernel_func { false };
+		bool is_vertex_func { false };
+		bool is_fragment_func { false };
+		bool was_modified { false };
+		ConstantFolder folder;
+		
+		// function input/originating pointers (i.e. parameters + allocas)
+		std::unordered_set<Value*> input_ptrs;
+		std::vector<GetElementPtrInst*> gep_ptrs;
+		std::vector<SelectInst*> select_ptrs;
+		std::vector<PHINode*> phi_ptrs;
+		
+		VulkanPreFinal() :
+		FunctionPass(ID) {
+			initializeVulkanPreFinalPass(*PassRegistry::getPassRegistry());
+		}
+		
+		void getAnalysisUsage(AnalysisUsage &AU) const override {
+			AU.addRequired<AAResultsWrapperPass>();
+			AU.addRequired<GlobalsAAWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<TargetLibraryInfoWrapperPass>();
+			AU.addRequired<AssumptionCacheTracker>();
+			AU.addRequired<DominatorTreeWrapperPass>();
+		}
+		
+		bool runOnFunction(Function &F) override {
+			is_kernel_func = F.getCallingConv() == CallingConv::FLOOR_KERNEL;
+			is_vertex_func = F.getCallingConv() == CallingConv::FLOOR_VERTEX;
+			is_fragment_func = F.getCallingConv() == CallingConv::FLOOR_FRAGMENT;
+			if(!is_kernel_func && !is_vertex_func && !is_fragment_func) return false;
+			
+			//
+			M = F.getParent();
+			ctx = &M->getContext();
+			func = &F;
+			input_ptrs.clear();
+			gep_ptrs.clear();
+			select_ptrs.clear();
+			phi_ptrs.clear();
+			
+			// store parameter pointers + globals used in this function
+			for(auto& arg : F.args()) {
+				if(arg.getType()->isPointerTy()) {
+					input_ptrs.emplace(&arg);
+				}
+			}
+			for (auto& GV : M->globals()) {
+				if(!GV.getType()->isPointerTy()) continue;
+				for(const auto& user : GV.users()) {
+					if(Instruction* instr = dyn_cast_or_null<Instruction>(user)) {
+						if(instr->getParent()->getParent() == &F) {
+							input_ptrs.emplace(&GV);
+							break;
+						}
+					}
+				}
+			}
+			
+			// gather all stuff
+			was_modified = false;
+			visit(F);
+			
+			// NOTE: disabled for now as we don't need this any more (probably ...)
+			// handle everything
+			//handle_pointers();
+			
+			return was_modified;
+		}
+		
+		// InstVisitor overrides...
+		using InstVisitor<VulkanPreFinal>::visit;
+		void visit(Instruction& I) {
+			InstVisitor<VulkanPreFinal>::visit(I);
+		}
+		
+		void visitAllocaInst(AllocaInst& AI) {
+			// store allocas
+			input_ptrs.emplace(&AI);
+		}
+		
+		// fuses "base_gep" and "gep" to a single GEP, possibly replacing "gep" with the new GEP
+		// NOTE: for simple cases, the returned GEP can be the input "gep" (with indices replaced)
+		std::pair<GetElementPtrInst*, bool /* abort */>
+		fuse_geps(GetElementPtrInst* base_gep, GetElementPtrInst* gep, const bool replace_old_gep) {
+			const auto fuse_index = [this](Value* lhs, Value* rhs, Instruction* insert_pos) -> Value* {
+				ConstantInt* lhs_const = dyn_cast_or_null<ConstantInt>(lhs);
+				ConstantInt* rhs_const = dyn_cast_or_null<ConstantInt>(rhs);
+				if(lhs_const != nullptr && rhs_const != nullptr) {
+					// both are constant -> simply fold them
+					return folder.CreateAdd(lhs_const, rhs_const);
+				}
+				else if(lhs_const != nullptr && lhs_const->getZExtValue() == 0) {
+					// lhs idx is 0
+					return rhs;
+				}
+				else if(rhs_const != nullptr && rhs_const->getZExtValue() == 0) {
+					// rhs idx is 0
+					return lhs;
+				}
+				else {
+					// need to create a proper addition
+					std::string name = ".gep.idx.add";
+					if(rhs->hasName()) name = rhs->getName().str() + name;
+					if(lhs->hasName()) name = lhs->getName().str() + name;
+					return BinaryOperator::CreateAdd(lhs, rhs, name, insert_pos);
+				}
+			};
+			
+			const auto base_idx_count = base_gep->getNumIndices();
+			const auto idx_count = gep->getNumIndices();
+			
+			DBG(errs() << ">>> fusing: " << *base_gep << " + " << *gep << "\n";)
+			
+			if(base_idx_count == 1 && replace_old_gep) {
+				// -> base is just an offset pointer, can simply fuse both GEPs
+				DBG(errs() << ">> fused (simple): " << *gep;)
+				
+				// replace pointer base
+				gep->setOperand(0, base_gep->getPointerOperand());
+				
+				// fuse first index
+				gep->setOperand(1, fuse_index(base_gep->getOperand(1), gep->getOperand(1), gep));
+				
+				DBG(errs() << ", with " << *gep << "\n";)
+				return { gep, false };
+			}
+			else {
+				// -> need to create a new GEP so that it can contain all indices from both GEPs
+				// NOTE: last idx in base + first in current always matches -> -1
+				const auto fused_idx_count = base_idx_count + idx_count - 1;
+				
+				// create fused idx list
+				SmallVector<Value*, 4> idx_list;
+				
+				// add base indices
+				for(uint32_t i = 1, count = base_gep->getNumIndices() + 1; i < count; ++i) {
+					idx_list.push_back(base_gep->getOperand(i));
+				}
+				
+				// fust last (base) + first (current) idx
+				idx_list[idx_list.size() - 1] = fuse_index(idx_list.back(), gep->getOperand(1), gep);
+				
+				// add current indices
+				for(uint32_t i = 2, count = gep->getNumIndices() + 1; i < count; ++i) {
+					idx_list.push_back(gep->getOperand(i));
+				}
+				assert(idx_list.size() == fused_idx_count && "invalid idx list size");
+				
+				// create the new GEP
+				auto gep_ptr = base_gep->getPointerOperand();
+				auto gep_ptr_type = cast<PointerType>(gep_ptr->getType()->getScalarType())->getElementType();
+				if (const auto idx_type = GetElementPtrInst::getIndexedType(gep_ptr_type, idx_list); !idx_type) {
+					// can't handle it -> ignore it
+					// NOTE: since variable pointers are now used by default, PtrAccessChain is possible -> we don't necessarily
+					// need fused GEPs (but it's still a good thing that enables other transformations)
+					return { gep, true };
+				}
+				auto fused_gep = GetElementPtrInst::CreateInBounds(gep_ptr_type, gep_ptr, idx_list,
+																   gep->getName() + ".gep.fused", gep);
+				fused_gep->setDebugLoc(gep->getDebugLoc());
+				
+				// replace this with new fused one
+				DBG(errs() << ">> fused: replacing " << *gep << ", with " << *fused_gep << "\n";)
+				if(replace_old_gep) {
+					gep->replaceAllUsesWith(fused_gep);
+					gep->eraseFromParent();
+				}
+				return { fused_gep, false };
+			}
+		}
+		
+		void visitGetElementPtrInst(GetElementPtrInst &I) {
+			// prefer i32 indices so that we don't need the Int64 capability
+			for(auto& op : I.operands()) {
+				if(op->getType()->isIntegerTy() &&
+				   !op->getType()->isIntegerTy(32)) {
+					if(const auto const_idx_op = dyn_cast_or_null<ConstantInt>(op)) {
+						op.set(ConstantInt::get(Type::getInt32Ty(*ctx),
+												(int32_t)const_idx_op->getValue().getZExtValue()));
+					}
+					else {
+						// TODO: does this make sense? would need Int64 cap either way
+						op.set(CastInst::CreateIntegerCast(op, Type::getInt32Ty(*ctx), false, "gep.idx.i32.cast", &I));
+					}
+				}
+			}
+			
+			// GEP fusion: we can't have GEPs into GEPs, so fuse them
+			// NOTE: this has to be done recursively of course
+			GetElementPtrInst* output_gep = &I;
+			bool did_fuse = false;
+			do {
+				did_fuse = false;
+				if(GetElementPtrInst* GEP = dyn_cast_or_null<GetElementPtrInst>(output_gep->getPointerOperand())) {
+					bool abort = false;
+					std::tie(output_gep, abort) = fuse_geps(GEP, output_gep, true /* replace_old_gep */);
+					if (abort) {
+						break;
+					}
+					did_fuse = true;
+				}
+			} while(did_fuse);
+			
+			gep_ptrs.push_back(output_gep);
+		}
+		
+		void visitSelectInst(SelectInst& SI) {
+			// we're only interested in pointer selects, as these aren't supported in vulkan
+			if(!SI.getType()->isPointerTy()) {
+				return;
+			}
+			select_ptrs.emplace_back(&SI);
+		}
+		
+		void visitPHINode(PHINode& PHI) {
+			// we're only interested in pointer selects, as these aren't supported in vulkan
+			if(!PHI.getType()->isPointerTy()) {
+				return;
+			}
+			phi_ptrs.emplace_back(&PHI);
+		}
+		
+		CallInst* insert_keep_block_marker(BasicBlock* keep_block) {
+			Function* keep_block_func = M->getFunction("floor.keep_block");
+			if(keep_block_func == nullptr) {
+				FunctionType* keep_block_type = FunctionType::get(llvm::Type::getVoidTy(*ctx), false);
+				keep_block_func = (Function*)M->getOrInsertFunction("floor.keep_block", keep_block_type).getCallee();
+				keep_block_func->setCallingConv(CallingConv::FLOOR_FUNC);
+				keep_block_func->setCannotDuplicate();
+				keep_block_func->setDoesNotThrow();
+				keep_block_func->setNotConvergent();
+				keep_block_func->setDoesNotRecurse();
+			}
+			CallInst* keep_block_call = CallInst::Create(keep_block_func, "", keep_block->getTerminator());
+			keep_block_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			return keep_block_call;
+		}
+		
+		void handle_pointers() {
+			DBG(errs() << "####################\n## in " << func->getName() << "\n";
+				for(const auto& iptr : input_ptrs) {
+					errs() << "input: " << *iptr << "\n";
+				}
+				for(const auto& gep : gep_ptrs) {
+					errs() << "GEP: " << *gep << "\n";
+				}
+				for(const auto& sel : select_ptrs) {
+					errs() << "select: " << *sel << "\n";
+				}
+				for(const auto& phi : phi_ptrs) {
+					errs() << "phi: " << *phi << "\n";
+				}
+			)
+			was_modified = true; // TODO: do this properly
+			
+			//
+			std::vector<GetElementPtrInst*> problem_geps;
+			for(const auto& gep : gep_ptrs) {
+				if(input_ptrs.count(gep->getPointerOperand()) == 0) {
+					problem_geps.emplace_back(gep);
+					continue;
+				}
+			}
+			DBG(errs() << "\n";
+				for(const auto& gep : problem_geps) {
+					errs() << "PROBLEM GEP: " << *gep << "\n";
+				}
+			)
+			
+			// needed for finding the origin pointers
+			auto DT = &getAnalysis<DominatorTreeWrapperPass>().getDomTree();
+			DT->recalculate(*func);
+			LoopInfo LI(*DT);
+			
+			// find set of problem instructions and their pointer origin(s)
+			std::deque<Instruction*> problem_instrs;
+			problem_instrs.insert(begin(problem_instrs), begin(select_ptrs), end(select_ptrs));
+			problem_instrs.insert(begin(problem_instrs), begin(phi_ptrs), end(phi_ptrs));
+			problem_instrs.insert(begin(problem_instrs), begin(problem_geps), end(problem_geps));
+			
+			struct ptr_set {
+				// pointer origin(s)
+				std::unordered_set<const Value*> src;
+				// pointer producers (PHIs, selects, GEPs)
+				// TODO: pointer bitcasts should be handled previously
+				std::unordered_set<Instruction*> producers;
+				// pointer consumers (loads, stores)
+				std::unordered_set<Instruction*> consumers;
+			};
+			std::vector<std::shared_ptr<ptr_set>> ptr_sets;
+			std::unordered_map<Instruction*, ptr_set*> ptr_set_map;
+			std::unordered_multimap<const Value*, ptr_set*> origin_map;
+			
+			// create initial ptr_sets from our "problem" instructions
+			for(; !problem_instrs.empty(); problem_instrs.pop_front()) {
+				const auto& instr = problem_instrs[0];
+				
+				// already handled elsewhere?
+				if(ptr_set_map.count(instr) > 0) continue;
+				
+				// figure out which origin pointer(s) this comes from
+				SmallVector<const Value*, 3> origins;
+				DBG(errs() << "\n-> " << *instr << "\n";)
+				getUnderlyingObjects(instr, origins, &LI, 0);
+				DBG(for(const auto& orig : origins) {
+					errs() << "     origin: " << *orig << "\n";
+				})
+				if(origins.empty()) {
+					ctx->emitError(instr, "instruction has no origin");
+					return;
+				}
+				
+				// check if we have a ptr_set for this origin combination yet
+				ptr_set* pset = nullptr;
+				auto orig_range = origin_map.equal_range(origins[0]);
+				for(auto oiter = orig_range.first; oiter != orig_range.second; ++oiter) {
+					// same size?
+					if(oiter->second->src.size() != origins.size()) {
+						continue;
+					}
+					
+					// same content?
+					bool equal_set = true;
+					for(const auto& origin : origins) {
+						if(oiter->second->src.count(origin) == 0) {
+							equal_set = false;
+							break;
+						}
+					}
+					if(!equal_set) continue;
+					
+					// found a ptr_set that matches
+					pset = oiter->second;
+					break;
+				}
+				
+				// create a ptr_set if none exists yet
+				if(pset == nullptr) {
+					auto new_ptr_set = std::make_shared<ptr_set>();
+					ptr_sets.push_back(new_ptr_set);
+					pset = new_ptr_set.get();
+					
+					pset->src.insert(origins.begin(), origins.end());
+					for(const auto& origin : origins) {
+						origin_map.emplace(origin, pset);
+					}
+				}
+				
+				// add this instruction
+				pset->producers.emplace(instr);
+				ptr_set_map.emplace(instr, pset);
+				
+				// we must also handle the pointers contained within instructions
+				// -> this makes sure that we recursively get the complete net/graph of pointers
+				if(PHINode* phi = dyn_cast_or_null<PHINode>(instr)) {
+					for(const auto& phi_inc_val : phi->incoming_values()) {
+						// only add the value/instruction if we haven't handled it yet + if it isn't a input ptr
+						const auto phi_inc_instr = dyn_cast_or_null<Instruction>(&phi_inc_val);
+						if(phi_inc_instr != nullptr &&
+						   input_ptrs.count(phi_inc_instr) == 0 &&
+						   ptr_set_map.count(phi_inc_instr) == 0) {
+							problem_instrs.emplace_back(phi_inc_instr);
+						}
+					}
+				}
+				else if(SelectInst* sel = dyn_cast_or_null<SelectInst>(instr)) {
+					const auto true_instr = dyn_cast_or_null<Instruction>(sel->getTrueValue());
+					const auto false_instr = dyn_cast_or_null<Instruction>(sel->getFalseValue());
+					if(true_instr != nullptr &&
+					   input_ptrs.count(true_instr) == 0 &&
+					   ptr_set_map.count(true_instr) == 0) {
+						problem_instrs.emplace_back(true_instr);
+					}
+					if(false_instr != nullptr &&
+					   input_ptrs.count(false_instr) == 0 &&
+					   ptr_set_map.count(false_instr) == 0) {
+						problem_instrs.emplace_back(false_instr);
+					}
+				}
+				else if(GetElementPtrInst* GEP = dyn_cast_or_null<GetElementPtrInst>(instr)) {
+					const auto gep_instr = dyn_cast_or_null<Instruction>(GEP->getPointerOperand());
+					if(gep_instr != nullptr &&
+					   input_ptrs.count(gep_instr) == 0 &&
+					   ptr_set_map.count(gep_instr) == 0) {
+						problem_instrs.emplace_back(gep_instr);
+					}
+				}
+			}
+			
+			// find all consumers of our producers
+			for(const auto& instr_pset : ptr_set_map) {
+				const auto& instr = instr_pset.first;
+				auto& pset = instr_pset.second;
+				
+				for(const auto& user : instr->users()) {
+					// only need to consider direct users this time
+					if(Instruction* user_instr = dyn_cast_or_null<Instruction>(user)) {
+						if(isa<GetElementPtrInst>(user_instr) ||
+						   isa<SelectInst>(user_instr) ||
+						   isa<PHINode>(user_instr)) {
+							// not interested in these, should already be handled elsewhere
+							const auto pset_iter = ptr_set_map.find(user_instr);
+							assert(pset_iter != ptr_set_map.end() && "unknown producer");
+							assert(pset_iter->second->producers.count(user_instr) > 0 && "unhandled producer");
+							continue;
+						}
+						// NOTE: we don't care about what kind of instructions consumers are, because
+						// we can simply iterate over all operands of the instruction and replace the
+						// producer(s) accordingly
+						pset->consumers.emplace(user_instr);
+					}
+				}
+			}
+			
+			// debug output
+			DBG(errs() << "\n\n## ptr sets:\n";
+				for(const auto& pset : ptr_sets) {
+					errs() << "# set\n";
+					errs() << "origins:\n";
+					for(const auto& origin : pset->src) {
+						errs() << "\t" << *origin << "\n";
+					}
+					errs() << "producers:\n";
+					for(const auto& prod : pset->producers) {
+						errs() << "\t" << *prod << "\n";
+					}
+					errs() << "consumers:\n";
+					for(const auto& cons : pset->consumers) {
+						errs() << "\t" << *cons << "\n";
+					}
+					
+					errs() << "\n";
+				}
+			)
+			
+			//
+			const auto idx_type = Type::getInt32Ty(*ctx);
+			const auto idx_zero = ConstantInt::get(idx_type, 0);
+			for(const auto& pset : ptr_sets) {
+				// ignore set if there are no consumers (that we need to handle)
+				if(pset->consumers.empty()) continue;
+				
+				if(pset->src.size() == 1) {
+					// simple case with a single origin ptr
+					// -> can replace everything with indices
+					
+					// ptr -> index map
+					std::unordered_map<Instruction*, Instruction*> index_map;
+					
+					// TODO: handle necessary index count (> 1)
+					// * check which op#s are constant (throughout?) -> no need to handle them
+					// * for non-constant -> use array? n-vector? multiple scalar indices?
+					
+					// create indices / replace pointers with integers - due to chain dependencies this is a bit tricky
+					const auto replace_with_idx = [&pset, &index_map, &idx_zero](Value* val) -> Value* {
+						// is this an origin pointer - if so, index is 0
+						if(pset->src.count(val) > 0) {
+							return idx_zero;
+						}
+						
+						// otherwise it must be an instruction
+						const auto instr = dyn_cast_or_null<Instruction>(val);
+						assert(instr != nullptr && "must be an instruction");
+						
+						const auto idx_iter = index_map.find(instr);
+						assert(idx_iter != index_map.end() && "no index for this instruction");
+						
+						return idx_iter->second;
+					};
+					
+					bool all_indices_created = false;
+					while(!all_indices_created) {
+						DBG(errs() << "... index round ...\n";)
+						all_indices_created = true;
+						for(const auto& prod : pset->producers) {
+							if(index_map.count(prod) > 0) continue;
+							
+							Instruction* idx = nullptr;
+							if(GetElementPtrInst* GEP = dyn_cast_or_null<GetElementPtrInst>(prod)) {
+								const auto base_ptr = GEP->getPointerOperand();
+								const auto base_ptr_instr = dyn_cast_or_null<Instruction>(base_ptr);
+								if(pset->src.count(base_ptr) > 0 ||
+								   (base_ptr_instr != nullptr && index_map.count(base_ptr_instr) > 0)) {
+									idx = BinaryOperator::CreateAdd(replace_with_idx(base_ptr), GEP->getOperand(1),
+																	GEP->getName() + ".idx", GEP);
+								}
+								else {
+									// can't handle this yet - do another round
+									DBG(errs() << " -- can't handle yet: " << *GEP << "\n";)
+									all_indices_created = false;
+									continue;
+								}
+							}
+							else if(PHINode* phi = dyn_cast_or_null<PHINode>(prod)) {
+								// NOTE: we will make sure this contains the proper incoming values later
+								idx = PHINode::Create(idx_type, phi->getNumIncomingValues(),
+													  phi->getName() + ".idx", phi);
+							}
+							else if(SelectInst* sel = dyn_cast_or_null<SelectInst>(prod)) {
+								// NOTE: we will make sure this contains the proper true/false values later
+								idx = SelectInst::Create(sel->getCondition(), idx_zero, idx_zero);
+							}
+							
+							if (auto dbg_loc = prod->getDebugLoc(); dbg_loc) {
+								idx->setDebugLoc(dbg_loc);
+							}
+							index_map.emplace(prod, idx);
+							DBG(errs() << "idx: " << *prod << " -> " << *idx << "\n";)
+						}
+					}
+					
+					// phi/select index fixup now that we have created all indices
+					for(const auto& prod : pset->producers) {
+						if(PHINode* phi = dyn_cast_or_null<PHINode>(prod)) {
+							auto idx_phi = dyn_cast<PHINode>(index_map[prod]);
+							for(uint32_t i = 0, count = phi->getNumIncomingValues(); i < count; ++i) {
+								idx_phi->addIncoming(replace_with_idx(phi->getIncomingValue(i)),
+													 phi->getIncomingBlock(i));
+							}
+							DBG(errs() << "updated phi: " << *idx_phi << "\n";)
+						}
+						else if(SelectInst* sel = dyn_cast_or_null<SelectInst>(prod)) {
+							auto idx_sel = dyn_cast<SelectInst>(index_map[prod]);
+							idx_sel->setTrueValue(replace_with_idx(sel->getTrueValue()));
+							idx_sel->setFalseValue(replace_with_idx(sel->getFalseValue()));
+							DBG(errs() << "updated sel: " << *idx_sel << "\n";)
+						}
+					}
+					
+					// materialize pointers/GEPs for users (from indices)
+					auto base_ptr = *pset->src.begin();
+					for(Instruction* cons : pset->consumers) {
+						DBG(errs() << ">> cons before: " << *cons << "\n";)
+						for(uint32_t i = 0, count = cons->getNumOperands(); i < count; ++i) {
+							const auto op = cons->getOperand(i);
+							const auto idx_iter = index_map.find((Instruction*)op);
+							if(idx_iter != index_map.end()) {
+								// replace with new materialized GEP
+								SmallVector<Value*, 3> idx_list;
+								idx_list.push_back(idx_zero);
+								idx_list.push_back(idx_iter->second);
+								auto val_ptr = const_cast<Value*>(base_ptr);
+								auto mat_gep = GetElementPtrInst::CreateInBounds(val_ptr->getType()->getScalarType()->getPointerElementType(),
+																				 val_ptr, idx_list,
+																				 op->getName() + ".idx.mat.gep", cons);
+								if(Instruction* op_instr = dyn_cast_or_null<Instruction>(op)) {
+									mat_gep->setDebugLoc(op_instr->getDebugLoc());
+								}
+								DBG(errs() << "> consumer op replace: " << *op << " -> " << *mat_gep << "\n";)
+								cons->setOperand(i, mat_gep);
+							}
+						}
+						DBG(errs() << ">> cons after: " << *cons << "\n";)
+					}
+				}
+				else if(pset->src.size() > 1) {
+					// complex case with 2 or more origin ptrs
+					
+					// go down the condition tree until we hit a leaf / origin pointer, in between:
+					//  * for GEPs: store them in a chain (in the same order as they were encountered) and pass/copy
+					//              them through to each tree branch (so that each branch has its individual chain)
+					//  * for SELs: create if/else branches for each SEL (using the same condition as the SEL),
+					//              then recursively go down each branch, meeting up again after the SEL instruction,
+					//              also insert a PHI if the consumer produces a value (replace consumer with that at the end)
+					//  * for PHIs: not handled yet - need to create a new integer condition, then use that for branching?
+					//  * for origin ptrs: fuse the GEP chain up to that point, replace pointer with the origin pointer (now
+					//                     unambiguous), replace use of producer pointer in the consumer with the new GEP
+					
+					// (upside-down) condition tree
+					struct condition_tree {
+						enum class TYPE : uint32_t {
+							COMPARE_BOOL,
+							COMPARE_PHI,
+							ORIGIN_PTR,
+							GEP,
+						};
+						TYPE type;
+						
+						// if COMPARE_BOOL: i1 compare value
+						// if COMPARE_PHI: i32 compare value
+						// if ORIGIN_PTR: actual origin ptr (+this is a leaf node)
+						// if GEP: intermediate GEP
+						Value* val;
+						
+						// pointers to the next condition node(s) or actual origin ptrs (if they are leaf nodes)
+						std::vector<condition_tree> nodes;
+					};
+					uint32_t leaf_count = 0;
+					const std::function<void(Value*, condition_tree&)> create_condition_tree =
+					[&create_condition_tree, &pset, &leaf_count, this](Value* prod, condition_tree& node) {
+						// origin ptr: attach as leaf node
+						if(pset->src.count(prod) > 0) {
+							node.type = condition_tree::TYPE::ORIGIN_PTR;
+							node.val = prod;
+							++leaf_count;
+							return;
+						}
+						
+						// intermediate GEP, continue with pointer op
+						if(GetElementPtrInst* GEP = dyn_cast_or_null<GetElementPtrInst>(prod)) {
+							node.type = condition_tree::TYPE::GEP;
+							node.val = GEP;
+							
+							condition_tree child;
+							create_condition_tree(GEP->getPointerOperand(), child);
+							node.nodes.push_back(child);
+						}
+						// select, store condition + continue traversal with true and false value
+						else if(SelectInst* sel = dyn_cast_or_null<SelectInst>(prod)) {
+							node.type = condition_tree::TYPE::COMPARE_BOOL;
+							node.val = sel->getCondition();
+							
+							// NOTE: must be in this order
+							condition_tree true_child, false_child;
+							create_condition_tree(sel->getTrueValue(), true_child);
+							create_condition_tree(sel->getFalseValue(), false_child);
+							node.nodes.push_back(true_child);
+							node.nodes.push_back(false_child);
+						}
+						else if(PHINode* phi = dyn_cast_or_null<PHINode>(prod)) {
+							// TODO: handle PHIs!
+							node.type = condition_tree::TYPE::COMPARE_PHI;
+							ctx->emitError(phi, "PHI nodes are not handled yet");
+							return;
+						}
+						else {
+							if(Instruction* instr = dyn_cast_or_null<Instruction>(prod)) {
+								ctx->emitError(instr, "unhandled producer while creating condition tree");
+							}
+							else {
+								ctx->emitError("unhandled non-instruction producer value while creating condition tree");
+							}
+						}
+					};
+					
+					// for debugging purposes
+					const std::function<void(const condition_tree&, const uint32_t)> dump_condition_tree =
+					[&dump_condition_tree](const condition_tree& node, const uint32_t level) {
+						errs() << std::string(level, '-') << "> ";
+						switch(node.type) {
+							case condition_tree::TYPE::COMPARE_BOOL:
+								errs() << "SELECT: " << *node.val << "\n";
+								dump_condition_tree(node.nodes[0], level + 1);
+								dump_condition_tree(node.nodes[1], level + 1);
+								break;
+							case condition_tree::TYPE::COMPARE_PHI:
+								errs() << "PHI: " << *node.val << "\n";
+								for(const auto& child : node.nodes) {
+									dump_condition_tree(child, level + 1);
+								}
+								break;
+							case condition_tree::TYPE::GEP:
+								errs() << "GEP: " << *node.val << "\n";
+								dump_condition_tree(node.nodes[0], level + 1);
+								break;
+							case condition_tree::TYPE::ORIGIN_PTR:
+								errs() << "LEAF: " << *node.val << "\n";
+								break;
+						}
+					};
+					
+					for(Instruction* cons : pset->consumers) {
+						// find producer
+						Instruction* producer = nullptr;
+						for(uint32_t i = 0, count = cons->getNumOperands(); i < count; ++i) {
+							const auto op = cons->getOperand(i);
+							const auto piter = pset->producers.find((Instruction*)op);
+							if(piter != pset->producers.end()) {
+								producer = *piter;
+								break;
+							}
+						}
+						assert(producer != nullptr && "consumer has no producer");
+						DBG(errs() << "-> cons/prod: " << *cons << " -> " << *producer << "\n";)
+						
+						// find the condition(s) on which we need to branch
+						condition_tree cond_tree;
+						create_condition_tree(producer, cond_tree);
+						DBG(dump_condition_tree(cond_tree, 0); errs() << "\n";)
+						
+						// TODO: once switches are supported, combine all conditions to a single large switch,
+						// instead of creating a condition tree
+						// TODO: try to merge multiple users to a single block per origin instead of per GEP/user
+						
+						BasicBlock* continue_block = nullptr; // code/block _after_ the consumer instruction
+						PHINode* continue_phi = nullptr;
+						const std::function<void(const condition_tree&, std::vector<GetElementPtrInst*>, Instruction*)>
+						handle_condition_tree = [this, &handle_condition_tree, &continue_block, &continue_phi, &leaf_count,
+												 &cons, &producer](const condition_tree& node,
+																   std::vector<GetElementPtrInst*> gep_chain,
+																   Instruction* branch_term) {
+							switch(node.type) {
+								case condition_tree::TYPE::COMPARE_BOOL: {
+									DBG(errs() << "@SEL: " << *node.val << "\n";)
+									// select: create an if-else branch
+									Value* cond = node.val;
+									
+									Instruction* true_term = nullptr;
+									Instruction* false_term = nullptr;
+									if(continue_block == nullptr) {
+										// create the first split and final continue block
+										SplitBlockAndInsertIfThenElse(cond, cons, &true_term, &false_term);
+										assert(true_term != nullptr && "failed to create true branch/term");
+										assert(false_term != nullptr && "failed to create false branch/term");
+										continue_block = cons->getParent();
+										continue_block->setName("continue.block");
+										
+										// insert continue phi if the consumer produces a value
+										if(!cons->getType()->isVoidTy()) {
+											continue_phi = PHINode::Create(cons->getType(), leaf_count, "continue.phi",
+																		   cons->getNextNode());
+										}
+									}
+									else {
+										// TODO: implement this
+										assert(false && "not implemented yet");
+										ctx->emitError("multi-select not implemented yet");
+										return;
+									}
+									
+									true_term->getParent()->setName("sel.true");
+									false_term->getParent()->setName("sel.false");
+									
+									// copy GEP chain into both branches
+									handle_condition_tree(node.nodes[0], gep_chain, true_term);
+									handle_condition_tree(node.nodes[1], gep_chain, false_term);
+									break;
+								}
+								case condition_tree::TYPE::COMPARE_PHI: {
+									DBG(errs() << "@PHI: " << *node.val << "\n";)
+									ctx->emitError("PHI nodes are not handled yet");
+									break;
+								}
+								case condition_tree::TYPE::GEP: {
+									DBG(errs() << "@GEP: " << *node.val << "\n";)
+									// add GEP to chain and continue
+									gep_chain.emplace_back(dyn_cast<GetElementPtrInst>(node.val));
+									handle_condition_tree(node.nodes[0], std::move(gep_chain), branch_term);
+									break;
+								}
+								case condition_tree::TYPE::ORIGIN_PTR: {
+									DBG(errs() << "@PTR: " << *node.val << "\n";)
+									// leaf: finally reached an origin pointer
+									assert(branch_term != nullptr && "must have a branch terminator");
+									
+									// fuse GEPs with origin ptr
+									GetElementPtrInst* mat_gep = (GetElementPtrInst*)gep_chain[0]->clone();
+									mat_gep->setOperand(0, node.val);
+									mat_gep->setName(gep_chain.back()->getName());
+									mat_gep->setDebugLoc(gep_chain.back()->getDebugLoc());
+									for(size_t i = 1, count = gep_chain.size(); i < count; ++i) {
+										bool abort = false;
+										std::tie(mat_gep, abort) = fuse_geps(mat_gep, gep_chain[i], false); // TODO
+										if (abort) {
+											break;
+										}
+									}
+									mat_gep->insertBefore(branch_term);
+									DBG(errs() << ">> mat_gep: " << *mat_gep << "\n";)
+									
+									// copy consumer
+									auto cons_clone = cons->clone();
+									if(cons->hasName()) {
+										cons_clone->setName(cons->getName());
+									}
+									cons_clone->setDebugLoc(cons->getDebugLoc());
+									cons_clone->insertBefore(branch_term);
+									
+									// replace consumer ptr
+									DBG(errs() << ">> cons before: " << *cons_clone << "\n";)
+									for(uint32_t i = 0, count = cons_clone->getNumOperands(); i < count; ++i) {
+										const auto op = cons_clone->getOperand(i);
+										if(op == producer) {
+											cons_clone->setOperand(i, mat_gep);
+										}
+									}
+									DBG(errs() << ">> cons after: " << *cons_clone << "\n";)
+									
+									// branch to continue block
+									auto br_inst = dyn_cast_or_null<BranchInst>(branch_term);
+									assert(br_inst != nullptr && "invalid terminator");
+									assert(br_inst->isUnconditional() && "branch must be unconditional");
+									br_inst->setSuccessor(0, continue_block);
+									
+									// update continue phi (if there is one)
+									if(continue_phi != nullptr) {
+										continue_phi->addIncoming(cons_clone, cons_clone->getParent());
+										DBG(errs() << ">> updated phi: " << *continue_phi << "\n";)
+									}
+									break;
+								}
+							}
+						};
+						handle_condition_tree(cond_tree, {}, nullptr);
+						
+						if(continue_phi != nullptr) {
+							cons->replaceAllUsesWith(continue_phi);
+						}
+						cons->eraseFromParent();
+					}
+				}
+			}
+			
+			// TODO: cleanup / DCE (will be done at the end anyways, but might be better to do here already)
+		}
+	};
+	
+	// VulkanFinalModuleCleanup:
+	// * strip unused functions/prototypes/externs
+	struct VulkanFinalModuleCleanup : public ModulePass {
+		static char ID; // Pass identification, replacement for typeid
+		
+		Module* M { nullptr };
+		LLVMContext* ctx { nullptr };
+		
+		VulkanFinalModuleCleanup() : ModulePass(ID) {
+			initializeVulkanFinalModuleCleanupPass(*PassRegistry::getPassRegistry());
+		}
+		
+		bool runOnModule(Module& Mod) override {
+			M = &Mod;
+			ctx = &M->getContext();
+			
+			// kill all functions named floor.builtin.* (we still need other floor.* functions)
+			bool module_modified = false;
+			for(auto func_iter = Mod.begin(); func_iter != Mod.end();) {
+				auto& func = *func_iter;
+				if(func.getName().startswith("floor.builtin.")) {
+					if(func.getNumUses() != 0) {
+						errs() << func.getName() << " should not have any uses at this point!\n";
+					}
+					++func_iter; // inc before erase
+					func.eraseFromParent();
+					module_modified = true;
+					continue;
+				}
+				++func_iter;
+			}
+			return module_modified;
+		}
+		
+	};
+	
+}
+
+char VulkanFinal::ID = 0;
+FunctionPass *llvm::createVulkanFinalPass() {
+	return new VulkanFinal();
+}
+INITIALIZE_PASS_BEGIN(VulkanFinal, "VulkanFinal", "VulkanFinal Pass", false, false)
+INITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass)
+INITIALIZE_PASS_END(VulkanFinal, "VulkanFinal", "VulkanFinal Pass", false, false)
+
+char VulkanBuiltinParamHandling::ID = 0;
+FunctionPass *llvm::createVulkanBuiltinParamHandlingPass() {
+	return new VulkanBuiltinParamHandling();
+}
+INITIALIZE_PASS_BEGIN(VulkanBuiltinParamHandling, "VulkanBuiltinParamHandling", "VulkanBuiltinParamHandling Pass", false, false)
+INITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass)
+INITIALIZE_PASS_END(VulkanBuiltinParamHandling, "VulkanBuiltinParamHandling", "VulkanBuiltinParamHandling Pass", false, false)
+
+char VulkanPreFinal::ID = 0;
+FunctionPass *llvm::createVulkanPreFinalPass() {
+	return new VulkanPreFinal();
+}
+INITIALIZE_PASS_BEGIN(VulkanPreFinal, "VulkanPreFinal", "VulkanPreFinal Pass", false, false)
+INITIALIZE_PASS_DEPENDENCY(AAResultsWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(GlobalsAAWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
+INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(TargetLibraryInfoWrapperPass)
+INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
+INITIALIZE_PASS_END(VulkanPreFinal, "VulkanPreFinal", "VulkanPreFinal Pass", false, false)
+
+char VulkanFinalModuleCleanup::ID = 0;
+ModulePass *llvm::createVulkanFinalModuleCleanupPass() {
+	return new VulkanFinalModuleCleanup();
+}
+INITIALIZE_PASS_BEGIN(VulkanFinalModuleCleanup, "VulkanFinal module cleanup", "VulkanFinal module cleanup Pass", false, false)
+INITIALIZE_PASS_END(VulkanFinalModuleCleanup, "VulkanFinal module cleanup", "VulkanFinal module cleanup Pass", false, false)
diff --git a/llvm/lib/Transforms/LibFloor/VulkanImage.cpp b/llvm/lib/Transforms/LibFloor/VulkanImage.cpp
new file mode 100644
index 000000000000..af419960686c
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/VulkanImage.cpp
@@ -0,0 +1,878 @@
+//===- VulkanImage.cpp - Vulkan-specific floor image transformations ------===//
+////
+//  Flo's Open libRary (floor)
+//  Copyright (C) 2004 - 2022 Florian Ziesche
+//
+//  This program is free software; you can redistribute it and/or modify
+//  it under the terms of the GNU General Public License as published by
+//  the Free Software Foundation; version 2 of the License only.
+//
+//  This program is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+//  GNU General Public License for more details.
+//
+//  You should have received a copy of the GNU General Public License along
+//  with this program; if not, write to the Free Software Foundation, Inc.,
+//  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass implements the SPIR-V-specific floor image transformations, i.e.
+// floor.opaque.<read/write function>.* -> SPIR-V image function call
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SetVector.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringExtras.h"
+#include "llvm/Analysis/AliasAnalysis.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/CallingConv.h"
+#include "llvm/IR/ConstantRange.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/Dominators.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/InlineAsm.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/IR/InstVisitor.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Metadata.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/LegacyPassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/PassRegistry.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/IPO/PassManagerBuilder.h"
+#include "llvm/Transforms/IPO.h"
+#include "llvm/Transforms/LibFloor.h"
+#include "llvm/Transforms/LibFloor/FloorImage.h"
+#include "llvm/Transforms/LibFloor/VulkanSampling.h"
+#include <unordered_map>
+using namespace llvm;
+
+#define DEBUG_TYPE "VulkanImage"
+
+#if 1
+#define DBG(x)
+#else
+#define DBG(x) x
+#endif
+
+namespace {
+	struct VulkanImage : public FloorImageBasePass {
+		static char ID; // Pass identification, replacement for typeid
+		
+		VulkanImage(const uint32_t image_capabilities_ = 0) :
+		FloorImageBasePass(ID, IMAGE_TYPE_ID::OPAQUE, image_capabilities_) {
+			initializeVulkanImagePass(*PassRegistry::getPassRegistry());
+		}
+		
+		llvm::Function* get_or_create_spirv_function(std::string func_name,
+													 llvm::Type* ret_type,
+													 const SmallVector<llvm::Type*, 8>& func_arg_types,
+													 const bool is_readnone = false) {
+			const auto func_type = llvm::FunctionType::get(ret_type, func_arg_types, false);
+			auto func = M->getFunction(func_name);
+			if(func == nullptr) { // only do this once
+				func = dyn_cast<Function>(M->getOrInsertFunction(func_name, func_type,
+																 is_readnone ?
+																 nounwind_readnone_attr :
+																 nounwind_attr).getCallee());
+				func->setCallingConv(CallingConv::FLOOR_FUNC);
+				// TODO: any other flags here?
+			}
+			return func;
+		}
+		
+		static const char* type_to_geom(const COMPUTE_IMAGE_TYPE& image_type) {
+			switch(image_type) {
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D:
+					return "11ocl_image1d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_ARRAY:
+					return "16ocl_image1darray";
+				case COMPUTE_IMAGE_TYPE::IMAGE_1D_BUFFER:
+					return "17ocl_image1dbuffer";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH:
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_STENCIL:
+					return "16ocl_image2ddepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D:
+					return "11ocl_image2d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_ARRAY:
+					return "21ocl_image2darraydepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_ARRAY:
+					return "16ocl_image2darray";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA:
+					return "20ocl_image2dmsaadepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA:
+					return "15ocl_image2dmsaa";
+				case COMPUTE_IMAGE_TYPE::IMAGE_3D:
+					return "11ocl_image3d";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_MSAA_ARRAY:
+					return "25ocl_image2darraymsaadepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_2D_MSAA_ARRAY:
+					return "20ocl_image2darraymsaa";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE:
+					return "18ocl_imagecubedepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_DEPTH_CUBE_ARRAY:
+					return "23ocl_imagecubearraydepth";
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE:
+					return "13ocl_imagecube";
+				case COMPUTE_IMAGE_TYPE::IMAGE_CUBE_ARRAY:
+					return "18ocl_imagecubearray";
+				default:
+					return nullptr;
+			}
+		}
+		
+		void handle_vk_coord(Instruction& I,
+							 llvm::Value* coord_arg,
+							 llvm::Value* layer_arg,
+							 const bool is_array,
+							 const bool is_msaa,
+							 const bool is_non_cube_array_depth_compare,
+							 // must have: true: int coords, false: float coords
+							 const bool must_have_int_args,
+							 std::string& vk_func_name,
+							 SmallVector<llvm::Type*, 8>& func_arg_types,
+							 SmallVector<llvm::Value*, 8>& func_args) {
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			const auto coord_dim = coord_vec_type->getNumElements();
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			const auto is_int_coord = coord_type->isIntegerTy();
+			if(!is_int_coord && !coord_type->isFloatTy()) {
+				ctx->emitError(&I, "invalid coordinate type (neither int nor float)");
+				return;
+			}
+			
+			auto vk_coord_dim = coord_dim + (is_array ? 1 : 0) + (is_non_cube_array_depth_compare ? 1 : 0);
+			assert(vk_coord_dim <= 4 && "can't have a coord vector type with dim > 4");
+			const auto vk_coord_scalar_type = (must_have_int_args ? llvm::Type::getInt32Ty(*ctx) : llvm::Type::getFloatTy(*ctx));
+			const auto vk_coord_type = (vk_coord_dim == 1 ?
+										vk_coord_scalar_type :
+										llvm::FixedVectorType::get(vk_coord_scalar_type, vk_coord_dim));
+			
+			//
+			bool convert_to_int = false, convert_to_float = false;
+			if(must_have_int_args && !is_int_coord) {
+				convert_to_int = true;
+			}
+			else if(!must_have_int_args && is_int_coord) {
+				convert_to_float = true;
+			}
+			const auto convert_val = [&convert_to_int, &convert_to_float,
+									  &vk_coord_scalar_type, this](llvm::Value* val) {
+				return (convert_to_int ?
+						builder->CreateFPToSI(val, vk_coord_scalar_type) :
+						(convert_to_float ?
+						 builder->CreateSIToFP(val, vk_coord_scalar_type) :
+						 val));
+			};
+			
+			// start with the specified coord arg, there are some cases where we can just use it without rebuilding
+			auto vk_coord_arg = coord_arg;
+			if(vk_coord_type != coord_vec_type) {
+				if(vk_coord_dim == 1) {
+					// just a scalar
+					vk_coord_arg = convert_val(builder->CreateExtractElement(coord_arg, builder->getInt32(0)));
+				}
+				else {
+					// create a new tmp coord, then copy coord elements (keep unused undef)
+					vk_coord_arg = UndefValue::get(vk_coord_type);
+					uint32_t coord_idx = 0;
+					for(; coord_idx < coord_dim; ++coord_idx) {
+						vk_coord_arg = builder->CreateInsertElement(vk_coord_arg,
+																	convert_val(builder->CreateExtractElement(coord_arg,
+																											  builder->getInt32(coord_idx))),
+																	builder->getInt32(coord_idx));
+					}
+					
+					// need to pull the layer index into the coordinate, including possible int -> float conversion
+					if(is_array) {
+						auto layer = layer_arg;
+						if(!must_have_int_args) {
+							// need to convert
+							layer = builder->CreateUIToFP(layer_arg, vk_coord_scalar_type);
+						}
+						vk_coord_arg = builder->CreateInsertElement(vk_coord_arg, layer, builder->getInt32(coord_idx++));
+					}
+					
+					// workaround a bug in nvidia drivers where another coord component is expected when doing depth comparison
+					// (this seems to be a remnant of glsl where Dref was stored as the last component instead of an additional argument)
+					if(is_non_cube_array_depth_compare) {
+						vk_coord_arg = builder->CreateInsertElement(vk_coord_arg,
+																	ConstantFP::get(llvm::Type::getFloatTy(*ctx), 0.0f),
+																	builder->getInt32(coord_idx++));
+					}
+				}
+			}
+			func_arg_types.push_back(vk_coord_arg->getType());
+			func_args.push_back(vk_coord_arg);
+			
+			if(vk_coord_dim > 1) {
+				vk_func_name += "Dv" + std::to_string(vk_coord_dim) + "_";
+			}
+			vk_func_name += (must_have_int_args ? "i" : "f");
+		}
+		
+		void handle_read_image(Instruction& I,
+							   const StringRef& func_name,
+							   llvm::Value* img_handle_arg,
+							   const COMPUTE_IMAGE_TYPE& image_type,
+							   llvm::ConstantInt* const_sampler_arg,
+							   llvm::Value* dyn_sampler_arg,
+							   llvm::Value* coord_arg,
+							   llvm::Value* layer_arg,
+							   llvm::Value* sample_arg,
+							   llvm::Value* offset_arg,
+							   const SmallVector<llvm::Value*, 3>& offset_elems,
+							   const bool is_offset,
+							   llvm::Value* lod_or_bias_arg,
+							   const bool is_lod_or_bias, // true: lod, false: bias
+							   llvm::Value* dpdx_arg,
+							   llvm::Value* dpdy_arg,
+							   const bool is_gradient,
+							   const COMPARE_FUNCTION& compare_function,
+							   llvm::Value* compare_value_arg,
+							   const bool is_compare) override {
+			SmallVector<llvm::Type*, 8> func_arg_types;
+			SmallVector<llvm::Value*, 8> func_args;
+			auto coord_vec_type = dyn_cast_or_null<FixedVectorType>(coord_arg->getType());
+			
+			// NOTE: read call will be constructed as follows ([arg] are optional args):
+			// read(image, sampler_idx, coord_with_layer,
+			//      lod_type, [lod_arg_0], [lod_arg_1],
+			//      bool is_offset, [offset],
+			//      [sample_idx],
+			//      [compare_val])
+			// -> this will use cxx mangling, since we still need to differentiate the calls later on
+			
+			// TODO: properly handle cube images -> must either use explicit/implicit sampling or read, not fetch
+			// TODO: properly handle depth compare -> must use explicit/implicit sampling, not fetch
+			
+			// must be constant/constexpr for now
+			if(const_sampler_arg == nullptr) {
+				ctx->emitError(&I, "sampler must be a constant");
+				return;
+			}
+			const vulkan_sampling::sampler sampler_val { (uint32_t)const_sampler_arg->getZExtValue() };
+			const auto is_fetch = ((sampler_val.value & vulkan_sampling::sampler::COORD_MODE::__COORD_MODE_MASK) ==
+								   vulkan_sampling::sampler::COORD_MODE::PIXEL);
+			
+			// get geom string / mangled name + flags
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(image_type);
+			const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type);
+			const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			// -> return data and vulkan function name
+			// NOTE: we don't have a c++ mangling support in here, so do it manually
+			// (this is actually easy enough, since everything is very static)
+			std::string vk_func_name;
+			llvm::Type* ret_type;
+			if(func_name.endswith(".float")) {
+				vk_func_name = "_Z11read_imagef";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4);
+			}
+			else if(func_name.endswith(".int")) {
+				vk_func_name = "_Z11read_imagei";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".uint")) {
+				vk_func_name = "_Z12read_imageui";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			}
+			else if(func_name.endswith(".half")) {
+				vk_func_name = "_Z11read_imageh";
+				ret_type = llvm::FixedVectorType::get(llvm::Type::getHalfTy(*ctx), 4);
+			}
+			// unknown -> ignore
+			else return;
+			
+			// -> geom/image
+			vk_func_name += geom;
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			// -> sampler
+			vk_func_name += "11ocl_sampler"; // technically "i"
+			func_arg_types.push_back(const_sampler_arg->getType());
+			func_args.push_back(const_sampler_arg);
+			
+			// -> coord
+			handle_vk_coord(I,
+							coord_arg,
+							layer_arg,
+							is_array,
+							is_msaa,
+							is_depth && is_compare && !(is_cube && is_array),
+							// fetch: always int coords, sample: always float coords
+							is_fetch,
+							vk_func_name,
+							func_arg_types,
+							func_args);
+			
+			// -> lod / bias / gradient
+			vulkan_sampling::LOD_TYPE lod_type = vulkan_sampling::LOD_TYPE::INVALID;
+			const auto const_fp_lod_or_bias = dyn_cast<ConstantFP>(lod_or_bias_arg);
+			if(!is_msaa) {
+				// -> gradient
+				if(is_gradient) {
+					lod_type = vulkan_sampling::LOD_TYPE::GRADIENT;
+				}
+				// -> lod / bias
+				else {
+					// -> lod
+					if(is_lod_or_bias) {
+						lod_type = vulkan_sampling::LOD_TYPE::EXPLICIT_LOD;
+					}
+					// -> bias
+					else {
+						// if this is inside a fragment shader and this isn't a fetch, use implicit lod
+						if(is_fragment_shader && !is_fetch) {
+							// if bias is 0, only use implicit lod as-is
+							// NOTE: also makes sure that bias is a float value
+							if(const_fp_lod_or_bias != nullptr &&
+							   const_fp_lod_or_bias->isZero()) {
+								lod_type = vulkan_sampling::LOD_TYPE::IMPLICIT_LOD;
+							}
+							// if bias is != 0 and we're not in fetch mode, specify the bias
+							else if(!is_fetch && lod_or_bias_arg->getType()->isFloatTy()) {
+								lod_type = vulkan_sampling::LOD_TYPE::IMPLICIT_LOD_WITH_BIAS;
+							}
+							// else: convert to explicit lod
+							else {
+								lod_type = vulkan_sampling::LOD_TYPE::EXPLICIT_LOD;
+							}
+						}
+						// for all others: convert to an explicit lod
+						else {
+							lod_type = vulkan_sampling::LOD_TYPE::EXPLICIT_LOD;
+						}
+					}
+				}
+			}
+			else {
+				// msaa allows no lod
+				lod_type = vulkan_sampling::LOD_TYPE::NO_LOD;
+			}
+			assert(lod_type != vulkan_sampling::LOD_TYPE::INVALID && "invalid lod type");
+			
+			vk_func_name += "i";
+			auto lod_type_arg = ConstantInt::get(Type::getInt32Ty(*ctx), (uint32_t)lod_type);
+			func_arg_types.push_back(lod_type_arg->getType());
+			func_args.push_back(lod_type_arg);
+			
+			const auto is_int_lod = (is_fetch);
+			switch(lod_type) {
+				case vulkan_sampling::LOD_TYPE::NO_LOD:
+				case vulkan_sampling::LOD_TYPE::IMPLICIT_LOD:
+					// nothing to emit here
+					break;
+				case vulkan_sampling::LOD_TYPE::GRADIENT: {
+					// gradient coord_dim must be equal to image coord dim (- layer)
+					const auto coord_dim = coord_vec_type->getNumElements();
+					if(coord_dim == 1) {
+						// extract scalar
+						vk_func_name += "ff";
+						func_arg_types.push_back(builder->getFloatTy());
+						func_args.push_back(builder->CreateExtractElement(dpdx_arg, builder->getInt32(0)));
+						func_arg_types.push_back(builder->getFloatTy());
+						func_args.push_back(builder->CreateExtractElement(dpdy_arg, builder->getInt32(0)));
+					}
+					else if(coord_dim == 2) {
+						// just pass-through
+						vk_func_name += "Dv2_fDv2_f";
+						func_arg_types.push_back(dpdx_arg->getType());
+						func_args.push_back(dpdx_arg);
+						func_arg_types.push_back(dpdy_arg->getType());
+						func_args.push_back(dpdy_arg);
+					}
+					else if(coord_dim == 3) {
+						// just pass-through
+						vk_func_name += "Dv3_fDv3_f";
+						func_arg_types.push_back(dpdx_arg->getType());
+						func_args.push_back(dpdx_arg);
+						func_arg_types.push_back(dpdy_arg->getType());
+						func_args.push_back(dpdy_arg);
+					}
+					else llvm_unreachable("invalid coord dim");
+					
+					break;
+				}
+				case vulkan_sampling::LOD_TYPE::IMPLICIT_LOD_WITH_BIAS:
+					vk_func_name += "f";
+					func_arg_types.push_back(lod_or_bias_arg->getType());
+					func_args.push_back(lod_or_bias_arg);
+					break;
+				case vulkan_sampling::LOD_TYPE::EXPLICIT_LOD: {
+					Value* lod = nullptr;
+					if(is_int_lod && lod_or_bias_arg->getType()->isFloatTy()) {
+						// convert to int
+						lod = builder->CreateFPToSI(lod_or_bias_arg, llvm::Type::getInt32Ty(*ctx));
+					}
+					else if(!is_int_lod && lod_or_bias_arg->getType()->isIntegerTy()) {
+						// convert to float
+						lod = builder->CreateSIToFP(lod_or_bias_arg, llvm::Type::getFloatTy(*ctx));
+					}
+					else lod = lod_or_bias_arg;
+					
+					vk_func_name += (is_int_lod ? "i" : "f");
+					func_arg_types.push_back(lod->getType());
+					func_args.push_back(lod);
+					break;
+				}
+				default: llvm_unreachable("invalid lod type");
+			}
+			
+			// -> offset
+			vk_func_name += "b";
+			const auto is_offset_arg = (is_offset ? ConstantInt::getTrue(*ctx) : ConstantInt::getFalse(*ctx));
+			func_arg_types.push_back(is_offset_arg->getType());
+			func_args.push_back(is_offset_arg);
+			
+			if(is_offset) {
+				// offset coord_dim must be equal to image coord dim (- layer)
+				const auto coord_dim = coord_vec_type->getNumElements();
+				if(coord_dim == 1) {
+					// extract scalar
+					vk_func_name += "i";
+					auto extracted_arg = builder->CreateExtractElement(offset_arg, builder->getInt32(0));
+					func_arg_types.push_back(extracted_arg->getType());
+					func_args.push_back(extracted_arg);
+				}
+				else if(coord_dim == 2) {
+					// just pass-through
+					vk_func_name += "Dv2_i";
+					func_arg_types.push_back(offset_arg->getType());
+					func_args.push_back(offset_arg);
+				}
+				else if(coord_dim == 3) {
+					// just pass-through
+					vk_func_name += "Dv3_i";
+					func_arg_types.push_back(offset_arg->getType());
+					func_args.push_back(offset_arg);
+				}
+				else llvm_unreachable("invalid coord dim");
+			}
+			// else: no arg
+			
+			// -> sample idx
+			if(is_msaa) {
+				if(!sample_arg->getType()->isIntegerTy()) {
+					ctx->emitError(&I, "msaa sample index must be integer");
+					return;
+				}
+				
+				vk_func_name += "i";
+				func_arg_types.push_back(sample_arg->getType());
+				func_args.push_back(sample_arg);
+			}
+			
+			// -> depth compare
+			if(is_depth && is_compare) {
+				if(!compare_value_arg->getType()->isFloatTy()) {
+					ctx->emitError(&I, "compare value must be float");
+					return;
+				}
+				
+				// return type is always a scalar float
+				ret_type = Type::getFloatTy(*ctx);
+				
+				vk_func_name += "f";
+				func_arg_types.push_back(compare_value_arg->getType());
+				func_args.push_back(compare_value_arg);
+			}
+			
+			// create the vulkan call
+			// NOTE: always returns a vector4
+			auto read_func = get_or_create_spirv_function(vk_func_name, ret_type, func_arg_types, true);
+			llvm::CallInst* read_call = builder->CreateCall(read_func, func_args);
+			read_call->setConvergent();
+			read_call->setOnlyAccessesArgMemory();
+			read_call->setDoesNotThrow();
+			read_call->setOnlyReadsMemory(); // all reads are readonly (can be optimized away if unused)
+			read_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			read_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			// if this is a depth compare, the return type is a float -> create a float4
+			llvm::Value* read_call_result = read_call;
+			if(is_depth && is_compare) {
+				read_call_result = UndefValue::get(llvm::FixedVectorType::get(llvm::Type::getFloatTy(*ctx), 4));
+				read_call_result = builder->CreateInsertElement(read_call_result, read_call, builder->getInt32(0));
+				// NOTE: rest of vector is undef/zero (and will be stripped away again anyways)
+			}
+			
+			//
+			I.replaceAllUsesWith(read_call_result);
+			I.eraseFromParent();
+			
+			//
+			simplify_image_handle(img_handle_arg);
+		}
+		
+		void handle_write_image(Instruction& I,
+								const StringRef& func_name,
+								llvm::Value* img_handle_arg,
+								const COMPUTE_IMAGE_TYPE& full_image_type,
+								const COMPUTE_IMAGE_TYPE& image_type,
+								const COMPUTE_IMAGE_TYPE& format_type,
+								const COMPUTE_IMAGE_TYPE& data_type,
+								llvm::Value* /* rt_image_type */,
+								const bool& is_normalized,
+								const uint32_t& image_channel_count,
+								llvm::Value* coord_arg,
+								llvm::Value* layer_arg,
+								llvm::Value* lod_arg,
+								const bool is_lod,
+								llvm::Value* data_arg) override {
+			SmallVector<llvm::Type*, 8> func_arg_types;
+			SmallVector<llvm::Value*, 8> func_args;
+			
+			// NOTE: write call will be constructed as follows ([arg] are optional args):
+			// write(image, coord_with_layer, data,
+			//       // NOTE: only explicit lod or no lod
+			//       lod_type, [lod_arg_0])
+			
+			//// more arg checking
+			auto coord_vec_type = dyn_cast_or_null<VectorType>(coord_arg->getType());
+			if(!coord_vec_type) {
+				ctx->emitError(&I, "invalid image coordinate argument (cast to vector failed)");
+				return;
+			}
+			
+			const auto coord_type = coord_vec_type->getElementType();
+			if(!coord_type->isIntegerTy()) {
+				ctx->emitError(&I, "coordinate type must be integer");
+				return;
+			}
+			
+			std::string vk_func_name, dtype;
+			if(func_name.endswith(".float")) {
+				vk_func_name = "_Z12write_imagef";
+				dtype = "f";
+			}
+			else if(func_name.endswith(".int")) {
+				vk_func_name = "_Z12write_imagei";
+				dtype = "i";
+			}
+			else if(func_name.endswith(".uint")) {
+				vk_func_name = "_Z13write_imageui";
+				dtype = "j";
+			}
+			else if(func_name.endswith(".half")) {
+				vk_func_name = "_Z12write_imageh";
+				dtype = "h";
+			}
+			// unknown -> ignore
+			else return;
+			
+			//// func replacement
+			// -> geom
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			std::string geom = geom_cstr;
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+			const auto is_msaa = has_flag<COMPUTE_IMAGE_TYPE::FLAG_MSAA>(image_type);
+			//const auto is_cube = has_flag<COMPUTE_IMAGE_TYPE::FLAG_CUBE>(image_type);
+			//const auto is_depth = has_flag<COMPUTE_IMAGE_TYPE::FLAG_DEPTH>(image_type);
+			
+			vk_func_name += geom;
+			
+			// TODO: vulkan/spir-v can write cube, depth, msaa images ?
+			
+			func_arg_types.push_back(img_handle_arg->getType());
+			func_args.push_back(img_handle_arg);
+			
+			// -> coord
+			handle_vk_coord(I,
+							coord_arg,
+							layer_arg,
+							is_array,
+							is_msaa,
+							false,
+							true, // must always have int coords for writes
+							vk_func_name,
+							func_arg_types,
+							func_args);
+			
+			// -> data
+			// data is always a vector4
+			vk_func_name += "Dv4_" + dtype;
+			func_arg_types.push_back(data_arg->getType());
+			func_args.push_back(data_arg);
+			
+			// -> lod
+			vk_func_name += "i";
+			func_arg_types.push_back(Type::getInt32Ty(*ctx));
+			func_args.push_back(ConstantInt::get(Type::getInt32Ty(*ctx),
+												 uint32_t(is_lod ?
+														  vulkan_sampling::LOD_TYPE::EXPLICIT_LOD :
+														  vulkan_sampling::LOD_TYPE::NO_LOD)));
+			if(is_lod) {
+				// always int
+				vk_func_name += "i";
+				func_arg_types.push_back(lod_arg->getType());
+				func_args.push_back(lod_arg);
+			}
+			
+			// -> sample idx
+			// TODO: !
+			
+			// create the vulkan call
+			auto write_func = get_or_create_spirv_function(vk_func_name, builder->getVoidTy(), func_arg_types, false);
+			llvm::CallInst* write_call = builder->CreateCall(write_func, func_args);
+			write_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+			write_call->setCallingConv(CallingConv::FLOOR_FUNC);
+			
+			//
+			I.replaceAllUsesWith(write_call);
+			I.eraseFromParent();
+			
+			//
+			simplify_image_handle(img_handle_arg);
+		}
+		
+		// in cases where the image handle is acquired through image arrays,
+		// replace any (bitcast-)gep-(bitcast-)load(-inttoptr) chains with a floor.image_array_load call
+		// this is necessary, because we need to handle these specially on the SPIR-V side
+		void simplify_image_handle(llvm::Value* handle) {
+			llvm::Value* img_handle = handle;
+			
+			// loaded handle might be "int-to-ptr" casted
+			IntToPtrInst* ITPtrI = dyn_cast_or_null<IntToPtrInst>(handle);
+			if (ITPtrI) {
+				img_handle = ITPtrI->getOperand(0);
+			}
+			
+			// directly abort if not a load (array element is always loaded)
+			LoadInst* LI = dyn_cast<LoadInst>(img_handle);
+			if(!LI) return;
+			
+			// load operand might be bitcasted
+			auto load_ptr_op = LI->getPointerOperand();
+			BitCastInst* load_ptr_op_bc = dyn_cast_or_null<BitCastInst>(load_ptr_op);
+			if (load_ptr_op_bc) {
+				load_ptr_op = load_ptr_op_bc->getOperand(0);
+			}
+			
+			// array element ptr must have come from a GEP
+			GetElementPtrInst* array_elem_gep = dyn_cast<GetElementPtrInst>(load_ptr_op);
+			if(!array_elem_gep) return;
+			
+			// GEP src pointer
+			auto src_ptr = array_elem_gep->getPointerOperand();
+			auto array_type_ = src_ptr->getType()->getPointerElementType();
+			if(!array_type_->isArrayTy()) return;
+			ArrayType* array_type = cast<ArrayType>(array_type_);
+			
+			// check if this is a 2D array, in which case there is another load
+			LoadInst* outer_load = dyn_cast<LoadInst>(src_ptr);
+			GetElementPtrInst* outer_array_elem_gep = nullptr;
+			bool is_2d_array = false;
+			if (outer_load) {
+				is_2d_array = true;
+				auto outer_load_ptr_op = outer_load->getPointerOperand();
+				// might be bitcasted as well
+				BitCastInst* outer_load_ptr_op_bc = dyn_cast_or_null<BitCastInst>(outer_load_ptr_op);
+				if (outer_load_ptr_op_bc) {
+					outer_load_ptr_op = outer_load_ptr_op_bc->getOperand(0);
+				}
+				
+				// again: array element ptr must have come from a GEP
+				outer_array_elem_gep = dyn_cast<GetElementPtrInst>(outer_load_ptr_op);
+				if (!outer_array_elem_gep) return;
+				
+				// actual GEP src pointer
+				src_ptr = outer_array_elem_gep->getPointerOperand();
+				array_type_ = src_ptr->getType()->getPointerElementType();
+				if (!array_type_->isArrayTy()) return;
+				array_type = cast<ArrayType>(array_type_);
+			}
+			
+			// check if it started out from a bitcast (used by dynamic indexing)
+			BitCastInst* BC = dyn_cast<BitCastInst>(src_ptr);
+			ArrayType* src_array_type = nullptr;
+			Value* img_array = nullptr;
+			if(BC) {
+				// abort if bitcast src isn't [N * something]*
+				img_array = BC->getOperand(0);
+				auto src_array_ptr_type = BC->getSrcTy();
+				if(!src_array_ptr_type->isPointerTy() ||
+				   !src_array_ptr_type->getPointerElementType()->isArrayTy()) {
+					return;
+				}
+				src_array_type = cast<ArrayType>(src_array_ptr_type->getPointerElementType());
+				
+				// abort if array elem type is a pointer (we're expecting [N x %"class.floor_image::const_image"])
+				if(array_type->getArrayElementType()->isPointerTy()) {
+					return;
+				}
+			} else {
+				// else: constant indexing (just a gep + load)
+				img_array = src_ptr;
+				src_array_type = array_type;
+			}
+			
+			// abort if not an opaque ptr type in an address space
+			// TODO: opaque type check?
+			llvm::Type* img_type = nullptr;
+			if (!is_2d_array) {
+				if (!src_array_type->getArrayElementType()->isPointerTy() ||
+					src_array_type->getArrayElementType()->getPointerAddressSpace() == 0) {
+					return;
+				}
+				img_type = src_array_type->getArrayElementType();
+			} else {
+				if (!src_array_type->getArrayElementType()->isPointerTy() ||
+					!src_array_type->getArrayElementType()->getPointerElementType()->isArrayTy() ||
+					!src_array_type->getArrayElementType()->getPointerElementType()->getArrayElementType()->isPointerTy() ||
+					src_array_type->getArrayElementType()->getPointerElementType()->getArrayElementType()->getPointerAddressSpace() == 0) {
+					return;
+				}
+				img_type = src_array_type->getArrayElementType()->getPointerElementType()->getArrayElementType();
+			}
+			
+			// create the call
+			std::vector<Value*> params {
+				img_array
+			};
+			// GEP ptr, 0, (%outer_idx,) %idx, ... -> want third operand(s)
+			if (is_2d_array) {
+				params.push_back(outer_array_elem_gep->getOperand(2));
+			}
+			params.push_back(array_elem_gep->getOperand(2));
+			
+			std::vector<Type*> param_types;
+			for (const auto& param : params) {
+				param_types.push_back(param->getType());
+			}
+			
+			const auto func_type = llvm::FunctionType::get(img_type, param_types, false);
+			
+			auto handle_instr = cast<Instruction>(handle);
+			std::string elem_counts_str = "." + std::to_string(src_array_type->getNumElements());
+			if (is_2d_array) {
+				auto inner_arr_type = src_array_type = cast<ArrayType>(src_array_type->getArrayElementType()->getPointerElementType());
+				elem_counts_str += "." + std::to_string(inner_arr_type->getNumElements());
+			}
+			auto CI = CallInst::Create(M->getOrInsertFunction("floor.image_array_load." +
+															  img_type->getPointerElementType()->getStructName().str() +
+															  elem_counts_str,
+															  func_type),
+									   params, "imgarrld", handle_instr);
+			
+			// if dynamic access (bitcast), then bitcast has the debug location, use gep location otherwise
+			CI->setDebugLoc(BC ? BC->getDebugLoc() : array_elem_gep->getDebugLoc());
+			
+			handle_instr->replaceAllUsesWith(CI);
+			handle_instr->eraseFromParent();
+		}
+		
+		void handle_get_image_dim(Instruction& I,
+								  const StringRef& func_name,
+								  llvm::Value* img_handle_arg,
+								  const COMPUTE_IMAGE_TYPE& /* full_image_type */,
+								  const COMPUTE_IMAGE_TYPE& image_type,
+								  llvm::Value* lod_arg) override {
+			// gather info
+			const auto dim_count = image_dim_count(image_type);
+			const auto is_array = has_flag<COMPUTE_IMAGE_TYPE::FLAG_ARRAY>(image_type);
+
+			const auto geom_cstr = type_to_geom(image_type);
+			if (!geom_cstr) {
+				ctx->emitError(&I, "unknown or incorrect image type");
+				return;
+			}
+			const std::string geom = geom_cstr;
+
+			// query/get function base
+			const auto query_image = [&](const std::string& query_name) {
+				SmallVector<llvm::Type*, 8> func_arg_types;
+				SmallVector<llvm::Value*, 8> func_args;
+				func_arg_types.push_back(img_handle_arg->getType());
+				func_args.push_back(img_handle_arg);
+				func_arg_types.push_back(lod_arg->getType());
+				func_args.push_back(lod_arg);
+				
+				// -> build get func name
+				const std::string get_func_name = query_name + geom + "i";
+				
+				// create the air call
+				const auto ret_type = llvm::Type::getInt32Ty(*ctx);
+				auto get_func = get_or_create_spirv_function(get_func_name, ret_type, func_arg_types, true);
+				llvm::CallInst* get_call = builder->CreateCall(get_func, func_args);
+				get_call->setConvergent();
+				get_call->setOnlyAccessesArgMemory();
+				get_call->setDoesNotThrow();
+				get_call->setOnlyReadsMemory(); // all get_* calls are readonly (can be optimized away if unused)
+				get_call->setDebugLoc(I.getDebugLoc()); // keep debug loc
+				get_call->setCallingConv(CallingConv::FLOOR_FUNC);
+				return get_call;
+			};
+			
+			// we have to a return a full image dim query for all dims of the image (type dependent)
+			// order is: width [, height] [, depth], [, layer_count]
+			// non-existing dims are set to 0
+			const auto ret_type = llvm::FixedVectorType::get(llvm::Type::getInt32Ty(*ctx), 4);
+			llvm::Value* ret_vec = UndefValue::get(ret_type);
+			uint32_t ret_vec_idx = 0;
+			// all images have a width
+			ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z15get_image_width"), builder->getInt32(ret_vec_idx++));
+			if (dim_count >= 2) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z16get_image_height"), builder->getInt32(ret_vec_idx++));
+			}
+			if (dim_count >= 3) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z15get_image_depth"), builder->getInt32(ret_vec_idx++));
+			}
+			if (is_array) {
+				ret_vec = builder->CreateInsertElement(ret_vec, query_image("_Z20get_image_array_size"), builder->getInt32(ret_vec_idx++));
+			}
+			// NOTE: while cube maps technically have 6 layers, this number is not stored in the image dim
+			
+			// fill remaining components with 0
+			for (uint32_t vec_idx = ret_vec_idx; vec_idx < 4; ++vec_idx) {
+				ret_vec = builder->CreateInsertElement(ret_vec, builder->getInt32(0), builder->getInt32(ret_vec_idx++));
+			}
+			
+			//
+			I.replaceAllUsesWith(ret_vec);
+			I.eraseFromParent();
+		}
+		
+	};
+}
+
+char VulkanImage::ID = 0;
+INITIALIZE_PASS_BEGIN(VulkanImage, "VulkanImage", "VulkanImage Pass", false, false)
+INITIALIZE_PASS_END(VulkanImage, "VulkanImage", "VulkanImage Pass", false, false)
+
+FunctionPass *llvm::createVulkanImagePass(const uint32_t image_capabilities) {
+	return new VulkanImage(image_capabilities);
+}
diff --git a/llvm/lib/Transforms/LibFloor/cfg/LICENSE b/llvm/lib/Transforms/LibFloor/cfg/LICENSE
new file mode 100644
index 000000000000..4362b49151d7
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/cfg/LICENSE
@@ -0,0 +1,502 @@
+                  GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 2.1, February 1999
+
+ Copyright (C) 1991, 1999 Free Software Foundation, Inc.
+ 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+[This is the first released version of the Lesser GPL.  It also counts
+ as the successor of the GNU Library Public License, version 2, hence
+ the version number 2.1.]
+
+                            Preamble
+
+  The licenses for most software are designed to take away your
+freedom to share and change it.  By contrast, the GNU General Public
+Licenses are intended to guarantee your freedom to share and change
+free software--to make sure the software is free for all its users.
+
+  This license, the Lesser General Public License, applies to some
+specially designated software packages--typically libraries--of the
+Free Software Foundation and other authors who decide to use it.  You
+can use it too, but we suggest you first think carefully about whether
+this license or the ordinary General Public License is the better
+strategy to use in any particular case, based on the explanations below.
+
+  When we speak of free software, we are referring to freedom of use,
+not price.  Our General Public Licenses are designed to make sure that
+you have the freedom to distribute copies of free software (and charge
+for this service if you wish); that you receive source code or can get
+it if you want it; that you can change the software and use pieces of
+it in new free programs; and that you are informed that you can do
+these things.
+
+  To protect your rights, we need to make restrictions that forbid
+distributors to deny you these rights or to ask you to surrender these
+rights.  These restrictions translate to certain responsibilities for
+you if you distribute copies of the library or if you modify it.
+
+  For example, if you distribute copies of the library, whether gratis
+or for a fee, you must give the recipients all the rights that we gave
+you.  You must make sure that they, too, receive or can get the source
+code.  If you link other code with the library, you must provide
+complete object files to the recipients, so that they can relink them
+with the library after making changes to the library and recompiling
+it.  And you must show them these terms so they know their rights.
+
+  We protect your rights with a two-step method: (1) we copyright the
+library, and (2) we offer you this license, which gives you legal
+permission to copy, distribute and/or modify the library.
+
+  To protect each distributor, we want to make it very clear that
+there is no warranty for the free library.  Also, if the library is
+modified by someone else and passed on, the recipients should know
+that what they have is not the original version, so that the original
+author's reputation will not be affected by problems that might be
+introduced by others.
+
+  Finally, software patents pose a constant threat to the existence of
+any free program.  We wish to make sure that a company cannot
+effectively restrict the users of a free program by obtaining a
+restrictive license from a patent holder.  Therefore, we insist that
+any patent license obtained for a version of the library must be
+consistent with the full freedom of use specified in this license.
+
+  Most GNU software, including some libraries, is covered by the
+ordinary GNU General Public License.  This license, the GNU Lesser
+General Public License, applies to certain designated libraries, and
+is quite different from the ordinary General Public License.  We use
+this license for certain libraries in order to permit linking those
+libraries into non-free programs.
+
+  When a program is linked with a library, whether statically or using
+a shared library, the combination of the two is legally speaking a
+combined work, a derivative of the original library.  The ordinary
+General Public License therefore permits such linking only if the
+entire combination fits its criteria of freedom.  The Lesser General
+Public License permits more lax criteria for linking other code with
+the library.
+
+  We call this license the "Lesser" General Public License because it
+does Less to protect the user's freedom than the ordinary General
+Public License.  It also provides other free software developers Less
+of an advantage over competing non-free programs.  These disadvantages
+are the reason we use the ordinary General Public License for many
+libraries.  However, the Lesser license provides advantages in certain
+special circumstances.
+
+  For example, on rare occasions, there may be a special need to
+encourage the widest possible use of a certain library, so that it becomes
+a de-facto standard.  To achieve this, non-free programs must be
+allowed to use the library.  A more frequent case is that a free
+library does the same job as widely used non-free libraries.  In this
+case, there is little to gain by limiting the free library to free
+software only, so we use the Lesser General Public License.
+
+  In other cases, permission to use a particular library in non-free
+programs enables a greater number of people to use a large body of
+free software.  For example, permission to use the GNU C Library in
+non-free programs enables many more people to use the whole GNU
+operating system, as well as its variant, the GNU/Linux operating
+system.
+
+  Although the Lesser General Public License is Less protective of the
+users' freedom, it does ensure that the user of a program that is
+linked with the Library has the freedom and the wherewithal to run
+that program using a modified version of the Library.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.  Pay close attention to the difference between a
+"work based on the library" and a "work that uses the library".  The
+former contains code derived from the library, whereas the latter must
+be combined with the library in order to run.
+
+                  GNU LESSER GENERAL PUBLIC LICENSE
+   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+  0. This License Agreement applies to any software library or other
+program which contains a notice placed by the copyright holder or
+other authorized party saying it may be distributed under the terms of
+this Lesser General Public License (also called "this License").
+Each licensee is addressed as "you".
+
+  A "library" means a collection of software functions and/or data
+prepared so as to be conveniently linked with application programs
+(which use some of those functions and data) to form executables.
+
+  The "Library", below, refers to any such software library or work
+which has been distributed under these terms.  A "work based on the
+Library" means either the Library or any derivative work under
+copyright law: that is to say, a work containing the Library or a
+portion of it, either verbatim or with modifications and/or translated
+straightforwardly into another language.  (Hereinafter, translation is
+included without limitation in the term "modification".)
+
+  "Source code" for a work means the preferred form of the work for
+making modifications to it.  For a library, complete source code means
+all the source code for all modules it contains, plus any associated
+interface definition files, plus the scripts used to control compilation
+and installation of the library.
+
+  Activities other than copying, distribution and modification are not
+covered by this License; they are outside its scope.  The act of
+running a program using the Library is not restricted, and output from
+such a program is covered only if its contents constitute a work based
+on the Library (independent of the use of the Library in a tool for
+writing it).  Whether that is true depends on what the Library does
+and what the program that uses the Library does.
+
+  1. You may copy and distribute verbatim copies of the Library's
+complete source code as you receive it, in any medium, provided that
+you conspicuously and appropriately publish on each copy an
+appropriate copyright notice and disclaimer of warranty; keep intact
+all the notices that refer to this License and to the absence of any
+warranty; and distribute a copy of this License along with the
+Library.
+
+  You may charge a fee for the physical act of transferring a copy,
+and you may at your option offer warranty protection in exchange for a
+fee.
+
+  2. You may modify your copy or copies of the Library or any portion
+of it, thus forming a work based on the Library, and copy and
+distribute such modifications or work under the terms of Section 1
+above, provided that you also meet all of these conditions:
+
+    a) The modified work must itself be a software library.
+
+    b) You must cause the files modified to carry prominent notices
+    stating that you changed the files and the date of any change.
+
+    c) You must cause the whole of the work to be licensed at no
+    charge to all third parties under the terms of this License.
+
+    d) If a facility in the modified Library refers to a function or a
+    table of data to be supplied by an application program that uses
+    the facility, other than as an argument passed when the facility
+    is invoked, then you must make a good faith effort to ensure that,
+    in the event an application does not supply such function or
+    table, the facility still operates, and performs whatever part of
+    its purpose remains meaningful.
+
+    (For example, a function in a library to compute square roots has
+    a purpose that is entirely well-defined independent of the
+    application.  Therefore, Subsection 2d requires that any
+    application-supplied function or table used by this function must
+    be optional: if the application does not supply it, the square
+    root function must still compute square roots.)
+
+These requirements apply to the modified work as a whole.  If
+identifiable sections of that work are not derived from the Library,
+and can be reasonably considered independent and separate works in
+themselves, then this License, and its terms, do not apply to those
+sections when you distribute them as separate works.  But when you
+distribute the same sections as part of a whole which is a work based
+on the Library, the distribution of the whole must be on the terms of
+this License, whose permissions for other licensees extend to the
+entire whole, and thus to each and every part regardless of who wrote
+it.
+
+Thus, it is not the intent of this section to claim rights or contest
+your rights to work written entirely by you; rather, the intent is to
+exercise the right to control the distribution of derivative or
+collective works based on the Library.
+
+In addition, mere aggregation of another work not based on the Library
+with the Library (or with a work based on the Library) on a volume of
+a storage or distribution medium does not bring the other work under
+the scope of this License.
+
+  3. You may opt to apply the terms of the ordinary GNU General Public
+License instead of this License to a given copy of the Library.  To do
+this, you must alter all the notices that refer to this License, so
+that they refer to the ordinary GNU General Public License, version 2,
+instead of to this License.  (If a newer version than version 2 of the
+ordinary GNU General Public License has appeared, then you can specify
+that version instead if you wish.)  Do not make any other change in
+these notices.
+
+  Once this change is made in a given copy, it is irreversible for
+that copy, so the ordinary GNU General Public License applies to all
+subsequent copies and derivative works made from that copy.
+
+  This option is useful when you wish to copy part of the code of
+the Library into a program that is not a library.
+
+  4. You may copy and distribute the Library (or a portion or
+derivative of it, under Section 2) in object code or executable form
+under the terms of Sections 1 and 2 above provided that you accompany
+it with the complete corresponding machine-readable source code, which
+must be distributed under the terms of Sections 1 and 2 above on a
+medium customarily used for software interchange.
+
+  If distribution of object code is made by offering access to copy
+from a designated place, then offering equivalent access to copy the
+source code from the same place satisfies the requirement to
+distribute the source code, even though third parties are not
+compelled to copy the source along with the object code.
+
+  5. A program that contains no derivative of any portion of the
+Library, but is designed to work with the Library by being compiled or
+linked with it, is called a "work that uses the Library".  Such a
+work, in isolation, is not a derivative work of the Library, and
+therefore falls outside the scope of this License.
+
+  However, linking a "work that uses the Library" with the Library
+creates an executable that is a derivative of the Library (because it
+contains portions of the Library), rather than a "work that uses the
+library".  The executable is therefore covered by this License.
+Section 6 states terms for distribution of such executables.
+
+  When a "work that uses the Library" uses material from a header file
+that is part of the Library, the object code for the work may be a
+derivative work of the Library even though the source code is not.
+Whether this is true is especially significant if the work can be
+linked without the Library, or if the work is itself a library.  The
+threshold for this to be true is not precisely defined by law.
+
+  If such an object file uses only numerical parameters, data
+structure layouts and accessors, and small macros and small inline
+functions (ten lines or less in length), then the use of the object
+file is unrestricted, regardless of whether it is legally a derivative
+work.  (Executables containing this object code plus portions of the
+Library will still fall under Section 6.)
+
+  Otherwise, if the work is a derivative of the Library, you may
+distribute the object code for the work under the terms of Section 6.
+Any executables containing that work also fall under Section 6,
+whether or not they are linked directly with the Library itself.
+
+  6. As an exception to the Sections above, you may also combine or
+link a "work that uses the Library" with the Library to produce a
+work containing portions of the Library, and distribute that work
+under terms of your choice, provided that the terms permit
+modification of the work for the customer's own use and reverse
+engineering for debugging such modifications.
+
+  You must give prominent notice with each copy of the work that the
+Library is used in it and that the Library and its use are covered by
+this License.  You must supply a copy of this License.  If the work
+during execution displays copyright notices, you must include the
+copyright notice for the Library among them, as well as a reference
+directing the user to the copy of this License.  Also, you must do one
+of these things:
+
+    a) Accompany the work with the complete corresponding
+    machine-readable source code for the Library including whatever
+    changes were used in the work (which must be distributed under
+    Sections 1 and 2 above); and, if the work is an executable linked
+    with the Library, with the complete machine-readable "work that
+    uses the Library", as object code and/or source code, so that the
+    user can modify the Library and then relink to produce a modified
+    executable containing the modified Library.  (It is understood
+    that the user who changes the contents of definitions files in the
+    Library will not necessarily be able to recompile the application
+    to use the modified definitions.)
+
+    b) Use a suitable shared library mechanism for linking with the
+    Library.  A suitable mechanism is one that (1) uses at run time a
+    copy of the library already present on the user's computer system,
+    rather than copying library functions into the executable, and (2)
+    will operate properly with a modified version of the library, if
+    the user installs one, as long as the modified version is
+    interface-compatible with the version that the work was made with.
+
+    c) Accompany the work with a written offer, valid for at
+    least three years, to give the same user the materials
+    specified in Subsection 6a, above, for a charge no more
+    than the cost of performing this distribution.
+
+    d) If distribution of the work is made by offering access to copy
+    from a designated place, offer equivalent access to copy the above
+    specified materials from the same place.
+
+    e) Verify that the user has already received a copy of these
+    materials or that you have already sent this user a copy.
+
+  For an executable, the required form of the "work that uses the
+Library" must include any data and utility programs needed for
+reproducing the executable from it.  However, as a special exception,
+the materials to be distributed need not include anything that is
+normally distributed (in either source or binary form) with the major
+components (compiler, kernel, and so on) of the operating system on
+which the executable runs, unless that component itself accompanies
+the executable.
+
+  It may happen that this requirement contradicts the license
+restrictions of other proprietary libraries that do not normally
+accompany the operating system.  Such a contradiction means you cannot
+use both them and the Library together in an executable that you
+distribute.
+
+  7. You may place library facilities that are a work based on the
+Library side-by-side in a single library together with other library
+facilities not covered by this License, and distribute such a combined
+library, provided that the separate distribution of the work based on
+the Library and of the other library facilities is otherwise
+permitted, and provided that you do these two things:
+
+    a) Accompany the combined library with a copy of the same work
+    based on the Library, uncombined with any other library
+    facilities.  This must be distributed under the terms of the
+    Sections above.
+
+    b) Give prominent notice with the combined library of the fact
+    that part of it is a work based on the Library, and explaining
+    where to find the accompanying uncombined form of the same work.
+
+  8. You may not copy, modify, sublicense, link with, or distribute
+the Library except as expressly provided under this License.  Any
+attempt otherwise to copy, modify, sublicense, link with, or
+distribute the Library is void, and will automatically terminate your
+rights under this License.  However, parties who have received copies,
+or rights, from you under this License will not have their licenses
+terminated so long as such parties remain in full compliance.
+
+  9. You are not required to accept this License, since you have not
+signed it.  However, nothing else grants you permission to modify or
+distribute the Library or its derivative works.  These actions are
+prohibited by law if you do not accept this License.  Therefore, by
+modifying or distributing the Library (or any work based on the
+Library), you indicate your acceptance of this License to do so, and
+all its terms and conditions for copying, distributing or modifying
+the Library or works based on it.
+
+  10. Each time you redistribute the Library (or any work based on the
+Library), the recipient automatically receives a license from the
+original licensor to copy, distribute, link with or modify the Library
+subject to these terms and conditions.  You may not impose any further
+restrictions on the recipients' exercise of the rights granted herein.
+You are not responsible for enforcing compliance by third parties with
+this License.
+
+  11. If, as a consequence of a court judgment or allegation of patent
+infringement or for any other reason (not limited to patent issues),
+conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot
+distribute so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you
+may not distribute the Library at all.  For example, if a patent
+license would not permit royalty-free redistribution of the Library by
+all those who receive copies directly or indirectly through you, then
+the only way you could satisfy both it and this License would be to
+refrain entirely from distribution of the Library.
+
+If any portion of this section is held invalid or unenforceable under any
+particular circumstance, the balance of the section is intended to apply,
+and the section as a whole is intended to apply in other circumstances.
+
+It is not the purpose of this section to induce you to infringe any
+patents or other property right claims or to contest validity of any
+such claims; this section has the sole purpose of protecting the
+integrity of the free software distribution system which is
+implemented by public license practices.  Many people have made
+generous contributions to the wide range of software distributed
+through that system in reliance on consistent application of that
+system; it is up to the author/donor to decide if he or she is willing
+to distribute software through any other system and a licensee cannot
+impose that choice.
+
+This section is intended to make thoroughly clear what is believed to
+be a consequence of the rest of this License.
+
+  12. If the distribution and/or use of the Library is restricted in
+certain countries either by patents or by copyrighted interfaces, the
+original copyright holder who places the Library under this License may add
+an explicit geographical distribution limitation excluding those countries,
+so that distribution is permitted only in or among countries not thus
+excluded.  In such case, this License incorporates the limitation as if
+written in the body of this License.
+
+  13. The Free Software Foundation may publish revised and/or new
+versions of the Lesser General Public License from time to time.
+Such new versions will be similar in spirit to the present version,
+but may differ in detail to address new problems or concerns.
+
+Each version is given a distinguishing version number.  If the Library
+specifies a version number of this License which applies to it and
+"any later version", you have the option of following the terms and
+conditions either of that version or of any later version published by
+the Free Software Foundation.  If the Library does not specify a
+license version number, you may choose any version ever published by
+the Free Software Foundation.
+
+  14. If you wish to incorporate parts of the Library into other free
+programs whose distribution conditions are incompatible with these,
+write to the author to ask for permission.  For software which is
+copyrighted by the Free Software Foundation, write to the Free
+Software Foundation; we sometimes make exceptions for this.  Our
+decision will be guided by the two goals of preserving the free status
+of all derivatives of our free software and of promoting the sharing
+and reuse of software generally.
+
+                            NO WARRANTY
+
+  15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO
+WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW.
+EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR
+OTHER PARTIES PROVIDE THE LIBRARY "AS IS" WITHOUT WARRANTY OF ANY
+KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE
+LIBRARY IS WITH YOU.  SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME
+THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN
+WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
+AND/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU
+FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR
+CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE
+LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING
+RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A
+FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF
+SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
+DAMAGES.
+
+                     END OF TERMS AND CONDITIONS
+
+           How to Apply These Terms to Your New Libraries
+
+  If you develop a new library, and you want it to be of the greatest
+possible use to the public, we recommend making it free software that
+everyone can redistribute and change.  You can do so by permitting
+redistribution under these terms (or, alternatively, under the terms of the
+ordinary General Public License).
+
+  To apply these terms, attach the following notices to the library.  It is
+safest to attach them to the start of each source file to most effectively
+convey the exclusion of warranty; and each file should have at least the
+"copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the library's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This library is free software; you can redistribute it and/or
+    modify it under the terms of the GNU Lesser General Public
+    License as published by the Free Software Foundation; either
+    version 2.1 of the License, or (at your option) any later version.
+
+    This library is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+    Lesser General Public License for more details.
+
+    You should have received a copy of the GNU Lesser General Public
+    License along with this library; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+
+Also add information on how to contact you by electronic and paper mail.
+
+You should also get your employer (if you work as a programmer) or your
+school, if any, to sign a "copyright disclaimer" for the library, if
+necessary.  Here is a sample; alter the names:
+
+  Yoyodyne, Inc., hereby disclaims all copyright interest in the
+  library `Frob' (a library for tweaking knobs) written by James Random Hacker.
+
+  <signature of Ty Coon>, 1 April 1990
+  Ty Coon, President of Vice
+
+That's all there is to it!
diff --git a/llvm/lib/Transforms/LibFloor/cfg/cfg_structurizer.cpp b/llvm/lib/Transforms/LibFloor/cfg/cfg_structurizer.cpp
new file mode 100644
index 000000000000..6cecb8f76e8f
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/cfg/cfg_structurizer.cpp
@@ -0,0 +1,3248 @@
+/*
+ * Copyright 2019-2021 Hans-Kristian Arntzen for Valve Corporation
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// dxil-spirv CFG structurizer adopted for LLVM use
+// ref: https://github.com/HansKristian-Work/dxil-spirv
+// @ 189cc855b471591763d9951d63e51c72649037ab
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor/cfg/cfg_structurizer.hpp"
+#include "llvm/Transforms/LibFloor/cfg/node.hpp"
+#include "llvm/IR/Instructions.h"
+#include <algorithm>
+#include <assert.h>
+
+#define LOGE printf
+#define LOGI printf
+#define LOGW printf
+
+namespace llvm {
+static std::string unique_phi_suffix() {
+  static uint32_t phi_name_counter = 0;
+  std::string suffix = "." + std::to_string(phi_name_counter);
+  ++phi_name_counter;
+  return suffix;
+}
+
+static PHINode *create_phi_node(llvm::Type *type,
+                                const uint32_t reserved_in_values,
+                                std::string name, BasicBlock &BB) {
+  auto unique_name = name + unique_phi_suffix();
+  auto first_non_phi = BB.getFirstNonPHI();
+  if (BB.empty()) {
+    // -> empty: just insert in BB
+    return llvm::PHINode::Create(type, reserved_in_values, unique_name, &BB);
+  } else if (first_non_phi == nullptr) {
+    // -> we don't have any non-PHIs in the BB, but probably other PHIs ->
+    // insert at the BB end
+    assert(BB.getTerminator() == nullptr);
+    return llvm::PHINode::Create(type, reserved_in_values, unique_name, &BB);
+  } else {
+    // -> insert before first non-PHI (after previous PHIs if there are any)
+    return llvm::PHINode::Create(type, reserved_in_values, unique_name,
+                                 first_non_phi);
+  }
+}
+
+CFGStructurizer::CFGStructurizer(CFGNode *entry, CFGNodePool &pool_,
+                                 Function &F_, LLVMContext &ctx_)
+    : entry_block(entry), pool(pool_), F(F_), ctx(ctx_) {
+  exit_block = pool.create_node("EXIT");
+}
+
+void CFGStructurizer::log_cfg_graphviz(const char *path) const {
+  FILE *file = fopen((std::string(path) + ".dot").c_str(), "w");
+  if (!file) {
+    LOGE("Failed to open graphviz dump path: %s\n", path);
+    return;
+  }
+
+  std::unordered_map<const CFGNode *, uint32_t> node_to_id;
+  uint32_t accum_id = 0;
+
+  const auto get_node_id = [&](const CFGNode *node) -> uint32_t {
+    auto itr = node_to_id.find(node);
+    if (itr == node_to_id.end()) {
+      const char *shape = nullptr;
+      if (node->merge == MergeType::Loop)
+        shape = "circle";
+      else if (node->merge == MergeType::Selection)
+        shape = "triangle";
+      else
+        shape = "box";
+
+      node_to_id[node] = ++accum_id;
+      fprintf(file, "%u [label=\"%s\", shape=\"%s\"];\n", accum_id,
+              node->name.c_str(), shape);
+      return accum_id;
+    } else
+      return itr->second;
+  };
+
+  fprintf(file, "digraph {\n");
+  for (auto index = forward_post_visit_order.size(); index; index--) {
+    auto *node = forward_post_visit_order[index - 1];
+    switch (node->ir.terminator.type) {
+    case Terminator::Type::Branch:
+      fprintf(file, "%u -> %u;\n", get_node_id(node),
+              get_node_id(node->ir.terminator.direct_block));
+      break;
+
+    case Terminator::Type::Condition:
+      fprintf(file, "%u -> %u;\n", get_node_id(node),
+              get_node_id(node->ir.terminator.true_block));
+      fprintf(file, "%u -> %u;\n", get_node_id(node),
+              get_node_id(node->ir.terminator.false_block));
+      break;
+
+    case Terminator::Type::Switch:
+      for (auto &c : node->ir.terminator.cases)
+        fprintf(file, "%u -> %u;\n", get_node_id(node), get_node_id(c.node));
+      fprintf(file, "%u -> %u;\n", get_node_id(node),
+              get_node_id(node->ir.terminator.default_node));
+      break;
+
+    default:
+      break;
+    }
+
+    if (node->merge == MergeType::Loop) {
+      if (node->pred_back_edge)
+        fprintf(file, "%u -> %u [style=\"dotted\"];\n", get_node_id(node),
+                get_node_id(node->pred_back_edge));
+      if (node->loop_merge_block)
+        fprintf(file, "%u -> %u [style=\"dashed\"];\n", get_node_id(node),
+                get_node_id(node->loop_merge_block));
+    } else if (node->merge == MergeType::Selection) {
+      if (node->selection_merge_block)
+        fprintf(file, "%u -> %u [style=\"dashed\"];\n", get_node_id(node),
+                get_node_id(node->selection_merge_block));
+    }
+  }
+
+  fprintf(file, "}\n");
+  fclose(file);
+}
+
+void CFGStructurizer::log_cfg(const char *tag) const {
+  LOGI("\n======== %s =========\n", tag);
+  for (auto index = forward_post_visit_order.size(); index; index--) {
+    auto *node = forward_post_visit_order[index - 1];
+
+    LOGI("%s:\n", node->name.c_str());
+    switch (node->ir.terminator.type) {
+    case Terminator::Type::Branch:
+      LOGI("  Branch -> %s\n", node->ir.terminator.direct_block->name.c_str());
+      break;
+
+    case Terminator::Type::Condition:
+      LOGI("  Cond -> %s | %s\n", node->ir.terminator.true_block->name.c_str(),
+           node->ir.terminator.false_block->name.c_str());
+      break;
+
+    case Terminator::Type::Return:
+      LOGI("  Return\n");
+      break;
+
+    case Terminator::Type::Unreachable:
+      LOGI("  Unreachable\n");
+      break;
+
+    case Terminator::Type::Switch:
+      LOGI("  Switch\n");
+      for (auto &c : node->ir.terminator.cases)
+        LOGI("    Case %llu -> %s\n", (uint64_t)c.value, c.node->name.c_str());
+      LOGI("    Default -> %s\n",
+           node->ir.terminator.default_node->name.c_str());
+      break;
+
+    case Terminator::Type::Kill:
+      LOGI("  Kill\n");
+      break;
+    }
+
+    switch (node->merge) {
+    case MergeType::Selection:
+      LOGI("  SelectionMerge -> %s\n",
+           node->selection_merge_block
+               ? node->selection_merge_block->name.c_str()
+               : "N/A");
+      break;
+
+    case MergeType::Loop:
+      LOGI("  LoopMerge -> %s\n", node->loop_merge_block
+                                      ? node->loop_merge_block->name.c_str()
+                                      : "N/A");
+      LOGI("    Continue -> %s\n",
+           node->pred_back_edge ? node->pred_back_edge->name.c_str() : "N/A");
+      break;
+
+    default:
+      break;
+    }
+
+    LOGI("\n");
+  }
+  LOGI("\n=====================\n");
+}
+
+//#define PHI_DEBUG
+#ifdef PHI_DEBUG
+static void validate_phi(const PHI &phi) {
+  auto incomings = phi.incoming;
+  std::sort(incomings.begin(), incomings.end(),
+            [](const IncomingValue &a, const IncomingValue &b) {
+              return a.block < b.block;
+            });
+  auto itr = std::unique(incomings.begin(), incomings.end(),
+                         [](const IncomingValue &a, const IncomingValue &b) {
+                           return a.block == b.block;
+                         });
+  if (itr != incomings.end())
+    abort();
+}
+
+static void validate_phi(const std::vector<PHI> &phis) {
+  for (auto &phi : phis)
+    validate_phi(phi);
+}
+#else
+#define validate_phi(phi) ((void)0)
+#endif
+
+void CFGStructurizer::eliminate_node_link_preds_to_succ(CFGNode *node) {
+  assert(node->succ.size() == 1);
+  auto *succ = node->succ.front();
+
+  validate_phi(succ->ir.phi);
+
+  std::vector<CFGNode *> break_nodes;
+  auto pred_copy = node->pred;
+  for (auto *pred : pred_copy) {
+    auto *break_node = pool.create_node(node->name + ".break." + pred->name);
+    break_node->ir.terminator.type = Terminator::Type::Branch;
+    break_node->ir.terminator.direct_block = succ;
+    break_node->add_branch(succ);
+    break_node->immediate_post_dominator = succ;
+    break_node->immediate_dominator = pred;
+    pred->retarget_branch(node, break_node);
+
+    break_nodes.push_back(break_node);
+
+    for (auto &phi : node->ir.phi)
+      for (auto &incoming : phi.incoming)
+        if (incoming.block == pred)
+          incoming.block = break_node;
+  }
+  assert(node->pred.empty());
+
+  for (auto &phi : succ->ir.phi) {
+    // Find incoming ID from the block we're splitting up.
+    auto incoming_itr = std::find_if(
+        phi.incoming.begin(), phi.incoming.end(),
+        [&](const IncomingValue &incoming) { return incoming.block == node; });
+    assert(incoming_itr != phi.incoming.end());
+    auto incoming_from_node = incoming_itr->value;
+    phi.incoming.erase(incoming_itr);
+
+    // Try to see if the ID is a PHI that was generated by this block.
+    auto outgoing_itr = std::find_if(
+        node->ir.phi.begin(), node->ir.phi.end(),
+        [&](const PHI &phi) { return phi.phi == incoming_from_node; });
+
+    if (outgoing_itr != node->ir.phi.end()) {
+      // If it was then we need to split up the PHI node. The break block will
+      // serve as a proxy incoming block instead.
+      phi.incoming.insert(phi.incoming.end(), outgoing_itr->incoming.begin(),
+                          outgoing_itr->incoming.end());
+      node->ir.phi.erase(outgoing_itr);
+      validate_phi(succ->ir.phi);
+    } else {
+      // A plain value is passed down to succ, most likely a constant which
+      // lives at global scope. We know this block does not generate this ID, so
+      // it must be either a value generated at global scope (constant), or a
+      // value created by a block which dominates this node, which also means it
+      // dominates all preds to this node.
+      for (auto *break_pred : break_nodes)
+        phi.incoming.push_back({break_pred, incoming_from_node});
+      validate_phi(succ->ir.phi);
+    }
+  }
+  assert(node->ir.phi.empty());
+}
+
+static bool node_has_phi_inputs_from(const CFGNode *from, const CFGNode *to) {
+  for (auto &phi : to->ir.phi)
+    for (auto &incoming : phi.incoming)
+      if (incoming.block == from)
+        return true;
+  return false;
+}
+
+void CFGStructurizer::cleanup_breaking_phi_constructs() {
+  bool did_work = false;
+
+  // There might be cases where we have a common break block from different
+  // scopes which only serves to PHI together some values before actually
+  // breaking, and passing that PHI node on to the actual break block. This
+  // causes problems because this looks very much like a merge, but it is
+  // actually not and forces validation errors. Another case is where the succ
+  // block takes PHI nodes from the breaking block only, which is relevant if
+  // only constants are somehow used in the PHI construct.
+
+  for (size_t i = forward_post_visit_order.size(); i; i--) {
+    auto *node = forward_post_visit_order[i - 1];
+
+    // Only bother with blocks which don't do anything useful work.
+    // The only opcodes they should have are PHI nodes and a direct branch.
+    if (!node->ir.operations.empty())
+      continue;
+    if (node->pred.size() <= 1)
+      continue;
+    if (node->succ.size() != 1)
+      continue;
+    if (node->ir.terminator.type != Terminator::Type::Branch)
+      continue;
+
+    auto *succ = node->succ.front();
+
+    // Checks if either the merge block or successor is sensitive to PHI
+    // somehow.
+    if (!node_has_phi_inputs_from(node, succ))
+      continue;
+
+    if (node->dominates(succ))
+      continue;
+
+    // Anything related to loop/continue blocks, we don't bother with.
+    if (node->succ_back_edge || node->pred_back_edge)
+      continue;
+
+    // This is a merge block candidate for a loop, don't split.
+    // It will only confuse things where we'll need to re-merge the split blocks
+    // anyways.
+    bool is_merge_block_candidate = false;
+    for (auto *pred : node->pred) {
+      if (pred->succ_back_edge) {
+        is_merge_block_candidate = true;
+        continue;
+      }
+    }
+
+    if (is_merge_block_candidate)
+      continue;
+
+    eliminate_node_link_preds_to_succ(node);
+    did_work = true;
+  }
+
+  if (did_work)
+    recompute_cfg();
+}
+
+bool CFGStructurizer::run() {
+  std::string graphviz_path;
+  if (const char *env = getenv("DXIL_SPIRV_GRAPHVIZ_PATH")) {
+    graphviz_path = env;
+    graphviz_path += F.getName().str() + "_";
+  }
+
+  // log_cfg("Input state");
+  if (!graphviz_path.empty()) {
+    reset_traversal();
+    visit(*entry_block);
+    auto graphviz_input = graphviz_path + ".input";
+    log_cfg_graphviz(graphviz_input.c_str());
+  }
+
+  recompute_cfg();
+
+  cleanup_breaking_phi_constructs();
+  if (!graphviz_path.empty()) {
+    auto graphviz_split = graphviz_path + ".phi-split";
+    log_cfg_graphviz(graphviz_split.c_str());
+  }
+
+  create_continue_block_ladders();
+
+  split_merge_scopes();
+  recompute_cfg();
+
+  // log_cfg("Split merge scopes");
+  if (!graphviz_path.empty()) {
+    auto graphviz_split = graphviz_path + ".split";
+    log_cfg_graphviz(graphviz_split.c_str());
+  }
+
+  // We will have generated lots of ladder blocks
+  // which might cause issues with further analysis, so
+  // nuke them as required.
+  eliminate_degenerate_blocks();
+
+  if (!graphviz_path.empty()) {
+    auto graphviz_split = graphviz_path + ".eliminate0";
+    log_cfg_graphviz(graphviz_split.c_str());
+  }
+
+  // LOGI("=== Structurize pass ===\n");
+  structurize(0);
+  update_structured_loop_merge_targets();
+
+  // log_cfg("Structurize pass 0");
+  if (!graphviz_path.empty()) {
+    auto graphviz_final = graphviz_path + ".struct0";
+    log_cfg_graphviz(graphviz_final.c_str());
+  }
+
+  // We will have generated lots of ladder blocks
+  // which might cause issues with further analysis, so
+  // nuke them as required.
+  eliminate_degenerate_blocks();
+
+  // log_cfg("Split merge scopes");
+  if (!graphviz_path.empty()) {
+    auto graphviz_split = graphviz_path + ".eliminate1";
+    log_cfg_graphviz(graphviz_split.c_str());
+  }
+
+  // LOGI("=== Structurize pass ===\n");
+  structurize(1);
+
+  validate_structured();
+  // log_cfg("Final");
+  if (!graphviz_path.empty()) {
+    auto graphviz_final = graphviz_path + ".final";
+    log_cfg_graphviz(graphviz_final.c_str());
+  }
+
+  insert_phi();
+
+  return true;
+}
+
+CFGNode *CFGStructurizer::get_entry_block() const { return entry_block; }
+
+void CFGStructurizer::create_continue_block_ladders() {
+  // It does not seem to be legal to merge directly to continue blocks.
+  // To make it possible to merge execution, we need to create a ladder block
+  // which we can merge to.
+  bool need_recompute_cfg = false;
+  for (auto *node : forward_post_visit_order) {
+    if (node->succ_back_edge && node->succ_back_edge != node) {
+      // LOGI("Creating helper pred block for continue block: %s\n",
+      // node->name.c_str());
+      create_helper_pred_block(node);
+      need_recompute_cfg = true;
+    }
+  }
+
+  if (need_recompute_cfg)
+    recompute_cfg();
+}
+
+void CFGStructurizer::update_structured_loop_merge_targets() {
+  // First, we need to do this before recomputing the CFG, since we lose
+  // normal loop merge targets when recomputing.
+  structured_loop_merge_targets.clear();
+  for (auto *node : forward_post_visit_order) {
+    if (node->loop_merge_block)
+      structured_loop_merge_targets.insert(node->loop_merge_block);
+    if (node->loop_ladder_block)
+      structured_loop_merge_targets.insert(node->loop_ladder_block);
+  }
+
+  recompute_cfg();
+
+  // Make sure we include merge blocks which are frozen merge targets in ladder
+  // blocks, which were not included in the post visit order yet.
+  for (auto *node : forward_post_visit_order) {
+    if (node->loop_merge_block)
+      structured_loop_merge_targets.insert(node->loop_merge_block);
+    if (node->loop_ladder_block)
+      structured_loop_merge_targets.insert(node->loop_ladder_block);
+  }
+}
+
+void CFGStructurizer::eliminate_degenerate_blocks() {
+  // After we create ladder blocks, we will likely end up with a lot of blocks
+  // which don't do much. We might also have created merge scenarios which
+  // should *not* merge, i.e. cleanup_breaking_phi_constructs(), except we
+  // caused it ourselves.
+
+  // Eliminate bottom-up. First eliminate B, in A -> B -> C, where B contributes
+  // nothing.
+  bool did_work = false;
+  for (auto *node : forward_post_visit_order) {
+    if (node->ir.operations.empty() && node->ir.phi.empty() &&
+        !node->pred_back_edge && !node->succ_back_edge &&
+        node->succ.size() == 1 &&
+        node->ir.terminator.type == Terminator::Type::Branch &&
+        node->merge == MergeType::None &&
+        // Loop merge targets are sacred, and must not be removed.
+        structured_loop_merge_targets.count(node) == 0 &&
+        !node_has_phi_inputs_from(node, node->succ.front())) {
+      // If any pred is a continue block, this block is also load-bearing, since
+      // it can be used as a merge block.
+      if (std::find_if(node->pred.begin(), node->pred.end(),
+                       [](const CFGNode *n) {
+                         return n->succ_back_edge != nullptr;
+                       }) != node->pred.end()) {
+        continue;
+      }
+
+      // If any succ is a continue block, this block is also load-bearing, since
+      // it can be used as a merge block (merge-to-continue ladder).
+      if (std::find_if(node->succ.begin(), node->succ.end(),
+                       [](const CFGNode *n) {
+                         return n->succ_back_edge != nullptr;
+                       }) != node->succ.end()) {
+        continue;
+      }
+
+      // If succ uses this block as an incoming block, we should keep the block
+      // around. We're only really interested in eliminating degenerate ladder
+      // blocks, which generally do not deal with PHI.
+      auto *succ = node->succ.front();
+      if (std::find_if(
+              succ->ir.phi.begin(), succ->ir.phi.end(), [node](const PHI &phi) {
+                return std::find_if(phi.incoming.begin(), phi.incoming.end(),
+                                    [node](const IncomingValue &incoming) {
+                                      return incoming.block == node;
+                                    }) != phi.incoming.end();
+              }) != succ->ir.phi.end()) {
+        continue;
+      }
+
+      if (node->pred.size() == 1 && node->post_dominates(node->pred.front())) {
+        // Trivial case.
+        did_work = true;
+        auto *pred = node->pred.front();
+        pred->retarget_branch(node, succ);
+      } else if (node->pred.size() >= 2 &&
+                 !node->dominates(node->succ.front()) &&
+                 node->succ.front()->post_dominates(node)) {
+        // If we have two or more preds, we have to be really careful.
+        // If this node is on a breaking path, without being important for
+        // merging control flow, it is fine to eliminate the block.
+        if (control_flow_is_escaping(node, node->succ.front()) &&
+            !block_is_load_bearing(node, node->succ.front())) {
+          did_work = true;
+          auto tmp_pred = node->pred;
+          for (auto *pred : tmp_pred)
+            pred->retarget_branch(node, node->succ.front());
+        }
+      }
+    }
+  }
+
+  if (did_work)
+    recompute_cfg();
+}
+
+void CFGStructurizer::prune_dead_preds() {
+  // We do not want to see unreachable preds.
+  // Having a pred means we need to map it to an incoming value when dealing
+  // with PHI.
+  for (auto *node : forward_post_visit_order) {
+    auto itr = std::remove_if(
+        node->pred.begin(), node->pred.end(),
+        [&](const CFGNode *node) { return reachable_nodes.count(node) == 0; });
+    node->pred.erase(itr, node->pred.end());
+  }
+}
+
+static void rewrite_consumed_ids(IRBlock &ir, Value *from, Value *to) {
+  // TODO: do this properly
+  assert(false && "rewrite_consumed_ids not implemented yet");
+  for (auto *op : ir.operations) {
+    for (auto &arg : op->operands()) {
+      if (arg == from)
+        arg = to;
+    }
+  }
+
+  if (ir.terminator.condition == from)
+    ir.terminator.condition = to;
+  if (ir.terminator.return_value == from)
+    ir.terminator.return_value = to;
+}
+
+void CFGStructurizer::fixup_broken_value_dominance() {
+  struct Origin {
+    CFGNode *node;
+  };
+
+  std::unordered_map<Value *, Origin> origin;
+  std::unordered_map<Value *, std::vector<CFGNode *>> id_to_non_local_consumers;
+
+  // First, scan through all blocks and figure out which block creates an ID.
+  for (auto *node : forward_post_visit_order) {
+    for (auto *op : node->ir.operations)
+      if (op)
+        origin[op] = {node};
+    for (auto &phi : node->ir.phi)
+      origin[phi.phi] = {node};
+  }
+
+  const auto sort_unique_node_vector = [](std::vector<CFGNode *> &nodes) {
+    // Fixup nodes in order.
+    std::sort(nodes.begin(), nodes.end(),
+              [](const CFGNode *a, const CFGNode *b) -> bool {
+                return a->forward_post_visit_order >
+                       b->forward_post_visit_order;
+              });
+    nodes.erase(std::unique(nodes.begin(), nodes.end()), nodes.end());
+  };
+
+  const auto mark_node_value_access = [&](CFGNode *node, Value *val) {
+    auto origin_itr = origin.find(val);
+    if (origin_itr == origin.end())
+      return;
+
+    auto *origin_node = origin_itr->second.node;
+    if (!origin_node->dominates(node)) {
+      // We have a problem. Mark that we need to rewrite a certain variable.
+      id_to_non_local_consumers[val].push_back(node);
+    }
+  };
+
+  // Now, scan through all blocks and figure out which values are consumed in
+  // different blocks.
+  for (auto *node : forward_post_visit_order) {
+    for (auto *op : node->ir.operations) {
+      for (auto &arg : op->operands())
+        mark_node_value_access(node, arg);
+    }
+
+    // Incoming PHI values are handled elsewhere by modifying the incoming block
+    // to the creating block. Ignore these kinds of usage here.
+
+    if (node->ir.terminator.condition != nullptr)
+      mark_node_value_access(node, node->ir.terminator.condition);
+    if (node->ir.terminator.return_value != nullptr)
+      mark_node_value_access(node, node->ir.terminator.return_value);
+  }
+
+  // Resolve these broken PHIs by using OpVariable. It is the simplest solution,
+  // and this is a very rare case to begin with.
+  struct Rewrite {
+    Value *val = nullptr;
+    const std::vector<CFGNode *> *consumers;
+  };
+  std::vector<Rewrite> rewrites;
+  rewrites.reserve(id_to_non_local_consumers.size());
+
+  for (auto &pair : id_to_non_local_consumers) {
+    sort_unique_node_vector(pair.second);
+    rewrites.push_back({pair.first, &pair.second});
+  }
+
+#if 0 // ordering isn't needed here
+  // Ensure ordering so that output remains stable.
+  std::sort(rewrites.begin(), rewrites.end(),
+            [](const Rewrite &a, const Rewrite &b) { return a.id < b.id; });
+#endif
+
+  for (auto &rewrite : rewrites) {
+    auto &orig = origin[rewrite.val];
+    AllocaInst *alloca_var = nullptr;
+    if (entry_block->BB.empty()) {
+      alloca_var = new AllocaInst(rewrite.val->getType(), 0, "rewrite.alloca",
+                                  &entry_block->BB);
+    } else {
+      alloca_var = new AllocaInst(rewrite.val->getType(), 0, "rewrite.alloca",
+                                  &entry_block->BB.front());
+    }
+
+    auto store_op = new StoreInst(alloca_var, rewrite.val, &orig.node->BB);
+    orig.node->ir.operations.push_back(store_op);
+
+    // For every non-local node which consumes ID, we load from the alloca'd
+    // variable instead. Rewrite all ID references to point to the loaded value.
+    for (auto *consumer : *rewrite.consumers) {
+      LoadInst *load_op = nullptr;
+      if (consumer->BB.empty()) {
+        load_op = new LoadInst(rewrite.val->getType(), alloca_var, "rewrite.ld",
+                               &consumer->BB);
+      } else {
+        load_op = new LoadInst(rewrite.val->getType(), alloca_var, "rewrite.ld",
+                               &consumer->BB.front());
+      }
+
+      rewrite_consumed_ids(consumer->ir, rewrite.val, load_op);
+
+      consumer->ir.operations.insert(consumer->ir.operations.begin(), load_op);
+    }
+  }
+}
+
+void CFGStructurizer::insert_phi() {
+  prune_dead_preds();
+
+  // It is possible that an SSA value was created in a block, and consumed in
+  // another. With CFG rewriting branches, it is possible that dominance
+  // relationship no longer holds and we must insert new dummy IDs to resolve
+  // this.
+  fixup_broken_value_dominance();
+
+  // Build a map of value ID -> creating block.
+  // This allows us to detect if a value is consumed in a situation where the
+  // declaration does not dominate use. This can happen when introducing ladder
+  // blocks or similar.
+  for (auto *node : forward_post_visit_order) {
+    unsigned phi_index = 0;
+    for (auto &phi : node->ir.phi) {
+      phi_nodes.push_back({node, phi_index});
+      if (phi.phi)
+        value_id_to_block[phi.phi] = node;
+      phi_index++;
+    }
+
+    for (auto *op : node->ir.operations)
+      if (op)
+        value_id_to_block[op] = node;
+  }
+
+  // Resolve phi-nodes top-down since PHI nodes may depend on other PHI nodes.
+  std::sort(phi_nodes.begin(), phi_nodes.end(),
+            [](const PHINode &a, const PHINode &b) {
+              return a.block->forward_post_visit_order >
+                     b.block->forward_post_visit_order;
+            });
+
+  for (auto &phi_node : phi_nodes) {
+    fixup_phi(phi_node);
+    insert_phi(phi_node);
+  }
+}
+
+std::vector<IncomingValue>::const_iterator CFGStructurizer::find_incoming_value(
+    const CFGNode *frontier_pred, const std::vector<IncomingValue> &incoming) {
+  // Find the incoming block which dominates frontier_pred and has the lowest
+  // post visit order. There are cases where two or more blocks dominate, but we
+  // want the most immediate dominator.
+  auto candidate = incoming.end();
+
+  for (auto itr = incoming.begin(); itr != incoming.end(); ++itr) {
+    auto *block = itr->block;
+    if (block->dominates(frontier_pred)) {
+      if (candidate == incoming.end() ||
+          (block->forward_post_visit_order <
+           candidate->block->forward_post_visit_order))
+        candidate = itr;
+    }
+  }
+
+  return candidate;
+}
+
+void CFGStructurizer::fixup_phi(PHINode &node) {
+  // We want to move any incoming block to where the ID was created.
+  // This avoids some problematic cases of crossing edges when using ladders.
+
+  for (auto &incoming : node.block->ir.phi[node.phi_index].incoming) {
+    auto itr = value_id_to_block.find(incoming.value);
+    if (itr == end(value_id_to_block)) {
+      // This is a global.
+      continue;
+    }
+
+    if (!itr->second->dominates(incoming.block)) {
+#ifdef PHI_DEBUG
+      LOGI("For node %s, move incoming node %s to %s.\n",
+           node.block->name.c_str(), incoming.block->name.c_str(),
+           itr->second->name.c_str());
+#endif
+      incoming.block = itr->second;
+      validate_phi(node.block->ir.phi[node.phi_index]);
+    }
+  }
+}
+
+bool CFGStructurizer::can_complete_phi_insertion(const PHI &phi,
+                                                 const CFGNode *block) {
+  // If all incoming values have at least one pred block they dominate, we can
+  // merge the final PHI.
+  auto &incoming_values = phi.incoming;
+  for (auto &incoming : incoming_values) {
+    auto itr = std::find_if(
+        block->pred.begin(), block->pred.end(),
+        [&](const CFGNode *n) { return incoming.block->dominates(n); });
+
+    if (itr == block->pred.end() &&
+        (!block->pred_back_edge ||
+         !incoming.block->dominates(block->pred_back_edge))) {
+      return false;
+    }
+  }
+
+  return true;
+}
+
+bool CFGStructurizer::query_reachability_through_back_edges(
+    const CFGNode &from, const CFGNode &to) const {
+  if (to.dominates(&from)) {
+    // If we're dominated by end node, only way we can reach is through a back
+    // edge.
+    return to.pred_back_edge && query_reachability(from, *to.pred_back_edge);
+  } else
+    return query_reachability(from, to);
+}
+
+bool CFGStructurizer::query_reachability_split_loop_header(
+    const CFGNode &from, const CFGNode &to, const CFGNode &end_node) const {
+  // A special query where from and to must lie on the same side of a loop
+  // header to be considered reachable.
+  if (!end_node.pred_back_edge)
+    return query_reachability(from, to);
+
+  bool from_reaches_header = query_reachability(from, end_node);
+  bool to_reaches_header = query_reachability(to, end_node);
+  if (from_reaches_header != to_reaches_header)
+    return false;
+
+  return query_reachability(from, to);
+}
+
+bool CFGStructurizer::phi_frontier_makes_forward_progress(
+    const PHI &phi, const CFGNode *frontier, const CFGNode *end_node) const {
+  // Not all PHI frontiers are nodes we need to care about.
+  // There are two conditions we must meet to disregard a placement.
+  // - We do not remove any inputs as a result.
+  // - The frontier can reach another incoming value.
+  // In this situation, a frontier is completely meaningless.
+  auto &incoming = phi.incoming;
+
+  for (auto &incoming_value : incoming) {
+    auto *incoming_block = incoming_value.block;
+    // We will remove an input, this is forward progress.
+    if (!exists_path_in_cfg_without_intermediate_node(incoming_block, end_node,
+                                                      frontier))
+      return true;
+  }
+
+  // Nothing is removed as a result, so check if the frontier can reach another
+  // incoming value. If end_node is a loop header, makes sure we only consider a
+  // node visible if they are both on the correct side of the loop header.
+  for (auto &incoming_value : incoming)
+    if (query_reachability_split_loop_header(*frontier, *incoming_value.block,
+                                             *end_node))
+      return false;
+
+  // Assume we make forward progress. Either way, we will never look at a
+  // frontier twice, so this should be safe. The only real risk is that we add
+  // some redundant PHI nodes.
+  return true;
+}
+
+void CFGStructurizer::insert_phi(PHINode &node) {
+  // We start off with N values defined in N blocks.
+  // These N blocks *used* to branch to the PHI node, but due to our
+  // structurizer, there might not be branch targets here anymore, primary
+  // example here is ladders. In order to fix this we need to follow control
+  // flow from these values and insert phi nodes as necessary to link up a set
+  // of values where dominance frontiers are shared.
+
+#ifdef PHI_DEBUG
+  LOGI("\n=== INSERT PHI FOR %s ===\n", node.block->name.c_str());
+#endif
+
+  auto &phi = node.block->ir.phi[node.phi_index];
+  auto &incoming_values = phi.incoming;
+
+  std::unordered_set<const CFGNode *> placed_frontiers;
+
+  for (;;) {
+#ifdef PHI_DEBUG
+    LOGI("\n=== PHI iteration ===\n");
+
+    for (auto &incoming : incoming_values)
+      LOGI("  Incoming value from %s\n", incoming.block->name.c_str());
+#endif
+
+    // Inside the CFG subset, find a dominance frontiers where we merge PHIs
+    // this iteration.
+    CFGNode *frontier = node.block;
+    if (!can_complete_phi_insertion(phi, node.block)) {
+      frontier = nullptr;
+
+      // We need some intermediate merge, so find a frontier node to work on.
+      for (auto &incoming : incoming_values) {
+        for (auto *candidate_frontier : incoming.block->dominance_frontier) {
+          if (placed_frontiers.count(candidate_frontier))
+            continue;
+
+          if (!phi_frontier_makes_forward_progress(phi, candidate_frontier,
+                                                   node.block)) {
+            // Makes sure we don't redundantly test this again.
+            placed_frontiers.insert(candidate_frontier);
+            continue;
+          }
+
+          // Only consider a frontier if we can reach node.block or its back
+          // edge from it.
+          if (query_reachability_through_back_edges(*candidate_frontier,
+                                                    *node.block)) {
+            if (frontier == nullptr ||
+                candidate_frontier->forward_post_visit_order >
+                    frontier->forward_post_visit_order) {
+              // Pick the earliest frontier in the CFG.
+              // We want to merge top to bottom.
+              frontier = candidate_frontier;
+            }
+          }
+        }
+      }
+
+      if (frontier)
+        placed_frontiers.insert(frontier);
+    }
+
+    // assert(frontier);
+    if (!frontier) {
+      // TODO: don't skip this
+      // printf(">> skipping phi: %s\n", phi.phi->getName().str().c_str());
+      break;
+    }
+
+    if (frontier == node.block) {
+      // NOTE: the "frontier->pred.size() == 1 && !frontier->pred_back_edge"
+      // case is ignored here, b/c it's handled later/elsewhere
+      {
+        std::vector<IncomingValue> final_incoming;
+
+        // Final merge.
+        for (auto *input : frontier->pred) {
+          auto itr = find_incoming_value(input, incoming_values);
+
+          IncomingValue value = {};
+          if (itr != incoming_values.end())
+            value.value = itr->value;
+          else
+            value.value = UndefValue::get(phi.phi->getType());
+
+          value.block = input;
+          final_incoming.push_back(value);
+        }
+
+        if (frontier->pred_back_edge) {
+          auto itr =
+              find_incoming_value(frontier->pred_back_edge, incoming_values);
+
+          IncomingValue value = {};
+          if (itr != incoming_values.end())
+            value.value = itr->value;
+          else
+            value.value = UndefValue::get(phi.phi->getType());
+
+          value.block = frontier->pred_back_edge;
+          final_incoming.push_back(value);
+        }
+
+        incoming_values = std::move(final_incoming);
+      }
+      return;
+    }
+
+    // A candidate dominance frontier is a place where we might want to place a
+    // PHI node in order to merge values. For a successful iteration, we need to
+    // find at least one candidate where we can merge PHI.
+
+#ifdef PHI_DEBUG
+    LOGI("Testing dominance frontier %s ...\n", frontier->name.c_str());
+#endif
+
+    // Remove old inputs.
+    PHI frontier_phi;
+    frontier_phi.phi =
+        create_phi_node(phi.phi->getType(), frontier->pred.size(),
+                        "frontier_phi_" + frontier->name, frontier->BB);
+
+    assert(!frontier->pred_back_edge);
+    for (auto *input : frontier->pred) {
+      auto itr = find_incoming_value(input, incoming_values);
+      if (itr != incoming_values.end()) {
+#ifdef PHI_DEBUG
+        auto *incoming_block = itr->block;
+        LOGI("   ... found incoming block %s for input %s.\n",
+             incoming_block->name.c_str(), input->name.c_str());
+        LOGI(" ... For pred %s (%p), found incoming value from %s (%p)\n",
+             input->name.c_str(), static_cast<const void *>(input),
+             incoming_block->name.c_str(),
+             static_cast<const void *>(incoming_block));
+#endif
+
+        IncomingValue value = {};
+        value.value = itr->value;
+        value.block = input;
+        frontier_phi.incoming.push_back(value);
+      } else {
+#ifdef PHI_DEBUG
+        LOGI("   ... creating undefined input for %s\n", input->name.c_str());
+#endif
+        // If there is no incoming value, we need to hallucinate an undefined
+        // value.
+        IncomingValue value = {};
+        value.value = UndefValue::get(phi.phi->getType());
+        value.block = input;
+        frontier_phi.incoming.push_back(value);
+      }
+    }
+
+    // Do we remove the incoming value now or not?
+    // If all paths from incoming value must go through frontier, we can remove
+    // it, otherwise, we might still need to use the incoming value somewhere
+    // else.
+    size_t num_alive_incoming_values = incoming_values.size();
+    for (size_t i = 0; i < num_alive_incoming_values;) {
+      auto *incoming_block = incoming_values[i].block;
+      if (!exists_path_in_cfg_without_intermediate_node(incoming_block,
+                                                        node.block, frontier)) {
+#ifdef PHI_DEBUG
+        LOGI("     ... removing input in %s\n", incoming_block->name.c_str());
+#endif
+        if (i != num_alive_incoming_values - 1)
+          std::swap(incoming_values[num_alive_incoming_values - 1],
+                    incoming_values[i]);
+        num_alive_incoming_values--;
+      } else {
+#ifdef PHI_DEBUG
+        LOGI("     ... keeping input in %s\n", incoming_block->name.c_str());
+#endif
+        i++;
+      }
+    }
+
+    // Need to clean up exhausted incoming values after the loop,
+    // since an incoming value can be used multiple times before a frontier PHI
+    // is resolved.
+    incoming_values.erase(incoming_values.begin() + num_alive_incoming_values,
+                          incoming_values.end());
+
+    IncomingValue *dominated_incoming = nullptr;
+    for (auto &incoming : incoming_values) {
+      if (frontier->dominates(incoming.block) &&
+          !exists_path_in_cfg_without_intermediate_node(frontier, node.block,
+                                                        incoming.block)) {
+        // There should be only one block the frontier can dominate.
+        // The candidate block must also post-dominate the frontier on the CFG
+        // subset which terminates at node.block, otherwise we will get a proper
+        // merge later anyways.
+        assert(!dominated_incoming);
+        dominated_incoming = &incoming;
+      }
+    }
+
+    if (dominated_incoming) {
+      // If our frontier dominates another incoming block, we need to merge two
+      // incoming values using an auxillary phi node as well as an OpSelect to
+      // resolve two conflicting values into one.
+
+      // For every pred edge of the frontier where pred did not dominate, we are
+      // now suddenly dominating. If we came from such a block, we should
+      // replace the incoming value of dominating_incoming rather than adding a
+      // new incoming value.
+      PHI merge_phi = {};
+
+      // Here we need to figure out if we have a cross branch which functions as
+      // a ladder. If we have such a special edge, the PHI value we find here
+      // will override any other value on this path. However, if we only have
+      // expected branches, there is nothing to override, and any PHI values we
+      // created along this path turned out to be irrelevant after all.
+
+      unsigned normal_branch_count = 0;
+      for (auto *input : frontier->pred) {
+        IncomingValue value = {};
+        auto itr = find_incoming_value(input, incoming_values);
+        if (itr != incoming_values.end()) {
+          // If the input does not dominate the frontier, this might be a case
+          // of cross-edge PHI merge. However, if we still have an incoming
+          // value which dominates the input block, ignore. This is considered a
+          // normal path and we will merge the actual result in a later
+          // iteration, because the frontier is not a post-dominator of the
+          // input value.
+          bool input_is_normal_edge = true;
+          if (!input->dominates(frontier)) {
+            input_is_normal_edge = false;
+            for (auto &incoming : incoming_values) {
+              if (incoming.block->dominates(input)) {
+                input_is_normal_edge = true;
+                break;
+              }
+            }
+          }
+
+          if (input_is_normal_edge)
+            normal_branch_count++;
+
+          value.value = ConstantInt::getBool(ctx, input_is_normal_edge);
+        } else {
+          // The input is undefined, so we don't really care. Just treat this as
+          // a normal edge.
+          normal_branch_count++;
+          value.value = ConstantInt::getBool(ctx, true);
+        }
+
+        value.block = input;
+        merge_phi.incoming.push_back(value);
+      }
+
+      if (normal_branch_count != frontier->pred.size()) {
+        merge_phi.phi = create_phi_node(
+            IntegerType::get(ctx, 1), merge_phi.incoming.size(),
+            "merged_phi_" + dominated_incoming->block->name, frontier->BB);
+
+        auto op = SelectInst::Create(merge_phi.phi, dominated_incoming->value,
+                                     frontier_phi.phi, "sel.incoming",
+                                     &dominated_incoming->block->BB);
+        dominated_incoming->block->ir.operations.push_back(op);
+        dominated_incoming->value = op;
+
+        frontier->ir.phi.push_back(std::move(merge_phi));
+      }
+    } else {
+      // Replace with merged value.
+      IncomingValue new_incoming = {};
+      new_incoming.value = frontier_phi.phi;
+      new_incoming.block = frontier;
+      incoming_values.push_back(new_incoming);
+    }
+
+#ifdef PHI_DEBUG
+    LOGI("=========================\n");
+#endif
+    frontier->ir.phi.push_back(std::move(frontier_phi));
+  }
+}
+
+void CFGStructurizer::compute_dominance_frontier() {
+  for (auto *node : forward_post_visit_order)
+    node->dominance_frontier.clear();
+  for (auto *node : forward_post_visit_order)
+    recompute_dominance_frontier(node);
+}
+
+void CFGStructurizer::compute_post_dominance_frontier() {
+  for (auto *node : backward_post_visit_order)
+    node->post_dominance_frontier.clear();
+  for (auto *node : backward_post_visit_order)
+    recompute_post_dominance_frontier(node);
+}
+
+void CFGStructurizer::build_immediate_dominators() {
+  for (auto i = forward_post_visit_order.size(); i; i--) {
+    auto *block = forward_post_visit_order[i - 1];
+    block->recompute_immediate_dominator();
+  }
+}
+
+void CFGStructurizer::build_immediate_post_dominators() {
+  for (auto i = backward_post_visit_order.size(); i; i--) {
+    auto *block = backward_post_visit_order[i - 1];
+    block->recompute_immediate_post_dominator();
+  }
+}
+
+void CFGStructurizer::reset_traversal() {
+  reachable_nodes.clear();
+  forward_post_visit_order.clear();
+  backward_post_visit_order.clear();
+  pool.for_each_node([](CFGNode &node) {
+    node.visited = false;
+    node.backward_visited = false;
+    node.traversing = false;
+    node.immediate_dominator = nullptr;
+    node.immediate_post_dominator = nullptr;
+    node.fake_pred.clear();
+    node.fake_succ.clear();
+
+    if (!node.freeze_structured_analysis) {
+      node.headers.clear();
+      node.merge = MergeType::None;
+      node.loop_merge_block = nullptr;
+      node.loop_ladder_block = nullptr;
+      node.selection_merge_block = nullptr;
+    }
+
+    if (node.succ_back_edge)
+      node.succ.push_back(node.succ_back_edge);
+    if (node.pred_back_edge)
+      node.pred.push_back(node.pred_back_edge);
+    node.succ_back_edge = nullptr;
+    node.pred_back_edge = nullptr;
+  });
+}
+
+struct LoopBacktracer {
+  void trace_to_parent(CFGNode *header, CFGNode *block);
+  std::unordered_set<CFGNode *> traced_blocks;
+};
+
+struct LoopMergeTracer {
+  explicit LoopMergeTracer(const LoopBacktracer &backtracer_)
+      : backtracer(backtracer_) {}
+
+  void trace_from_parent(CFGNode *header);
+  const LoopBacktracer &backtracer;
+  std::unordered_set<CFGNode *> loop_exits;
+  std::unordered_set<CFGNode *> traced_blocks;
+};
+
+void LoopBacktracer::trace_to_parent(CFGNode *header, CFGNode *block) {
+  if (block == header) {
+    traced_blocks.insert(block);
+    return;
+  }
+
+  if (traced_blocks.count(block) == 0) {
+    traced_blocks.insert(block);
+    for (auto *p : block->pred)
+      trace_to_parent(header, p);
+
+    // A backtrace will not pick up continue blocks which only branch back to
+    // header, and thus they will be considered loop exists by mistake. Start
+    // traversing from the continue block to catch these nodes as well. If a
+    // loop header is part of an outer loop construct, the loop body must also
+    // be part of the loop construct.
+    if (block->pred_back_edge)
+      trace_to_parent(header, block->pred_back_edge);
+  }
+}
+
+void LoopMergeTracer::trace_from_parent(CFGNode *header) {
+  if (backtracer.traced_blocks.count(header) == 0) {
+    loop_exits.insert(header);
+    return;
+  }
+
+  for (auto *succ : header->succ) {
+    if (traced_blocks.count(succ) == 0) {
+      trace_from_parent(succ);
+      traced_blocks.insert(succ);
+    }
+  }
+}
+
+void CFGStructurizer::backwards_visit() {
+  std::vector<CFGNode *> leaf_nodes;
+
+  // Traverse from leaf nodes, back through their preds instead.
+  // Clear out some state set by forward visit earlier.
+  for (auto *node : forward_post_visit_order) {
+    node->backward_visited = false;
+    node->traversing = false;
+
+    // For loops which can only exit from their header block,
+    // certain loops will be unreachable when doing a backwards traversal.
+    // We'll visit them explicitly later.
+    if (node->succ.empty() && !node->succ_back_edge)
+      leaf_nodes.push_back(node);
+  }
+
+  for (auto *leaf : leaf_nodes)
+    backwards_visit(*leaf);
+
+  // It might be case that some continue blocks are not reachable through
+  // backwards traversal. This effectively means that our flipped CFG is not
+  // reducible, which is rather annoying. To work around this, we fake some
+  // branches from the continue block out to other blocks. This way, we ensure
+  // that every forward-reachable block is reachable in a backwards traversal as
+  // well. The algorithm works where given the innermost loop header A, a block
+  // B (A dom B) and continue block C, For successors of B, we will observe some
+  // successors which can reach C ({E}), and some successors which can not reach
+  // C. C will add fake successor edges to {E}.
+  bool need_revisit = false;
+  for (auto *node : forward_post_visit_order) {
+    if (node->pred_back_edge) {
+      if (!node->pred_back_edge->backward_visited) {
+        LoopBacktracer tracer;
+        tracer.trace_to_parent(node, node->pred_back_edge);
+        LoopMergeTracer merge_tracer(tracer);
+        merge_tracer.trace_from_parent(node);
+        for (auto *f : merge_tracer.loop_exits)
+          node->pred_back_edge->add_fake_branch(f);
+        need_revisit = true;
+      }
+    }
+  }
+
+  if (need_revisit) {
+    for (auto *node : forward_post_visit_order) {
+      node->backward_visited = false;
+      node->traversing = false;
+      node->backward_post_visit_order = 0;
+    }
+
+    for (auto *leaf : leaf_nodes)
+      backwards_visit(*leaf);
+  }
+
+  exit_block->backward_post_visit_order = backward_post_visit_order.size();
+  exit_block->immediate_post_dominator = exit_block;
+  exit_block->backward_visited = true;
+  for (auto *leaf : leaf_nodes)
+    leaf->immediate_post_dominator = exit_block;
+}
+
+void CFGStructurizer::backwards_visit(CFGNode &entry) {
+  entry.backward_visited = true;
+
+  for (auto *pred : entry.pred)
+    if (!pred->backward_visited)
+      backwards_visit(*pred);
+
+  for (auto *pred : entry.fake_pred)
+    if (!pred->backward_visited)
+      backwards_visit(*pred);
+
+  entry.backward_post_visit_order = backward_post_visit_order.size();
+  backward_post_visit_order.push_back(&entry);
+}
+
+void CFGStructurizer::visit(CFGNode &entry) {
+  entry.visited = true;
+  entry.traversing = true;
+  reachable_nodes.insert(&entry);
+
+  for (auto *succ : entry.succ) {
+    if (succ->traversing) {
+      // For now, only support one back edge.
+      // DXIL seems to obey this.
+      assert(!entry.succ_back_edge || entry.succ_back_edge == succ);
+      entry.succ_back_edge = succ;
+
+      // For now, only support one back edge.
+      // DXIL seems to obey this.
+      assert(!succ->pred_back_edge || succ->pred_back_edge == &entry);
+      succ->pred_back_edge = &entry;
+    } else if (!succ->visited)
+      visit(*succ);
+  }
+
+  // Any back edges need to be handled specifically, only keep forward edges in
+  // succ/pred lists. This avoids any infinite loop scenarios and needing to
+  // special case a lot of checks.
+  if (entry.succ_back_edge) {
+    auto itr =
+        std::find(entry.succ.begin(), entry.succ.end(), entry.succ_back_edge);
+    if (itr != entry.succ.end())
+      entry.succ.erase(itr);
+  }
+
+  if (entry.pred_back_edge) {
+    auto itr =
+        std::find(entry.pred.begin(), entry.pred.end(), entry.pred_back_edge);
+    if (itr != entry.pred.end())
+      entry.pred.erase(itr);
+  }
+
+  entry.traversing = false;
+  entry.forward_post_visit_order = forward_post_visit_order.size();
+  forward_post_visit_order.push_back(&entry);
+}
+
+void CFGStructurizer::merge_to_succ(CFGNode *node, unsigned index) {
+  node->succ[index]->headers.push_back(node);
+  node->selection_merge_block = node->succ[index];
+  node->merge = MergeType::Selection;
+  // LOGI("Fixup selection merge %s -> %s\n", node->name.c_str(),
+  // node->selection_merge_block->name.c_str());
+}
+
+void CFGStructurizer::isolate_structured(std::unordered_set<CFGNode *> &nodes,
+                                         const CFGNode *header,
+                                         const CFGNode *merge) {
+  for (auto *pred : merge->pred) {
+    if (pred != header && nodes.count(pred) == 0) {
+      nodes.insert(pred);
+      isolate_structured(nodes, header, pred);
+    }
+  }
+}
+
+std::vector<CFGNode *>
+CFGStructurizer::isolate_structured_sorted(const CFGNode *header,
+                                           const CFGNode *merge) {
+  std::unordered_set<CFGNode *> nodes;
+  isolate_structured(nodes, header, merge);
+
+  std::vector<CFGNode *> sorted;
+  sorted.reserve(nodes.size());
+
+  for (auto *node : nodes)
+    sorted.push_back(node);
+
+  std::sort(sorted.begin(), sorted.end(),
+            [](const CFGNode *a, const CFGNode *b) {
+              return a->forward_post_visit_order > b->forward_post_visit_order;
+            });
+  return sorted;
+}
+
+bool CFGStructurizer::block_is_load_bearing(const CFGNode *node,
+                                            const CFGNode *merge) const {
+  return node->pred.size() >= 2 &&
+         !exists_path_in_cfg_without_intermediate_node(
+             node->immediate_dominator, merge, node);
+}
+
+bool CFGStructurizer::control_flow_is_escaping(const CFGNode *node,
+                                               const CFGNode *merge) const {
+  if (node == merge)
+    return false;
+
+  assert(merge->post_dominates(node));
+  bool escaping_path = false;
+
+  // First, test the loop scenario.
+  // If we're inside a loop, we're a break construct if we can prove that:
+  // - node has a loop header which dominates it.
+  // - node cannot reach the continue block.
+  // - Continue block cannot reach node.
+  // - All post-domination frontiers can reach the continue block, meaning that
+  // at some point control flow
+  //   decided to break out of the loop construct.
+  auto *innermost_loop_header =
+      entry_block->get_innermost_loop_header_for(node);
+  if (innermost_loop_header && innermost_loop_header->pred_back_edge) {
+    bool dominates_merge = node->dominates(merge);
+    bool can_reach_continue =
+        query_reachability(*node, *innermost_loop_header->pred_back_edge);
+    bool continue_can_reach =
+        query_reachability(*innermost_loop_header->pred_back_edge, *node);
+    bool pdf_can_reach_continue = true;
+
+    for (auto *frontier : node->post_dominance_frontier) {
+      bool header_dominates_frontier =
+          innermost_loop_header->dominates(frontier);
+      bool fronter_is_inside_loop_construct =
+          query_reachability(*frontier, *innermost_loop_header->pred_back_edge);
+      if (!header_dominates_frontier || !fronter_is_inside_loop_construct) {
+        pdf_can_reach_continue = false;
+        break;
+      }
+    }
+
+    if (!dominates_merge && !continue_can_reach && !can_reach_continue &&
+        pdf_can_reach_continue)
+      escaping_path = true;
+  }
+
+  if (!escaping_path) {
+    // Try to test if our block is load bearing, in which case it cannot be
+    // considered a break block. If the only path from idom to merge goes
+    // through node, it must be considered load bearing, since removing break
+    // paths must not change reachability.
+    bool load_bearing_escape = block_is_load_bearing(node, merge);
+
+    if (!load_bearing_escape) {
+      // If we cannot prove the escape through loop analysis, we might be able
+      // to deduce it from domination frontiers. If control flow is not
+      // escaping, then there must exist a dominance frontier node A, where
+      // merge strictly post-dominates A. This means that control flow can merge
+      // somewhere before we hit the merge block, and we consider that normal
+      // structured control flow.
+
+      escaping_path = true;
+      for (auto *frontier : node->dominance_frontier) {
+        if (merge != frontier && merge->post_dominates(frontier)) {
+          escaping_path = false;
+          break;
+        }
+      }
+    }
+  }
+
+  return escaping_path;
+}
+
+void CFGStructurizer::fixup_broken_selection_merges(unsigned pass) {
+  // Here we deal with selection branches where one path breaks and one path
+  // merges. This is common case for ladder blocks where we need to merge to the
+  // "true" merge block. The selection header has two succs, but the merge block
+  // might only have one pred block, which means it was not considered a merge
+  // candidate earlier in find_selection_merges().
+  for (auto *node : forward_post_visit_order) {
+    if (node->succ.size() != 2)
+      continue;
+    if (node->merge != MergeType::None)
+      continue;
+
+    // A continue block will never need to merge execution, but it shouldn't
+    // have succ.size() == 2, but rather succ.size() == 1 and a back edge.
+    if (node->succ_back_edge)
+      continue;
+
+    bool dominates_a = node->dominates(node->succ[0]);
+    bool dominates_b = node->dominates(node->succ[1]);
+
+    bool merge_a_has_header = !node->succ[0]->headers.empty();
+    bool merge_b_has_header = !node->succ[1]->headers.empty();
+
+    int trivial_merge_index = -1;
+
+    // Only allow the obvious merge candidates in pass 1.
+    // In pass 0, we might have a clear merge candidate,
+    // but the other path might be an escaping edge, which needs to be
+    // considered.
+    if (dominates_a && !dominates_b && !merge_a_has_header) {
+      // A is obvious candidate. B is a direct break/continue construct target
+      // most likely.
+      merge_to_succ(node, 0);
+      trivial_merge_index = 0;
+    } else if (dominates_b && !dominates_a && !merge_b_has_header) {
+      // B is obvious candidate. A is a direct break/continue construct target
+      // most likely.
+      merge_to_succ(node, 1);
+      trivial_merge_index = 1;
+    } else if (dominates_a && dominates_b && !merge_a_has_header &&
+               merge_b_has_header) {
+      // Not as obvious of a candidate, but this can happen if one path hits
+      // continue block, and other path hits a ladder merge block. For
+      // do/while(false) style loop, the loop body may dominate the merge block.
+      merge_to_succ(node, 0);
+      trivial_merge_index = 0;
+    } else if (dominates_a && dominates_b && !merge_b_has_header &&
+               merge_a_has_header) {
+      // Not as obvious of a candidate, but this can happen if one path hits
+      // continue block, and other path hits a ladder merge block. For do/while
+      // style loop, the loop body may dominate the merge block.
+      merge_to_succ(node, 1);
+      trivial_merge_index = 1;
+    } else if (dominates_a && dominates_b && !merge_a_has_header &&
+               !merge_b_has_header) {
+      // We could merge to both, no obvious merge point.
+      // Figure out where execution reconvenes.
+      // If we have a "break"-like construct inside a selection construct, we
+      // will not end up dominating the merge block. This will be fixed up with
+      // ladder constructs later in first pass.
+
+      // In second pass, we will have redirected any branches which escape
+      // through a ladder block. If we find that one path of the selection
+      // construct must go through that ladder block, we know we have a break
+      // construct.
+      CFGNode *merge = CFGStructurizer::find_common_post_dominator(node->succ);
+      if (merge) {
+        bool dominates_merge = node->dominates(merge);
+        bool merges_to_continue = merge && merge->succ_back_edge;
+        if (dominates_merge && !merge->headers.empty()) {
+          // Here we have a likely case where one block is doing a clean "break"
+          // out of a loop, and the other path continues as normal, and then
+          // conditionally breaks in a continue block or something similar.
+          bool a_path_is_break = control_flow_is_escaping(node->succ[0], merge);
+          bool b_path_is_break = control_flow_is_escaping(node->succ[1], merge);
+
+          if (a_path_is_break && b_path_is_break) {
+            // If either path branches to the other,
+            // we should be able to merge to the node which has not committed to
+            // the break path yet.
+            if (node->succ[1]->can_backtrace_to(node->succ[0]))
+              merge_to_succ(node, 0);
+            else if (node->succ[0]->can_backtrace_to(node->succ[1]))
+              merge_to_succ(node, 1);
+            else {
+              node->merge = MergeType::Selection;
+              node->selection_merge_block = nullptr;
+              // LOGI("Merging %s -> Unreachable\n", node->name.c_str());
+            }
+          } else if (b_path_is_break)
+            merge_to_succ(node, 0);
+          else
+            merge_to_succ(node, 1);
+        } else if (!merges_to_continue &&
+                   (merge->headers.empty() || pass == 0)) {
+          // Happens first iteration. We'll have to split blocks, so register a
+          // merge target where we want it. Otherwise, this is the easy case if
+          // we observe it in pass 1. This shouldn't really happen though, as
+          // we'd normally resolve this earlier in find_selection_merges.
+          assert(merge);
+          node->selection_merge_block = merge;
+          node->merge = MergeType::Selection;
+          merge->headers.push_back(node);
+          // LOGI("Merging %s -> %s\n", node->name.c_str(),
+          // node->selection_merge_block->name.c_str());
+        } else {
+          // We don't dominate the merge block in pass 1. We cannot split blocks
+          // now. Check to see which paths can actually reach the merge target
+          // without going through a ladder block. If we don't go through ladder
+          // it means an outer scope will actually reach the merge node. If we
+          // reach a ladder it means a block we dominate will make the escape.
+
+          // Another case is when one path is "breaking" out to a continue block
+          // which we don't dominate. We should not attempt to do ladder
+          // breaking here in pass 0 since it's unnecessary.
+
+          bool a_path_is_break = control_flow_is_escaping(node->succ[0], merge);
+          bool b_path_is_break = control_flow_is_escaping(node->succ[1], merge);
+          if (a_path_is_break && b_path_is_break) {
+            // Both paths break, so we never merge. Merge against Unreachable
+            // node if necessary ...
+            node->merge = MergeType::Selection;
+            node->selection_merge_block = nullptr;
+            // LOGI("Merging %s -> Unreachable\n", node->name.c_str());
+          } else if (b_path_is_break)
+            merge_to_succ(node, 0);
+          else
+            merge_to_succ(node, 1);
+        }
+      } else {
+        // We likely had one side of the branch take an "exit", in which case
+        // there is no common post-dominator.
+        bool a_dominates_exit = node->succ[0]->dominates_all_reachable_exits();
+        bool b_dominates_exit = node->succ[1]->dominates_all_reachable_exits();
+        if (!a_dominates_exit && b_dominates_exit)
+          merge_to_succ(node, 0);
+        else if (!b_dominates_exit && a_dominates_exit)
+          merge_to_succ(node, 1);
+        else {
+          // Both paths lead to exit. Do we even need to merge here?
+          // In worst case we can always merge to an unreachable node in the
+          // CFG.
+          node->merge = MergeType::Selection;
+          node->selection_merge_block = nullptr;
+          node->selection_merge_exit = true;
+        }
+      }
+    } else if (pass == 0) {
+      // No possible merge target. Just need to pick whatever node is the merge
+      // block here. Only do this in first pass, so that we can get a proper
+      // ladder breaking mechanism in place if we are escaping.
+      CFGNode *merge = CFGStructurizer::find_common_post_dominator(node->succ);
+
+      if (merge) {
+        // Don't try to merge to our switch block.
+        auto *inner_header = node->get_outer_header_dominator();
+        bool conditional_switch_break =
+            inner_header && inner_header->merge == MergeType::Selection &&
+            inner_header->selection_merge_block == merge;
+
+        if (!conditional_switch_break) {
+          node->selection_merge_block = merge;
+          node->merge = MergeType::Selection;
+          merge->headers.push_back(node);
+          // LOGI("Merging %s -> %s\n", node->name.c_str(),
+          // node->selection_merge_block->name.c_str());
+        }
+      } else {
+        // LOGI("Cannot find a merge target for block %s ...\n",
+        // node->name.c_str());
+      }
+    }
+
+    if (trivial_merge_index >= 0 && pass == 0) {
+      CFGNode *merge = CFGStructurizer::find_common_post_dominator(node->succ);
+      if (merge && !node->dominates(merge)) {
+        if (!merge->headers.empty()) {
+          // We might have a trivial merge, yet the other branch direction
+          // is a breaking construct. We will have to split some blocks.
+          merge->headers.push_back(node);
+        }
+
+        auto *current_candidate = node->succ[trivial_merge_index];
+        auto *other_candidate = node->succ[1 - trivial_merge_index];
+
+        bool current_escapes =
+            control_flow_is_escaping(current_candidate, merge) ||
+            current_candidate == merge;
+        bool other_escapes = control_flow_is_escaping(other_candidate, merge) ||
+                             other_candidate == merge;
+
+        // If we tried to merge in a direction which is a breaking construct,
+        // this means that the other path is actual desired break path.
+        if (current_escapes && !other_escapes) {
+          auto *target_block = node->succ[1 - trivial_merge_index];
+          // We kinda want to merge the other way, but to do that, we need an
+          // interim block.
+          auto *ladder = pool.create_node(node->name + "." +
+                                          target_block->name + ".interim");
+          ladder->add_branch(target_block);
+          ladder->ir.terminator.type = Terminator::Type::Branch;
+          ladder->ir.terminator.direct_block = target_block;
+          ladder->immediate_dominator = node;
+          ladder->immediate_post_dominator = target_block;
+          ladder->dominance_frontier.push_back(target_block);
+          ladder->forward_post_visit_order = node->forward_post_visit_order;
+          ladder->backward_post_visit_order = node->backward_post_visit_order;
+          node->retarget_branch(target_block, ladder);
+          node->selection_merge_block = ladder;
+        }
+      }
+    }
+  }
+}
+
+void CFGStructurizer::rewrite_selection_breaks(CFGNode *header,
+                                               CFGNode *ladder_to) {
+  // Don't rewrite loops here (since this is likely a loop merge block),
+  // unless we're rewriting header -> inner construct scenario.
+  // Check if the ladder_to block has a path to continue block.
+  // If it does, it is part of the loop construct, and cannot be a loop merge
+  // block.
+  if (header->pred_back_edge &&
+      !header->pred_back_edge->can_backtrace_to(ladder_to))
+    return;
+
+  // Don't rewrite switch blocks either.
+  if (header->ir.terminator.type == Terminator::Type::Switch)
+    return;
+
+  // LOGI("Rewriting selection breaks %s -> %s\n", header->name.c_str(),
+  // ladder_to->name.c_str());
+
+  std::unordered_set<CFGNode *> nodes;
+  std::unordered_set<CFGNode *> construct;
+
+  // Be careful about rewriting branches in continuing constructs.
+  CFGNode *inner_continue_block = nullptr;
+  CFGNode *inner_continue_succ = nullptr;
+  bool ladder_to_dominates_continue = false;
+  bool break_post_dominates_ladder_to = false;
+  auto *innermost_loop_header =
+      entry_block->get_innermost_loop_header_for(header);
+  if (innermost_loop_header && innermost_loop_header->pred_back_edge)
+    inner_continue_block = innermost_loop_header->pred_back_edge;
+  if (inner_continue_block && inner_continue_block->succ.size() == 1) {
+    inner_continue_succ = inner_continue_block->succ.front();
+    break_post_dominates_ladder_to =
+        inner_continue_succ->post_dominates(ladder_to);
+    ladder_to_dominates_continue = ladder_to->dominates(inner_continue_block);
+  }
+
+  header->traverse_dominated_blocks([&](CFGNode *node) -> bool {
+    // Inner loop headers are not candidates for a rewrite. They are split in
+    // split_merge_blocks. Similar with switch blocks. Also, we need to stop
+    // traversing when we hit the target block ladder_to.
+    if (node != ladder_to && nodes.count(node) == 0) {
+      nodes.insert(node);
+      if (!query_reachability(*node, *ladder_to))
+        return false;
+
+      bool branch_is_loop_or_switch =
+          node->pred_back_edge ||
+          node->ir.terminator.type == Terminator::Type::Switch;
+
+      // If our candidate scope splits a loop scope in half, ignore this
+      // candidate.
+      if (break_post_dominates_ladder_to && !ladder_to_dominates_continue &&
+          node->dominates(inner_continue_block)) {
+        return false;
+      }
+
+      if (node->succ.size() >= 2 && !branch_is_loop_or_switch) {
+        auto *outer_header =
+            get_post_dominance_frontier_with_cfg_subset_that_reaches(
+                node, ladder_to, nullptr);
+        if (outer_header == header)
+          construct.insert(node);
+      }
+      return true;
+    } else
+      return false;
+  });
+
+  std::vector<CFGNode *> sorted_construct;
+  sorted_construct.reserve(construct.size());
+  for (auto *inner_block : construct)
+    sorted_construct.push_back(inner_block);
+
+  // Emit inner constructs before outer constructs.
+  // This way we get natural nesting in case of certain if/else if ladders.
+  std::sort(sorted_construct.begin(), sorted_construct.end(),
+            [](const CFGNode *a, const CFGNode *b) {
+              return a->forward_post_visit_order < b->forward_post_visit_order;
+            });
+
+  for (auto *inner_block : sorted_construct) {
+    // LOGI("Header: %s, Inner: %s.\n", header->name.c_str(),
+    // inner_block->name.c_str());
+    auto *ladder =
+        pool.create_node(ladder_to->name + "." + inner_block->name + ".ladder");
+    // LOGI("Walking dominated blocks of %s, rewrite branches %s -> %s.\n",
+    // inner_block->name.c_str(),
+    //     ladder_to->name.c_str(), ladder->name.c_str());
+
+    ladder->add_branch(ladder_to);
+    ladder->ir.terminator.type = Terminator::Type::Branch;
+    ladder->ir.terminator.direct_block = ladder_to;
+    ladder->immediate_post_dominator = ladder_to;
+    ladder->dominance_frontier.push_back(ladder_to);
+    ladder->forward_post_visit_order = ladder_to->forward_post_visit_order;
+    ladder->backward_post_visit_order = ladder_to->backward_post_visit_order;
+
+    // Stop rewriting once we hit a merge block.
+    inner_block->traverse_dominated_blocks_and_rewrite_branch(
+        ladder_to, ladder, [inner_block](CFGNode *node) -> bool {
+          return inner_block->selection_merge_block != node;
+        });
+
+    ladder->recompute_immediate_dominator();
+    rewrite_selection_breaks(inner_block, ladder);
+  }
+}
+
+bool CFGStructurizer::header_and_merge_block_have_entry_exit_relationship(
+    CFGNode *header, CFGNode *merge) {
+  if (!merge->post_dominates(header))
+    return false;
+
+  // If there are other blocks which need merging, and that idom is the header,
+  // then header is some kind of exit block.
+  bool found_inner_merge_target = false;
+
+  std::unordered_set<CFGNode *> traversed;
+
+  header->traverse_dominated_blocks([&](CFGNode *node) {
+    if (node == merge)
+      return false;
+    if (traversed.count(node))
+      return false;
+    traversed.insert(node);
+
+    if (node->num_forward_preds() <= 1)
+      return true;
+    auto *idom = node->immediate_dominator;
+    if (idom == header) {
+      found_inner_merge_target = true;
+      return false;
+    }
+    return true;
+  });
+  return found_inner_merge_target;
+}
+
+void CFGStructurizer::split_merge_scopes() {
+  for (auto *node : forward_post_visit_order) {
+    // Setup a preliminary merge scope so we know when to stop traversal.
+    // We don't care about traversing inner scopes, out starting from merge
+    // block as well.
+    if (node->num_forward_preds() <= 1)
+      continue;
+
+    // The idom is the natural header block.
+    auto *idom = node->immediate_dominator;
+    assert(idom->succ.size() >= 2);
+
+    if (idom->merge == MergeType::None) {
+      idom->merge = MergeType::Selection;
+      idom->selection_merge_block = node;
+    }
+    node->headers.push_back(idom);
+  }
+
+  for (auto *node : forward_post_visit_order) {
+    if (node->num_forward_preds() <= 1)
+      continue;
+
+    // Continue blocks can always be branched to, from any scope, so don't
+    // rewrite anything here.
+    if (node->succ_back_edge)
+      continue;
+
+    // The idom is the natural header block.
+    auto *idom = node->immediate_dominator;
+    assert(idom->succ.size() >= 2);
+
+    // If we find a construct which is a typical entry <-> exit scenario, do not
+    // attempt to rewrite any branches. The real merge block might be contained
+    // inside this construct, and this block merely serves as the exit merge
+    // point. It should generally turn into a loop merge later.
+    if (header_and_merge_block_have_entry_exit_relationship(idom, node))
+      continue;
+
+    // Now we want to deal with cases where we are using this selection merge
+    // block as "goto" target for inner selection constructs. Using a loop
+    // header might be possible, but we will need to split up blocks to make
+    // sure that we don't end up with headers where the only branches are either
+    // merges or breaks.
+
+    // This case is relevant when we have something like:
+    // A -> B -> C -> D -> M
+    // A -> M
+    // B -> M
+    // C -> M
+    // D -> M
+    // We'll need intermediate blocks which merge each layer of the selection
+    // "onion".
+    rewrite_selection_breaks(idom, node);
+  }
+
+  recompute_cfg();
+}
+
+bool CFGStructurizer::query_reachability(const CFGNode &from,
+                                         const CFGNode &to) const {
+  if (&from == &to)
+    return true;
+
+  const uint32_t *src_reachability =
+      &reachability_bitset[from.forward_post_visit_order * reachability_stride];
+  return (src_reachability[to.forward_post_visit_order / 32] &
+          (1u << (to.forward_post_visit_order & 31u))) != 0;
+}
+
+void CFGStructurizer::visit_reachability(const CFGNode &node) {
+  uint32_t *dst_reachability =
+      &reachability_bitset[node.forward_post_visit_order * reachability_stride];
+
+  for (auto *succ : node.succ) {
+    // Inherit reachability from all successors.
+    const uint32_t *src_reachability =
+        &reachability_bitset[succ->forward_post_visit_order *
+                             reachability_stride];
+    for (unsigned i = 0; i < reachability_stride; i++)
+      dst_reachability[i] |= src_reachability[i];
+  }
+
+  // We can reach ourselves.
+  dst_reachability[node.forward_post_visit_order / 32] |=
+      1u << (node.forward_post_visit_order & 31u);
+}
+
+void CFGStructurizer::build_reachability() {
+  reachability_stride = (forward_post_visit_order.size() + 31) / 32;
+  reachability_bitset.clear();
+  reachability_bitset.resize(reachability_stride *
+                             forward_post_visit_order.size());
+  for (auto *node : forward_post_visit_order)
+    visit_reachability(*node);
+}
+
+void CFGStructurizer::recompute_cfg() {
+  reset_traversal();
+  visit(*entry_block);
+  // Need to prune dead preds before computing dominance.
+  prune_dead_preds();
+  build_immediate_dominators();
+  build_reachability();
+
+  backwards_visit();
+  build_immediate_post_dominators();
+
+  compute_dominance_frontier();
+  compute_post_dominance_frontier();
+}
+
+bool CFGStructurizer::find_switch_blocks(unsigned pass) {
+  bool modified_cfg = false;
+  for (auto index = forward_post_visit_order.size(); index; index--) {
+    auto *node = forward_post_visit_order[index - 1];
+    if (node->ir.terminator.type != Terminator::Type::Switch)
+      continue;
+
+    auto *merge = find_common_post_dominator(node->succ);
+
+    if (pass == 0) {
+      // Maintain the original switch block order if possible to avoid awkward
+      // churn in reference output.
+      uint64_t order = 0;
+      for (auto &c : node->ir.terminator.cases) {
+        // We'll need to increment global order up to N times in the worst case.
+        // Use 64-bit here as a safeguard in case the module is using a
+        // ridiculous amount of case labels.
+        c.global_order = order * node->ir.terminator.cases.size();
+        order++;
+      }
+
+      // First, sort so that any fallthrough parent comes before fallthrough
+      // target.
+      std::sort(node->ir.terminator.cases.begin(),
+                node->ir.terminator.cases.end(),
+                [](const Terminator::Case &a, const Terminator::Case &b) {
+                  return a.node->forward_post_visit_order >
+                         b.node->forward_post_visit_order;
+                });
+
+      // Look at all potential fallthrough candidates and reassign global order.
+      for (size_t i = 1, n = node->ir.terminator.cases.size(); i < n; i++) {
+        for (size_t j = 0; j < i; j++) {
+          auto &a = node->ir.terminator.cases[j];
+          auto &b = node->ir.terminator.cases[i];
+
+          // A case label might be the merge block candidate of the switch.
+          // Don't consider case fallthrough if b post-dominates the entire
+          // switch statement.
+          if (b.node != merge && a.node != b.node &&
+              b.node->can_backtrace_to(a.node))
+            b.global_order = a.global_order + 1;
+        }
+      }
+
+      // Sort again, but this time, by global order.
+      std::stable_sort(
+          node->ir.terminator.cases.begin(), node->ir.terminator.cases.end(),
+          [](const Terminator::Case &a, const Terminator::Case &b) {
+            return a.global_order < b.global_order;
+          });
+    }
+
+    // We cannot rewrite the CFG in pass 1 safely, this should have happened in
+    // pass 0.
+    if (pass == 0 && !node->dominates(merge)) {
+      // We did not rewrite switch blocks w.r.t. selection breaks.
+      // We might be in a situation where the switch block is trying to merge to
+      // a block which is already being merged to. Create a ladder which the
+      // switch block could merge to.
+      auto *ladder = pool.create_node(merge->name + ".switch-merge");
+      ladder->add_branch(merge);
+      ladder->ir.terminator.type = Terminator::Type::Branch;
+      ladder->ir.terminator.direct_block = merge;
+      ladder->immediate_post_dominator = merge;
+      ladder->immediate_dominator = merge->immediate_dominator;
+      ladder->dominance_frontier.push_back(merge);
+      ladder->forward_post_visit_order = merge->forward_post_visit_order;
+      ladder->backward_post_visit_order = merge->backward_post_visit_order;
+      node->traverse_dominated_blocks_and_rewrite_branch(merge, ladder);
+
+      merge = find_common_post_dominator(node->succ);
+      modified_cfg = true;
+    }
+
+    if (node->dominates(merge)) {
+      // LOGI("Switch merge: %p (%s) -> %p (%s)\n", static_cast<const void
+      // *>(node), node->name.c_str(),
+      //     static_cast<const void *>(merge), merge->name.c_str());
+      node->merge = MergeType::Selection;
+      node->selection_merge_block = merge;
+      merge->add_unique_header(node);
+    } else {
+      // We got a switch block where someone is escaping. Similar idea as for
+      // loop analysis. Find a post-dominator where we ignore branches which are
+      // "escaping".
+      auto *dominated_merge_target =
+          find_common_post_dominator_with_ignored_break(node->succ, merge);
+      if (node->dominates(dominated_merge_target)) {
+        node->merge = MergeType::Selection;
+        node->selection_merge_block = merge;
+        dominated_merge_target->add_unique_header(node);
+        merge->add_unique_header(node);
+      }
+    }
+
+    // A switch header might also be a loop header. Create a helper succ block
+    // for this case.
+    if (pass == 0 && node->pred_back_edge) {
+      node = create_helper_succ_block(node);
+      modified_cfg = true;
+    }
+  }
+
+  return modified_cfg;
+}
+
+void CFGStructurizer::find_selection_merges(unsigned pass) {
+  for (auto *node : forward_post_visit_order) {
+    if (node->num_forward_preds() <= 1)
+      continue;
+
+    // If there are 2 or more pred edges, try to merge execution.
+
+    // The idom is the natural header block.
+    auto *idom = node->immediate_dominator;
+    assert(idom->succ.size() >= 2);
+
+    // Check for case fallthrough here. In this case, we do not have a merge
+    // scenario, just ignore.
+    auto *inner_header = node->get_outer_selection_dominator();
+    if (inner_header &&
+        inner_header->ir.terminator.type == Terminator::Type::Switch) {
+      if (inner_header->selection_merge_block == node) {
+        // We just found a switch block which we have already handled.
+        continue;
+      }
+
+      if (std::find(inner_header->succ.begin(), inner_header->succ.end(),
+                    node) != inner_header->succ.end()) {
+        // Fallthrough.
+        continue;
+      }
+    }
+
+    for (auto *header : node->headers) {
+      // If we have a loop header already associated with this block, treat that
+      // as our idom.
+      if (header->forward_post_visit_order > idom->forward_post_visit_order)
+        idom = header;
+    }
+
+    if (pass == 0) {
+      // Check that we're not merging ourselves into the aether.
+      // This is a scenario that can happen if we attempt to merge to a block
+      // which terminates the CFG (return or unreachable), but does not
+      // post-dominate the idom candidate, i.e. the selection construct needs to
+      // break to some other scope. If this happens, we won't be able to
+      // register a typical breaking scenario (since post-domination analysis
+      // won't help us), and we need to do some magic fixups.
+
+      auto *merge_candidate = CFGNode::find_common_post_dominator(idom, node);
+      bool post_dominator_is_exit_node =
+          merge_candidate &&
+          merge_candidate->immediate_post_dominator == merge_candidate;
+      bool merged_into_terminating_path =
+          post_dominator_is_exit_node && node->dominates_all_reachable_exits();
+
+      // If our candidate idom post dominates the entry block, we consider this
+      // the main path of execution.
+      if (merged_into_terminating_path && idom->post_dominates(entry_block))
+        merged_into_terminating_path = false;
+
+      if (merged_into_terminating_path) {
+        // Similar to loops, find the break target for this construct.
+        auto *break_target =
+            find_break_target_for_selection_construct(idom, node);
+
+        // Have not observed any scenario where we won't have a dominated break
+        // target we can use.
+        if (break_target && idom->dominates(break_target) &&
+            break_target->headers.empty()) {
+          // Enclose this scope in a loop.
+          auto *helper_pred = create_helper_pred_block(idom);
+          helper_pred->merge = MergeType::Loop;
+          helper_pred->loop_merge_block = break_target;
+          helper_pred->freeze_structured_analysis = true;
+          break_target->headers.push_back(helper_pred);
+        }
+      }
+    }
+
+    if (idom->merge == MergeType::None || idom->merge == MergeType::Selection) {
+      // We just found a switch block which we have already handled.
+      if (idom->ir.terminator.type == Terminator::Type::Switch)
+        continue;
+
+      // If the idom is already a selection construct, this must mean
+      // we have some form of breaking construct inside this inner construct.
+      // This fooled find_selection_merges() to think we had a selection merge
+      // target at the break target. Fix this up here, where we rewrite the
+      // outer construct as a fixed loop instead.
+      if (idom->merge == MergeType::Selection) {
+        if (pass == 0) {
+          assert(idom->selection_merge_block);
+
+          // If we turn the outer selection construct into a loop,
+          // we remove the possibility to break further out (without adding
+          // ladders like we do for loops). To make this work, we must ensure
+          // that the new merge block post-dominates the loop and selection
+          // merge.
+          auto *merge_candidate = CFGNode::find_common_post_dominator(
+              idom->selection_merge_block, idom);
+
+          if (!merge_candidate ||
+              merge_candidate == idom->selection_merge_block) {
+            idom->loop_merge_block = idom->selection_merge_block;
+          } else {
+            // Make sure we split merge scopes. Pretend we have a true loop.
+            idom->loop_ladder_block = idom->selection_merge_block;
+            idom->loop_merge_block = merge_candidate;
+          }
+
+          idom->loop_merge_block->add_unique_header(idom);
+
+          idom->merge = MergeType::Loop;
+          idom->selection_merge_block = nullptr;
+          idom->freeze_structured_analysis = true;
+          idom = create_helper_succ_block(idom);
+        } else
+          LOGW("Mismatch headers in pass 1 ... ?\n");
+      }
+
+      idom->merge = MergeType::Selection;
+      node->add_unique_header(idom);
+      assert(node);
+      idom->selection_merge_block = node;
+      // LOGI("Selection merge: %p (%s) -> %p (%s)\n", static_cast<const void
+      // *>(idom), idom->name.c_str(),
+      //     static_cast<const void *>(node), node->name.c_str());
+    } else if (idom->merge == MergeType::Loop) {
+      if (idom->loop_merge_block == node && idom->loop_ladder_block) {
+        // We need to create an outer shell for this header since we need to
+        // ladder break to this node.
+        auto *loop = create_helper_pred_block(idom);
+        loop->merge = MergeType::Loop;
+        loop->loop_merge_block = node;
+        loop->freeze_structured_analysis = true;
+        node->add_unique_header(loop);
+        // LOGI("Loop merge: %p (%s) -> %p (%s)\n", static_cast<const void
+        // *>(loop), loop->name.c_str(),
+        //     static_cast<const void *>(node), node->name.c_str());
+      } else if (idom->loop_merge_block != node) {
+        auto *selection_idom = create_helper_succ_block(idom);
+        // If we split the loop header into the loop header -> selection merge
+        // header, then we can merge into a continue block for example.
+        selection_idom->merge = MergeType::Selection;
+        selection_idom->selection_merge_block = node;
+        node->add_unique_header(selection_idom);
+        // LOGI("Selection merge: %p (%s) -> %p (%s)\n", static_cast<const void
+        // *>(selection_idom),
+        //     selection_idom->name.c_str(), static_cast<const void *>(node),
+        //     node->name.c_str());
+      }
+    } else {
+      // We are hosed. There is no obvious way to merge execution here.
+      // This might be okay.
+      LOGW("Cannot merge execution for node %p (%s).\n",
+           static_cast<const void *>(node), node->name.c_str());
+    }
+  }
+}
+
+CFGStructurizer::LoopExitType
+CFGStructurizer::get_loop_exit_type(const CFGNode &header,
+                                    const CFGNode &node) const {
+  // If there exists an inner loop which dominates this exit, we treat it as an
+  // inner loop exit.
+  const CFGNode *innermost_loop_header =
+      header.get_innermost_loop_header_for(&node);
+  bool is_innermost_loop_header = &header == innermost_loop_header;
+
+  if (header.dominates(&node) && node.dominates_all_reachable_exits()) {
+    if (is_innermost_loop_header)
+      return LoopExitType::Exit;
+    else
+      return LoopExitType::InnerLoopExit;
+  }
+
+  if (header.dominates(&node)) {
+    if (is_innermost_loop_header) {
+      // Even if we dominate node, we might not be able to merge to it.
+      if (!header.can_loop_merge_to(&node))
+        return LoopExitType::Escape;
+
+      return LoopExitType::Merge;
+    } else {
+      // Try to detect if this is a degenerate inner loop merge.
+      // If the inner loop header is the only way to exit the loop construct,
+      // the loop exit block is a false exit.
+      // This is the case if the candidate must pass through the back edge, and
+      // the back edge can only branch to header. In this case, the loop will
+      // not be visible through back-propagation, but it is definitely part of
+      // the loop construct.
+
+      if (!innermost_loop_header->pred_back_edge ||
+          innermost_loop_header->pred_back_edge->ir.terminator.type !=
+              Terminator::Type::Branch)
+        return LoopExitType::InnerLoopMerge;
+
+      auto *post =
+          find_common_post_dominator({const_cast<CFGNode *>(&node),
+                                      innermost_loop_header->pred_back_edge});
+      if (post == innermost_loop_header->pred_back_edge)
+        return LoopExitType::InnerLoopFalsePositive;
+      else
+        return LoopExitType::InnerLoopMerge;
+    }
+  } else
+    return LoopExitType::Escape;
+}
+
+CFGNode *CFGStructurizer::create_helper_pred_block(CFGNode *node) {
+  auto *pred_node = pool.create_node(node->name + ".pred");
+
+  // Fixup visit order later.
+  pred_node->forward_post_visit_order = node->forward_post_visit_order;
+  pred_node->backward_post_visit_order = node->backward_post_visit_order;
+
+  std::swap(pred_node->pred, node->pred);
+  for (auto *header : node->headers)
+    header->fixup_merge_info_after_branch_rewrite(node, pred_node);
+  node->headers.clear();
+
+  pred_node->immediate_dominator = node->immediate_dominator;
+  pred_node->immediate_post_dominator = node;
+  node->immediate_dominator = pred_node;
+
+  retarget_pred_from(pred_node, node);
+
+  pred_node->add_branch(node);
+
+  if (node == entry_block)
+    entry_block = pred_node;
+
+  pred_node->ir.terminator.type = Terminator::Type::Branch;
+  pred_node->ir.terminator.direct_block = node;
+
+  return pred_node;
+}
+
+void CFGStructurizer::retarget_pred_from(CFGNode *new_node, CFGNode *old_succ) {
+  for (auto *p : new_node->pred) {
+    for (auto &s : p->succ)
+      if (s == old_succ)
+        s = new_node;
+
+    auto &p_term = p->ir.terminator;
+    if (p_term.direct_block == old_succ)
+      p_term.direct_block = new_node;
+    if (p_term.true_block == old_succ)
+      p_term.true_block = new_node;
+    if (p_term.false_block == old_succ)
+      p_term.false_block = new_node;
+    if (p_term.default_node == old_succ)
+      p_term.default_node = new_node;
+    for (auto &c : p_term.cases)
+      if (c.node == old_succ)
+        c.node = new_node;
+  }
+
+  // Do not swap back edges.
+
+  // Retarget immediate post dominators.
+  for (auto *n : forward_post_visit_order)
+    if (n->immediate_post_dominator == old_succ)
+      n->immediate_post_dominator = new_node;
+}
+
+void CFGStructurizer::retarget_succ_from(CFGNode *new_node, CFGNode *old_pred) {
+  for (auto *s : new_node->succ)
+    for (auto &p : s->pred)
+      if (p == old_pred)
+        p = new_node;
+
+  for (auto *node : forward_post_visit_order) {
+    if (node != old_pred) {
+      // Don't override immediate dominator for entry block.
+      if (node->immediate_dominator == old_pred)
+        node->immediate_dominator = new_node;
+    }
+  }
+  new_node->immediate_dominator = old_pred;
+
+  // Do not swap back edges.
+}
+
+CFGNode *CFGStructurizer::create_helper_succ_block(CFGNode *node) {
+  auto *succ_node = pool.create_node(node->name + ".succ");
+
+  // Fixup visit order later.
+  succ_node->forward_post_visit_order = node->forward_post_visit_order;
+  succ_node->backward_post_visit_order = node->backward_post_visit_order;
+
+  std::swap(succ_node->succ, node->succ);
+  // Do not swap back edges, only forward edges.
+
+  succ_node->immediate_post_dominator = node->immediate_post_dominator;
+  node->immediate_post_dominator = succ_node;
+
+  succ_node->ir.terminator = node->ir.terminator;
+  node->ir.terminator.type = Terminator::Type::Branch;
+  node->ir.terminator.direct_block = succ_node;
+
+  retarget_succ_from(succ_node, node);
+
+  node->add_branch(succ_node);
+  return succ_node;
+}
+
+CFGNode *CFGStructurizer::find_common_post_dominator(
+    const std::vector<CFGNode *> &candidates) {
+  if (candidates.empty())
+    return nullptr;
+  else if (candidates.size() == 1)
+    return candidates.front();
+
+  CFGNode *common_post =
+      CFGNode::find_common_post_dominator(candidates[0], candidates[1]);
+  for (size_t i = 2; i < candidates.size(); i++)
+    common_post =
+        CFGNode::find_common_post_dominator(common_post, candidates[i]);
+  return common_post != common_post->immediate_post_dominator ? common_post
+                                                              : nullptr;
+}
+
+CFGNode *
+CFGStructurizer::find_break_target_for_selection_construct(CFGNode *idom,
+                                                           CFGNode *merge) {
+  std::vector<CFGNode *> new_visit_queue;
+  std::unordered_set<CFGNode *> visited;
+  std::vector<CFGNode *> visit_queue;
+  std::vector<CFGNode *> candidates;
+
+  visit_queue.push_back(idom);
+  do {
+    for (auto *n : visit_queue) {
+      if (visited.count(n))
+        continue;
+      visited.insert(n);
+
+      if (query_reachability(*n, *merge)) {
+        for (auto *succ : n->succ)
+          new_visit_queue.push_back(succ);
+      } else {
+        candidates.push_back(n);
+      }
+    }
+
+    visit_queue = new_visit_queue;
+    new_visit_queue.clear();
+  } while (!visit_queue.empty());
+
+  if (candidates.empty())
+    return nullptr;
+  else
+    return find_common_post_dominator(candidates);
+}
+
+CFGNode *CFGStructurizer::find_common_post_dominator_with_ignored_break(
+    std::vector<CFGNode *> candidates, const CFGNode *ignored_node) {
+  if (candidates.empty())
+    return nullptr;
+
+  std::vector<CFGNode *> next_nodes;
+  const auto add_unique_next_node = [&](CFGNode *node) {
+    if (node != ignored_node)
+      if (std::find(next_nodes.begin(), next_nodes.end(), node) ==
+          next_nodes.end())
+        next_nodes.push_back(node);
+  };
+
+  while (candidates.size() != 1) {
+    // Sort candidates by post visit order.
+    std::sort(candidates.begin(), candidates.end(),
+              [](const CFGNode *a, const CFGNode *b) {
+                return a->forward_post_visit_order >
+                       b->forward_post_visit_order;
+              });
+
+    // We reached exit without merging execution, there is no common post
+    // dominator.
+    if (candidates.front()->succ.empty())
+      return nullptr;
+
+    for (auto *succ : candidates.front()->succ)
+      add_unique_next_node(succ);
+    for (auto itr = candidates.begin() + 1; itr != candidates.end(); ++itr)
+      add_unique_next_node(*itr);
+
+    candidates.clear();
+    std::swap(candidates, next_nodes);
+  }
+
+  if (candidates.empty())
+    return nullptr;
+
+  return candidates.front();
+}
+
+void CFGStructurizer::find_loops() {
+  for (auto index = forward_post_visit_order.size(); index; index--) {
+    // Visit in reverse order so we resolve outer loops first,
+    // this lets us detect ladder-breaking loops.
+    auto *node = forward_post_visit_order[index - 1];
+
+    if (node->freeze_structured_analysis) {
+      // If we have a pre-created dummy loop for ladding breaking,
+      // just propagate the header information and be done with it.
+      if (node->merge == MergeType::Loop) {
+        node->loop_merge_block->headers.push_back(node);
+        continue;
+      }
+    }
+
+    if (!node->has_pred_back_edges())
+      continue;
+
+    // There are back-edges here, this must be a loop header.
+    node->merge = MergeType::Loop;
+
+    // Now, we need to figure out which blocks belong in the loop construct.
+    // The way to figure out a natural loop is any block which is dominated by
+    // loop header and control flow passes to one of the back edges.
+
+    // Unfortunately, it can be ambiguous which block is the merge block for a
+    // loop. Ideally, there is a unique block which is the loop exit block, but
+    // if there are multiple breaks there are multiple blocks which are not part
+    // of the loop construct.
+
+    LoopBacktracer tracer;
+    auto *pred = node->pred_back_edge;
+
+    // Back-trace from here.
+    // The CFG is reducible, so node must dominate pred.
+    // Since node dominates pred, there is no pred chain we can follow without
+    // eventually hitting node, and we'll stop traversal there.
+
+    // All nodes which are touched during this traversal must be part of the
+    // loop construct.
+    tracer.trace_to_parent(node, pred);
+
+    LoopMergeTracer merge_tracer(tracer);
+    merge_tracer.trace_from_parent(node);
+
+    std::vector<CFGNode *> direct_exits;
+    std::vector<CFGNode *> inner_direct_exits;
+    std::vector<CFGNode *> dominated_exit;
+    std::vector<CFGNode *> inner_dominated_exit;
+    std::vector<CFGNode *> non_dominated_exit;
+
+    for (auto *loop_exit : merge_tracer.loop_exits) {
+      auto exit_type = get_loop_exit_type(*node, *loop_exit);
+      switch (exit_type) {
+      case LoopExitType::Exit:
+        direct_exits.push_back(loop_exit);
+        break;
+
+      case LoopExitType::InnerLoopExit:
+        // It's not an exit for us, but the inner loop.
+        inner_direct_exits.push_back(loop_exit);
+        break;
+
+      case LoopExitType::Merge:
+        dominated_exit.push_back(loop_exit);
+        break;
+
+      case LoopExitType::InnerLoopMerge:
+        inner_dominated_exit.push_back(loop_exit);
+        break;
+
+      case LoopExitType::InnerLoopFalsePositive:
+        // In this case, the inner loop can only exit at the loop header,
+        // and thus post-dominance analysis will always fail.
+        // Ignore this case as it's a false exit.
+        break;
+
+      case LoopExitType::Escape:
+        non_dominated_exit.push_back(loop_exit);
+        break;
+      }
+    }
+
+    // If the only merge candidates we have are inner dominated, treat them as
+    // true dominated exits.
+    if (dominated_exit.empty() && !inner_dominated_exit.empty())
+      std::swap(dominated_exit, inner_dominated_exit);
+
+    // If there are no direct exists, treat inner direct exists as direct exits.
+    if (direct_exits.empty())
+      direct_exits = std::move(inner_direct_exits);
+
+    // A direct exit can be considered a dominated exit if there are no better
+    // candidates.
+    if (dominated_exit.empty() && !direct_exits.empty())
+      std::swap(dominated_exit, direct_exits);
+
+    // If we only have one direct exit, consider it our merge block.
+    // Pick either Merge or Escape.
+    if (direct_exits.size() == 1 && dominated_exit.empty() &&
+        non_dominated_exit.empty()) {
+      if (node->dominates(direct_exits.front()))
+        std::swap(dominated_exit, direct_exits);
+      else
+        std::swap(non_dominated_exit, direct_exits);
+    }
+
+    if (dominated_exit.size() >= 2) {
+      // Try to see if we can reduce the number of merge blocks to just 1.
+      // This is relevant if we have various "clean" break blocks.
+      auto *post_dominator = find_common_post_dominator(dominated_exit);
+      if (std::find(dominated_exit.begin(), dominated_exit.end(),
+                    post_dominator) != dominated_exit.end()) {
+        dominated_exit.clear();
+        dominated_exit.push_back(post_dominator);
+      }
+    }
+
+    if (dominated_exit.empty() && inner_dominated_exit.empty() &&
+        non_dominated_exit.empty()) {
+      // There can be zero loop exits, i.e. infinite loop. This means we have no
+      // merge block. We will invent a merge block to satisfy SPIR-V validator,
+      // and declare it as unreachable.
+      node->loop_merge_block = nullptr;
+      // LOGI("Loop without merge: %p (%s)\n", static_cast<const void *>(node),
+      // node->name.c_str());
+    } else if (dominated_exit.size() == 1 && non_dominated_exit.empty() &&
+               inner_dominated_exit.empty()) {
+      // Clean merge.
+      // This is a unique merge block. There can be no other merge candidate.
+      node->loop_merge_block = dominated_exit.front();
+
+      const_cast<CFGNode *>(node->loop_merge_block)->add_unique_header(node);
+      // LOGI("Loop with simple merge: %p (%s) -> %p (%s)\n", static_cast<const
+      // void *>(node), node->name.c_str(),
+      //     static_cast<const void *>(node->loop_merge_block),
+      //     node->loop_merge_block->name.c_str());
+    } else if (dominated_exit.empty() && inner_dominated_exit.empty() &&
+               non_dominated_exit.size() == 1) {
+      // Single-escape merge.
+      // It is unique, but we need workarounds later.
+      auto *merge_block = non_dominated_exit.front();
+
+      // We can make the non-dominated exit dominated by
+      // adding a ladder block in-between. This allows us to merge the loop
+      // cleanly before breaking out.
+
+      auto *ladder = pool.create_node(node->name + ".merge");
+      ladder->add_branch(merge_block);
+      ladder->ir.terminator.type = Terminator::Type::Branch;
+      ladder->ir.terminator.direct_block = merge_block;
+      ladder->immediate_post_dominator = merge_block;
+      ladder->forward_post_visit_order = merge_block->forward_post_visit_order;
+      ladder->backward_post_visit_order =
+          merge_block->backward_post_visit_order;
+
+      node->traverse_dominated_blocks_and_rewrite_branch(merge_block, ladder);
+      node->loop_ladder_block = nullptr;
+      node->loop_merge_block = ladder;
+      ladder->recompute_immediate_dominator();
+
+      const_cast<CFGNode *>(node->loop_merge_block)->add_unique_header(node);
+      // LOGI("Loop with ladder merge: %p (%s) -> %p (%s)\n", static_cast<const
+      // void *>(node), node->name.c_str(),
+      //     static_cast<const void *>(node->loop_merge_block),
+      //     node->loop_merge_block->name.c_str());
+    } else {
+      // We have multiple blocks which are merge candidates. We need to figure
+      // out where execution reconvenes.
+      std::vector<CFGNode *> merges;
+      merges.reserve(inner_dominated_exit.size() + dominated_exit.size() +
+                     non_dominated_exit.size());
+      merges.insert(merges.end(), inner_dominated_exit.begin(),
+                    inner_dominated_exit.end());
+      merges.insert(merges.end(), dominated_exit.begin(), dominated_exit.end());
+      merges.insert(merges.end(), non_dominated_exit.begin(),
+                    non_dominated_exit.end());
+      CFGNode *merge = CFGStructurizer::find_common_post_dominator(merges);
+
+      CFGNode *dominated_merge = nullptr;
+
+      // Try to find the sensible target first.
+      // If one of our merge blocks is the successor of the continue block,
+      // this is a prime candidate for a ladder block.
+      if (node->pred_back_edge && node->pred_back_edge->succ.size() == 1 &&
+          std::find(dominated_exit.begin(), dominated_exit.end(),
+                    node->pred_back_edge->succ.front()) !=
+              dominated_exit.end()) {
+        dominated_merge = node->pred_back_edge->succ.front();
+      } else if (merge && !node->dominates(merge) &&
+                 dominated_exit.size() > 1) {
+        // Now, we might have Merge blocks which end up escaping out of the loop
+        // construct. We might have to remove candidates which end up being
+        // break blocks after all.
+        std::vector<CFGNode *> non_breaking_exits;
+        non_breaking_exits.reserve(dominated_exit.size());
+        for (auto *exit : dominated_exit)
+          if (!control_flow_is_escaping(exit, merge))
+            non_breaking_exits.push_back(exit);
+
+        dominated_merge =
+            CFGStructurizer::find_common_post_dominator(non_breaking_exits);
+      } else {
+        dominated_merge =
+            CFGStructurizer::find_common_post_dominator(dominated_exit);
+      }
+
+      if (!dominated_merge) {
+        LOGW("There is no candidate for ladder merging.\n");
+      }
+
+      if (dominated_merge && !node->dominates(dominated_merge)) {
+        LOGW("We don't dominate the merge target ...\n");
+        dominated_merge = nullptr;
+      }
+
+      if (!merge) {
+        LOGW("Failed to find a common merge point ...\n");
+      } else {
+        node->loop_merge_block = merge;
+        const_cast<CFGNode *>(node->loop_merge_block)->add_unique_header(node);
+
+        if (node->can_loop_merge_to(merge)) {
+          // Clean merge.
+          // This is a unique merge block. There can be no other merge
+          // candidate.
+          // LOGI("Loop with simple multi-exit merge: %p (%s) -> %p (%s)\n",
+          // static_cast<const void *>(node),
+          //     node->name.c_str(), static_cast<const void
+          //     *>(node->loop_merge_block),
+          //     node->loop_merge_block->name.c_str());
+        } else {
+          // Single-escape merge.
+          // It is unique, but we need workarounds later.
+          // LOGI("Loop with ladder multi-exit merge: %p (%s) -> %p (%s)\n",
+          // static_cast<const void *>(node),
+          //     node->name.c_str(), static_cast<const void
+          //     *>(node->loop_merge_block),
+          //     node->loop_merge_block->name.c_str());
+
+          if (dominated_merge) {
+            // LOGI("    Ladder block: %p (%s)\n", static_cast<const void
+            // *>(dominated_merge),
+            //     dominated_merge->name.c_str());
+          }
+
+          // We will use this block as a ladder.
+          node->loop_ladder_block = dominated_merge;
+        }
+      }
+    }
+  }
+}
+
+CFGNode *
+CFGStructurizer::get_target_break_block_for_inner_header(const CFGNode *node,
+                                                         size_t header_index) {
+  CFGNode *target_header = nullptr;
+  for (size_t j = header_index; j; j--) {
+    if (node->headers[j - 1]->merge == MergeType::Loop) {
+      // We might have two loops, each at equal scopes.
+      // In order to break out to an outer loop, we must verify that the loops
+      // actually nest. We must not introduce any backwards branches here.
+      auto *candidate_header = node->headers[j - 1];
+      CFGNode *candidate_merge = nullptr;
+      if (candidate_header->loop_ladder_block)
+        candidate_merge = candidate_header->loop_ladder_block;
+      else if (candidate_header->loop_merge_block)
+        candidate_merge = candidate_header->loop_merge_block;
+
+      if (candidate_merge &&
+          !candidate_merge->dominates(node->headers[header_index])) {
+        target_header = candidate_header;
+        break;
+      }
+    }
+  }
+
+  return target_header;
+}
+
+CFGNode *CFGStructurizer::get_or_create_ladder_block(CFGNode *node,
+                                                     size_t header_index) {
+  auto *header = node->headers[header_index];
+  auto *loop_ladder = header->loop_ladder_block;
+
+  if (!loop_ladder) {
+    // We don't have a ladder, because the loop merged to an outer scope, so we
+    // need to fake a ladder. If we hit this case, we did not hit the simpler
+    // case in find_loops().
+    auto *ladder = pool.create_node(node->name + ".merge");
+    ladder->add_branch(node);
+    ladder->ir.terminator.type = Terminator::Type::Branch;
+    ladder->ir.terminator.direct_block = node;
+    ladder->immediate_post_dominator = node;
+    ladder->forward_post_visit_order = node->forward_post_visit_order;
+    ladder->backward_post_visit_order = node->backward_post_visit_order;
+
+    header->traverse_dominated_blocks_and_rewrite_branch(node, ladder);
+    header->loop_ladder_block = ladder;
+    ladder->recompute_immediate_dominator();
+
+    // If this is the second outermost scope, we don't need to deal with
+    // ladders. ladder is a dummy branch straight out to the outer merge point.
+    if (header_index > 1)
+      loop_ladder = header->loop_ladder_block;
+  }
+
+  return loop_ladder;
+}
+
+CFGNode *CFGStructurizer::build_enclosing_break_target_for_loop_ladder(
+    CFGNode *&node, CFGNode *loop_ladder) {
+  // A loop ladder needs to break out somewhere. If we don't have a candidate
+  // place to break out to, we will need to create one for the outer scope.
+  // This is the purpose of the full_break_target fallback.
+
+  bool ladder_to_merge_is_trivial =
+      loop_ladder->succ.size() == 1 && loop_ladder->succ.front() == node;
+  CFGNode *full_break_target = nullptr;
+
+  // We have to break somewhere, turn the outer selection construct into
+  // a loop.
+  if (!ladder_to_merge_is_trivial) {
+    // Selection merge to this dummy instead.
+    auto *new_selection_merge = create_helper_pred_block(node);
+
+    // This is now our fallback loop break target.
+    full_break_target = node;
+
+    auto *loop = create_helper_pred_block(new_selection_merge->headers[0]);
+
+    // Reassign header node.
+    assert(new_selection_merge->headers[0]->merge == MergeType::Selection);
+    new_selection_merge->headers[0]->selection_merge_block =
+        new_selection_merge;
+    new_selection_merge->headers[0] = loop;
+
+    loop->merge = MergeType::Loop;
+    loop->loop_merge_block = node;
+    loop->freeze_structured_analysis = true;
+
+    // After the loop ladder, make sure we always branch to the break target.
+    loop_ladder->traverse_dominated_blocks_and_rewrite_branch(
+        new_selection_merge, node);
+
+    node = new_selection_merge;
+  }
+
+  return full_break_target;
+}
+
+CFGNode *CFGStructurizer::build_ladder_block_for_escaping_edge_handling(
+    CFGNode *node, CFGNode *header, CFGNode *loop_ladder,
+    CFGNode *target_header, CFGNode *full_break_target,
+    const std::unordered_set<const CFGNode *> &normal_preds) {
+  CFGNode *new_ladder_block = nullptr;
+
+  if (target_header || full_break_target) {
+    // If we have a ladder block, there exists a merge candidate which the loop
+    // header dominates. We create a ladder block before the merge block, which
+    // becomes the true merge block. In this ladder block, we can detect with
+    // Phi nodes whether the break was "clean", or if we had an escape edge. If
+    // we have an escape edge, we can break to outer level, and continue the
+    // ladder that way. Otherwise we branch to the existing merge block and
+    // continue as normal. We'll also need to rewrite a lot of Phi nodes this
+    // way as well.
+    auto *ladder = create_helper_pred_block(loop_ladder);
+    new_ladder_block = ladder;
+
+    // Merge to ladder instead.
+    header->traverse_dominated_blocks_and_rewrite_branch(node, ladder);
+
+    ladder->ir.terminator.type = Terminator::Type::Condition;
+    ladder->ir.terminator.false_block = loop_ladder;
+
+    PHI phi;
+    phi.phi = create_phi_node(IntegerType::get(ctx, 1), ladder->pred.size(),
+                              "ladder_phi_" + loop_ladder->name, ladder->BB);
+    ladder->ir.terminator.condition = phi.phi;
+
+    for (auto *pred : ladder->pred) {
+      IncomingValue incoming = {};
+      incoming.block = pred;
+      bool is_breaking_pred = normal_preds.count(pred) == 0;
+      incoming.value = ConstantInt::getBool(ctx, is_breaking_pred);
+      phi.incoming.push_back(incoming);
+    }
+    ladder->ir.phi.push_back(std::move(phi));
+
+    // Ladder breaks out to outer scope.
+    if (target_header && target_header->loop_ladder_block) {
+      ladder->ir.terminator.true_block = target_header->loop_ladder_block;
+      ladder->add_branch(target_header->loop_ladder_block);
+    } else if (target_header && target_header->loop_merge_block) {
+      ladder->ir.terminator.true_block = target_header->loop_merge_block;
+      ladder->add_branch(target_header->loop_merge_block);
+    } else if (full_break_target) {
+      ladder->ir.terminator.true_block = full_break_target;
+      ladder->add_branch(full_break_target);
+    } else
+      LOGW("No loop merge block?\n");
+
+    // This can happen in some scenarios, fixup the branch to be a direct one
+    // instead.
+    if (ladder->ir.terminator.true_block == ladder->ir.terminator.false_block) {
+      ladder->ir.terminator.direct_block = ladder->ir.terminator.true_block;
+      ladder->ir.terminator.type = Terminator::Type::Branch;
+    }
+  } else {
+    // Here, loop_ladder -> final merge is a trivial, direct branch.
+
+    if (loop_ladder->ir.operations.empty()) {
+      // Simplest common case.
+      // If the loop ladder just branches to outer scope, and this block does
+      // not perform any operations we can avoid messing around with ladder PHI
+      // variables and just execute the branch. This block will likely become a
+      // frontier node when merging PHI instead. This is a common case when
+      // breaking out of a simple for loop.
+      header->traverse_dominated_blocks_and_rewrite_branch(node, loop_ladder);
+    } else {
+      // We have a case where we're trivially breaking out of a selection
+      // construct, but the loop ladder block contains operations which we must
+      // not execute, since we were supposed to branch directly out to node. We
+      // cannot directly break out of a selection construct, so our ladder must
+      // be a bit more sophisticated. ladder-pre -> merge -> ladder-post ->
+      // selection merge
+      //      \-------------------/
+      auto *ladder_pre = create_helper_pred_block(loop_ladder);
+      auto *ladder_post = create_helper_succ_block(loop_ladder);
+      ladder_pre->add_branch(ladder_post);
+
+      ladder_pre->ir.terminator.type = Terminator::Type::Condition;
+      ladder_pre->ir.terminator.true_block = ladder_post;
+      ladder_pre->ir.terminator.false_block = loop_ladder;
+
+      // Merge to ladder instead.
+      header->traverse_dominated_blocks_and_rewrite_branch(node, ladder_pre);
+      new_ladder_block = ladder_pre;
+
+      PHI phi;
+      phi.phi =
+          create_phi_node(IntegerType::get(ctx, 1), ladder_pre->pred.size(),
+                          "ladder_phi_" + loop_ladder->name, ladder_pre->BB);
+      ladder_pre->ir.terminator.condition = phi.phi;
+      for (auto *pred : ladder_pre->pred) {
+        IncomingValue incoming = {};
+        incoming.block = pred;
+        bool is_breaking_pred = normal_preds.count(pred) == 0;
+        incoming.value = ConstantInt::getBool(ctx, is_breaking_pred);
+        phi.incoming.push_back(incoming);
+      }
+      ladder_pre->ir.phi.push_back(std::move(phi));
+    }
+  }
+
+  return new_ladder_block;
+}
+
+void CFGStructurizer::split_merge_blocks() {
+  for (auto *node : forward_post_visit_order) {
+    if (node->headers.size() <= 1)
+      continue;
+
+    // If this block was the merge target for more than one construct,
+    // we will need to split the block. In SPIR-V, a merge block can only be the
+    // merge target for one construct. However, we can set up a chain of merges
+    // where inner scope breaks to outer scope with a dummy basic block. The
+    // outer scope comes before the inner scope merge.
+
+    // We cannot fully trust a sort on post-visit order, since if we have two
+    // split blocks here, they will have the same post-visit order until we
+    // recompute them.
+    // FIXME: Should probably be smarter about this ...
+    std::sort(node->headers.begin(), node->headers.end(),
+              [](const CFGNode *a, const CFGNode *b) -> bool {
+                if (a->dominates(b))
+                  return true;
+                else if (b->dominates(a))
+                  return false;
+                else
+                  return a->forward_post_visit_order >
+                         b->forward_post_visit_order;
+              });
+
+    // LOGI("Splitting merge blocks for %s\n", node->name.c_str());
+    // for (auto *header : node->headers)
+    //	LOGI("  Header: %s.\n", header->name.c_str());
+
+    CFGNode *full_break_target = nullptr;
+
+    // Before we start splitting and rewriting branches, we need to know which
+    // preds are considered "normal", and which branches are considered ladder
+    // breaking branches (rewritten branches). This will influence if a pred
+    // block gets false or true when emitting ladder breaking blocks later.
+    std::vector<std::unordered_set<const CFGNode *>> normal_preds(
+        node->headers.size());
+    for (size_t i = 0; i < node->headers.size(); i++)
+      if (node->headers[i]->loop_ladder_block)
+        for (auto *pred : node->headers[i]->loop_ladder_block->pred)
+          normal_preds[i].insert(pred);
+
+    // Start from innermost scope, and rewrite all escape branches to a merge
+    // block which is dominated by the loop header in question. The merge block
+    // for the loop must have a ladder block before the old merge block. This
+    // ladder block will break to outer scope, or keep executing the old merge
+    // block.
+    for (size_t i = node->headers.size() - 1; i; i--) {
+      // Find innermost loop header scope we can break to when resolving
+      // ladders.
+      CFGNode *target_header = get_target_break_block_for_inner_header(node, i);
+
+      if (node->headers[i]->merge == MergeType::Loop) {
+        auto *loop_ladder = get_or_create_ladder_block(node, i);
+
+        // The loop ladder needs to break to somewhere.
+        // Either this is an outer loop scope, or we need to create a fake loop
+        // we can break out of if the break is non-trivial.
+        if (loop_ladder && !target_header && !full_break_target)
+          full_break_target =
+              build_enclosing_break_target_for_loop_ladder(node, loop_ladder);
+
+        CFGNode *new_ladder_block = nullptr;
+        if (loop_ladder) {
+          new_ladder_block = build_ladder_block_for_escaping_edge_handling(
+              node, node->headers[i], loop_ladder, target_header,
+              full_break_target, normal_preds[i]);
+        }
+
+        // We won't analyze this again, so make sure header knows
+        // about the new merge block.
+        if (node->headers[i]->freeze_structured_analysis) {
+          if (new_ladder_block)
+            node->headers[i]->loop_ladder_block = new_ladder_block;
+          node->headers[i]->loop_merge_block =
+              node->headers[i]->loop_ladder_block;
+          node->headers[i]->loop_ladder_block = nullptr;
+        }
+      } else if (node->headers[i]->merge == MergeType::Selection) {
+        if (target_header) {
+          // Breaks out to outer available scope.
+          CFGNode *rewrite_to = nullptr;
+          if (target_header->loop_ladder_block)
+            rewrite_to = target_header->loop_ladder_block;
+          else if (target_header->loop_merge_block)
+            rewrite_to = target_header->loop_merge_block;
+
+          if (rewrite_to)
+            node->headers[i]->traverse_dominated_blocks_and_rewrite_branch(
+                node, rewrite_to);
+          else
+            LOGW("No loop merge block?\n");
+        } else if (full_break_target) {
+          node->headers[i]->traverse_dominated_blocks_and_rewrite_branch(
+              node, full_break_target);
+        } else {
+          // The outer scope *must* now become a loop, no matter what.
+          // We cannot rely on a traversal to rewrite breaking constructs in the
+          // entire loop, so "everything" must essentially become a break
+          // instead.
+          full_break_target = node;
+          assert(node->headers[0]->merge == MergeType::Selection);
+          node->headers[0]->merge = MergeType::Loop;
+          node->headers[0]->freeze_structured_analysis = true;
+
+          assert(node->headers[0]->selection_merge_block == node);
+          node->headers[0]->loop_merge_block =
+              node->headers[0]->selection_merge_block;
+          node->headers[0]->selection_merge_block = nullptr;
+        }
+      } else
+        LOGE("Invalid merge type.\n");
+    }
+  }
+}
+
+void CFGStructurizer::structurize(unsigned pass) {
+  if (find_switch_blocks(pass)) {
+    recompute_cfg();
+    if (find_switch_blocks(pass)) {
+      LOGE("Fatal, detected infinite loop.\n");
+      abort();
+    }
+  }
+
+  find_loops();
+  find_selection_merges(pass);
+  fixup_broken_selection_merges(pass);
+  if (pass == 0)
+    split_merge_blocks();
+}
+
+bool CFGStructurizer::exists_path_in_cfg_without_intermediate_node(
+    const CFGNode *start_block, const CFGNode *end_block,
+    const CFGNode *stop_block) const {
+  if (query_reachability(*start_block, *end_block) &&
+      query_reachability(*start_block, *stop_block) &&
+      query_reachability(*stop_block, *end_block)) {
+    auto *frontier = get_post_dominance_frontier_with_cfg_subset_that_reaches(
+        stop_block, end_block, start_block);
+    // We already know start_block reaches the frontier.
+    return frontier != nullptr;
+  } else {
+    bool ret = query_reachability(*start_block, *end_block);
+    return ret;
+  }
+}
+
+CFGNode *
+CFGStructurizer::get_post_dominance_frontier_with_cfg_subset_that_reaches(
+    const CFGNode *node, const CFGNode *must_reach,
+    const CFGNode *must_reach_frontier) const {
+  std::unordered_set<const CFGNode *> promoted_post_dominators;
+  promoted_post_dominators.insert(node);
+  auto frontiers = node->post_dominance_frontier;
+
+  assert(query_reachability(*node, *must_reach));
+
+  if (frontiers.empty())
+    return nullptr;
+
+  while (!frontiers.empty()) {
+    // We might not be interested in post-domination-frontiers that we cannot
+    // reach. Filter our search based on this.
+    if (must_reach_frontier) {
+      auto itr = std::remove_if(
+          frontiers.begin(), frontiers.end(), [&](CFGNode *candidate) {
+            return !query_reachability(*must_reach_frontier, *candidate);
+          });
+      frontiers.erase(itr, frontiers.end());
+    }
+
+    if (frontiers.size() > 1) {
+      std::sort(frontiers.begin(), frontiers.end(),
+                [](const CFGNode *a, const CFGNode *b) {
+                  return a->backward_post_visit_order <
+                         b->backward_post_visit_order;
+                });
+      frontiers.erase(std::unique(frontiers.begin(), frontiers.end()),
+                      frontiers.end());
+    } else if (frontiers.empty())
+      break;
+
+    auto &frontier = frontiers.back();
+
+    // For a frontier to be discounted, we look at all successors and check
+    // if there no node in promoted_post_dominators that post-dominate the
+    // successor, that path cannot reach must_reach. If a post dominance
+    // frontier satisfies this rule, it is promoted to be considered an alias of
+    // node.
+
+    bool all_succs_must_go_via_node = true;
+    for (auto *succ : frontier->succ) {
+      bool promote = true;
+      if (query_reachability(*succ, *must_reach)) {
+        promote = false;
+        for (auto *pdom : promoted_post_dominators) {
+          if (pdom->post_dominates(succ)) {
+            promote = true;
+            break;
+          }
+        }
+      }
+
+      if (!promote) {
+        all_succs_must_go_via_node = false;
+        break;
+      }
+    }
+
+    if (!all_succs_must_go_via_node) {
+      return frontier;
+    } else {
+      promoted_post_dominators.insert(frontier);
+      frontiers.pop_back();
+      for (auto *pdoms : frontier->post_dominance_frontier)
+        frontiers.push_back(pdoms);
+    }
+  }
+
+  return frontiers.empty() ? nullptr : frontiers.front();
+}
+
+void CFGStructurizer::recompute_post_dominance_frontier(CFGNode *node) {
+  for (auto *pred : node->pred) {
+    if (pred->immediate_post_dominator != node &&
+        std::find(node->post_dominance_frontier.begin(),
+                  node->post_dominance_frontier.end(),
+                  pred) == node->post_dominance_frontier.end()) {
+      node->post_dominance_frontier.push_back(pred);
+    }
+
+    if (auto *ipdom = node->immediate_post_dominator) {
+      for (auto *frontier_node : node->post_dominance_frontier) {
+        if (!ipdom->post_dominates(frontier_node) &&
+            std::find(ipdom->post_dominance_frontier.begin(),
+                      ipdom->post_dominance_frontier.end(),
+                      frontier_node) == ipdom->post_dominance_frontier.end()) {
+          ipdom->post_dominance_frontier.push_back(frontier_node);
+        }
+      }
+    }
+  }
+}
+
+void CFGStructurizer::recompute_dominance_frontier(CFGNode *node) {
+  for (auto *succ : node->succ) {
+    if (succ->immediate_dominator != node &&
+        std::find(node->dominance_frontier.begin(),
+                  node->dominance_frontier.end(),
+                  succ) == node->dominance_frontier.end()) {
+      node->dominance_frontier.push_back(succ);
+    }
+
+    if (auto *idom = node->immediate_dominator) {
+      for (auto *frontier_node : node->dominance_frontier) {
+        if (!idom->dominates(frontier_node) &&
+            std::find(idom->dominance_frontier.begin(),
+                      idom->dominance_frontier.end(),
+                      frontier_node) == idom->dominance_frontier.end()) {
+          idom->dominance_frontier.push_back(frontier_node);
+        }
+      }
+    }
+  }
+}
+
+void CFGStructurizer::validate_structured() {
+  for (auto *node : forward_post_visit_order) {
+    if (node->headers.size() > 1) {
+      LOGE("Node %s has %u headers!\n", node->name.c_str(),
+           unsigned(node->headers.size()));
+      return;
+    }
+
+    if (node->merge == MergeType::Loop) {
+      if (!node->dominates(node->loop_merge_block) &&
+          !node->loop_merge_block->pred.empty()) {
+        LOGE("Node %s does not dominate its merge block %s!\n",
+             node->name.c_str(), node->loop_merge_block->name.c_str());
+        return;
+      }
+    } else if (node->merge == MergeType::Selection) {
+      if (!node->selection_merge_block) {
+        if (!node->selection_merge_exit)
+          LOGE("No selection merge block for %s\n", node->name.c_str());
+      } else if (!node->dominates(node->selection_merge_block) &&
+                 !node->selection_merge_block->pred.empty()) {
+        LOGE("Node %s does not dominate its selection merge block %s!\n",
+             node->name.c_str(), node->selection_merge_block->name.c_str());
+        return;
+      }
+    }
+
+    if (node->succ.size() >= 2 && node->merge == MergeType::None) {
+      // This might not be critical.
+      LOGW("Node %s has %u successors, but no merge header.\n",
+           node->name.c_str(), unsigned(node->succ.size()));
+    }
+  }
+  // LOGI("Successful CFG validation!\n");
+}
+
+void CFGStructurizer::traverse(BlockEmissionInterface &iface) {
+  // Make sure all blocks are known to the backend before we emit code.
+  // Prefer that IDs grow the further down the function we go.
+  for (auto itr = forward_post_visit_order.rbegin();
+       itr != forward_post_visit_order.rend(); ++itr) {
+    //(*itr)->id = 0;
+    iface.register_block(*itr);
+  }
+
+  // Need to emit blocks such that dominating blocks come before dominated
+  // blocks.
+  for (auto index = forward_post_visit_order.size(); index; index--) {
+    auto *block = forward_post_visit_order[index - 1];
+
+    auto &merge = block->ir.merge_info;
+
+    switch (block->merge) {
+    case MergeType::Selection:
+      merge.merge_block = block->selection_merge_block;
+      if (merge.merge_block)
+        iface.register_block(merge.merge_block);
+      merge.merge_type = block->merge;
+      iface.emit_basic_block(block);
+      break;
+
+    case MergeType::Loop:
+      merge.merge_block = block->loop_merge_block;
+      merge.merge_type = block->merge;
+      merge.continue_block = block->pred_back_edge;
+      if (merge.merge_block)
+        iface.register_block(merge.merge_block);
+      if (merge.continue_block)
+        iface.register_block(merge.continue_block);
+
+      iface.emit_basic_block(block);
+      break;
+
+    default:
+      iface.emit_basic_block(block);
+      break;
+    }
+  }
+}
+} // namespace llvm
diff --git a/llvm/lib/Transforms/LibFloor/cfg/cfg_translator.cpp b/llvm/lib/Transforms/LibFloor/cfg/cfg_translator.cpp
new file mode 100644
index 000000000000..f89e43a92654
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/cfg/cfg_translator.cpp
@@ -0,0 +1,610 @@
+/*
+ * Copyright 2021 Florian Ziesche
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// LLVM compat/translator for the dxil-spirv CFG structurizer
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor/cfg/cfg_translator.hpp"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/Intrinsics.h"
+#include "llvm/IR/Module.h"
+#include <algorithm>
+#include <assert.h>
+
+namespace llvm {
+
+cfg_translator::cfg_translator(Function &F_, LLVMContext &ctx_,
+                               CFGNodePool &pool_)
+    : F(F_), M(*F.getParent()), ctx(ctx_), pool(pool_) {
+  run();
+}
+
+void cfg_translator::run() {
+  // create nodes for all BBs first
+  for (auto &BB : F) {
+    bb_map[&BB] = pool.create_node(BB.getName().str(), &BB);
+  }
+
+  // translate instructions in all BBs + connect BBs
+  for (auto &BB : F) {
+    translate_bb(*bb_map.at(&BB));
+  }
+
+  // set entry block
+  entry = bb_map.at(&F.getEntryBlock());
+}
+
+static inline Terminator::Type get_terminator_type(Instruction &instr) {
+  if (auto br = dyn_cast_or_null<BranchInst>(&instr)) {
+    return (br->isConditional() ? Terminator::Type::Condition
+                                : Terminator::Type::Branch);
+  } else if (auto ret = dyn_cast_or_null<ReturnInst>(&instr)) {
+    return Terminator::Type::Return;
+  } else if (auto unreachable = dyn_cast_or_null<UnreachableInst>(&instr)) {
+    if (auto CI = dyn_cast_or_null<CallInst>(instr.getPrevNode());
+        CI && CI->getCalledFunction()->getName() == "floor.discard_fragment") {
+      return Terminator::Type::Kill;
+    }
+    return Terminator::Type::Unreachable;
+  } else if (auto sw = dyn_cast_or_null<SwitchInst>(&instr)) {
+    return Terminator::Type::Switch;
+  }
+  assert(false && "unsupported terminator instruction");
+  abort();
+}
+
+void cfg_translator::translate_bb(CFGNode &node) {
+  // translate all BB content
+  for (auto &instr : node.BB) {
+    if (auto phi = dyn_cast_or_null<PHINode>(&instr)) {
+      PHI ph{.phi = phi};
+      std::unordered_set<BasicBlock *> phi_bbs;
+      for (uint32_t i = 0, count = phi->getNumIncomingValues(); i < count;
+           ++i) {
+        auto in_bb = phi->getIncomingBlock(i);
+        if (phi_bbs.count(in_bb) > 0) {
+          // ignore duplicate incoming blocks
+          continue;
+        } else {
+          phi_bbs.emplace(in_bb);
+        }
+
+        ph.incoming.emplace_back(IncomingValue{
+            .block = bb_map.at(in_bb),
+            .value = phi->getIncomingValue(i),
+        });
+      }
+      node.ir.phi.emplace_back(ph);
+    } else if (instr.isTerminator()) {
+      node.ir.terminator.terminator = &instr;
+      node.ir.terminator.type = get_terminator_type(instr);
+      switch (node.ir.terminator.type) {
+      case Terminator::Type::Condition: {
+        auto br = dyn_cast<BranchInst>(&instr);
+        node.ir.terminator.condition = br->getCondition();
+        node.ir.terminator.true_block = bb_map.at(br->getSuccessor(0));
+        node.ir.terminator.false_block = bb_map.at(br->getSuccessor(1));
+        node.add_branch(node.ir.terminator.true_block);
+        node.add_branch(node.ir.terminator.false_block);
+        break;
+      }
+      case Terminator::Type::Branch: {
+        auto br = dyn_cast<BranchInst>(&instr);
+        node.ir.terminator.direct_block = bb_map.at(br->getSuccessor(0));
+        node.add_branch(node.ir.terminator.direct_block);
+        break;
+      }
+      case Terminator::Type::Return: {
+        auto ret = dyn_cast<ReturnInst>(&instr);
+        node.ir.terminator.return_value = ret->getReturnValue();
+        break;
+      }
+      case Terminator::Type::Unreachable:
+      case Terminator::Type::Kill:
+        // NOTE: we don't have a specific terminator for Kill instructions
+        // (reuses Unreachable)
+        break;
+      case Terminator::Type::Switch: {
+        auto sw = dyn_cast<SwitchInst>(&instr);
+        node.ir.terminator.default_node = bb_map.at(sw->getDefaultDest());
+        node.ir.terminator.condition = sw->getCondition();
+        node.add_branch(node.ir.terminator.default_node);
+        for (auto &cs : sw->cases()) {
+          node.ir.terminator.cases.emplace_back(Terminator::Case{
+              .node = bb_map.at(cs.getCaseSuccessor()),
+              .value = cs.getCaseValue(),
+          });
+          node.add_branch(node.ir.terminator.cases.back().node);
+        }
+        break;
+      }
+      }
+    } else {
+      // normal instruction
+      node.ir.operations.emplace_back(&instr);
+    }
+  }
+}
+
+void cfg_translator::add_or_update_terminator(CFGNode &node) {
+  // remove existing if there is one
+  if (auto existing_term = node.BB.getTerminator(); existing_term) {
+    existing_term->eraseFromParent();
+  }
+
+  // add new LLVM terminator
+  auto &term = node.ir.terminator;
+  switch (term.type) {
+  case Terminator::Type::Condition: {
+    BranchInst::Create(&term.true_block->BB, &term.false_block->BB,
+                       term.condition, &node.BB);
+    break;
+  }
+  case Terminator::Type::Branch: {
+    BranchInst::Create(&term.direct_block->BB, &node.BB);
+    break;
+  }
+  case Terminator::Type::Return: {
+    if (term.return_value) {
+      ReturnInst::Create(ctx, term.return_value, &node.BB);
+    } else {
+      ReturnInst::Create(ctx, &node.BB);
+    }
+    break;
+  }
+  case Terminator::Type::Kill: {
+    bool create_discard_fragment = false;
+    if (node.BB.empty()) {
+      create_discard_fragment = true;
+    } else {
+      if (auto CI = dyn_cast_or_null<CallInst>(&node.BB.back());
+          CI &&
+          CI->getCalledFunction()->getName() == "floor.discard_fragment") {
+        // -> already exists
+      } else {
+        create_discard_fragment = true;
+      }
+    }
+    if (create_discard_fragment) {
+      Function *discard_func = M.getFunction("floor.discard_fragment");
+      assert(discard_func &&
+             "discard function must have already existed if we get here");
+      CallInst *discard_call = CallInst::Create(discard_func, "", &node.BB);
+      discard_call->setCallingConv(CallingConv::FLOOR_FUNC);
+      discard_call->setCannotDuplicate();
+    }
+  }
+    LLVM_FALLTHROUGH;
+  case Terminator::Type::Unreachable:
+    new UnreachableInst(ctx, &node.BB);
+    break;
+  case Terminator::Type::Switch: {
+    auto sw = SwitchInst::Create(term.condition, &term.default_node->BB,
+                                 term.cases.size(), &node.BB);
+    for (auto &cs : term.cases) {
+      sw->addCase(cs.value, &cs.node->BB);
+    }
+    break;
+  }
+  }
+}
+
+void cfg_translator::cfg_to_llvm_ir(CFGNode *updated_entry_block,
+                                    const bool add_merge_annotations) {
+  if (entry != updated_entry_block) {
+    // move new entry to the front
+    F.getBasicBlockList().erase(&updated_entry_block->BB);
+    F.getBasicBlockList().push_front(&updated_entry_block->BB);
+  }
+  entry = updated_entry_block;
+
+  // update terminators
+  pool.for_each_node([this](CFGNode &node) {
+    auto terminator = node.BB.getTerminator();
+    if (!terminator) {
+      // probably a new BB w/o an LLVM terminator -> create one
+      add_or_update_terminator(node);
+    } else {
+      // check if the existing terminator is equal to the CFG terminator
+      const auto term_type = get_terminator_type(*terminator);
+      if (term_type != node.ir.terminator.type) {
+        // different type -> update
+        add_or_update_terminator(node);
+      } else {
+        // equal type, check if operands are the same
+        switch (term_type) {
+        case Terminator::Type::Condition: {
+          auto br = dyn_cast<BranchInst>(terminator);
+          assert(br->getNumSuccessors() == 2);
+          assert(node.ir.terminator.true_block &&
+                 node.ir.terminator.false_block);
+          if (br->getCondition() != node.ir.terminator.condition ||
+              br->getSuccessor(0) != &node.ir.terminator.true_block->BB ||
+              br->getSuccessor(1) != &node.ir.terminator.false_block->BB) {
+            add_or_update_terminator(node);
+          }
+          break;
+        }
+        case Terminator::Type::Branch: {
+          auto br = dyn_cast<BranchInst>(terminator);
+          assert(br->getNumSuccessors() == 1);
+          assert(node.ir.terminator.direct_block);
+          if (br->getSuccessor(0) != &node.ir.terminator.direct_block->BB) {
+            add_or_update_terminator(node);
+          }
+          break;
+        }
+        case Terminator::Type::Return: {
+          auto ret = dyn_cast<ReturnInst>(terminator);
+          if (ret->getReturnValue() != node.ir.terminator.return_value) {
+            add_or_update_terminator(node);
+          }
+          break;
+        }
+        case Terminator::Type::Unreachable:
+        case Terminator::Type::Kill:
+          // no operands to check
+          break;
+        case Terminator::Type::Switch: {
+          auto sw = dyn_cast<SwitchInst>(terminator);
+          if (sw->getDefaultDest() != &node.ir.terminator.default_node->BB ||
+              sw->getCondition() != node.ir.terminator.condition ||
+              sw->getNumCases() != node.ir.terminator.cases.size()) {
+            add_or_update_terminator(node);
+          } else {
+            auto case_iter = sw->case_begin();
+            for (uint32_t i = 0, count = sw->getNumCases(); i < count;
+                 ++i, ++case_iter) {
+              auto &llvm_case = *case_iter;
+              auto &cfg_case = node.ir.terminator.cases[i];
+              if (llvm_case.getCaseSuccessor() != &cfg_case.node->BB ||
+                  llvm_case.getCaseValue() != cfg_case.value) {
+                add_or_update_terminator(node);
+                break;
+              }
+            }
+          }
+          break;
+        }
+        }
+      }
+    }
+  });
+
+  // compute (simple) reachability
+  std::unordered_set<const BasicBlock *> reachable_blocks;
+  std::function<void(const BasicBlock &BB)> compute_simple_reachability =
+      [&compute_simple_reachability, &reachable_blocks](const BasicBlock &BB) {
+        // already visited?
+        if (reachable_blocks.count(&BB) > 0) {
+          return;
+        }
+        reachable_blocks.emplace(&BB);
+
+        // visit children
+        auto term = BB.getTerminator();
+        assert(term != nullptr);
+        if (auto br = dyn_cast_or_null<BranchInst>(term)) {
+          for (auto succ : br->successors()) {
+            compute_simple_reachability(*succ);
+          }
+        } else if (auto sw = dyn_cast_or_null<SwitchInst>(term)) {
+          for (uint32_t succ_idx = 0, succ_count = sw->getNumSuccessors();
+               succ_idx < succ_count; ++succ_idx) {
+            compute_simple_reachability(*sw->getSuccessor(succ_idx));
+          }
+        } else if (auto ret = dyn_cast_or_null<ReturnInst>(term)) {
+          // nop
+        } else if (auto ur = dyn_cast_or_null<UnreachableInst>(term)) {
+          // nop
+        } else {
+          assert(false && "unknown/unhandled terminator type");
+        }
+      };
+  compute_simple_reachability(entry->BB);
+
+  // update PHIs
+  pool.for_each_node(
+      [&reachable_blocks](CFGNode &node) {
+        assert(!node.BB.empty());
+
+        // skip unreachable BBs (i.e. w/o a predecessor / unreachable ones ->
+        // will be killed later)
+        if (reachable_blocks.count(&node.BB) == 0) {
+          return;
+        }
+
+        for (auto &instr : node.BB) {
+          auto phi = dyn_cast_or_null<PHINode>(&instr);
+          if (!phi) {
+            continue;
+          }
+
+          // remove existing incoming values
+          // NOTE: this also ensures that unreachable BBs are cleared out -> no
+          // longer have any users
+          for (uint32_t idx = 0, count = phi->getNumIncomingValues();
+               idx < count; ++idx) {
+            phi->removeIncomingValue(0u, false /* do NOT delete the phi once no incoming values are left */);
+          }
+          assert(phi->getNumIncomingValues() == 0);
+
+          // find corresponding PHI entry
+          auto piter =
+              find_if(node.ir.phi.begin(), node.ir.phi.end(),
+                      [&phi](const PHI &elem) { return elem.phi == phi; });
+          if (piter == node.ir.phi.end()) {
+            assert(false && "couldn't find corresponding PHI");
+            continue;
+          }
+          assert(!piter->incoming.empty());
+
+          // add actual/updated incoming values
+          for (auto &incoming : piter->incoming) {
+            if (reachable_blocks.count(&incoming.block->BB) > 0) {
+              phi->addIncoming(incoming.value, &incoming.block->BB);
+            } else {
+              assert(false && "phi incoming BB unreachable");
+            }
+          }
+
+          // handle the awkwardness that is duplicate predecessor blocks in LLVM
+          // ...
+          if (node.BB.hasNPredecessorsOrMore(phi->getNumIncomingValues() + 1)) {
+            std::unordered_map<const BasicBlock *, Value *> unique_preds;
+            std::vector<BasicBlock *> dup_preds;
+            uint32_t ignored_bbs = 0;
+            for (auto pred : predecessors(&node.BB)) {
+              if (reachable_blocks.count(pred) == 0) {
+                // ignore unreachable predecessors that will be removed next
+                ++ignored_bbs;
+                continue;
+              }
+              if (unique_preds.count(pred) == 0) {
+                unique_preds.emplace(pred, phi->getIncomingValueForBlock(pred));
+              } else {
+                dup_preds.emplace_back(pred);
+              }
+            }
+            for (auto &dup_pred : dup_preds) {
+              phi->addIncoming(unique_preds.at(dup_pred), dup_pred);
+            }
+          }
+        }
+      });
+
+  // remove BBs without a predecessor
+  // -> pass #1: gather all instructions inside BBs and drop their references
+  std::vector<Instruction *> instr_kill_list;
+  pool.for_each_node([&reachable_blocks, &instr_kill_list](CFGNode &node) {
+    if (reachable_blocks.count(&node.BB) > 0) {
+      return;
+    }
+    for (auto &instr : node.BB) {
+      // need to drop all references, because this might be ref'ed by another
+      // instruction that will be deleted
+      instr.dropAllReferences();
+      instr_kill_list.emplace_back(&instr);
+    }
+  });
+  // -> pass #2: remove all gathered instructions
+  for (auto &instr : instr_kill_list) {
+    instr->eraseFromParent();
+  }
+  // -> pass #3: actually remove the BBs
+  std::vector<CFGNode *> rem_nodes;
+  pool.for_each_node([&reachable_blocks, &rem_nodes](CFGNode &node) {
+    if (reachable_blocks.count(&node.BB) > 0) {
+      return;
+    }
+    assert(node.BB.users().empty() && "unreachable BB still has users");
+    assert(node.BB.uses().empty() && "unreachable BB still has uses");
+    node.BB.eraseFromParent();
+    rem_nodes.emplace_back(&node);
+  });
+  for (auto &rem_node : rem_nodes) {
+    pool.remove_node(*rem_node);
+  }
+
+  // add merge annotations
+  if (add_merge_annotations) {
+    pool.for_each_node([this](CFGNode &node) {
+      if (node.merge == MergeType::None) {
+        return;
+      }
+
+      auto term = node.BB.getTerminator();
+      assert(term != nullptr &&
+             "BB with merge annotation must have a terminator");
+      if (node.merge == MergeType::Selection) {
+        // special case: no selection merge block, because at least one BB exits
+        if (node.selection_merge_block == nullptr &&
+            node.selection_merge_exit) {
+          if (node.ir.terminator.type == Terminator::Type::Condition) {
+            auto br = dyn_cast_or_null<BranchInst>(term);
+            assert(br != nullptr);
+            assert(br->getNumSuccessors() == 2);
+            auto unreach_0 = dyn_cast_or_null<UnreachableInst>(
+                br->getSuccessor(0)->getTerminator());
+            auto unreach_1 = dyn_cast_or_null<UnreachableInst>(
+                br->getSuccessor(1)->getTerminator());
+            if (unreach_0 && !unreach_1) {
+              // 0 is unreachable, 1 is not -> merge to 1
+              create_selection_merge(term, br->getSuccessor(1));
+            } else if (!unreach_0 && unreach_1) {
+              // 1 is unreachable, 0 is not -> merge to 0
+              create_selection_merge(term, br->getSuccessor(0));
+            }
+            // else: ignore this
+            return;
+          } else if (node.ir.terminator.type == Terminator::Type::Switch) {
+            assert(
+                false &&
+                "can't handle exit selection merge on switch instruction yet");
+            return;
+          }
+          assert(false && "invalid terminator");
+        }
+
+        assert(node.selection_merge_block != nullptr);
+        if (node.ir.terminator.type == Terminator::Type::Condition ||
+            node.ir.terminator.type == Terminator::Type::Switch) {
+          if (node.ir.terminator.type == Terminator::Type::Condition) {
+            auto br = dyn_cast_or_null<BranchInst>(term);
+            assert(br != nullptr);
+            assert(br->getNumSuccessors() == 2);
+          } else if (node.ir.terminator.type == Terminator::Type::Switch) {
+            auto sw = dyn_cast_or_null<SwitchInst>(term);
+            assert(sw != nullptr);
+            assert(sw->getNumSuccessors() > 0);
+          }
+          create_selection_merge(term, &node.selection_merge_block->BB);
+        } else {
+          llvm_unreachable("invalid selection merge");
+        }
+      } else if (node.merge == MergeType::Loop) {
+        if (node.loop_merge_block != nullptr) {
+          if (node.pred_back_edge != nullptr) {
+            create_loop_merge(term, &node.loop_merge_block->BB,
+                              &node.pred_back_edge->BB);
+          } else {
+            // continue block does not exist -> need to create a fake incoming
+            // block
+            auto continue_block = BasicBlock::Create(
+                ctx, node.name + ".fake_continue", &F, &node.BB);
+            BranchInst::Create(&node.BB, continue_block);
+            create_loop_merge(term, &node.loop_merge_block->BB, continue_block);
+
+            // update phis: need to insert incoming undef value for the new
+            // continue block
+            for (auto &phi : node.BB.phis()) {
+              phi.addIncoming(UndefValue::get(phi.getType()), continue_block);
+            }
+          }
+        } else {
+          llvm_unreachable("invalid loop merge");
+        }
+      }
+    });
+  }
+}
+
+CallInst *cfg_translator::insert_merge_block_marker(BasicBlock *merge_block) {
+  const std::string merge_block_func_name = "floor.merge_block";
+  Function *merge_block_func = M.getFunction(merge_block_func_name);
+  if (merge_block_func == nullptr) {
+    FunctionType *merge_block_type =
+        FunctionType::get(llvm::Type::getVoidTy(ctx), false);
+    merge_block_func =
+        (Function *)M
+            .getOrInsertFunction(merge_block_func_name, merge_block_type)
+            .getCallee();
+    merge_block_func->setCallingConv(CallingConv::FLOOR_FUNC);
+    merge_block_func->setCannotDuplicate();
+    merge_block_func->setConvergent();
+    merge_block_func->setDoesNotRecurse();
+  }
+
+  CallInst *merge_block_call =
+      CallInst::Create(merge_block_func, "", merge_block->getTerminator());
+  merge_block_call->setCallingConv(CallingConv::FLOOR_FUNC);
+  merge_block_call->setConvergent();
+  merge_block_call->setCannotDuplicate();
+  return merge_block_call;
+}
+
+CallInst *
+cfg_translator::insert_continue_block_marker(BasicBlock *continue_block) {
+  const std::string continue_block_func_name = "floor.continue_block";
+  Function *continue_block_func = M.getFunction(continue_block_func_name);
+  if (continue_block_func == nullptr) {
+    FunctionType *continue_block_type =
+        FunctionType::get(llvm::Type::getVoidTy(ctx), false);
+    continue_block_func =
+        (Function *)M
+            .getOrInsertFunction(continue_block_func_name, continue_block_type)
+            .getCallee();
+    continue_block_func->setCallingConv(CallingConv::FLOOR_FUNC);
+    continue_block_func->setCannotDuplicate();
+    continue_block_func->setConvergent();
+    continue_block_func->setDoesNotRecurse();
+  }
+
+  CallInst *continue_block_call = CallInst::Create(
+      continue_block_func, "", continue_block->getTerminator());
+  continue_block_call->setCallingConv(CallingConv::FLOOR_FUNC);
+  continue_block_call->setConvergent();
+  continue_block_call->setCannotDuplicate();
+  return continue_block_call;
+}
+
+void cfg_translator::create_loop_merge(Instruction *insert_before,
+                                       BasicBlock *bb_merge,
+                                       BasicBlock *bb_continue) {
+  Function *loop_merge_func = M.getFunction("floor.loop_merge");
+  if (loop_merge_func == nullptr) {
+    llvm::Type *loop_merge_arg_types[]{llvm::Type::getLabelTy(ctx),
+                                       llvm::Type::getLabelTy(ctx)};
+    FunctionType *loop_merge_type = FunctionType::get(
+        llvm::Type::getVoidTy(ctx), loop_merge_arg_types, false);
+    loop_merge_func =
+        (Function *)M.getOrInsertFunction("floor.loop_merge", loop_merge_type)
+            .getCallee();
+    loop_merge_func->setCallingConv(CallingConv::FLOOR_FUNC);
+    loop_merge_func->setCannotDuplicate();
+    loop_merge_func->setDoesNotThrow();
+    loop_merge_func->setNotConvergent();
+    loop_merge_func->setDoesNotRecurse();
+  }
+  llvm::Value *merge_vars[]{bb_merge, bb_continue};
+  CallInst *loop_merge_call =
+      CallInst::Create(loop_merge_func, merge_vars, "", insert_before);
+  loop_merge_call->setCallingConv(CallingConv::FLOOR_FUNC);
+
+  insert_merge_block_marker(bb_merge);
+  insert_continue_block_marker(bb_continue);
+}
+
+void cfg_translator::create_selection_merge(Instruction *insert_before,
+                                            BasicBlock *merge_block) {
+  Function *sel_merge_func = M.getFunction("floor.selection_merge");
+  if (sel_merge_func == nullptr) {
+    llvm::Type *sel_merge_arg_types[]{llvm::Type::getLabelTy(ctx)};
+    FunctionType *sel_merge_type = FunctionType::get(
+        llvm::Type::getVoidTy(ctx), sel_merge_arg_types, false);
+    sel_merge_func =
+        (Function *)M
+            .getOrInsertFunction("floor.selection_merge", sel_merge_type)
+            .getCallee();
+    sel_merge_func->setCallingConv(CallingConv::FLOOR_FUNC);
+    sel_merge_func->setCannotDuplicate();
+    sel_merge_func->setDoesNotThrow();
+    sel_merge_func->setNotConvergent();
+    sel_merge_func->setDoesNotRecurse();
+  }
+  llvm::Value *merge_vars[]{merge_block};
+  CallInst *sel_merge_call =
+      CallInst::Create(sel_merge_func, merge_vars, "", insert_before);
+  sel_merge_call->setCallingConv(CallingConv::FLOOR_FUNC);
+
+  insert_merge_block_marker(merge_block);
+}
+
+} // namespace llvm
diff --git a/llvm/lib/Transforms/LibFloor/cfg/node.cpp b/llvm/lib/Transforms/LibFloor/cfg/node.cpp
new file mode 100644
index 000000000000..e61df9f372b5
--- /dev/null
+++ b/llvm/lib/Transforms/LibFloor/cfg/node.cpp
@@ -0,0 +1,516 @@
+/*
+ * Copyright 2019-2021 Hans-Kristian Arntzen for Valve Corporation
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+//==-----------------------------------------------------------------------===//
+//
+// dxil-spirv CFG structurizer adopted for LLVM use
+// ref: https://github.com/HansKristian-Work/dxil-spirv
+// @ 189cc855b471591763d9951d63e51c72649037ab
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/LibFloor/cfg/node.hpp"
+
+#include <algorithm>
+#include <assert.h>
+
+namespace llvm {
+CFGNode::CFGNode(CFGNodePool &pool_, BasicBlock &BB_, std::string name_)
+    : BB(BB_), name(name_), pool(pool_) {}
+
+void CFGNode::add_unique_pred(CFGNode *node) {
+  auto itr = std::find(pred.begin(), pred.end(), node);
+  if (itr == pred.end())
+    pred.push_back(node);
+}
+
+void CFGNode::add_unique_fake_pred(CFGNode *node) {
+  auto itr = std::find(fake_pred.begin(), fake_pred.end(), node);
+  if (itr == fake_pred.end())
+    fake_pred.push_back(node);
+}
+
+void CFGNode::add_unique_header(CFGNode *node) {
+  auto itr = std::find(headers.begin(), headers.end(), node);
+  if (itr == headers.end())
+    headers.push_back(node);
+}
+
+void CFGNode::add_branch(CFGNode *to) {
+  add_unique_succ(to);
+  to->add_unique_pred(this);
+}
+
+void CFGNode::add_fake_branch(CFGNode *to) {
+  add_unique_fake_succ(to);
+  to->add_unique_fake_pred(this);
+}
+
+void CFGNode::add_unique_succ(CFGNode *node) {
+  auto itr = std::find(succ.begin(), succ.end(), node);
+  if (itr == succ.end())
+    succ.push_back(node);
+}
+
+void CFGNode::add_unique_fake_succ(CFGNode *node) {
+  auto itr = std::find(fake_succ.begin(), fake_succ.end(), node);
+  if (itr == fake_succ.end())
+    fake_succ.push_back(node);
+}
+
+unsigned CFGNode::num_forward_preds() const { return unsigned(pred.size()); }
+
+bool CFGNode::has_pred_back_edges() const { return pred_back_edge != nullptr; }
+
+bool CFGNode::dominates(const CFGNode *other) const {
+  // Follow immediate dominator graph. Either we end up at this, or entry block.
+  while (this != other) {
+    if (!other->immediate_dominator || other == other->immediate_dominator)
+      break;
+    other = other->immediate_dominator;
+  }
+
+  return this == other;
+}
+
+bool CFGNode::can_loop_merge_to(const CFGNode *other) const {
+  if (!dominates(other))
+    return false;
+
+  auto *c = pred_back_edge;
+
+  if (c && !c->succ.empty()) {
+    // If the continue block branches to something which is not the loop header,
+    // it must be the merge block we're after, i.e., it must be a clean break
+    // (or we are kind of screwed). Detect a "fake" merge branch here. E.g., we
+    // have a fake merge branch if an escaping edge is branching to one block
+    // beyond the real merge block. This can happen after split_merge_scopes()
+    // transform where inner loop tries to break out of multiple loops and
+    // multiple selection scopes at the same time. We can still dominate this
+    // escape target, but it's still an escape which must be resolved some other
+    // way with ladders.
+    if (std::find(c->succ.begin(), c->succ.end(), other) == c->succ.end())
+      return false;
+  }
+
+  return true;
+}
+
+bool CFGNode::can_backtrace_to(
+    const CFGNode *parent,
+    std::unordered_set<const CFGNode *> &node_cache) const {
+  if (node_cache.count(this))
+    return false;
+  node_cache.insert(this);
+
+  for (auto *p : pred)
+    if (p == parent || p->can_backtrace_to(parent, node_cache))
+      return true;
+
+  return false;
+}
+
+bool CFGNode::can_backtrace_to(const CFGNode *parent) const {
+  // If parent can branch to this, then post_order(parent) must be greater than
+  // post_order(this).
+  if (parent->forward_post_visit_order < forward_post_visit_order)
+    return false;
+
+  std::unordered_set<const CFGNode *> node_cache;
+  return can_backtrace_to(parent, node_cache);
+}
+
+const CFGNode *
+CFGNode::get_innermost_loop_header_for(const CFGNode *other) const {
+  while (this != other) {
+    // Entry block case.
+    if (other->pred.empty())
+      break;
+
+    // Found a loop header. This better be the one.
+    if (other->pred_back_edge)
+      break;
+
+    assert(other->immediate_dominator);
+    other = other->immediate_dominator;
+  }
+
+  return other;
+}
+
+bool CFGNode::is_innermost_loop_header_for(const CFGNode *other) const {
+  return this == get_innermost_loop_header_for(other);
+}
+
+bool CFGNode::branchless_path_to(const CFGNode *to) const {
+  const auto *node = this;
+  while (node != to) {
+    if (node->succ.size() != 1 || node->succ_back_edge)
+      return false;
+    node = node->succ.front();
+  }
+
+  return true;
+}
+
+bool CFGNode::post_dominates(const CFGNode *start_node) const {
+  while (start_node != this) {
+    // Reached exit node.
+    if (!start_node->immediate_post_dominator ||
+        start_node == start_node->immediate_post_dominator)
+      break;
+    start_node = start_node->immediate_post_dominator;
+  }
+
+  return this == start_node;
+}
+
+bool CFGNode::dominates_all_reachable_exits(
+    std::unordered_set<const CFGNode *> &completed,
+    const CFGNode &header) const {
+  if (!completed.count(this)) {
+    if (succ_back_edge)
+      return false;
+
+    for (auto *node : succ)
+      if (!header.dominates(node) ||
+          !node->dominates_all_reachable_exits(completed, header))
+        return false;
+
+    completed.insert(this);
+  }
+
+  return true;
+}
+
+bool CFGNode::dominates_all_reachable_exits() const {
+  std::unordered_set<const CFGNode *> completed;
+  return dominates_all_reachable_exits(completed, *this);
+}
+
+CFGNode *CFGNode::find_common_post_dominator(CFGNode *a, CFGNode *b) {
+  assert(a);
+  assert(b);
+
+  while (a != b) {
+    if (!a->immediate_post_dominator) {
+      for (auto *p : a->succ)
+        p->recompute_immediate_post_dominator();
+      a->recompute_immediate_post_dominator();
+    }
+
+    if (!b->immediate_post_dominator) {
+      for (auto *p : b->succ)
+        p->recompute_immediate_post_dominator();
+      b->recompute_immediate_post_dominator();
+    }
+
+    if (a->backward_post_visit_order == b->backward_post_visit_order) {
+      // Should not normally happen, but when we insert ladder blocks,
+      // we might have assigned temporary visit orders which can alias with
+      // other nodes in some cases. Fixing this up requires a full traversal of
+      // the entire CFG, so as a fallback we can do direct reachability and
+      // domination analysis.
+      if (b->post_dominates(a))
+        return const_cast<CFGNode *>(b);
+      else if (a->post_dominates(b))
+        return const_cast<CFGNode *>(a);
+
+      // If there is no clear domination relationship, then we need to iterate
+      // both a and b. This is fine as we now know that neither a nor b can be
+      // the common node.
+      assert(a->immediate_post_dominator);
+      assert(b->immediate_post_dominator);
+      a = a->immediate_post_dominator;
+      b = b->immediate_post_dominator;
+    } else if (a->backward_post_visit_order < b->backward_post_visit_order) {
+      assert(a->immediate_post_dominator);
+      a = a->immediate_post_dominator;
+    } else {
+      assert(b->immediate_post_dominator);
+      b = b->immediate_post_dominator;
+    }
+  }
+  return const_cast<CFGNode *>(a);
+}
+
+CFGNode *CFGNode::find_common_dominator(CFGNode *a, CFGNode *b) {
+  assert(a);
+  assert(b);
+
+  while (a != b) {
+    if (!a->immediate_dominator) {
+      for (auto *p : a->pred)
+        p->recompute_immediate_dominator();
+      a->recompute_immediate_dominator();
+    }
+
+    if (!b->immediate_dominator) {
+      for (auto *p : b->pred)
+        p->recompute_immediate_dominator();
+      b->recompute_immediate_dominator();
+    }
+
+    if (a->forward_post_visit_order == b->forward_post_visit_order) {
+      // Should not normally happen, but when we insert ladder blocks,
+      // we might have assigned temporary visit orders which can alias with
+      // other nodes in some cases. Fixing this up requires a full traversal of
+      // the entire CFG, so as a fallback we can do direct reachability and
+      // domination analysis.
+      if (b->dominates(a))
+        return const_cast<CFGNode *>(b);
+      else if (a->dominates(b))
+        return const_cast<CFGNode *>(a);
+
+      // If there is no clear domination relationship, then we need to iterate
+      // both a and b. This is fine as we now know that neither a nor b can be
+      // the common node.
+      assert(a->immediate_dominator);
+      assert(b->immediate_dominator);
+      if (a == a->immediate_dominator)
+        return b;
+      else if (b == b->immediate_dominator)
+        return a;
+      a = a->immediate_dominator;
+      b = b->immediate_dominator;
+    } else if (a->forward_post_visit_order < b->forward_post_visit_order) {
+      // Awkward case which can happen when nodes are unreachable in the CFG.
+      // Can occur with the dummy blocks we create.
+      if (a == a->immediate_dominator)
+        return b;
+      a = a->immediate_dominator;
+    } else {
+      // Awkward case which can happen when nodes are unreachable in the CFG.
+      // Can occur with the dummy blocks we create.
+      if (b == b->immediate_dominator)
+        return a;
+
+      b = b->immediate_dominator;
+    }
+  }
+  return const_cast<CFGNode *>(a);
+}
+
+CFGNode *CFGNode::get_immediate_dominator_loop_header() {
+  assert(immediate_dominator);
+  auto *node = this;
+  while (!node->pred_back_edge) {
+    if (node->pred.empty())
+      return nullptr;
+
+    assert(node->immediate_dominator);
+    node = node->immediate_dominator;
+  }
+
+  return node;
+}
+
+void CFGNode::retarget_branch_with_intermediate_node(CFGNode *to_prev,
+                                                     CFGNode *to_next) {
+  auto *intermediate =
+      pool.create_node(name + ".intermediate." + to_next->name);
+  intermediate->ir.terminator.type = Terminator::Type::Branch;
+  intermediate->ir.terminator.direct_block = to_next;
+  intermediate->add_branch(to_next);
+  intermediate->immediate_post_dominator = to_next;
+  intermediate->immediate_dominator = this;
+  intermediate->forward_post_visit_order = forward_post_visit_order;
+  intermediate->backward_post_visit_order = backward_post_visit_order;
+  retarget_branch(to_prev, intermediate);
+  to_next->recompute_immediate_dominator();
+}
+
+void CFGNode::retarget_branch(CFGNode *to_prev, CFGNode *to_next) {
+  // LOGI("Retargeting branch for %s: %s -> %s\n", name.c_str(),
+  // to_prev->name.c_str(), to_next->name.c_str());
+  assert(std::find(succ.begin(), succ.end(), to_prev) != succ.end());
+  assert(std::find(to_prev->pred.begin(), to_prev->pred.end(), this) !=
+         to_prev->pred.end());
+  assert(std::find(succ.begin(), succ.end(), to_next) == succ.end());
+  assert(std::find(to_next->pred.begin(), to_next->pred.end(), this) ==
+         to_next->pred.end());
+
+  to_prev->pred.erase(
+      std::find(to_prev->pred.begin(), to_prev->pred.end(), this));
+
+  // Modify succ in place so we don't invalidate iterator in
+  // traverse_dominated_blocks_and_rewrite_branch.
+  *std::find(succ.begin(), succ.end(), to_prev) = to_next;
+
+  auto replace_itr =
+      std::find(to_next->pred.begin(), to_next->pred.end(), to_prev);
+  // If to_prev now becomes unreachable, just replace pred in-place to avoid a
+  // stale pred. The stale pred will be cleaned up later when recomputing CFG.
+  if (to_prev->pred.empty() && !to_prev->pred_back_edge &&
+      replace_itr != to_next->pred.end())
+    *replace_itr = this;
+  else
+    to_next->add_unique_pred(this);
+
+  // Branch targets have changed, so recompute immediate dominators.
+  if (to_prev->forward_post_visit_order > to_next->forward_post_visit_order) {
+    to_prev->recompute_immediate_dominator();
+    to_next->recompute_immediate_dominator();
+  } else {
+    to_next->recompute_immediate_dominator();
+    to_prev->recompute_immediate_dominator();
+  }
+
+  // ... and post dominator for ourself.
+  // I am not sure if it's technically possible that we have to recompute the
+  // entire post domination graph now?
+  recompute_immediate_post_dominator();
+
+  if (ir.terminator.direct_block == to_prev)
+    ir.terminator.direct_block = to_next;
+  if (ir.terminator.true_block == to_prev)
+    ir.terminator.true_block = to_next;
+  if (ir.terminator.false_block == to_prev)
+    ir.terminator.false_block = to_next;
+  if (ir.terminator.default_node == to_prev)
+    ir.terminator.default_node = to_next;
+  for (auto &c : ir.terminator.cases)
+    if (c.node == to_prev)
+      c.node = to_next;
+}
+
+void CFGNode::retarget_fake_succ(CFGNode *to_prev, CFGNode *to_next) {
+  assert(std::find(fake_succ.begin(), fake_succ.end(), to_prev) !=
+         fake_succ.end());
+  assert(std::find(to_prev->fake_pred.begin(), to_prev->fake_pred.end(),
+                   this) != to_prev->fake_pred.end());
+  assert(std::find(fake_succ.begin(), fake_succ.end(), to_next) ==
+         fake_succ.end());
+  assert(std::find(to_next->fake_pred.begin(), to_next->fake_pred.end(),
+                   this) == to_next->fake_pred.end());
+
+  // Modify fake_succ in place so we don't invalidate iterator in
+  // traverse_dominated_blocks_and_rewrite_branch.
+  *std::find(fake_succ.begin(), fake_succ.end(), to_prev) = to_next;
+  to_next->add_unique_fake_pred(this);
+
+  recompute_immediate_post_dominator();
+}
+
+void CFGNode::fixup_merge_info_after_branch_rewrite(CFGNode *from,
+                                                    CFGNode *to) {
+  // If we end up re-seating merge sites, make sure we add it to headers in the
+  // target block, since we might have to keep splitting merge scopes in
+  // innermost scopes.
+  if (std::find(from->headers.begin(), from->headers.end(), this) !=
+      from->headers.end()) {
+    if (std::find(to->headers.begin(), to->headers.end(), this) ==
+        to->headers.end())
+      to->headers.push_back(this);
+    if (selection_merge_block == from)
+      selection_merge_block = to;
+    if (loop_merge_block == from)
+      loop_merge_block = to;
+    if (loop_ladder_block == from)
+      loop_ladder_block = to;
+  }
+}
+
+void CFGNode::traverse_dominated_blocks_and_rewrite_branch(CFGNode *from,
+                                                           CFGNode *to) {
+  traverse_dominated_blocks_and_rewrite_branch(
+      *this, from, to, [](const CFGNode *node) -> bool { return true; });
+  fixup_merge_info_after_branch_rewrite(from, to);
+}
+
+void CFGNode::recompute_immediate_dominator() {
+  if (pred.empty()) {
+    // For entry block only.
+    immediate_dominator = this;
+  } else {
+    immediate_dominator = nullptr;
+
+    for (auto *edge : pred) {
+      if (immediate_dominator)
+        immediate_dominator =
+            CFGNode::find_common_dominator(immediate_dominator, edge);
+      else
+        immediate_dominator = edge;
+    }
+  }
+}
+
+void CFGNode::recompute_immediate_post_dominator() {
+  if (!succ.empty() || !fake_succ.empty()) {
+    // For non-leaf blocks only. The immediate post dominator is already set up
+    // to be the exit node in leaf nodes.
+    immediate_post_dominator = nullptr;
+    for (auto *edge : succ) {
+      if (immediate_post_dominator)
+        immediate_post_dominator =
+            CFGNode::find_common_post_dominator(immediate_post_dominator, edge);
+      else
+        immediate_post_dominator = edge;
+    }
+
+    for (auto *edge : fake_succ) {
+      if (immediate_post_dominator)
+        immediate_post_dominator =
+            CFGNode::find_common_post_dominator(immediate_post_dominator, edge);
+      else
+        immediate_post_dominator = edge;
+    }
+  }
+}
+
+CFGNode *CFGNode::get_outer_selection_dominator() {
+  assert(immediate_dominator);
+  auto *node = immediate_dominator;
+
+  // We need to find an immediate dominator which we do not post-dominate.
+  // That first idom is considered the outer selection header.
+  while (node->ir.terminator.type != Terminator::Type::Switch &&
+         post_dominates(node)) {
+    if (node->pred.empty())
+      break;
+
+    // Skip from merge block to header.
+    while (std::find(node->headers.begin(), node->headers.end(),
+                     node->immediate_dominator) != node->headers.end())
+      node = node->immediate_dominator;
+
+    if (post_dominates(node)) {
+      assert(node->immediate_dominator);
+      node = node->immediate_dominator;
+    }
+  }
+
+  return node;
+}
+
+CFGNode *CFGNode::get_outer_header_dominator() {
+  assert(immediate_dominator);
+  auto *node = immediate_dominator;
+  while (node->succ.size() == 1 &&
+         node->ir.terminator.type != Terminator::Type::Switch &&
+         !node->pred_back_edge) {
+    if (node->pred.empty())
+      break;
+
+    assert(node->immediate_dominator);
+    node = node->immediate_dominator;
+  }
+
+  return node;
+}
+
+} // namespace llvm
diff --git a/llvm/lib/Transforms/Scalar/ADCE.cpp b/llvm/lib/Transforms/Scalar/ADCE.cpp
index b693acceb3f6..cb33b4ac6233 100644
--- a/llvm/lib/Transforms/Scalar/ADCE.cpp
+++ b/llvm/lib/Transforms/Scalar/ADCE.cpp
@@ -121,6 +121,8 @@ class AggressiveDeadCodeElimination {
   DominatorTree *DT;
   PostDominatorTree &PDT;
 
+  bool allow_cfg_removal { true };
+
   /// Mapping of blocks to associated information, an element in BlockInfoVec.
   /// Use MapVector to get deterministic iteration order.
   MapVector<BasicBlock *, BlockInfoType> BlockInfo;
@@ -194,8 +196,8 @@ class AggressiveDeadCodeElimination {
 
 public:
   AggressiveDeadCodeElimination(Function &F, DominatorTree *DT,
-                                PostDominatorTree &PDT)
-      : F(F), DT(DT), PDT(PDT) {}
+                                PostDominatorTree &PDT, bool allow_cfg_removal_)
+      : F(F), DT(DT), PDT(PDT), allow_cfg_removal(allow_cfg_removal_) {}
 
   bool performDeadCodeElimination();
 };
@@ -247,7 +249,7 @@ void AggressiveDeadCodeElimination::initialize() {
     if (isAlwaysLive(I))
       markLive(&I);
 
-  if (!RemoveControlFlowFlag)
+  if (!RemoveControlFlowFlag || !allow_cfg_removal)
     return;
 
   if (!RemoveLoops) {
@@ -335,7 +337,7 @@ bool AggressiveDeadCodeElimination::isAlwaysLive(Instruction &I) {
   }
   if (!I.isTerminator())
     return false;
-  if (RemoveControlFlowFlag && (isa<BranchInst>(I) || isa<SwitchInst>(I)))
+  if (RemoveControlFlowFlag && allow_cfg_removal && (isa<BranchInst>(I) || isa<SwitchInst>(I)))
     return false;
   return true;
 }
@@ -688,17 +690,17 @@ void AggressiveDeadCodeElimination::makeUnconditional(BasicBlock *BB,
 // Pass Manager integration code
 //
 //===----------------------------------------------------------------------===//
-PreservedAnalyses ADCEPass::run(Function &F, FunctionAnalysisManager &FAM) {
+PreservedAnalyses ADCEPass::run(Function &F, FunctionAnalysisManager &FAM, bool allow_cfg_removal) {
   // ADCE does not need DominatorTree, but require DominatorTree here
   // to update analysis if it is already available.
   auto *DT = FAM.getCachedResult<DominatorTreeAnalysis>(F);
   auto &PDT = FAM.getResult<PostDominatorTreeAnalysis>(F);
-  if (!AggressiveDeadCodeElimination(F, DT, PDT).performDeadCodeElimination())
+  if (!AggressiveDeadCodeElimination(F, DT, PDT, allow_cfg_removal).performDeadCodeElimination())
     return PreservedAnalyses::all();
 
   PreservedAnalyses PA;
   // TODO: We could track if we have actually done CFG changes.
-  if (!RemoveControlFlowFlag)
+  if (!RemoveControlFlowFlag || !allow_cfg_removal)
     PA.preserveSet<CFGAnalyses>();
   else {
     PA.preserve<DominatorTreeAnalysis>();
@@ -711,8 +713,9 @@ namespace {
 
 struct ADCELegacyPass : public FunctionPass {
   static char ID; // Pass identification, replacement for typeid
+  bool allow_cfg_removal { true };
 
-  ADCELegacyPass() : FunctionPass(ID) {
+  ADCELegacyPass(bool allow_cfg_removal_ = true) : FunctionPass(ID), allow_cfg_removal(allow_cfg_removal_) {
     initializeADCELegacyPassPass(*PassRegistry::getPassRegistry());
   }
 
@@ -725,13 +728,13 @@ struct ADCELegacyPass : public FunctionPass {
     auto *DTWP = getAnalysisIfAvailable<DominatorTreeWrapperPass>();
     auto *DT = DTWP ? &DTWP->getDomTree() : nullptr;
     auto &PDT = getAnalysis<PostDominatorTreeWrapperPass>().getPostDomTree();
-    return AggressiveDeadCodeElimination(F, DT, PDT)
+    return AggressiveDeadCodeElimination(F, DT, PDT, allow_cfg_removal)
         .performDeadCodeElimination();
   }
 
   void getAnalysisUsage(AnalysisUsage &AU) const override {
     AU.addRequired<PostDominatorTreeWrapperPass>();
-    if (!RemoveControlFlowFlag)
+    if (!RemoveControlFlowFlag || !allow_cfg_removal)
       AU.setPreservesCFG();
     else {
       AU.addPreserved<DominatorTreeWrapperPass>();
@@ -751,4 +754,4 @@ INITIALIZE_PASS_DEPENDENCY(PostDominatorTreeWrapperPass)
 INITIALIZE_PASS_END(ADCELegacyPass, "adce", "Aggressive Dead Code Elimination",
                     false, false)
 
-FunctionPass *llvm::createAggressiveDCEPass() { return new ADCELegacyPass(); }
+FunctionPass *llvm::createAggressiveDCEPass(bool allow_cfg_removal) { return new ADCELegacyPass(allow_cfg_removal); }
diff --git a/llvm/lib/Transforms/Scalar/GVN.cpp b/llvm/lib/Transforms/Scalar/GVN.cpp
index c0e7e3cdad41..711f3d63c13c 100644
--- a/llvm/lib/Transforms/Scalar/GVN.cpp
+++ b/llvm/lib/Transforms/Scalar/GVN.cpp
@@ -1917,6 +1917,22 @@ bool GVN::processLoad(LoadInst *L) {
   // ... to a pointer that has been loaded from before...
   MemDepResult Dep = MD->getDependency(L);
 
+  // prevent unwanted int <-> vector bitcasts on Metal/Vulkan
+  const Triple triple(L->getModule()->getTargetTriple());
+  if (triple.getArch() == Triple::ArchType::spir64 ||
+      triple.getArch() == Triple::ArchType::air64) {
+    if (const auto dep_instr = Dep.getInst(); dep_instr) {
+      if (dep_instr->getType()->isVectorTy() != L->getType()->isVectorTy()) {
+        return false;
+      }
+      if (const auto dep_store_instr = dyn_cast_or_null<StoreInst>(dep_instr); dep_store_instr) {
+        if (dep_store_instr->getValueOperand()->getType()->isVectorTy() != L->getType()->isVectorTy()) {
+          return false;
+        }
+      }
+    }
+  }
+
   // If it is defined in another block, try harder.
   if (Dep.isNonLocal())
     return processNonLocalLoad(L);
diff --git a/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp b/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
index 67702520511b..d0750eb15179 100644
--- a/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
+++ b/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp
@@ -73,6 +73,11 @@ using namespace llvm;
 
 #define DEBUG_TYPE "loop-unroll"
 
+/// A magic value for use with the Threshold parameter to indicate
+/// that the loop unroll should be performed regardless of how much
+/// code expansion would result.
+static const unsigned NoThreshold = std::numeric_limits<unsigned>::max();
+
 cl::opt<bool> llvm::ForgetSCEVInLoopUnroll(
     "forget-scev-loop-unroll", cl::init(false), cl::Hidden,
     cl::desc("Forget everything in SCEV when doing LoopUnroll, instead of just"
@@ -141,8 +146,9 @@ static cl::opt<unsigned> UnrollMaxUpperBound(
     cl::desc(
         "The max of trip count upper bound that is considered in unrolling"));
 
+// NOTE: explicit requests for unrolls should *always* be considered
 static cl::opt<unsigned> PragmaUnrollThreshold(
-    "pragma-unroll-threshold", cl::init(16 * 1024), cl::Hidden,
+    "pragma-unroll-threshold", cl::init(NoThreshold), cl::Hidden,
     cl::desc("Unrolled size limit for loops with an unroll(full) or "
              "unroll_count pragma."));
 
@@ -175,11 +181,6 @@ static cl::opt<unsigned>
                            cl::desc("Default threshold (max size of unrolled "
                                     "loop), used in all but O3 optimizations"));
 
-/// A magic value for use with the Threshold parameter to indicate
-/// that the loop unroll should be performed regardless of how much
-/// code expansion would result.
-static const unsigned NoThreshold = std::numeric_limits<unsigned>::max();
-
 /// Gather the various unrolling parameters based on the defaults, compiler
 /// flags, TTI overrides and user specified parameters.
 TargetTransformInfo::UnrollingPreferences llvm::gatherUnrollingPreferences(
@@ -670,7 +671,7 @@ unsigned llvm::ApproximateLoopSize(
     const SmallPtrSetImpl<const Value *> &EphValues, unsigned BEInsns) {
   CodeMetrics Metrics;
   for (BasicBlock *BB : L->blocks())
-    Metrics.analyzeBasicBlock(BB, TTI, EphValues);
+    Metrics.analyzeBasicBlock(BB, TTI, EphValues, false, true /* allow duplicates */);
   NumCalls = Metrics.NumInlineCandidates;
   NotDuplicatable = Metrics.notDuplicatable;
   Convergent = Metrics.convergent;
diff --git a/llvm/lib/Transforms/Scalar/NewGVN.cpp b/llvm/lib/Transforms/Scalar/NewGVN.cpp
index 9855de49485c..2e87e78b2a1a 100644
--- a/llvm/lib/Transforms/Scalar/NewGVN.cpp
+++ b/llvm/lib/Transforms/Scalar/NewGVN.cpp
@@ -3464,7 +3464,11 @@ bool NewGVN::runGVN() {
   Changed |= !InstructionsToErase.empty();
 
   // Delete all unreachable blocks.
+  // NOTE: for Vulkan, ignore fake continue blocks
+  const auto is_vulkan = Triple(F.getParent()->getTargetTriple()).getEnvironment() == Triple::EnvironmentType::Vulkan;
   auto UnreachableBlockPred = [&](const BasicBlock &BB) {
+    if (is_vulkan && BB.getName().endswith(".fake_continue"))
+      return false;
     return !ReachableBlocks.count(&BB);
   };
 
diff --git a/llvm/lib/Transforms/Utils/LoopRotationUtils.cpp b/llvm/lib/Transforms/Utils/LoopRotationUtils.cpp
index 692e60a9701e..79c59d3f7d84 100644
--- a/llvm/lib/Transforms/Utils/LoopRotationUtils.cpp
+++ b/llvm/lib/Transforms/Utils/LoopRotationUtils.cpp
@@ -312,6 +312,7 @@ bool LoopRotate::rotateLoop(Loop *L, bool SimplifiedLatch) {
 
       CodeMetrics Metrics;
       Metrics.analyzeBasicBlock(OrigHeader, *TTI, EphValues, PrepareForLTO);
+#if 0 // TODO/NOTE: ignoring this for now, duplicates can very well exist in the same scope
       if (Metrics.notDuplicatable) {
         LLVM_DEBUG(
                    dbgs() << "LoopRotation: NOT rotating - contains non-duplicatable"
@@ -325,6 +326,7 @@ bool LoopRotate::rotateLoop(Loop *L, bool SimplifiedLatch) {
                    L->dump());
         return Rotated;
       }
+#endif
       if (Metrics.NumInsts > MaxHeaderSize) {
         LLVM_DEBUG(dbgs() << "LoopRotation: NOT rotating - contains "
                           << Metrics.NumInsts
diff --git a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
index 57b11e9414ba..6810eec8e4f8 100644
--- a/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
+++ b/llvm/lib/Transforms/Vectorize/VectorCombine.cpp
@@ -64,9 +64,9 @@ class VectorCombine {
 public:
   VectorCombine(Function &F, const TargetTransformInfo &TTI,
                 const DominatorTree &DT, AAResults &AA, AssumptionCache &AC,
-                bool ScalarizationOnly)
+                bool ScalarizationOnly, bool isVulkan_ = false)
       : F(F), Builder(F.getContext()), TTI(TTI), DT(DT), AA(AA), AC(AC),
-        ScalarizationOnly(ScalarizationOnly) {}
+        isVulkan(isVulkan_), ScalarizationOnly(ScalarizationOnly) {}
 
   bool run();
 
@@ -77,6 +77,7 @@ private:
   const DominatorTree &DT;
   AAResults &AA;
   AssumptionCache &AC;
+  const bool isVulkan { false };
 
   /// If true only perform scalarization combines and do not introduce new
   /// vector operations.
@@ -124,6 +125,11 @@ private:
 } // namespace
 
 bool VectorCombine::vectorizeLoadInsert(Instruction &I) {
+  // never allowed in Vulkan
+  if (isVulkan) {
+    return false;
+  }
+
   // Match insert into fixed vector of scalar value.
   // TODO: Handle non-zero insert index.
   auto *Ty = dyn_cast<FixedVectorType>(I.getType());
@@ -1178,7 +1184,8 @@ namespace {
 class VectorCombineLegacyPass : public FunctionPass {
 public:
   static char ID;
-  VectorCombineLegacyPass() : FunctionPass(ID) {
+  const bool isVulkan { false };
+  VectorCombineLegacyPass(bool isVulkan_ = false) : FunctionPass(ID), isVulkan(isVulkan_) {
     initializeVectorCombineLegacyPassPass(*PassRegistry::getPassRegistry());
   }
 
@@ -1202,7 +1209,7 @@ public:
     auto &TTI = getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
     auto &DT = getAnalysis<DominatorTreeWrapperPass>().getDomTree();
     auto &AA = getAnalysis<AAResultsWrapperPass>().getAAResults();
-    VectorCombine Combiner(F, TTI, DT, AA, AC, false);
+    VectorCombine Combiner(F, TTI, DT, AA, AC, false, isVulkan);
     return Combiner.run();
   }
 };
@@ -1216,17 +1223,18 @@ INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
 INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
 INITIALIZE_PASS_END(VectorCombineLegacyPass, "vector-combine",
                     "Optimize scalar/vector ops", false, false)
-Pass *llvm::createVectorCombinePass() {
-  return new VectorCombineLegacyPass();
+Pass *llvm::createVectorCombinePass(bool isVulkan) {
+  return new VectorCombineLegacyPass(isVulkan);
 }
 
 PreservedAnalyses VectorCombinePass::run(Function &F,
-                                         FunctionAnalysisManager &FAM) {
+                                         FunctionAnalysisManager &FAM,
+                                         bool isVulkan) {
   auto &AC = FAM.getResult<AssumptionAnalysis>(F);
   TargetTransformInfo &TTI = FAM.getResult<TargetIRAnalysis>(F);
   DominatorTree &DT = FAM.getResult<DominatorTreeAnalysis>(F);
   AAResults &AA = FAM.getResult<AAManager>(F);
-  VectorCombine Combiner(F, TTI, DT, AA, AC, ScalarizationOnly);
+  VectorCombine Combiner(F, TTI, DT, AA, AC, ScalarizationOnly, isVulkan);
   if (!Combiner.run())
     return PreservedAnalyses::all();
   PreservedAnalyses PA;
diff --git a/llvm/projects/spirv b/llvm/projects/spirv
new file mode 160000
index 000000000000..cf7742c67caf
--- /dev/null
+++ b/llvm/projects/spirv
@@ -0,0 +1 @@
+Subproject commit cf7742c67caff2c103f76f656df27553ef37c5bb
diff --git a/llvm/tools/bugpoint/CMakeLists.txt b/llvm/tools/bugpoint/CMakeLists.txt
index d64481df1c1c..3202aa3a5633 100644
--- a/llvm/tools/bugpoint/CMakeLists.txt
+++ b/llvm/tools/bugpoint/CMakeLists.txt
@@ -13,6 +13,7 @@ set(LLVM_LINK_COMPONENTS
   AggressiveInstCombine
   InstCombine
   Instrumentation
+  LibFloor
   Linker
   ObjCARCOpts
   ScalarOpts
diff --git a/llvm/tools/bugpoint/bugpoint.cpp b/llvm/tools/bugpoint/bugpoint.cpp
index 937ec23231b0..05c4d8ea5ea3 100644
--- a/llvm/tools/bugpoint/bugpoint.cpp
+++ b/llvm/tools/bugpoint/bugpoint.cpp
@@ -148,6 +148,7 @@ int main(int argc, char **argv) {
   PassRegistry &Registry = *PassRegistry::getPassRegistry();
   initializeCore(Registry);
   initializeScalarOpts(Registry);
+  initializeLibFloor(Registry);
   initializeObjCARCOpts(Registry);
   initializeVectorization(Registry);
   initializeIPO(Registry);
diff --git a/llvm/tools/llc/CMakeLists.txt b/llvm/tools/llc/CMakeLists.txt
index f0bc5825574b..32ffc558b089 100644
--- a/llvm/tools/llc/CMakeLists.txt
+++ b/llvm/tools/llc/CMakeLists.txt
@@ -9,6 +9,7 @@ set(LLVM_LINK_COMPONENTS
   CodeGen
   Core
   IRReader
+  LibFloor
   MC
   MIRParser
   Remarks
diff --git a/llvm/tools/llc/llc.cpp b/llvm/tools/llc/llc.cpp
index 9d80f062c8f9..77db3b32368e 100644
--- a/llvm/tools/llc/llc.cpp
+++ b/llvm/tools/llc/llc.cpp
@@ -362,6 +362,7 @@ int main(int argc, char **argv) {
   initializeUnreachableBlockElimLegacyPassPass(*Registry);
   initializeConstantHoistingLegacyPassPass(*Registry);
   initializeScalarOpts(*Registry);
+  initializeLibFloor(*Registry);
   initializeVectorization(*Registry);
   initializeScalarizeMaskedMemIntrinLegacyPassPass(*Registry);
   initializeExpandReductionsPass(*Registry);
diff --git a/llvm/tools/llvm-isel-fuzzer/CMakeLists.txt b/llvm/tools/llvm-isel-fuzzer/CMakeLists.txt
index 6ce0835f0465..1515410ee1d2 100644
--- a/llvm/tools/llvm-isel-fuzzer/CMakeLists.txt
+++ b/llvm/tools/llvm-isel-fuzzer/CMakeLists.txt
@@ -11,6 +11,7 @@ set(LLVM_LINK_COMPONENTS
     Core
     FuzzMutate
     IRReader
+    LibFloor
     MC
     ScalarOpts
     SelectionDAG
diff --git a/llvm/tools/llvm-opt-fuzzer/CMakeLists.txt b/llvm/tools/llvm-opt-fuzzer/CMakeLists.txt
index 67a401f0ef7f..10f744a7a6c3 100644
--- a/llvm/tools/llvm-opt-fuzzer/CMakeLists.txt
+++ b/llvm/tools/llvm-opt-fuzzer/CMakeLists.txt
@@ -14,6 +14,7 @@ set(LLVM_LINK_COMPONENTS
   AggressiveInstCombine
   InstCombine
   Instrumentation
+  LibFloor
   FuzzMutate
   MC
   ObjCARCOpts
diff --git a/llvm/tools/llvm-opt-fuzzer/llvm-opt-fuzzer.cpp b/llvm/tools/llvm-opt-fuzzer/llvm-opt-fuzzer.cpp
index 7cf7b1feb3b7..2c7d7a6ae1e4 100644
--- a/llvm/tools/llvm-opt-fuzzer/llvm-opt-fuzzer.cpp
+++ b/llvm/tools/llvm-opt-fuzzer/llvm-opt-fuzzer.cpp
@@ -194,6 +194,7 @@ extern "C" LLVM_ATTRIBUTE_USED int LLVMFuzzerInitialize(
   initializeCore(Registry);
   initializeCoroutines(Registry);
   initializeScalarOpts(Registry);
+  initializeLibFloor(Registry);
   initializeObjCARCOpts(Registry);
   initializeVectorization(Registry);
   initializeIPO(Registry);
diff --git a/llvm/tools/metallib-dis/CMakeLists.txt b/llvm/tools/metallib-dis/CMakeLists.txt
new file mode 100644
index 000000000000..5b1bb7e5a61f
--- /dev/null
+++ b/llvm/tools/metallib-dis/CMakeLists.txt
@@ -0,0 +1,12 @@
+set(LLVM_LINK_COMPONENTS
+  BitReader
+  Core
+  Support
+  )
+
+add_llvm_tool(metallib-dis
+  metallib-dis.cpp
+  
+  DEPENDS
+  intrinsics_gen
+  )
diff --git a/llvm/tools/metallib-dis/metallib-dis.cpp b/llvm/tools/metallib-dis/metallib-dis.cpp
new file mode 100644
index 000000000000..d4569df074dd
--- /dev/null
+++ b/llvm/tools/metallib-dis/metallib-dis.cpp
@@ -0,0 +1,786 @@
+//===-- metallib-dis.cpp - The low-level MetalLib + LLVM disassembler -----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This utility may be invoked in the following manner:
+//  metallib-dis [options]            - Read MetalLib from stdin, write asm
+//                                      to stdout
+//  metallib-dis [options] x.metallib - Read MetalLib from the x.metallib file,
+//                                      write asm to the x.txt file.
+//  Options:
+//      --help   - Output information about command line switches
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/Bitcode/BitcodeReader.h"
+#include "llvm/IR/AssemblyAnnotationWriter.h"
+#include "llvm/IR/DebugInfo.h"
+#include "llvm/IR/DiagnosticInfo.h"
+#include "llvm/IR/DiagnosticPrinter.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Support/Error.h"
+#include "llvm/Support/FileSystem.h"
+#include "llvm/Support/FormattedStream.h"
+#include "llvm/Support/InitLLVM.h"
+#include "llvm/Support/ManagedStatic.h"
+#include "llvm/Support/MemoryBuffer.h"
+#include "llvm/Support/PrettyStackTrace.h"
+#include "llvm/Support/Signals.h"
+#include "llvm/Support/ToolOutputFile.h"
+#include "llvm/Support/WithColor.h"
+#include <system_error>
+#include <iostream>
+#include <fstream>
+#include <vector>
+#include <string>
+#include <sstream>
+#include <optional>
+using namespace llvm;
+using namespace std;
+
+static cl::opt<std::string>
+InputFilename(cl::Positional, cl::desc("<input metallib>"), cl::init("-"));
+
+static cl::opt<std::string>
+OutputFilename("o", cl::desc("Override output filename"),
+               cl::value_desc("filename"));
+
+static cl::opt<bool>
+Force("f", cl::desc("Enable binary output on terminals"));
+
+static cl::opt<bool>
+DontPrint("disable-output", cl::desc("Don't output the .txt file"), cl::Hidden);
+
+static cl::opt<bool>
+ShowAnnotations("show-annotations",
+                cl::desc("Add informational comments to the LLVM IR output"));
+
+static cl::opt<bool> PreserveAssemblyUseListOrder(
+    "preserve-ll-uselistorder",
+    cl::desc("Preserve use-list order when writing LLVM assembly."),
+    cl::init(false), cl::Hidden);
+
+/* .metallib layout (as of Metal 2.4 / macOS 12.0)
+ 
+ versioning:
+ [magic: char[4] = MTLB]
+ [container version]
+      [major version + macOS flag: uint16_t - MSB is macOS flag]
+      [minor version: uint16_t]
+      [bugfix version: uint16_t]
+ [flags]
+      [flags #1: 1:7 bit-field (uint8_t), MSB: stub flag, rest: file type (0 is metallib executable)s]
+      [flags #2: 1:7 bit-field (uint8_t), MSB: 64-bit flag, rest: platform (macOS: 1, iOS: 2, tvOS: 3, watchOS: 4]
+ [platform version: 16:8:8 bit-field (32-bit) = major.minor.update]
+ 
+ header:
+ [program metadata offset: uint64_t]
+ [program metadata length: uint64_t]
+ [reflection metadata offset: uint64_t]
+ [reflection metadata length: uint64_t]
+ [debug metadata offset: uint64_t]
+ [debug metadata length: uint64_t]
+ [bitcode offset: uint64_t]
+ [bitcode length: uint64_t]
+ 
+ program metadata:
+     [program count: uint32_t] // NOTE: not included by "program metadata length"
+     [program metadata ...]
+ 
+ additional program metadata (not included in "program metadata"!)
+ [optional: embedded source code: char[4] = "HSRD"]
+ 	[tag length: uint16_t = 16]
+    [embedded source code offset: uint64_t]
+    [embedded source code length: uint64_t]
+ [optional: metallib UUID: char[4] = "UUID"]
+ 	[tag length: uint16_t = 16]
+    [UUID: uint8_t[16]]
+ [additional program metadata terminator: char[4] = "ENDT"]
+ 
+ [reflection metadata ...]
+ [debug metadata ...]
+ 
+ bitcode:
+ [LLVM 5.0 bitcode binaries ...]
+ 
+ (opt) embedded source code:
+ [source archive count: uint32_t]
+ [linker command line info: \0-terminated string]
+ [working direction: \0-terminated string]
+ source archives:
+ 	[source archive length: uint32_t]
+    [source archive...]
+        [magic: char[4] = SARC] // NOTE: program source offsets point here (+"embedded source code offset")
+        [archive length: uint32_t]
+        [archive number in ASCII: uint16_t = 0x30/'0'...]
+        [bzip2 compressed .a archive]
+ 
+ */
+
+//
+static constexpr const char metallib_magic[4] { 'M', 'T', 'L', 'B' };
+
+struct __attribute__((packed)) metallib_version {
+	// container/file version
+	uint16_t container_version_major : 15;
+	uint16_t is_macos_target : 1;
+	uint16_t container_version_minor;
+	uint16_t container_version_bugfix;
+	
+	// flags
+	uint8_t file_type : 7;
+	uint8_t is_stub : 1;
+	uint8_t platform : 7;
+	uint8_t is_64_bit : 1;
+	
+	// platform version
+	uint32_t platform_version_major : 16;
+	uint32_t platform_version_minor : 8;
+	uint32_t platform_version_update : 8;
+};
+static_assert(sizeof(metallib_version) == 12, "invalid version header length");
+
+struct __attribute__((packed)) metallib_header_control {
+	uint64_t programs_offset;
+	uint64_t programs_length;
+	uint64_t reflection_offset;
+	uint64_t reflection_length;
+	uint64_t debug_offset;
+	uint64_t debug_length;
+	uint64_t bitcode_offset;
+	uint64_t bitcode_length;
+};
+static_assert(sizeof(metallib_header_control) == 64, "invalid program info length");
+
+struct __attribute__((packed)) metallib_header {
+	const char magic[4]; // == metallib_magic
+	const metallib_version version;
+	const uint64_t file_length;
+	const metallib_header_control header_control;
+};
+static_assert(sizeof(metallib_header) == 4 + sizeof(metallib_version) + sizeof(uint64_t) + sizeof(metallib_header_control), "invalid metallib header size");
+
+struct metallib_program_info {
+	uint32_t length; // including length itself
+	
+	// NOTE: tag types are always 32-bit
+	// NOTE: tag types are always followed by a uint16_t that specifies the length of the tag data
+#define make_tag_type(a, b, c, d) ((uint32_t(d) << 24u) | (uint32_t(c) << 16u) | (uint32_t(b) << 8u) | uint32_t(a))
+	enum TAG_TYPE : uint32_t {
+		// used in initial header section
+		NAME		= make_tag_type('N', 'A', 'M', 'E'),
+		TYPE		= make_tag_type('T', 'Y', 'P', 'E'),
+		HASH		= make_tag_type('H', 'A', 'S', 'H'),
+		MD_SIZE		= make_tag_type('M', 'D', 'S', 'Z'),
+		OFFSET		= make_tag_type('O', 'F', 'F', 'T'),
+		VERSION		= make_tag_type('V', 'E', 'R', 'S'),
+		SOFF        = make_tag_type('S', 'O', 'F', 'F'),
+		// used in reflection section
+		CNST        = make_tag_type('C', 'N', 'S', 'T'),
+		VATT        = make_tag_type('V', 'A', 'T', 'T'),
+		VATY        = make_tag_type('V', 'A', 'T', 'Y'),
+		RETR        = make_tag_type('R', 'E', 'T', 'R'),
+		ARGR        = make_tag_type('A', 'R', 'G', 'R'),
+		// used in debug section
+		DEBI        = make_tag_type('D', 'E', 'B', 'I'),
+		DEPF        = make_tag_type('D', 'E', 'P', 'F'),
+		// additional metadata
+		HSRD        = make_tag_type('H', 'S', 'R', 'D'),
+		UUID        = make_tag_type('U', 'U', 'I', 'D'),
+		// used for source code/archive
+		SARC        = make_tag_type('S', 'A', 'R', 'C'),
+		// TODO/TBD
+		LAYR        = make_tag_type('L', 'A', 'Y', 'R'),
+		TESS        = make_tag_type('T', 'E', 'S', 'S'),
+		// generic end tag
+		END         = make_tag_type('E', 'N', 'D', 'T'),
+	};
+#undef make_tag_type
+	
+	enum class PROGRAM_TYPE : uint8_t {
+		VERTEX = 0,
+		FRAGMENT = 1,
+		KERNEL = 2,
+		// TODO: tessellation?
+		NONE = 255
+	};
+	
+	struct version_info {
+		uint32_t major : 16;
+		uint32_t minor : 8;
+		uint32_t rev : 8;
+	};
+	
+	struct offset_info {
+		// NOTE: these are all relative offsets -> add to metallib_header_control offsets to get absolute offsets
+		uint64_t reflection_offset;
+		uint64_t debug_offset;
+		uint64_t bitcode_offset;
+	};
+	
+	struct debug_entry {
+		std::string source_file_name;
+		uint32_t function_line { 0 };
+		std::string dependent_file;
+	};
+	
+	struct entry {
+		uint32_t length;
+		string name; // NOTE: limited to 65536 - 1 ('\0')
+		PROGRAM_TYPE type { PROGRAM_TYPE::NONE };
+		uint8_t sha256_hash[32] {
+			0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+			0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+		};
+		offset_info offset { 0, 0, 0 };
+		uint64_t bitcode_size { 0 }; // always 8 bytes
+		version_info metal_version { 0, 0, 0 };
+		version_info metal_language_version { 0, 0, 0 };
+		uint8_t tess_info { 0 };
+		uint64_t source_offset { 0 };
+		std::optional<debug_entry> debug;
+	};
+	vector<entry> entries;
+};
+
+//
+namespace {
+
+static void printDebugLoc(const DebugLoc &DL, formatted_raw_ostream &OS) {
+  OS << DL.getLine() << ":" << DL.getCol();
+  if (DILocation *IDL = DL.getInlinedAt()) {
+    OS << "@";
+    printDebugLoc(IDL, OS);
+  }
+}
+class CommentWriter : public AssemblyAnnotationWriter {
+public:
+  void emitFunctionAnnot(const Function *F,
+                         formatted_raw_ostream &OS) override {
+    OS << "; [#uses=" << F->getNumUses() << ']';  // Output # uses
+    OS << '\n';
+  }
+  void printInfoComment(const Value &V, formatted_raw_ostream &OS) override {
+    bool Padded = false;
+    if (!V.getType()->isVoidTy()) {
+      OS.PadToColumn(50);
+      Padded = true;
+      // Output # uses and type
+      OS << "; [#uses=" << V.getNumUses() << " type=" << *V.getType() << "]";
+    }
+    if (const Instruction *I = dyn_cast<Instruction>(&V)) {
+      if (const DebugLoc &DL = I->getDebugLoc()) {
+        if (!Padded) {
+          OS.PadToColumn(50);
+          Padded = true;
+          OS << ";";
+        }
+        OS << " [debug line = ";
+        printDebugLoc(DL,OS);
+        OS << "]";
+      }
+      if (const DbgDeclareInst *DDI = dyn_cast<DbgDeclareInst>(I)) {
+        if (!Padded) {
+          OS.PadToColumn(50);
+          OS << ";";
+        }
+        OS << " [debug variable = " << DDI->getVariable()->getName() << "]";
+      }
+      else if (const DbgValueInst *DVI = dyn_cast<DbgValueInst>(I)) {
+        if (!Padded) {
+          OS.PadToColumn(50);
+          OS << ";";
+        }
+        OS << " [debug variable = " << DVI->getVariable()->getName() << "]";
+      }
+    }
+  }
+};
+
+struct MetalLibDisDiagnosticHandler : public DiagnosticHandler {
+  char *Prefix;
+  MetalLibDisDiagnosticHandler(char *PrefixPtr) : Prefix(PrefixPtr) {}
+  bool handleDiagnostics(const DiagnosticInfo &DI) override {
+    raw_ostream &OS = errs();
+    OS << Prefix << ": ";
+    switch (DI.getSeverity()) {
+      case DS_Error: WithColor::error(OS); break;
+      case DS_Warning: WithColor::warning(OS); break;
+      case DS_Remark: OS << "remark: "; break;
+      case DS_Note: WithColor::note(OS); break;
+    }
+
+    DiagnosticPrinterRawOStream DP(OS);
+    DI.print(DP);
+    OS << '\n';
+
+    if (DI.getSeverity() == DS_Error)
+      exit(1);
+    return true;
+  }
+};
+} // end anon namespace
+
+static void hex_dump(raw_fd_ostream& os, const char* ptr, const size_t length, const char* name) {
+	os << '\n' << name << ":\n";
+	static constexpr const uint32_t row_width = 16;
+	for (uint32_t row = 0, row_count = ((length + row_width - 1) / row_width); row < row_count; ++row) {
+		for (uint32_t i = 0; i < row_width; ++i) {
+			const auto ch_idx = row * row_width + i;
+			if (ch_idx < length) {
+				const auto ch_ptr = (const uint8_t*)ptr + ch_idx;
+				llvm::write_hex(os, (uint64_t)*ch_ptr, HexPrintStyle::Upper, 2);
+				os << " ";
+			} else {
+				os << "   ";
+			}
+		}
+		os << " |  ";
+		for (uint32_t i = 0; i < row_width; ++i) {
+			const auto ch_idx = row * row_width + i;
+			if (ch_idx < length) {
+				const auto ch_ptr = ptr + ch_idx;
+				if (*ch_ptr >= 0x20) {
+					os << *ch_ptr;
+				} else if (*ch_ptr == 0) {
+					os << "";
+				} else {
+					os << ' ';
+				}
+			}
+		}
+		os << '\n';
+	}
+}
+
+static Expected<bool> openInputFile(char** argv, std::unique_ptr<ToolOutputFile>& Out) {
+	auto& os = Out->os();
+	
+	//
+	ErrorOr<std::unique_ptr<MemoryBuffer>> input_data = MemoryBuffer::getFileOrSTDIN(InputFilename);
+	if (!input_data) {
+		return errorCodeToError(input_data.getError());
+	}
+	const auto& buffer = (*input_data)->getBuffer();
+	const auto& data = buffer.data();
+	
+	// sanity check
+	if(buffer.size() < sizeof(metallib_header)) {
+		return make_error<StringError>("invalid header size", inconvertibleErrorCode());
+	}
+	
+	//
+	const auto& header = *(const metallib_header*)data;
+	if(memcmp(metallib_magic, header.magic, 4) != 0) {
+		return make_error<StringError>("invalid magic", inconvertibleErrorCode());
+	}
+	
+	// dump
+	os << "[header]" << '\n';
+	os << "container version: " << header.version.container_version_major << "." << header.version.container_version_minor << "." << header.version.container_version_bugfix << '\n';
+	os << "macOS: " << (header.version.is_macos_target ? "yes" : "no") << '\n';
+	os << "64-bit: " << (header.version.is_64_bit ? "yes" : "no") << '\n';
+	os << "stub: " << (header.version.is_stub ? "yes" : "no") << '\n';
+	os << "file-type: ";
+	switch (header.version.file_type) {
+		case 0:
+			os << "execute";
+			break;
+		case 1:
+			os << "CI";
+			break;
+		case 2:
+			os << "dylib";
+			break;
+		case 3:
+			os << "companion";
+			break;
+		default:
+			os << "unknown";
+			break;
+	}
+	os << '\n';
+	os << "platform: ";
+	switch (header.version.platform) {
+		case 1:
+			os << "macOS";
+			break;
+		case 2:
+			os << "iOS";
+			break;
+		case 3:
+			os << "tvOS";
+			break;
+		case 4:
+			os << "watchOS";
+			break;
+		default:
+			os << "unknown";
+			break;
+	}
+	os << '\n';
+	os << "platform version: " << header.version.platform_version_major << "." << header.version.platform_version_minor << "." << header.version.platform_version_update << '\n';
+	os << "length: " << header.file_length << '\n';
+	
+	os << '\n';
+	os << "programs_offset: " << header.header_control.programs_offset << '\n';
+	os << "programs_length: " << header.header_control.programs_length << '\n';
+	os << "reflection_offset: " << header.header_control.reflection_offset << '\n';
+	os << "reflection_length: " << header.header_control.reflection_length << '\n';
+	os << "debug_offset: " << header.header_control.debug_offset << '\n';
+	os << "debug_length: " << header.header_control.debug_length << '\n';
+	os << "bitcode_offset: " << header.header_control.bitcode_offset << '\n';
+	os << "bitcode_length: " << header.header_control.bitcode_length << '\n';
+	
+	// read programs info
+	if(buffer.size() < header.header_control.programs_offset + header.header_control.programs_length + 4u) {
+		return make_error<StringError>("invalid size", inconvertibleErrorCode());
+	}
+	
+	metallib_program_info info;
+	auto program_ptr = &data[header.header_control.programs_offset];
+	const auto add_program_md_offset = header.header_control.programs_offset + 4 + header.header_control.programs_length;
+	auto add_program_md_ptr = &data[add_program_md_offset];
+	const auto add_program_md_end = find((const uint32_t*)add_program_md_ptr,
+										 (const uint32_t*)add_program_md_ptr + (header.header_control.reflection_offset -
+																				add_program_md_offset) / 4u,
+										 0x54444E45u /* rev(ENDT) */) + 1;
+	const auto add_program_md_length = std::distance(add_program_md_ptr, (const char*)add_program_md_end);
+	auto refl_ptr = &data[header.header_control.reflection_offset];
+	auto debug_ptr = &data[header.header_control.debug_offset];
+	
+	const auto program_count = *(const uint32_t*)program_ptr; program_ptr += 4;
+	os << "program_count: " << program_count << '\n';
+	info.entries.resize(program_count);
+	
+	hex_dump(os, program_ptr, header.header_control.programs_length, "program metadata");
+	hex_dump(os, add_program_md_ptr, add_program_md_length, "additional program metadata");
+	hex_dump(os, refl_ptr, header.header_control.reflection_length, "reflection metadata");
+	hex_dump(os, debug_ptr, header.header_control.debug_length, "debug metadata");
+	
+	for(uint32_t i = 0; i < program_count; ++i) {
+		auto& entry = info.entries[i];
+		
+		entry.length = *(const uint32_t*)program_ptr; program_ptr += 4;
+		
+		bool found_end_tag = false;
+		while(!found_end_tag) {
+			const auto tag = *(const metallib_program_info::TAG_TYPE*)program_ptr; program_ptr += 4;
+			uint32_t tag_length = 0;
+			if(tag != metallib_program_info::TAG_TYPE::END) {
+				tag_length = *(const uint16_t*)program_ptr;
+				program_ptr += 2;
+				
+				if(tag_length == 0) {
+					return make_error<StringError>("tag " + to_string(uint32_t(tag)) + " should not be empty",
+												   inconvertibleErrorCode());
+				}
+			}
+			
+			switch(tag) {
+				case metallib_program_info::TAG_TYPE::NAME: {
+					entry.name = string((const char*)program_ptr, tag_length - 1u);
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::TYPE: {
+					entry.type = *(const metallib_program_info::PROGRAM_TYPE*)program_ptr;
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::HASH: {
+					if(tag_length != 32) {
+						return make_error<StringError>("invalid hash size: " + to_string(tag_length),
+													   inconvertibleErrorCode());
+					}
+					memcpy(entry.sha256_hash, program_ptr, 32u);
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::OFFSET: {
+					entry.offset = *(const metallib_program_info::offset_info*)program_ptr;
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::VERSION: {
+					entry.metal_version = *(const metallib_program_info::version_info*)program_ptr;
+					entry.metal_language_version = *((const metallib_program_info::version_info*)program_ptr + 1u);
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::MD_SIZE: {
+					entry.bitcode_size = *(const uint64_t*)program_ptr;
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::TESS: {
+					entry.tess_info = *(const uint8_t*)program_ptr;
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::SOFF: {
+					if(tag_length != 8) {
+						return make_error<StringError>("invalid SOFF size: " + to_string(tag_length),
+													   inconvertibleErrorCode());
+					}
+					entry.source_offset = *(const uint64_t*)program_ptr;
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::END: {
+					found_end_tag = true;
+					break;
+				}
+				default:
+					return make_error<StringError>("invalid program metadata tag: " + to_string((uint32_t)tag),
+												   inconvertibleErrorCode());
+			}
+			program_ptr += tag_length;
+		}
+	}
+	if(info.entries.size() != program_count) {
+		return make_error<StringError>("invalid entry count", inconvertibleErrorCode());
+	}
+	
+	// parse additional metadata
+	{
+		bool found_end_tag = false;
+		while (!found_end_tag) {
+			const auto tag = *(const metallib_program_info::TAG_TYPE*)add_program_md_ptr; add_program_md_ptr += 4;
+			uint32_t tag_length = 0;
+			if (tag != metallib_program_info::TAG_TYPE::END) {
+				tag_length = *(const uint16_t*)add_program_md_ptr;
+				add_program_md_ptr += 2;
+				
+				if (tag_length == 0) {
+					return make_error<StringError>("tag " + to_string(uint32_t(tag)) + " should not be empty",
+												   inconvertibleErrorCode());
+				}
+			}
+			
+			switch(tag) {
+				case metallib_program_info::TAG_TYPE::HSRD: {
+					const auto src_archives_offset = *(const uint64_t*)add_program_md_ptr;
+					const auto src_archives_length = *(const uint64_t*)(add_program_md_ptr + 8u);
+					os << "\nembedded source archives:\n\toffset: " << src_archives_offset << "\n\tlength: " << src_archives_length << '\n';
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::UUID: {
+					os << "\nUUID: ";
+					raw_ostream::uuid_t uuid;
+					memcpy(&uuid, add_program_md_ptr, sizeof(uuid));
+					os.write_uuid(uuid);
+					os << '\n';
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::END: {
+					found_end_tag = true;
+					break;
+				}
+				default:
+					return make_error<StringError>("invalid additional program metadata tag: " + to_string((uint32_t)tag),
+												   inconvertibleErrorCode());
+			}
+			add_program_md_ptr += tag_length;
+		}
+	}
+	
+	// parse debug info
+	for (uint32_t i = 0; i < program_count; ++i) {
+		auto& entry = info.entries[i];
+		
+		metallib_program_info::debug_entry dbg_entry {};
+		//const auto dbg_entry_length = *(const uint32_t*)debug_ptr;
+		debug_ptr += 4;
+		
+		bool found_end_tag = false;
+		while (!found_end_tag) {
+			const auto tag = *(const metallib_program_info::TAG_TYPE*)debug_ptr; debug_ptr += 4;
+			uint32_t tag_length = 0;
+			if(tag != metallib_program_info::TAG_TYPE::END) {
+				tag_length = *(const uint16_t*)debug_ptr;
+				debug_ptr += 2;
+				
+				if(tag_length == 0) {
+					return make_error<StringError>("tag " + to_string(uint32_t(tag)) + " should not be empty",
+												   inconvertibleErrorCode());
+				}
+			}
+			
+			switch(tag) {
+				case metallib_program_info::TAG_TYPE::DEBI: {
+					dbg_entry.function_line = *(const uint32_t*)debug_ptr;
+					dbg_entry.source_file_name = string((const char*)debug_ptr + 4, tag_length - 5u);
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::DEPF: {
+					dbg_entry.dependent_file = string((const char*)debug_ptr, tag_length - 1u);
+					break;
+				}
+				case metallib_program_info::TAG_TYPE::END: {
+					found_end_tag = true;
+					break;
+				}
+				default:
+					return make_error<StringError>("invalid debug metadata tag: " + to_string((uint32_t)tag),
+												   inconvertibleErrorCode());
+			}
+			debug_ptr += tag_length;
+		}
+		
+		// only set this debug entry if it actually contains anything
+		if (!dbg_entry.source_file_name.empty() || !dbg_entry.dependent_file.empty()) {
+			entry.debug = move(dbg_entry);
+		}
+	}
+	
+	//
+	for(const auto& prog : info.entries) {
+		os << '\n';
+		os << "################################################################################\n";
+		os << '\n';
+		os << "[program]" << '\n';
+		os << "\tname: " << prog.name << '\n';
+		os << "\ttype: ";
+		switch(prog.type) {
+			case metallib_program_info::PROGRAM_TYPE::FRAGMENT:
+				os << "fragment";
+				break;
+			case metallib_program_info::PROGRAM_TYPE::VERTEX:
+				os << "vertex";
+				break;
+			case metallib_program_info::PROGRAM_TYPE::KERNEL:
+				os << "kernel";
+				break;
+			case metallib_program_info::PROGRAM_TYPE::NONE:
+				os << "NONE";
+				break;
+		}
+		os << '\n';
+		os << "\tversion: " << prog.metal_version.major << "." << prog.metal_version.minor << "." << prog.metal_version.rev << '\n';
+		os << "\tlanguage: " << prog.metal_language_version.major << "." << prog.metal_language_version.minor << "." << prog.metal_language_version.rev << '\n';
+		os << "\trel offsets (refl, dbg, bc): " << prog.offset.reflection_offset << ", " << prog.offset.debug_offset << ", " << prog.offset.bitcode_offset << '\n';
+		os << "\tbitcode size: " << prog.bitcode_size << '\n';
+		os << "\thash: ";
+		
+		stringstream hash_hex;
+		hash_hex << hex << uppercase;
+		for(uint32_t i = 0; i < 32; ++i) {
+			if(prog.sha256_hash[i] < 0x10) {
+				hash_hex << '0';
+			}
+			hash_hex << uint32_t(prog.sha256_hash[i]);
+		}
+		os << hash_hex.str() << '\n';
+		os << "\ttess info: " << uint32_t(prog.tess_info) << '\n';
+		os << "\tsource offset: " << uint32_t(prog.source_offset) << '\n';
+		
+		// TODO: could use stringref?
+		auto bc_mem = WritableMemoryBuffer::getNewUninitMemBuffer(prog.bitcode_size, "bc_module");
+		const auto bc_offset = header.header_control.bitcode_offset + prog.offset.bitcode_offset;
+		os << "\toffset: " << bc_offset << '\n';
+		os << "\tsize: " << bc_mem->getBufferSize();
+		if (bc_mem->getBufferSize() != prog.bitcode_size) {
+			os << " != expected: " << prog.bitcode_size << " (!!!)";
+		}
+		os << '\n';
+		if (prog.debug) {
+			os << "\tdebug:\n";
+			if (!prog.debug->source_file_name.empty()) {
+				os << "\t\tsource file: " << prog.debug->source_file_name << '\n';
+			}
+			if (prog.debug->function_line != 0) {
+				os << "\t\tsource line: " << prog.debug->function_line << '\n';
+			}
+			if (!prog.debug->dependent_file.empty()) {
+				os << "\t\tdependent file: " << prog.debug->dependent_file << '\n';
+			}
+		}
+		os << '\n';
+		
+		// output LLVM IR
+		memcpy((char*)bc_mem->getBufferStart(), data + bc_offset, bc_mem->getBufferSize());
+		
+		LLVMContext Context;
+		Context.setDiagnosticHandler(std::make_unique<MetalLibDisDiagnosticHandler>(argv[0]));
+		auto bc_mod = parseBitcodeFile(*bc_mem, Context);
+		if(bc_mod) {
+			std::unique_ptr<AssemblyAnnotationWriter> Annotator;
+			if (ShowAnnotations) {
+				Annotator.reset(new CommentWriter());
+			}
+			
+			if ((*bc_mod)->materializeAll()) {
+				return make_error<StringError>("failed to materialize", inconvertibleErrorCode());
+			}
+			(*bc_mod)->print(Out->os(), Annotator.get(), PreserveAssemblyUseListOrder);
+			
+			if(Out->os().has_error()) {
+				Out->os().clear_error();
+			}
+		} else {
+			os << "bc parse error" << '\n';
+			// TODO: better error handling
+			handleAllErrors(bc_mod.takeError(), [&](ErrorInfoBase &EIB) {
+				errs() << "bc module: ";
+				EIB.log(errs());
+				errs() << '\n';
+			});
+			return make_error<StringError>("failed to parse bitcode module", inconvertibleErrorCode());
+		}
+	}
+	
+	return true;
+}
+
+static ExitOnError ExitOnErr;
+
+int main(int argc, char **argv) {
+  InitLLVM X(argc, argv);
+
+  ExitOnErr.setBanner(std::string(argv[0]) + ": error: ");
+
+  LLVMContext Context;
+  Context.setDiagnosticHandler(
+      std::make_unique<MetalLibDisDiagnosticHandler>(argv[0]));
+
+  cl::ParseCommandLineOptions(argc, argv, ".metallib -> .txt disassembler\n");
+
+  // Just use stdout.  We won't actually print anything on it.
+  if (DontPrint)
+    OutputFilename = "-";
+
+  if (OutputFilename.empty()) { // Unspecified output, infer it.
+    if (InputFilename == "-") {
+      OutputFilename = "-";
+    } else {
+      StringRef IFN = InputFilename;
+      OutputFilename = (IFN.endswith(".metallib") ? IFN.drop_back(9) : IFN).str();
+      OutputFilename += ".txt";
+    }
+  }
+
+  std::error_code EC;
+  std::unique_ptr<ToolOutputFile> Out(new ToolOutputFile(OutputFilename, EC, sys::fs::OF_None));
+  if (EC) {
+    errs() << EC.message() << '\n';
+    return -1;
+  }
+
+  Expected<bool> SuccessOrErr = openInputFile(argv, Out);
+  if (!SuccessOrErr) {
+    handleAllErrors(SuccessOrErr.takeError(), [&](ErrorInfoBase &EIB) {
+      errs() << argv[0] << ": ";
+      EIB.log(errs());
+      errs() << '\n';
+    });
+    return 1;
+  }
+
+  // Declare success.
+  Out->keep();
+
+  return 0;
+}
diff --git a/llvm/tools/opt/CMakeLists.txt b/llvm/tools/opt/CMakeLists.txt
index 19c85768fec4..bab77e708a39 100644
--- a/llvm/tools/opt/CMakeLists.txt
+++ b/llvm/tools/opt/CMakeLists.txt
@@ -7,6 +7,7 @@ set(LLVM_LINK_COMPONENTS
   Analysis
   AsmParser
   BitWriter
+  BitWriter50
   CodeGen
   Core
   Coroutines
@@ -15,6 +16,7 @@ set(LLVM_LINK_COMPONENTS
   IRReader
   InstCombine
   Instrumentation
+  LibFloor
   MC
   ObjCARCOpts
   Remarks
diff --git a/llvm/tools/opt/opt.cpp b/llvm/tools/opt/opt.cpp
index 7793a5471793..91f7858d5f02 100644
--- a/llvm/tools/opt/opt.cpp
+++ b/llvm/tools/opt/opt.cpp
@@ -537,6 +537,7 @@ int main(int argc, char **argv) {
   initializeCore(Registry);
   initializeCoroutines(Registry);
   initializeScalarOpts(Registry);
+  initializeLibFloor(Registry);
   initializeObjCARCOpts(Registry);
   initializeVectorization(Registry);
   initializeIPO(Registry);
@@ -569,6 +570,7 @@ int main(int argc, char **argv) {
   initializeExpandVectorPredicationPass(Registry);
   initializeWasmEHPreparePass(Registry);
   initializeWriteBitcodePassPass(Registry);
+  initializeWriteBitcodePass50Pass(Registry);
   initializeHardwareLoopsPass(Registry);
   initializeTypePromotionPass(Registry);
   initializeReplaceWithVeclibLegacyPass(Registry);
